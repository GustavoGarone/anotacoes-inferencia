```{julia}
#| output: false
using Pkg; Pkg.add(["Distributions", "Plots", "Random", "StatsBase", "LaTeXStrings", "SpecialFunctions"])
ENV["QT_STYLE_OVERRIDE"] = "Fusion"
```
# Intervalos de Confiança - Aprofundamento

Seja $\boldsymbol{X}_n = (X_1,\dots,X_n)$ [amostra aleatória](populacao-e-amostra.qmd#sec-aa) e $X \sim f_\theta, \theta \in \Theta$.

Dizemos que $[I_1(\boldsymbol{X}_n), I_2(\boldsymbol{X}_n)]$ é um [intervalo de confiança](estimador-intervalar.qmd) *exato*
para $g(\theta)$ com coeficiente de confiança $\gamma \in (0,1)$ se, e somente se:
$$
P_\theta\left(I_1(\boldsymbol{X}_n) \leq g(\theta) \leq I_2(\boldsymbol{X}_n)\right) = \gamma, \forall \theta \in \Theta.
$$
em que $I_1(\boldsymbol{X}_n), I_2(\boldsymbol{X}_n)$ são [estatísticas](estatisticas.qmd).

:::{.callout-note title="Observação"}
Se $P_\theta\left(I_1(\boldsymbol{X}_n) \leq g(\theta) \leq I_2(\boldsymbol{X}_n)\right) \geq \gamma, \forall \theta \in \Theta$,
então $[I_1(\boldsymbol{X}_n), I_2(\boldsymbol{X}_n)]$ é um Intervalo de Confiança (IC) de *pelo menos* $\gamma$
:::

:::{.callout-note title="Notação"}
$$
\mathrm{IC}(g(\theta), \gamma) = [I_1(\boldsymbol{X}_n), I_2(\boldsymbol{X}_n)]
$$
:::

Observe que $\mathrm{IC}(g(\theta),\gamma)$ é um *intervalo aleatório* que não depende de "$\theta$".

Na prática, observamos a amostra $\boldsymbol{x}_n = (x_1, \dots, x_n)$ e calculamos o IC *observado*

:::{.callout-note title="Notação"}
$$
\mathrm{IC}_{\mathrm{Obs}}(g(\theta), \gamma) = [I_1(\boldsymbol{x}_n), I_2(\boldsymbol{x}_n)]
$$

Observe que este é um intervalo *numérico*
:::

Portanto,
$$
P_\theta\left(I_1(\boldsymbol{X}_n) \leq g(\theta) \leq I_2(\boldsymbol{X}_n)\right) =
\begin{cases}
1,& g(\theta) \in \mathrm{IC}_{\mathrm{Obs}}(g(\theta), \gamma) \\
0,& \mathrm{c.c.}
\end{cases}
$$

## Interpretação em termos de repetições:

Se repetirmos o experimento, mantendo as mesmas condições, então esperamos que em $\gamma \cdot 100\%$ dos experimentos
os ICs contenham $g(\theta)$. Em outras palavras,
$$
\begin{aligned}
\#\frac{(g(\theta) \in \mathrm{IC}_{\mathrm{Obs}})}{N} \approx \gamma \\
\left[
\frac{1}{N} \sum \mathbb{1}_{\{\mathrm{IC}_{\mathrm{Obs}}^{(i)}\}}(g(\theta)) \stackrel{N \uparrow \infty}{\rightarrow} \gamma
\right]
\end{aligned}
$$

Dizemos que
$$
\frac{1}{N} \sum \mathbb{1}_{\{\mathrm{IC}_{\mathrm{Obs}}^{(i)}\}}(g(\theta))
$$
é a cobertura de $\mathrm{IC}(g(\theta),\gamma)$

:::{.callout-tip title="O que dizer sobre $\mathrm{IC}_{\mathrm{Obs}}$ em relação a $g(\theta)$?"}
Temos uma **confiança** de $\gamma\cdot 100\%$ de que $g(\theta) \in \mathrm{IC}_{\mathrm{Obs}}$.
:::

## Exemplos

### Exemplo Normal

Seja $\boldsymbol{X}_n$ a.a. de $X \sim N(\mu,\sigma^2), \theta = (\mu, \sigma^2) \in \Theta = \mathbb{R}\times\mathbb{R}^+$.
Encontre um IC com coeficiente de confiança $\gamma = 95\%$...

...para $g(\theta = \mu)$

Sabemos que
$$
\bar{X} = \frac{1}{n} \sum X_i \sim N(\mu, \sigma^2/n).
$$
Além disso,
$$
\sum \frac{(X_i - \bar{X})^2}{\sigma^2} \sim \chi^2_{n-1}
$$

Por definição de t-student, com as V.As das distribuições independentes,
$$
\begin{aligned}
\frac{N(0,1)}{\sqrt{\frac{\chi^2_k}{k}}} &\sim t_k  \\
\Rightarrow\frac{\sqrt{n} \frac{(\bar{X} - \mu)}{\sigma}}{\sqrt{\frac{\sum \frac{(X_i - \bar{X})^2}{\sigma^2}}{n-1}}} &\sim t_{(n-1)} \\
\Rightarrow \frac{\sqrt{n} (\bar{X} - \mu)}{\sqrt{S^2_{n-1}(\boldsymbol{X}_n)}} &\sim t_{(n-1)}
\end{aligned}
$$
logo, podemos sempre encontrar $c_{1, \gamma}, c_{2, \gamma}$ tais que
$$
P_\theta\left(c_{2, \gamma} \leq \frac{\sqrt{n} (\bar{X} - \mu)}{\sqrt{S^2_{n-1}(\boldsymbol{X}_n)}} \leq c_{1, \gamma}\right) = \gamma
$$

Note que
$$
c_{2, \gamma} \leq \frac{\sqrt{n} (\bar{X} - \mu)}{\sqrt{S^2_{n-1}(\boldsymbol{X}_n)}} \leq c_{1, \gamma} \iff
\bar{X} - c_{2,\gamma} \sqrt{\frac{S^2_{n-1}(\boldsymbol{X}_n)}{n}} \leq \mu \leq \bar{X} - c_{1,\gamma} \sqrt{\frac{S^2_{n-1}(\boldsymbol{X}_n)}{n}}
$$

Portanto,
$$
P_\theta\left(\bar{X} - c_{2,\gamma} \sqrt{\frac{S^2_{n-1}(\boldsymbol{X}_n)}{n}} \leq g(\theta) \leq \bar{X} - c_{1,\gamma} \sqrt{\frac{S^2_{n-1}(\boldsymbol{X}_n)}{n}}\right) = \gamma
$$

Pela definição de Intervalo de Confiança,
$$
\mathrm{IC}(\mu,\gamma) = \left[
\bar{X} - c_{2,\gamma} \sqrt{\frac{S^2_{n-1}(\boldsymbol{X}_n)}{n}}, \bar{X} - c_{1,\gamma} \sqrt{\frac{S^2_{n-1}(\boldsymbol{X}_n)}{n}}
\right]
$$
em que
$$
S^2_{n-1}(\boldsymbol{X}_n) = \frac{1}{n-1} \sum  (X_i - \bar{X})^2
$$
e $c_{1,\gamma}, c_{2,\gamma}$ são os quantis obtidos da distribuição t-student com $n-1$ graus de liberdade que satisfaçam
$$
P_\theta\left(\bar{X} - c_{2,\gamma} \sqrt{\frac{S^2_{n-1}(\boldsymbol{X}_n)}{n}} \leq g(\theta) \leq \bar{X} -
c_{1,\gamma} \sqrt{\frac{S^2_{n-1}(\boldsymbol{X}_n)}{n}}\right) = \gamma
$$
no caso simétrico (minimiza o IC para distribuições como a Normal), $c_{2,\gamma} = - c_{1,\gamma}$. Note que não é
possível construir ICs simétricos dessa forma para distribuições estritamente positivas, como a qui-quadrado.

## Quantidades Pivotais {#sec-quantpivot}

Dizemos que $Q(g(\theta), \boldsymbol{X}_n)$ é uma quantidade pivotal para $g(\theta)$ se, e somente se,

1. $Q(g(\theta), \boldsymbol{X}_n)$ depende de $g(\theta)$

2. A distribuição de $Q(g(\theta), \boldsymbol{X}_n)$ não depende de "$\theta$"

3. Existem $a_1, a_2$, que não dependem de $g(\theta)$, tais que $c_1 \leq Q(g(\theta),\boldsymbol{X}_n) \leq c_2 \iff a_1 \leq g(\theta) \leq a_2$

### Exemplos
$X \sim N(\mu, \sigma^2)$

1. Se $\sigma^2$ é conhecido e $\theta = \mu. g(\theta) = \mu$, então uma quantidade pivotal é dada por
$$
Q(\mu, \boldsymbol{X}_n) = \sqrt{n}\frac{\bar{X} - \mu}{\sqrt{\sigma^2}} \sim N(0,1)
$$

$$
\mathrm{IC}(\mu,\gamma) = \bar{X} \mp c_{\gamma} \sqrt{\frac{\sigma^2}{n}}
$$

2. Se $\sigma^2$ é desconhecido e $\theta = (\mu, \sigma^2), g(\theta) = \mu$, então uma quantidade pivotal é dada por
$$
Q(\mu, \boldsymbol{X}_n) = \sqrt{n}\frac{\bar{X}-\mu}{\sqrt{S^2_{n-1}(\boldsymbol{X}_n)}} \sim t_{(n-1)}
$$

$$
\mathrm{IC}(\mu,\gamma) = \bar{X} \mp c_{\gamma} \sqrt{\frac{S^2_{n-1}(\boldsymbol{X}_n)}{n}}
$$
essa é também uma quantidade pivotal para 1., mas o contrário não vale.

3. Se $\mu$ é conhecido e $\theta = \sigma^2, g(\theta) = \sigma^2$, então

$$
Q(\sigma^2, \boldsymbol{X}_n) = \sum \frac{(X_i - \bar{X})^2}{\sigma^2} \sim \chi^2_{n}
$$

$$
\mathrm{IC}(\sigma^2,\gamma) = \left[\frac{\sum (X_i - \mu)^2}{c_{2,\gamma}}, \frac{\sum (X_i - \mu)^2}{c_{1,\gamma}}\right]
$$

4. Se $\mu$ é desconhecido e $\theta = (\mu, \sigma^2), g(\theta) = \sigma^2$, então
$$
Q(\sigma^2, \boldsymbol{X}_n) = \sum \frac{(X_i - \bar{X})}{\sigma^2} \sim \chi^2_{(n-1)}
$$

$$
\mathrm{IC}(\sigma^2,\gamma) = \left[\frac{\sum (X_i - \bar{X})^2}{c_{2,\gamma}}, \frac{\sum (X_i - \bar{X})^2}{c_{1,\gamma}}\right]
$$

#### Exponencial

Seja $X \sim \mathrm{Exp}(\theta), \theta > 0$. Encontre uma quantidade pivotal para $\theta$.

Note que $\sum X_i \sim \mathrm{Gama}(n,\theta)$ com F.G.M. dada por
$$
M_{\sum X_i}(t) = \left(\frac{\theta}{\theta-t}\right)^n
$$
note ainda que
$$
\begin{aligned}
M_{\sum X_i}(t) &= E(\mathrm{e}^{t \sum X_i}) =  \left(\frac{\theta}{\theta-t}\right)^n \\
\Rightarrow M_{\theta \sum X_i}(t) &= E(\mathrm{e}^{t\theta \sum X_i}) \\
&=M_{\sum X_i}(t \theta) = \left(\frac{\theta}{\theta-\theta t}\right)^n = \left(\frac{1}{1-t}\right)^n \\
\Rightarrow M_{2\theta\sum X_i}(t) &= \left(\frac{1}{1-2t}\right)^n \\
\Rightarrow &2\theta \sum X_i \sim \chi^2_{(2n)}
\end{aligned}
$$

Portanto, $Q(\theta, \boldsymbol{X}_n) = 2\theta \sum X_i$ é uma quantidade de interesse para $\theta$

$$
\begin{aligned}
c_{1,\gamma} &\leq Q(\theta, \boldsymbol{X}_n) \leq c_{2,\gamma} \\
\iff c_{1,\gamma} &\leq 2\theta \sum X_i \leq c_{2\gamma} \iff \frac{c_{1,\gamma}}{2\sum X_i} \leq \theta \leq \frac{c_{2,\gamma}}{2\sum X_i}
\end{aligned}
$$

#### Uniforme

Seja $X \sim \mathrm{Unif}(0, \theta), \theta > 0$. Encontre uma quantidade pivotal para $\theta$.

Note que $X_{(n)} = \max \boldsymbol{X}_n$ [é uma estatística suficiente](estatisticas-suficientes.qmd#sec-exunif) cuja f.d.p.
é dada por
$$
f_(\theta)^{\boldsymbol{X}_{(n)}}(x) = \frac{n x^{n-1}}{\theta^n} \mathrm{1}_{(0,\theta]}(x)
$$

Seja $Y = \frac{X_{(n)}}{\theta}$, então 
$$
f_{\theta}^{Y}(y) = f_{\theta}^{\boldsymbol{X}_{(n)}}(y \theta) \cdot |J|
$$
em que $J = \theta$ (determinante jacobiano)

$$
f_{\theta}^{Y}(y) = \frac{n (y\theta)^{n-1}}{\theta^n} \theta \mathbb{1}_{(0,\theta]}(y \theta) = n y^{n-1} \mathbb{1}_{(0,1]}(y)
$$
não depende de "$\theta$"! Logo, $Q(\theta, \boldsymbol{X}_n) = \frac{\boldsymbol{X}_{(n)}}{\theta}$ é uma quantidade pivotal
para $\theta$.

$$
\begin{aligned}
\frac{\boldsymbol{X}_{(n)}}{c_{2,\gamma}} \leq \theta \leq \frac{\boldsymbol{X}_{(n)}}{c_{1,\gamma}}
\Rightarrow \mathrm{IC}(\theta,\gamma) = \left[
\frac{\boldsymbol{X}_{(n)}}{c_{2,\gamma}}, \frac{\boldsymbol{X}_{(n)}}{c_{1,\gamma}}
\right]
\end{aligned}
$$
em que os quantis são obtidos da distribuição de $Y$, nesse caso, $Y \sim \mathrm{Beta}(n,1)$:

$$
\begin{aligned}
\int^{c_{1,\gamma}}_0 ny^{n-1} dy = \frac{1-\gamma}{2}&\ \ \ \int_{c_{2,\gamma}}^{1} ny^{n-1}dy = \frac{1-\gamma}{2} \\
\Rightarrow \begin{cases}
y^n \rvert_0^{c_{1,\gamma}} = \frac{1-\gamma}{2} \\
y^n \rvert^1_{c_{2,\gamma}} = \frac{1-\gamma}{2} \\
\end{cases}
\Rightarrow c_{1,\gamma} &= \left(\frac{1-\gamma}{2}\right)^{1/2}\ \ \ \ c_{2,\gamma} = \left(\frac{1+\gamma}{2}\right)^{1/2}
\end{aligned}
$$

:::{.callout-note title="Quantis que minimizam a amplitude do IC"}
Seja $Q(g(\theta), \boldsymbol{X}_n)$ uma quantidade pivotal com função densidade de probabilidade $f$.

$c_{1,\gamma}, c_{2,\gamma}$ devem ser obtidos
$$
\int_{c_{1,\gamma}}^{c_{2,\gamma}} f(x) dx = \gamma
$${#eq-obterquantis}

Note que em geral infinitas combinações desses quantis satisfazem ([-@eq-obterquantis]). Podemos usar o par que satisfaz
$$
\int^{c_{1,\gamma}}_{-\infty} f(y) dy = \frac{1-\gamma}{2}\ \ \mathrm{e} \ \ \int_{c_{2,\gamma}}^{\infty} f(y)dy = \frac{1-\gamma}{2}
$$

Esse método produz um intervalo de confiança simétrico, não necessariamente o de menor amplitude, mas é mais fácil de
encontrar. O intervalo de confiança com menor amplitude com quantis que satisfaçam ([-@eq-obterquantis]) é obtido minimizando
$|c_{2,\gamma} - c_{1,\gamma}|$ sujeito a ([-@eq-obterquantis]).

Se $f$ for *unimodal* e *bicaudal*, pode-se demonstrar que $c_{1,\gamma}, c_2{\gamma}$ que produzem amplitude mínima e satisfazem ([-@eq-obterquantis]) são
tais que
$$
f(c_{1,\gamma}) = f(c_{2,\gamma})
$$
ou seja, tem mesma densidade.
:::

```{julia}
#| echo: true
using Distributions, Random, StatsBase, LaTeXStrings

Random.seed!(8)

theta0 = 4
n = 10
MC = 10000

I1 = []
I2 = []
for _ in 1:MC
    d = Exponential(1 / theta0) # Parâmetro média => 1/theta parâmetro taxa
    x = rand(d, n)

    # Construindo um IC com 95% de confiança
    gamma = 0.95
    quiquadrado = Chisq(2 * n)
    c1 = quantile(quiquadrado, (1 - gamma) / 2)
    c2 = quantile(quiquadrado, gamma + (1 - gamma) / 2)
    push!(I1, round(c1 / (2 * sum(x)), digits = 4))
    push!(I2, round(c2 / (2 * sum(x)), digits = 4))
end

acertos = [I1 .<= theta0 .<= I2]
display(L"\mathrm{IC}_{\mathrm{Obs (1)}}(\theta,0.95)=[%$(I1[1]), %$(I2[1])]")
display(L"\text{ICs que contém}\ \theta: %$((sum(acertos[1]))/MC * 100)\%")
```

### Quantidades pivotais aproximadas ou assintóticas
Dizemos que $Q(g(\theta), \boldsymbol{X}_n)$ é uma quantidade pivotal aproximada ou assintótica se, e somente se

1. $Q(g(\theta), \boldsymbol{X}_n)$ depende de $g(\theta)$;

2. A distribuição assintótica de $Q(g(\theta), \boldsymbol{X}_n)$ não depende de $g(\theta)$;

3. Existem $a_1, a_2$, que não dependem de $g(\theta)$, tais que $c_1 \leq Q(g(\theta),\boldsymbol{X}_n) \leq c_2 \iff a_1 \leq g(\theta) \leq a_2$.
Esses valores dependem apenas de $\boldsymbol{X}_n$.

Podemos usar o [TLC](tlc.qmd) para [estimadores](estimadores.qmd):

Se $T(\boldsymbol{X}_n)$ for um estimador para $\theta$ [assintoticamente normal](prop-est.qmd#sec-assinorm), então

$$
\sqrt{n} (T(\boldsymbol{X}_n) - \theta)  \stackrel{\mathcal{D}}{\rightarrow} N_p(0, V_\theta), \forall \theta \in \Theta.
$$
em que $V_\theta$ é uma matriz positiva definida. Para $p=1$,
$$
\sqrt{n} (T(\boldsymbol{X}_n) - \theta)  \stackrel{\mathcal{D}}{\rightarrow} N_1(0, V_\theta), \forall \theta \in \Theta.
$$
em que $V_\theta > 0$.

Se $g: \Theta \rightarrow \mathbb{R}$ for uma função tal que $g'(\theta) \neq 0$ $g'(\theta)$ é contínua, então,
$$
\sqrt{n} (g(T(\boldsymbol{X}_n)) - g(\theta))  \stackrel{\mathcal{D}}{\rightarrow} N_1(0, g'(\theta)^2V_\theta), \forall \theta \in \Theta.
$$

Além disso,

1. 
$$
\frac{\sqrt{n} (g(T(\boldsymbol{X}_n)) - g(\theta))}{\sqrt{g'(\theta)^{2}V_\theta}}  \stackrel{\mathcal{D}}{\rightarrow} N_1(0, 1), \forall \theta \in \Theta.
$$

2. Pelo [teorema de Slutsky](slutsky.qmd)
$$
\frac{\sqrt{n} (g(T(\boldsymbol{X}_n)) - g(\theta))}{\sqrt{g'(T(\boldsymbol{X}_n))^{2}V_{T(\boldsymbol{X}_n)}}}  \stackrel{\mathcal{D}}{\rightarrow} N_1(0, 1), \forall \theta \in \Theta.
$$

Ou seja,
$$
Q(g(\theta), \boldsymbol{X}_n) = \frac{\sqrt{n} (g(T(\boldsymbol{X}_n)) - g(\theta))}{\sqrt{g'(T(\boldsymbol{X}_n))^{2}V_{T(\boldsymbol{X}_n)}}}
$$
é uma quantidade pivotal aproximada para $g(\theta)$. Note que, com $T(\boldsymbol{X}_n) = \hat(\theta)$,
$$
\begin{aligned}
&c_1 \leq Q(g(\theta), \boldsymbol{X}_n) \leq c_2 \\
\iff& c_1 \leq \frac{\sqrt{n} (g(T(\boldsymbol{X}_n)) - g(\theta))}{\sqrt{g'(T(\boldsymbol{X}_n))^{2}V_{T(\boldsymbol{X}_n)}}} \leq c_2 \\
\iff& c_1 \sqrt{\frac{g'(\hat\theta)^2}{n} V_{\hat\theta}} \leq g(\hat\theta) - g(\theta) \leq c_2\sqrt{\frac{g'(\hat\theta)^2}{n} V_{\hat\theta}} \\
\iff& g(\hat\theta) - c_2 \sqrt{\frac{g'(\hat\theta)^2}{n} V_{\hat\theta}} \leq g(\theta) \leq g(\hat\theta) - c_1\sqrt{\frac{g'(\hat\theta)^2}{n} V_{\hat\theta}} \\
\end{aligned}
$$
Tomando $c_1 = -c_2$, pois a distribuição assintótica é normal, temos
$$
\mathrm{IC}^a(g(\theta),\gamma) = \left[
g(\hat\theta) - c_2 \sqrt{\frac{g'(\hat\theta)^2}{n} V_{\hat\theta}}, g(\hat\theta) + c_2 \sqrt{\frac{g'(\hat\theta)^2}{n} V_{\hat\theta}}
\right]
$$
em que $c_2$ é obtido dos quantis da normal padrão tal que
$$
P(-c_2 \leq N(0,1) \leq c_2) = \gamma
$$

:::{.callout-note title="Erro padrão"}
A quantidade
$$
\sqrt{\frac{g'(\hat\theta)^2}{n} V_{\hat\theta}}
$$
é o erro-padrão do estimador (seu desvio padrão).
:::

Vamos conferir por [simulações de monte carlo](monte-carlo.qmd) e o [método de Newton-Raphson](metodo-nr.qmd)

a) Para $\theta$
b) Para $g(\theta) = P(X < 950) \Rightarrow g'(\theta) = f_\theta(950)$
c) Para $g(\theta) = \ln f_\theta(x) g'(\theta) = - \psi_1(\theta) + \ln(\theta) - 1/\theta$
d) Para $g(\theta) = f_\theta(x)$

```{julia}
#| echo: true
#| output: false
using SpecialFunctions, Distributions, Random, LaTeXStrings
# Modelo Gama(θ, 1)
# IC assintótico
# Calcule o IC
#a-) para θ
#b-) para g(θ) = P(X < 950) => g'(θ) = f_θ(950)
#c-) para g_x(θ) = ln f_θ(x) g'(θ) = - ψ_1(θ) + ln(θ) - 1/θ. x = θ
#d-) para g_x(θ) = f_\theta(x). x = θ


# Sabemos que θ.MV ~a~ N(θ, I_n(θ)^-1)
# => IC^a(θ,γ) = θ.MV ∓ c_2 * √(I_1(θ))

Random.seed!(13)

function newton_raphson(x)

    theta::Vector{Float64} = []
    append!(theta, mean(x)) # Chute inicial = média
    erromax = 10^(-5)
    erro = Inf
    i = 1
    # iteracoesMax = 6 # Podemos também definir apenas um erro máximo
    while erro > erromax # && i < iteracoesMax
        append!(
            theta, theta[i] - (sum(log.(x)) - n * digamma(theta[i])) /
                (-n * trigamma(theta[i]))
        )
        erro = abs(theta[i + 1] - theta[i])
        # println("Erro na iteração $i: $erro")
        i += 1
    end
    # println("Theta final: $(theta[length(theta)])")
    # println("Total de iterações: $i")
    return theta[end]
end

# Resolução do item a
function monte_carlo_a(M, γ, n, theta0)
    d = Gamma(theta0, 1)
    function mc()
        x = rand(d, n)
        EMV = newton_raphson(x)
        c2 = quantile(Normal(), γ + (1 - γ) / 2)
        I1 = EMV - c2 * sqrt(1 / (n * trigamma(EMV)))
        I2 = EMV + c2 * sqrt(1 / (n * trigamma(EMV)))
        return I1, I2
    end

    inferiores = []; superiores = []
    for _ in 1:M
        intervalo = mc()
        push!(inferiores, intervalo[1])
        push!(superiores, intervalo[2])
    end

    acertos = inferiores .<= theta0 .<= superiores
    return mean(acertos)

end
```
Rodando o código para o item a, temos
```{julia}
M = 10_000; γ = 0.95; n = 100; θ_0 = 100
confiança = monte_carlo_a(M, γ, n, θ_0)
display(L"\text{Confiança para } %$M \text{ simulações com alvo } %$γ, n = %$n, \theta_0 = %$(θ_0):\ %$(confiança * 100)\%")
M = 10_000; γ = 0.99; n = 100; θ_0 = 100
confiança = monte_carlo_a(M, γ, n, θ_0)
display(L"\text{Confiança para } %$M \text{ simulações com alvo } %$γ, n = %$n, \theta_0 = %$(θ_0):\ %$(confiança * 100)\%")
M = 10_000; γ = 0.99; n = 5; θ_0 = 75
confiança = monte_carlo_a(M, γ, n, θ_0)
display(L"\text{Confiança para } %$M \text{ simulações com alvo } %$γ, n = %$n, \theta_0 = %$(θ_0):\ %$(confiança * 100)\%")
```

Para o item b,
```{julia}
#| echo: true
#| output: false

# Resolução do item b
function monte_carlo_b(M, γ, n, theta0)
    d = Gamma(theta0, 1)
    g(a) = cdf(Gamma(a, 1), 950)
    g1(a) = pdf(Gamma(a, 1), 950)
    function mc()
        x = rand(d, n)
        EMV = newton_raphson(x)
        c2 = quantile(Normal(), γ + (1 - γ) / 2)
        I1 = g(EMV) - c2 * sqrt(g1(EMV)^2 / (n * trigamma(EMV)))
        I2 = g(EMV) + c2 * sqrt(g1(EMV)^2 / (n * trigamma(EMV)))
        return I1, I2
    end

    inferiores = []; superiores = []
    for _ in 1:M
        intervalo = mc()
        push!(inferiores, intervalo[1])
        push!(superiores, intervalo[2])
    end

    acertos = inferiores .<= g(theta0) .<= superiores
    return mean(acertos)

end
```

Rodando o código para o item b, temos
```{julia}
M = 10_000; γ = 0.95; n = 100; θ_0 = 1000
confiança = monte_carlo_b(M, γ, n, θ_0)
display(L"\text{Confiança para } %$M \text{ simulações com alvo } %$γ, n = %$n, \theta_0 = %$(θ_0):\ %$(confiança * 100)\%")
M = 10_000; γ = 0.99; n = 100; θ_0 = 951
confiança = monte_carlo_b(M, γ, n, θ_0)
display(L"\text{Confiança para } %$M \text{ simulações com alvo } %$γ, n = %$n, \theta_0 = %$(θ_0):\ %$(confiança * 100)\%")
M = 10_000; γ = 0.99; n = 5; θ_0 = 951
confiança = monte_carlo_b(M, γ, n, θ_0)
display(L"\text{Confiança para } %$M \text{ simulações com alvo } %$γ, n = %$n, \theta_0 = %$(θ_0):\ %$(confiança * 100)\%")
```

Para o item c, precisaremos calcurar as derivadas:
$$
\begin{aligned}
g(\theta) &= \ln f_\theta(\theta) = - \ln \Gamma(\theta) + (\theta-1) \ln (\theta) - \theta \\
\Rightarrow g'(\theta) &= -\psi_1(\theta) + \ln(\theta) + \frac{\theta -1}{\theta} - 1 \\
&= -\psi_1(\theta) + \ln(\theta) - \frac{-1}{\theta} = g1(\theta)
\end{aligned}
$$

```{julia}
#| echo: true
#| output: false

# Resolução do item c
function monte_carlo_c(M, γ, n, theta0)
    d = Gamma(theta0, 1)
    g(a) = log(pdf(Gamma(a, 1), a))
    g1(a) = -digamma(a) + log(a) - 1 / a
    function mc()
        x = rand(d, n)
        EMV = newton_raphson(x)
        c2 = quantile(Normal(), γ + (1 - γ) / 2)
        I1 = g(EMV) - c2 * sqrt(g1(EMV)^2 / (n * trigamma(EMV)))
        I2 = g(EMV) + c2 * sqrt(g1(EMV)^2 / (n * trigamma(EMV)))
        return I1, I2
    end

    inferiores = []; superiores = []
    for _ in 1:M
        intervalo = mc()
        push!(inferiores, intervalo[1])
        push!(superiores, intervalo[2])
    end
    acertos = inferiores .<= g(theta0) .<= superiores
    return mean(acertos)
end
```

Rodando o código para o item c, temos
```{julia}
M = 10_000; γ = 0.95; n = 100; θ_0 = 100
confiança = monte_carlo_c(M, γ, n, θ_0)
display(L"\text{Confiança para } %$M \text{ simulações com alvo } %$γ, n = %$n, \theta_0 = %$(θ_0):\ %$(confiança * 100)\%")
M = 10_000; γ = 0.99; n = 100; θ_0 = 100
confiança = monte_carlo_c(M, γ, n, θ_0)
display(L"\text{Confiança para } %$M \text{ simulações com alvo } %$γ, n = %$n, \theta_0 = %$(θ_0):\ %$(confiança * 100)\%")
M = 10_000; γ = 0.99; n = 5; θ_0 = 75
confiança = monte_carlo_c(M, γ, n, θ_0)
display(L"\text{Confiança para } %$M \text{ simulações com alvo } %$γ, n = %$n, \theta_0 = %$(θ_0):\ %$(confiança * 100)\%")
```

Para o item d, note que
$$
g'(\theta) = \mathrm{e}^{\ln f_\theta(x)} (-\psi_1(\theta) + \ln(\theta) - 1/\theta)
$$

```{julia}
#| echo: true
#| output: false

# Resolução do item d
function monte_carlo_d(M, γ, n, theta0)
    d = Gamma(theta0, 1)
    g(a) = exp(log(pdf(Gamma(a, 1), a))) # usado a expp(log), poderia usar direto
    g1(a) = exp(log(pdf(Gamma(a, 1), a))) * (-digamma(a) + log(a) - 1 / a)
    function mc()
        x = rand(d, n)
        EMV = newton_raphson(x)
        c2 = quantile(Normal(), γ + (1 - γ) / 2)
        I1 = g(EMV) - c2 * sqrt(g1(EMV)^2 / (n * trigamma(EMV)))
        I2 = g(EMV) + c2 * sqrt(g1(EMV)^2 / (n * trigamma(EMV)))
        return I1, I2
    end

    inferiores = []; superiores = []
    for _ in 1:M
        intervalo = mc()
        push!(inferiores, intervalo[1])
        push!(superiores, intervalo[2])
    end
    acertos = inferiores .<= g(theta0) .<= superiores
    return mean(acertos)
end
```

Rodando o código para o item d, temos
```{julia}
M = 10_000; γ = 0.95; n = 100; θ_0 = 100
confiança = monte_carlo_d(M, γ, n, θ_0)
display(L"\text{Confiança para } %$M \text{ simulações com alvo } %$γ, n = %$n, \theta_0 = %$(θ_0):\ %$(confiança * 100)\%")
M = 10_000; γ = 0.99; n = 100; θ_0 = 100
confiança = monte_carlo_d(M, γ, n, θ_0)
display(L"\text{Confiança para } %$M \text{ simulações com alvo } %$γ, n = %$n, \theta_0 = %$(θ_0):\ %$(confiança * 100)\%")
M = 10_000; γ = 0.99; n = 5; θ_0 = 75
confiança = monte_carlo_d(M, γ, n, θ_0)
display(L"\text{Confiança para } %$M \text{ simulações com alvo } %$γ, n = %$n, \theta_0 = %$(θ_0):\ %$(confiança * 100)\%")
```

#### Comparação entre o exato e aproximado

```{julia}
#| echo: true
#| output: false
using Plots, LaTeXStrings, Distributions, StatsBase, Random

function monte_carlo_realxaprox()
    theta0 = 10
    M = 100_000
    nn = 100

    γ = 0.95

    g(a) = a
    g1(a) = 1

    w = 0
    cober = zeros(nn)
    cobera = zeros(nn)
    for n in 1:nn
        w += 1
        I1 = zeros(M)
        Ia1 = zeros(M)
        I2 = zeros(M)
        Ia2 = zeros(M)

        for i in 1:M
            x = rand(Exponential(1 / theta0), n)
            c1 = quantile(Chisq(2n), (1 - γ) / 2)
            c2 = quantile(Chisq(2n), γ + (1 - γ) / 2)
            I1[i] = c1 / (2 * sum(x))
            I2[i] = c2 / (2 * sum(x))

            ca2 = quantile(Normal(), γ + (1 - γ) / 2)
            EMV = 1 / mean(x)
            Ia1[i] = g(EMV) - ca2 * sqrt(g1(EMV)^2 * EMV^2 / n)
            Ia2[i] = g(EMV) + ca2 * sqrt(g1(EMV)^2 * EMV^2 / n)
        end
        cober[w] = mean(I1 .<= theta0 .<= I2)
        cobera[w] = mean(Ia1 .<= g(theta0) .<= Ia2)
    end
    preal = scatter(
        1:10:100, cober, color = "blue",
        label = "Cobertura do IC real observado",
        title = "Comparação entre ICs: Cobertura",
        ylims = (γ - 0.1, γ + 0.1),
        xlabel = "Cobertura",
        ylabel = L"n"
    )
    scatter!(1:10:100, cobera, color = "red", label = "Cobertura IC aproximado")
    hline!([γ], label = L"\gamma")
    return preal
end
```

<!-- TODO: adicionar comparação da amplitude, corrigir ylabel -->

```{julia}
display(monte_carlo_realxaprox())
```

#### Exemplo poisson

Note que0
$$
\begin{aligned}
\hat{\theta}_{\mathrm{MV}}(\boldsymbol{X}_n) &= \bar{X} \\
\sqrt{n}\frac{\hat{\theta}_{\mathrm{MV}}(\boldsymbol{X}_n) - \theta}{\sqrt{\theta}} &\stackrel{\mathcal{D}}{\rightarrow} N(0,1) \\
\sqrt{n}\frac{\hat{\theta}_{\mathrm{MV}}(\boldsymbol{X}_n) - \theta}{\sqrt{\hat{\theta}_{\mathrm{MV}}(\boldsymbol{X}_n)}} &\stackrel{\mathcal{D}}{\rightarrow} N(0,1) \\
\end{aligned}
$$

```{julia}
#| echo: true
#| output: false
function monte_carlo_poiss()
    theta0 = 10
    d = Poisson(theta0)
    g(a) = a
    g1(a) = 1
    nn = 1000
    amplitude = zeros(nn)
    cober = zeros(nn)
    M = 10_000
    for n in nn
        I1 = zeros(M)
        I2 = zeros(M)
        for i in 1:M
            x = rand(d, n)
            EMV = mean(x)
            c2 = quantile(Normal(), γ + (1 - γ) / 2)
            I1[i] = EMV - c2 * sqrt(g1(EMV)^2 * EMV / n)
            I2[i] = EMV + c2 * sqrt(g1(EMV)^2 * EMV / n)
        end
        amplitude[n] = mean(I2 .- I1)
        cober[n] = mean(I1 .<= g(theta0) .<= I2)
    end
    amp = scatter(
        10:100:1000, amplitude, title = "Amplitude dos ICs", label = "",
        color = :blue, xlabel = "n"
    )
    cob = scatter(
        10:100:1000, cober, title = "Cobertura dos ICs", label = "",
        color = :red, xlabel = "n"
    )
    plt = plot(cob, amp)
    return plt
end
```

```{julia}
display(monte_carlo_poiss())
```

## Regiões de confiança

Dizemos que $\mathrm{RC}^{(a)}(g(\theta),\gamma)$ é uma região de confança para $g(\theta)$ com coeficiente de confiança $\gamma$
se, e somente se,
$$
P_\theta(\theta \in \mathrm{RC}(g(\theta),\gamma)) = \gamma, \forall \theta \in \Theta.
$$

Se $\lim_{n\rightarrow \infty} P_\theta(\theta \in \mathrm{RC}(g(\theta),\gamma)) = \gamma, \forall \theta \in \Theta$, dizemos
que $\mathrm{RC}^{(a)}(g(\theta),\gamma)$ é uma região de confiança assintótica.

:::{.callout-note title="Observação"}
Se
$$
P_\theta(\theta \in \mathrm{RC}(g(\theta),\gamma)) \geq \gamma,
$$
então dizemos que $\mathrm{RC}(g(\theta),\gamma))$ tem confiança de pelo menos $\gamma$.
:::

:::{.callout-note title="Observação"}
$\mathrm{RC}(g(\theta),\gamma))$ é uma região aleatória que depende apenas da [amostra aleatória](populacao-e-amostra.qmd#sec-aa)
e do coeficiente $\gamma$
:::

### Construção da Região de Confiança

Seja $\hat{\theta}(\boldsymbol{X}_n)$ um [estimador](estimadores.qmd) para "$\theta$" tal que

1.
$$
\sqrt{n}(\hat{\theta}(\boldsymbol{X}_n) - \theta) \stackrel{\mathcal{D}}{\rightarrow} N_p(\boldsymbol{0}, V_\theta), \forall \theta \in \Theta.
$$

2.
$$
\sqrt{n}V_\theta^{-1/2}(\hat{\theta}(\boldsymbol{X}_n) - \theta) \stackrel{\mathcal{D}}{\rightarrow} N_p(\boldsymbol{0}, I), \forall \theta \in \Theta.
$$

3.
$$
\sqrt{n}V_{\hat{\theta}(\boldsymbol{X}_n)}^{-1/2}(\hat{\theta}(\boldsymbol{X}_n) - \theta) \stackrel{\mathcal{D}}{\rightarrow} N_p(\boldsymbol{0}, I), \forall \theta \in \Theta.
$$

4.
$$
n(\hat{\theta}(\boldsymbol{X}_n) - \theta)^T V_{\hat{\theta}(\boldsymbol{X}_n)}^{-1} (\hat{\theta}(\boldsymbol{X}_n) - \theta) \stackrel{\mathcal{D}}{\rightarrow} \chi^2_p, \forall \theta \in \Theta.
$$

Podemos usar 4. para construir uma RC para "$\theta$" assintótica:
$$
\mathrm{RC}^{(a)}(\theta, \gamma) = \left\{\theta \in \Theta : W_n(\theta) \leq q_\gamma\right\},
$$
em que $q_\gamma$ satisfaz
$$
P(\chi^2_p \leq q_\gamma) = \gamma.
$$
Assim, por construição,
$$
\lim_{n\rightarrow\infty} P_\theta(\theta \in \mathrm{RC}^{(a)}(\theta, \gamma)) = \gamma, \forall \theta \in \Theta.
$$

Para o caso $p=1$, ou seja, $\Theta \in \mathbb{R}$, note que
$$
n(\hat{\theta}(\boldsymbol{X}_n) - \theta)^T V_{\hat{\theta}(\boldsymbol{X}_n)}^{-1} (\hat{\theta}(\boldsymbol{X}_n) - \theta) =
\frac{n(\hat{\theta}(\boldsymbol{X}_n) - \theta)^2}{V_{\hat{\theta}(\boldsymbol{X}_n)}} = W_n(\theta).
$$
Portanto,
$$
\begin{aligned}
\mathrm{RC}(\theta,\gamma) &= \left\{
\theta \in \Theta : 
\frac{n(\hat{\theta}(\boldsymbol{X}_n) - \theta)^2}{V_{\hat{\theta}(\boldsymbol{X}_n)}} \leq q_\gamma
\right\} \\
\iff
\mathrm{RC}(\theta,\gamma) &= \left\{
\theta \in \Theta : 
\lvert \hat{\theta}(\boldsymbol{X}_n) - \theta \rvert \leq \sqrt{q_\gamma \cdot \frac{V_{\hat{\theta}(\boldsymbol{X}_n)}}{n}}
\right\} \\
\iff
\mathrm{RC}(\theta,\gamma) &= \left\{
\theta \in \Theta : 
-\sqrt{q_\gamma \cdot \frac{V_{\hat{\theta}(\boldsymbol{X}_n)}}{n}} \leq \hat{\theta}(\boldsymbol{X}_n) - \theta
\leq \sqrt{q_\gamma \cdot \frac{V_{\hat{\theta}(\boldsymbol{X}_n)}}{n}}
\right\} \\
\iff
\mathrm{RC}(\theta,\gamma) &= \left\{
\theta \in \Theta : 
\hat{\theta}(\boldsymbol{X}_n) -\sqrt{q_\gamma} \sqrt{\frac{V_{\hat{\theta}(\boldsymbol{X}_n)}}{n}} \leq \theta
\leq \hat{\theta}(\boldsymbol{X}_n) + \sqrt{q_\gamma} \sqrt{\frac{V_{\hat{\theta}(\boldsymbol{X}_n)}}{n}}
\right\}
\end{aligned}
$$
em que $q_\gamma$ é tal que $P(\chi^2_1 \leq q_\gamma) = \gamma$.

No caso em que $\Theta$ é uma semireta ou intervalo, a região de confiança coincide com o intervalo de confiança.

:::{.callout-note title="Observação"}
$$
P(\chi^2_1 \leq q_\gamma) = P(-\sqrt{q_\gamma} \leq N(0,1) \leq \sqrt{q_\gamma}) = \gamma
$$
:::

Para o caso $p=2$, ou seja, $\Theta = \mathbb{R}^2$, a região de confiança aproximada é
$$
\mathrm{RC}(\theta, \gamma) = \left\{\theta \in \Theta : W_n(\theta) \leq q_\gamma\right\},
$$

<!-- TODO: gráfico da elipse -->
