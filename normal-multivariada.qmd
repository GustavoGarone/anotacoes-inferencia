# Normal Multivariada

Dizemos que $\boldsymbol{X} = \left(\begin{array}{c}X_1 \\ \vdots \\ X_n\end{array}\right)$ tem distribuição multivariada
$N_d(\boldsymbol{\mu}, \Sigma)$ se, e somente se, a sua função densidade de probabilidade é dada por:

$$
f^{\boldsymbol{X}}(\boldsymbol{x}) = \frac{1}{(2\pi)^{\frac{d}{2}} \lvert \Sigma \rvert^{\frac{1}{2}}}
\mathrm{Exp}\left\{-\frac{1}{2} (\boldsymbol{x} - \boldsymbol{\mu})^T \Sigma^{-1}(\boldsymbol{x}-\boldsymbol{\mu})\right\}
$$

em que $\mu=\left(\begin{array}{c} \mu_1, \vdots, \mu_n\end{array}\right)$ e $\Sigma = \left[\begin{array}{cccc}
\sigma_{11} & \sigma_{12} & \dots & \sigma_{1d} \\
\sigma_{21} & \sigma_{22} & \dots & \sigma_{2d} \\
\vdots & \vdots & \ddots & \vdots \\
\sigma_{d1} & \dots & \sigma_{dd-1} & \sigma_{dd}
\end{array}\right]
$
é uma matriz simétrica positiva definida, ou seja, $\boldsymbol{y}^T \Sigma \boldsymbol{y} > 0, \forall y \in \mathbb{R}^d \setminus \{\boldsymbol{0}\}$

:::{.callout-note title="Notação"}
$$
\boldsymbol{X} \sim N_d(\boldsymbol{\mu}, \Sigma)
$$
:::

:::{.callout-note title="Obs 1: Caso particular: Normal univariada"}
Se $d=1$, então 
$\boldsymbol{X} = X_1$ e $X_1 \sim N_1(\mu_1,\sigma_{11})$
:::


:::{.callout-note title="Obs 2: Esperança e (co)variância"}
Pode-se mostrar que:
$$
E(\boldsymbol{X}) = \left(
\begin{array}{c}
E(X_1) \\
\vdots \\
E(X_d)
\end{array}
\right) = \boldsymbol{\mu}
$$
e
$$
\mathrm{Var}(\boldsymbol{X}) = E((\boldsymbol{X} - \boldsymbol{\mu})(\boldsymbol{X} - \boldsymbol{\mu})^T) = \Sigma
$$
ou seja,
$$
\mathrm{Cov}(X_i, X_j) = \sigma_{i,j}, i, j = 1, \dots, d
$$
:::


:::{.callout-note title="Obs 3: Covariância implica independência para normais"}
Se
$$
\Sigma = \mathrm{diag}(\sigma_{11}\dots\sigma_{dd}) = \left[\begin{array}{cccc}
\sigma_{11} & 0 & \dots & 0 \\
0 & \sigma_{22} & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & \dots & 0 & \sigma_{dd}
\end{array}\right]
$$
então $X_1, \dots, X_d$ são independentes.

Ou seja, se $X_1, \dots, X_d$ forem não correlacionadas (covariância zero), então serão independentes. Em outras
distribuições multivariadas, isso não é certo.
:::



:::{.callout-note title="Obs 4: Caso de independência"}
Se $\Sigma = \mathrm{diag}(\sigma_{11} \dots \sigma_{dd})$, então
$$
\lvert \Sigma \rvert  = \prod^d_{i=1} \sigma_{ii}, \Sigma^{-1} =
\left[\begin{array}{cccc}
\sigma_{11}^{-1} & 0 & \dots & 0 \\
0 & \sigma_{22}^{-1} & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & \dots & 0 & \sigma_{dd}^{-1}
\end{array}\right]
$$
e
$$
\begin{aligned}
(\boldsymbol{x} - \boldsymbol{\mu})^{T}\Sigma^{-1}(\boldsymbol{x} - \boldsymbol{\mu}) &=
\left(\begin{array}{c}
x_1 - \mu_1 \\
\vdots \\
x_d - \mu_d \\
\end{array}\right)^T
\left[\begin{array}{cccc}
\sigma_{11}^{-1} & 0 & \dots & 0 \\
0 & \sigma_{22}^{-1} & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & \dots & 0 & \sigma_{dd}^{-1}
\end{array}\right]
\left(\begin{array}{c}
x_1 - \mu_1 \\
\vdots \\
x_d - \mu_d \\
\end{array}\right) \\
&=
\left(
\frac{(x_1 - \mu_1}{\sigma_{11}} \dots \frac{x_d - \mu_d}{\sigma_{dd}}
\right)
\left(\begin{array}{c}
x_1 - \mu_1 \\
\vdots \\
x_d - \mu_d \\
\end{array}\right) \\
&= \frac{(x_1 - \mu_1)^2}{\sigma_{11}} +\dots +\frac{(x_d - \mu_d)^2}{\sigma_{dd}}
\end{aligned}
$$
Portanto, a f.d.p de $X$ quando $\Sigma = \mathrm{diag}(\sigma_{11} \dots \sigma_{dd})$ é
$$
\begin{aligned}
f^{\boldsymbol{X}}(\boldsymbol{x}) &= \frac{1}{(2\pi)^{\frac{d}{2}} \left(\prod \sigma_{ii}\right)^{\frac{1}{2}}}
\mathrm{Exp}\left\{-\frac{1}{2}\sum^d \frac{(x_i - \mu_i)^2}{(\sigma_{ii})}\right\} \\
&= \prod \left\{
\frac{1}{(2\pi)^{\frac{d}{2}} \sigma_{ii}^{\frac{1}{2}}}
\mathrm{Exp}\left\{-\frac{1}{2} \frac{(x_i - \mu_i)^2 }{(\sigma_{ii})}\right\}
\right\} \\
&= \prod^d_{i=1} f^{(i)}(x_i)
\end{aligned}
$$
em que $f^{(i)}(x_i)$ é  a f.d.p de $N(\mu_i, \sigma_{ii})$
:::

:::{.callout-note title="Obs 5: Função geradora de momentos"}
A função geradora de momentos de $\boldsymbol{X} \sim N_d(\boldsymbol{\mu} \Sigma)$ é dada por
$$
M_{\boldsymbol{X}}(\boldsymbol{t}) = \mathrm{Exp}\left\{
\boldsymbol{\mu}^T\boldsymbol{t} + \frac{1}{2} \boldsymbol{t}^T \Sigma \boldsymbol{t}, \forall \boldsymbol{t} \in \mathbb{R}^d
\right\}
$$
:::

:::{.callout-note title="Obs 6: Transformações para normais generalizadas"}
Se $\boldsymbol{Y} = \boldsymbol{a} + B\boldsymbol{X}$, em que $\boldsymbol{a} \in \mathbb{R}^m$ e $B$ é uma matriz $m\times d$, em que
$m \leq d$ cujas linhas são linearmente independentes, então,
$$
\boldsymbol{Y} \sim N_m(\boldsymbol{a} + B \boldsymbol{\mu}, B \Sigma B^T).
$$
Prova por F.G.M:
$$
\begin{aligned}
M_{\boldsymbol{Y}}(\boldsymbol{t} &= E(\mathrm{e}^{\boldsymbol{Y}^T\boldsymbol{t}}) \\
&= E(\mathrm{e}^{\boldsymbol{a}^T \boldsymbol{t} + \boldsymbol{X}^T B^T \boldsymbol{t}}) \\
&= \mathrm{e}^{\boldsymbol{a}^T\boldsymbol{t}}E(\mathrm{e}^{\boldsymbol{X}^T B^T \boldsymbol{t}}) \\
&= \mathrm{e}^{\boldsymbol{a}^T\boldsymbol{t}} M_{\boldsymbol{X}}(B^T \boldsymbol{t}) \\
&= \mathrm{e}^{\boldsymbol{a}^T\boldsymbol{t}} \mathrm{e}^{\boldsymbol{\mu}(B^T\boldsymbol{t}) + \frac{1}{2} \boldsymbol{t}^T B \Sigma B^T \boldsymbol{t}}\\
&= \mathrm{e}^{(\boldsymbol{a} + B\boldsymbol{\mu})^T\boldsymbol{t} + \frac{1}{2} \boldsymbol{t}^T B \Sigma B^T \boldsymbol{t}}
\end{aligned}
$$

Observações adicionais:

Se tomarmos $B = \Sigma^{-\frac{1}{2}}$, então $B\cdot B = \Sigma^{-1}$ e $B \Sigma B^{T} = I$. Ademais, tomando
$\boldsymbol{a} = - \Sigma^{-\frac{1}{2}}\boldsymbol{\mu}$, temos que
$$
\boldsymbol{Y} = \Sigma^{-\frac{1}{2}} \boldsymbol{X} - \Sigma^{-\frac{1}{2}}\boldsymbol{\mu} = \Sigma^{-\frac{1}{2}}(\boldsymbol{X} - \boldsymbol{\mu}) \sim N_d(0,I)
$$

Como $\Sigma$ é uma matriz quadrada simétrica e positiva definida, todos seus autovalores são estritamente positivas. Logo,
$$
\Sigma = \Gamma \Lambda \Gamma^T
$$
em que $\Gamma$ é a matriz diagonal de autovetores de $\Sigma$ e $\Lambda$ é a matriz diagonal de autovalores de $\Sigma$.

Pode-se mostrar que $\Sigma^{-1} = \Gamma \Lambda^{-1}\Gamma^T$ e $\Sigma^{-\frac{1}{2}} = \Gamma \Lambda^{-\frac{1}{2}}\Gamma^T$
em que
$$
\Lambda^k = \begin{bmatrix}
\lambda_1^k & 0 & \dots & 0 \\
0 & \lambda_2^k & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & \dots & 0 & \lambda_d^k\\
\end{bmatrix}
$$

Note ainda que
$$
\lvert \Sigma \rvert = \lvert \Gamma \Lambda \Gamma^T \rvert = \lvert \Lambda \Gamma^T \Gamma \rvert
$$
como $\Gamma^T \Gamma = I$,
$$
\lvert \Sigma \rvert = \lvert \Lambda \rvert = \prod^d_{i=1} \lambda_i
$$

Além disso,
$$
\mathrm{tr}\{\Sigma\} = \mathrm{tr}\{\Gamma \Lambda \Gamma^T\} = \mathrm{tr}\{\Lambda\Gamma^T\Gamma\} = \mathrm{tr}\{\Lambda\} = \sum^d_{i=1} \lambda_1
$$

```{julia}
#|eval: false
using LinearAlgebra, Distributions, Random
Random.seed!(10)
d = Normal(3,1)
a = rand(d, 3, 3)
b = a * transpose(a)
L = eigvals(b)
G = eigvecs(b)
println("Gama: $G")
println("Lambda: $L")
println("Matriz raiz quadrada (direto): $(b^(-1/2))")
println("Matriz raiz quadrada (decomposição): $(G * sqrt.(L) * transpose(T))")
println("Traço (direto): $(tr(b))")
println("Traço (por Lambda): $(tr(L))")
```
:::

:::{.callout-note title="Obs 7: Normais marginais"}
Se $\boldsymbol{X} \sim N_d(\boldsymbol{\mu}, \Sigma)$, então $X_i \sim N(\mu_i, \sigma_{ii}), i = 1,\dots d$

Demonstração:

Tomando $\boldsymbol{a} = 0$ e $B_i = (0 \dots 1 \dots 0)$ ($1$ na $i$-ésima posição), temos que
$$
a + B_i \boldsymbol{X} = X_i \sim N_1(\mu_i, \sigma_{ii})
$$
pelo resultado 6. Note que
$$
B_i \Sigma B^T_i = \sigma_{ii}.
$$
:::


## Normal bivariada $d=2$

$$
\boldsymbol{X} = \left(\begin{array}{c} X_1 \\ X_2 \end{array}\right) \sim N_2 (\boldsymbol{\mu}, \Sigma)
$$
em que $\boldsymbol{\mu} =\left(\begin{array}{c} \mu_1 \\ \mu_2 \end{array}\right)$ e
$\Sigma = \left(\begin{array}{cc} \sigma_{11} & \sigma_{12} \\ \sigma_{12} & \sigma_{22} \end{array}\right)$ é simétrica
positiva definida.

$$
\begin{aligned}
\lvert \Sigma \rvert &= \sigma_{11} \sigma_{22} - \sigma_{12}^2 \\
\Sigma^{-1} =& \frac{1}{\sigma_{11} \sigma_{22} - \sigma_{12}^2}
\left(\begin{array}{cc} \sigma_{11} & -\sigma_{12} \\ -\sigma_{12} & \sigma_{22} \end{array}\right) \\
\Rightarrow f^{\boldsymbol{X}}(x_1, x_2)\\ = \frac{1}{2\pi \sqrt{\sigma_{11}\sigma_{22} - \sigma_{12}^2}}
&\mathrm{Exp}\underbracket{\left\{
-\frac{1}{2} \left(\begin{array}{c} X_1 - \mu_1 \\ X_2 - \mu_2 \end{array}\right)^T
\left(\begin{array}{cc} \sigma_{11} & -\sigma_{12} \\ -\sigma_{12} & \sigma_{22} \end{array}\right)
\left(\begin{array}{c} X_1 - \mu_1 \\ X_2 - \mu_2 \end{array}\right) \frac{1}{\sigma_{11}\sigma_{22} - \sigma_{12}^2}
\right\}}_{*} \\ \\
*&= [(x_1 - \mu_1)\sigma_{22} - (x_2 - \mu_2)\sigma_{12} - (x_1-\mu_1)\sigma_{12}+(x_2 - \mu_2)\sigma_{11}]
\left(\begin{array}{c} X_1 - \mu_1 \\ X_2 - \mu_2 \end{array}\right)^T \\
&= (x_1 - \mu_1)^2 \sigma_{22} - 2(x_1 - \mu_1)(x_2 - \mu_2)\sigma_{12} + (x_2 - \mu_2)^2 \sigma_{11} \\
\Rightarrow \frac{*}{\sigma_{11}\sigma_{22} - \sigma_{12}^2} &=
\frac{(x_1 - \mu_1)^2}{\frac{\sigma_{11}\sigma_{22} - \sigma_{12}^2}{\sigma_{22}}} -
\frac{2(x_1 - \mu_1)(x_2 - \mu_2)}{\frac{\sigma_{11}\sigma_{22} - \sigma{12}^2}{\sigma_{22}}} +
\frac{(x_2 - \mu_2)^2}{\frac{\sigma_{11}\sigma_{22} - \sigma_{12}^2}{\sigma_{22}}} \\
\mathrm{Tome}\ \rho = \frac{\sigma_{12}}{\sqrt{\sigma_{11} \sigma_{22}}} \\ \\
A.\ \frac{\sigma_{11}\sigma_{22} - \sigma_{12}^2}{\sigma_{22}} &= \sigma_{11} - \frac{\sigma_{12}^2}{\sigma_{22}} \\
&= \sigma_{11} - \sigma_{11}\frac{\sigma_{12}^2}{\sigma_{11}\sigma_{22}} \\
&= \sigma_{11} - \sigma_{11} \rho^2  \\
&= \sigma_{11}(1-\rho^2)\\ \\
B.\ \frac{\sigma_{11} \sigma_{22} - \sigma_{12}^2}{\sigma_{11}} &= \sigma_{22}(1-\rho^2) \\ \\
C.\ \frac{\sigma_{11}\sigma_{22}}{\sigma_{12}} - \sigma_{12} &= \sigma_{12} \frac{\sigma_{11}\sigma_{22}}{\sigma_{12}^2} - \sigma_{12} \\
&= \sigma_{12}(\rho^{-2} - 1) = \sigma_{12} \left(\frac{1 - \rho^2}{\rho^2}\right) \\
\Rightarrow \frac{*}{\sigma_{11}\sigma_{22}-\sigma_{12}^2} &= \frac{(x_1-\mu_1)^2}{\sigma_{11}(1-\rho^2)}
-2\rho^2\frac{(x_1 - \mu_1)(x_2 - \mu_2)}{\sigma_{12}(1-\rho^2)} + \frac{(x_2 - \mu_2)^2}{\sigma_{22}(1-\rho^2)}
\end{aligned}
$$

Além disso,
$$
\sqrt{\sigma_{11}\sigma_{22} - \sigma_{12}^2} = \sqrt{\sigma_{11}\sigma_{22}-\rho^2\sigma_{11}\sigma_{22}} = \sqrt{\sigma_{11} \sigma_{22}} \sqrt{1-\rho^2}
$$

Assim temos que
$$
\begin{aligned}
&f^{\boldsymbol{X}}(x_1, x_2) \\&= \frac{1}{2\pi \sqrt{\sigma_{11}, \sigma_{22}} \sqrt{1-\rho^2}}
\mathrm{Exp} \left\{
-\frac{1}{2(1-\rho^2)} \left(\frac{(x_1-\mu_1)^2}{\sigma_{11}}
-2\rho\frac{(x_1 - \mu_1)(x_2 - \mu_2)}{\sqrt{\sigma_{11}\sigma_{22}}} + \frac{(x_2 - \mu_2)^2}{\sigma_{22}} \right)
\right\}
\end{aligned}
$$

## [Modelo estatístico](./modelo-estatistico.qmd)

Dizemos que $\boldsymbol{X} = \left(\begin{array}{c} X_1 \\ \vdots \\ X_d \end{array}\right)$ é um vetor aleatório
populacional normal multivariado se $\boldsymbol{X}_n \sim N_d(\boldsymbol{\mu}, \Sigma)$. em que o vetor de parâmetros
é
$$
\theta = \left[\begin{array}{c}
\mu_1 \\
\vdots \\
\mu_d \\
\sigma_{11} \\
\vdots \\
\sigma_{d1} \\
\vdots \\
\sigma_{dd}
\end{array}\right] =
\left[\begin{array}{c}
\boldsymbol{\mu} \\
\mathrm{vech}\Sigma
\end{array}\right] = (\boldsymbol{\mu}^T, (\mathrm{vech}\Sigma)^T)^T
$$

em que $\mathrm{vech}\Sigma$ é o vetor coluna com os elementos da triangular superior da matriz removidos:
$$
\mathrm{vech} \left(\begin{array}{cc}\sigma_{11} & \sigma_{12} \\ \sigma_{12} & \sigma_{22} \end{array}\right) =
\left(\begin{array}{c} \sigma_{11} \\ \sigma_{12} \\ \sigma_{22} \end{array}\right)
$$

## [Amostra aleatória](populacao-e-amostra.qmd#sec-aa) de uma normal multivariada

Dizemos que $\boldsymbol{X}_1,\dots, \boldsymbol{X}_n$ é uma amostra aleatória de $\boldsymbol{X} \sim N_d(\boldsymbol{\mu}, \Sigma)$ se,
e somente se, $\boldsymbol{X}_1,\dots,\boldsymbol{X}_n$ são independentes e $\boldsymbol{X}_i \sim N_d(\boldsymbol{\mu}, \Sigma), i = 1, \dots, n$.

:::{.callout-note title="Notação"}
$$
\boldsymbol{X}_n^* = (\boldsymbol{X}_1, \dots, \boldsymbol{X}_n)\ \mathrm{a.a.}\ \text{de}\ \boldsymbol{X} \sim N_d(\boldsymbol{\mu}, \Sigma) 
$$
:::

## [Função de verossimilhança](funcao-verossimilhanca.qmd)

Seja $\boldsymbol{X}^*$ a.a. de $\boldsymbol{X} \sim N_d(\boldsymbol{\mu}, \Sigma)$, em que
$$
\begin{aligned}
\theta &= (\boldsymbol{\mu}^T, \mathrm{vech}(\Sigma)^T)^T \\
\in \Theta &= \left\{(\boldsymbol{\mu}^T,\mathrm{vech}(\Sigma)^T) \in \mathbb{R}^d\times \mathbb{R}^{\frac{d(d+1)}{2}}
: \Sigma\ \text{é positiva definida (p.d.)}\right\}\subseteq \mathbb{R}^{d + \frac{d(d+1)}{2}}
\end{aligned}
$$

A função de verossimilhança é:
$$
\begin{aligned}
\mathcal{L}_{\boldsymbol{x}^*}(\theta) &=  \prod^n_{i=1} f_\theta^{\boldsymbol{X}}(\boldsymbol{x}_i) =
\prod \left\{
\frac{1}{(2\pi)^{\frac{d}{2}}\lvert\Sigma\rvert^{\frac{1}{2}}} \mathrm{Exp}\left\{
-\frac{1}{2}(\boldsymbol{x}_i - \boldsymbol{\mu})^T \Sigma^{-1}(\boldsymbol{x}_i - \boldsymbol{\mu})
\right\}
\right\} \\
&=  \frac{1}{(2\pi)^{\frac{nd}{2}}\lvert\Sigma\rvert^{\frac{1}{2}}} \mathrm{Exp}\left\{
-\frac{1}{2}\underbracket{\sum^n(\boldsymbol{x}_i - \boldsymbol{\mu})^T \Sigma^{-1}(\boldsymbol{x}_i - \boldsymbol{\mu})}_{\dagger}
\right\}
\end{aligned}
$$

Defina $\bar{\boldsymbol{x}} = \frac{1}{n} \sum^n \boldsymbol{x}_i$, então
$$
\begin{aligned}
\dagger &= \sum^n (\boldsymbol{x}_i - \bar{\boldsymbol{x}} + \bar{\boldsymbol{x}} - \boldsymbol{\mu})^T
\Sigma^{-1} (\boldsymbol{x}_i - \bar{\boldsymbol{x}} + \bar{\boldsymbol{x}} - \boldsymbol{\mu}) \\
&= \sum^n ((\boldsymbol{x}_i - \bar{\boldsymbol{x}}) - (\boldsymbol{\mu} - \bar{\boldsymbol{x}}))^T
\Sigma^{-1} ((\boldsymbol{x}_i - \bar{\boldsymbol{x}}) - (\boldsymbol{\mu} - \bar{\boldsymbol{x}})) \\
&= \sum^n \left\{
(\boldsymbol{x} - \bar{\boldsymbol{x}})^T \Sigma^{-1} (\boldsymbol{x}_i - \bar{\boldsymbol{x}})
-\underbracket{(\boldsymbol{\mu} - \bar{\boldsymbol{x}})^T \Sigma^{-1} (\boldsymbol{x}_i - \bar{\boldsymbol{x}})}_{A} \right .\\
&\left.-\underbracket{(\boldsymbol{x}_i - \bar{\boldsymbol{x}})^T \Sigma^{-1} (\boldsymbol{\mu} - \bar{\boldsymbol{x}})}_{B}
+(\boldsymbol{\mu} - \bar{\boldsymbol{x}})^T \Sigma^{-1} (\boldsymbol{\mu} - \bar{\boldsymbol{x}})
\right\}
\end{aligned}
$$

Como $n\bar{\boldsymbol{x}} = \sum^n \boldsymbol{x}_i$, temos que

$$
\begin{cases} 
A = \sum^n (\boldsymbol{\mu} - \bar{\boldsymbol{x}})^T \Sigma^{-1} (\boldsymbol{x}_i - \bar{\boldsymbol{x}}) =
(\boldsymbol{\mu} - \bar{\boldsymbol{x}})^T \Sigma^{-1} (\sum^n \boldsymbol{x}_i - \bar{\boldsymbol{x}}) = 0 \\
B = \sum^n (\boldsymbol{x}_i - \bar{\boldsymbol{x}})^T \Sigma^{-1} (\boldsymbol{\mu} - \bar{\boldsymbol{x}}) =
(\sum^n \boldsymbol{x}_i - \bar{\boldsymbol{x}})^T \Sigma^{-1} (\boldsymbol{\mu} - \bar{\boldsymbol{x}}) = 0
\end{cases}
$$

Portanto,
$$
\begin{aligned}
\dagger &= 
\sum^n (\boldsymbol{x} - \bar{\boldsymbol{x}})^T \Sigma^{-1} (\boldsymbol{x}_i - \bar{\boldsymbol{x}})
+n(\boldsymbol{\mu} - \bar{\boldsymbol{x}})^T \Sigma^{-1} (\boldsymbol{\mu} - \bar{\boldsymbol{x}}) \\
&=  \sum^n \mathrm{tr}\{(\boldsymbol{x} - \bar{\boldsymbol{x}})^T \Sigma^{-1} (\boldsymbol{x}_i - \bar{\boldsymbol{x}})\}
+n(\boldsymbol{\mu} - \bar{\boldsymbol{x}})^T \Sigma^{-1} (\boldsymbol{\mu} - \bar{\boldsymbol{x}}) \\
&\stackrel{\text{Prop. tr}}{=}  \sum^n \mathrm{tr}\{\Sigma^{-1}(\boldsymbol{x} - \bar{\boldsymbol{x}})(\boldsymbol{x}_i - \bar{\boldsymbol{x}})^T\}
+n(\boldsymbol{\mu} - \bar{\boldsymbol{x}})^T \Sigma^{-1} (\boldsymbol{\mu} - \bar{\boldsymbol{x}}) \\
\end{aligned}
$$

Note que $\sum^n \mathrm{tr}\{A \boldsymbol{y}_i\} = \mathrm{tr}\{A \sum \boldsymbol{y}_i\}$, usando essa propriedade,
$$
\begin{aligned}
\dagger &= n \mathrm{tr} \{\Sigma^{-1}S^2(\boldsymbol{x}_n^*)\}
+n(\boldsymbol{\mu} - \bar{\boldsymbol{x}})^T \Sigma^{-1} (\boldsymbol{\mu} - \bar{\boldsymbol{x}}) \\
\end{aligned}
$$
em que $S^2(\boldsymbol{x}_n^*) = \frac{1}{n} \sum^n (\boldsymbol{x}_i - \bar{\boldsymbol{x}})(\boldsymbol{x}_i - \bar{\boldsymbol{x}})^T$.

Dessa forma, podemos escrever a função de verossimilhança da seguinte forma:

$$
\mathcal{L}_{\boldsymbol{x}^*}(\theta) =
\frac{1}{(2\pi)^{\frac{nd}{2}}\lvert\Sigma\rvert^{\frac{1}{2}}} \mathrm{Exp}\left\{
-\frac{n}{2}\left( \mathrm{tr}\{\Sigma^{-1}S^2(\boldsymbol{x}_n^*)\}
+(\boldsymbol{\mu} - \bar{\boldsymbol{x}})^T \Sigma^{-1}(\boldsymbol{\mu} - \bar{\boldsymbol{x}})\right)
\right\}
$$

Pelo [critério da fatoração de Neyman-Fisher](estatisticas-suficientes.qmd#sec-crit-fat), temos que $(\bar{\boldsymbol{X}}, S^2(\boldsymbol{X}_n^*))$
é uma [estatística suficiente](estatisticas-suficientes.qmd) para o modelo normal multivariado. Como $S^2(\boldsymbol{X}^*_n)$
é uma matriz simétrica, podemos considerar apenas $\mathrm{vech}(S^2(\boldsymbol{X}_n^*))$

Para maximizar a função de verossimilhança em relação a $\boldsymbol{\mu}$, note que basta minimizar
$$
(\boldsymbol{\mu} - \bar{\boldsymbol{x}})^T \Sigma^{-1}(\boldsymbol{\mu} - \bar{\boldsymbol{x}})
$$
note que, como $\Sigma$ é positiva definida (p.d.), $\Sigma^{-1}$ também é p.d. Portanto, pela definição,
$\boldsymbol{y}^T \Sigma^{-1} \boldsymbol{y} > 0, \forall \boldsymbol{y} \in \mathbb{R}^d \setminus \{0\}$ e
$(\boldsymbol{\mu} - \bar{\boldsymbol{x}})^T \Sigma^{-1}(\boldsymbol{\mu} - \bar{\boldsymbol{x}})$ é mínimo quando $\boldsymbol{\mu} = \bar{\boldsymbol{x}}$.

Para encontrar $\Sigma$ que maximiza a função de verossimilhança, substituimos $\boldsymbol{\mu}$ por $\bar{\boldsymbol{x}}$
e tentamos encontrar seu valor. Portanto, devemos maximizar
$$
\frac{1}{(2\pi)^{\frac{nd}{2}}\lvert\Sigma\rvert^{\frac{1}{2}}} \mathrm{Exp}\left\{
-\frac{n}{2} \mathrm{tr}\{\Sigma^{-1}S^2(\boldsymbol{x}_n^*)\}
\right\}
$$
em relação a $\Sigma$. Aplicando $\ln$, temos
$$
-\frac{nd}{2} \ln(2\pi) - \frac{n}{2} \ln\lvert\Sigma\rvert - \frac{n}{2} \mathrm{tr}\{\Sigma^{-1}S^2(\boldsymbol{x}_n^*)\} =
c - \frac{n}{2}\left( \ln\lvert\Sigma\rvert +  \mathrm{tr}\{\Sigma^{-1}S^2(\boldsymbol{x}_n^*)\}\right)
$$

Note que, para qualquer $\lambda > 0$, temos
$$
\lambda - \ln\lambda \geq 1.
$$

Sejam $\lambda_1, \dots, \lambda_d$ autovalores de uma matriz positiva definida $M$.
$$
\begin{aligned}
\lambda_i &- \ln \lambda_i \geq 1 \forall i = 1, \dots, d \\
\Rightarrow &\sum^d \lambda_i - \sum^d \ln_i \geq d \\
\iff& \sum^d \lambda_i - \ln(\prod^d \lambda_i) \geq d \\
\Rightarrow &\mathrm{tr}\{M\} - \ln\lvert M\rvert \geq d
\end{aligned}
$$
