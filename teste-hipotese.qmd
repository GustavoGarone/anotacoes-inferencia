```{julia}
#| output: false
using Pkg; Pkg.add(["Distributions", "Plots", "StatsPlots", "LaTeXStrings"])
```
# Teste de Hipótese simples

Um dos principais objetivos da estática é testar hipóteses. Veja algumas dessas hipóteses potenciais:

1. A moeda é honesta?

2. O medicamento proposto é melhor que o vendido no mercado?

3. O número médio de acidentes aumentou em relação ao ano passado?

4. A altura interfere na performance num determinado esporte?

5. Um suspeito é culpado?

6. O mercado financeiro está em equilíbrio?

7. Dona Maria terá dinheiro para comprar o pão do próximo mês?

## Etapas de um teste de hipótese
1. Formular as hipóteses de interesse;

	1. Na estatística clássica, pela abordagem Fisheriana (uma hipótese) ou Neyman-Pearson (mais de uma hipótese).

2. Observar dados experimentais do estudo relacionado ao problema;

3. Elaborar uma conclusão utilizando um procedimento estatístico.


## Exemplo da hipótese 5.
Considere uma pessoa que está sendo acusada de ter cometido um crime.

As duas hipóteses envolvidas aqui são (abordagem de Neyman-Pearson):

1. “O suspeito não é culpado” -> $h_{0}$ Hipótese Nula ou de não-efeito;

2. “O suspeito é culpado” -> $h_{1}$ Hipótese alternativa ou hipótese que contém o efeito.

- Após coletar as evidências, dizemos que, se houver evidências de que o suspeito cometeu o crime, a pessoa é culpada. 
- Se não, concluímos que não é culpado.

Contudo, devemos nos atentar aos erros de decisão

### Erros de decisão
$$
\begin{array}{c|cc}
& H_{0} & H_{1} \\
\hline
\text{Decisão}  & \text{Não cometeu o crime}  & \text{Cometeu o crime}\\
\hline
\text{Inocente} & \text{Acerto} & \text{Erro Tipo II}\\
\text{Culpado} & \text{Erro Tipo I} & \text{Acerto}\\
\hline
\end{array}
$$
- Erro Tipo I: Decidir que o acusado é culpado quando na verdade é inocente (Rejeitar $H_{0}$).
- Erro Tipo II: Decidir que o acusado é inocente quando na verdade é culpado (Rejeitar $H_{1}$).

## Exemplo da hipótese 1.
Estamos interessados em verificar se uma moeda é honesta. Executaremos $n$ experimentos de Bernoulli e verificaremos se
a face voltada para cima após o lançamento é cara. Dessa forma, sendo $X$ o resultado dum lançamento, teremos a
[a.a](populacao-e-amostra.qmd#sec-aa)
$(\pmb{X}_{n})$ de $X\sim \mathrm{Ber}(\theta), \theta \in \Theta = [0,1]$. Suspeitamos que a moeda é honesta ou que
$\theta = 0.9$.

Nossas hipóteses são

1. $H_{0}$ -> $\theta = 0.5$

2. $H_{1}$ -> $\theta = 0.9$

### Erros Tipo I e II

Note que $\bar{X}$ é um [estimador](estimadores.qmd) para $\theta$ e $\bar{x}$ é uma [estimativa](estimadores.qmd#estimativa).
- Se $\bar{x}>0.7$, rejeitaremos a hipótese nula $h_{0}$, a moeda não seria honesta e haveria um viés agindo sobre seus lançamentos.
- Se $\bar{x}\leq 0.7$, concluiremos que a moeda é honesta.
$$
\begin{array}{c|cc}
& H_{0} & H_{1} \\
\hline
\text{Decisão}  & \text{Honesta} & \text{Viesada}\\
\hline
\bar{x} < 0.7 & \text{Acerto} & \text{Erro Tipo II}\\
\bar{x} \geq 0.7& \text{Erro Tipo I} & \text{Acerto}\\
\hline
\end{array}
$$

- Erro Tipo I: Rejeitar que a moeda é honesta (rejeitar $h_{0}$) quando na verdade é.

- Erro Tipo II: Rejeitar que a moeda é enviesada (rejeitar $h_{1}$) quando na verdade é.

Calculando a probabilidade dos erros
$$
\begin{aligned}
P(\text{Erro Tipo I}) &= P(\text{Probabilidade de rejeitar $h_{0}$}|\text{$h_{0}$ é verdadeiro}) \\
\text{Incorreto na Estatística Clássica} &= P(\bar{X} > 0.7 | \theta = 0.5) \\
\text{Correto na Estatística Clássica} & = P_{0.5}(\bar{X} > 0.7)
\end{aligned}
$$

Na segunda notação, correta na estatística frequentista, $P$ está sob a hipótese nula $h_{0}$ verdadeira.
Dessa forma
$$
P(\text{Erro Tipo II}) = P_{0.9}(\bar{X}\leq 0.7)
$$

Calcule as probabilidades dos erros considerando $n=10$ e a aproximação pela distribuição normal.

#### Calculando Exato e pela aproximação do [Teorema do Limite Central](tlc.qmd)
Sabemos que $\sum ^{n}_{i=1}X_{i}\sim \mathrm{Bin}(n, \theta)$, logo
$$
\begin{aligned}
\alpha &= P(\text{Erro Tipo I}) \stackrel{\text{Sob $h_{0}$}}{=} P_{0.5}\left( \frac{1}{n} \sum^n_{i=1}X_{i}> 0.7\right)
= P_{0.5}\left( \sum^{10}_{i=1}X_{i} > 7 \right) \\
&= P_{0.5}\left( \sum^{10}_{i=1} X_{i} \geq 8 \right) \\
&= \binom{10}{8} 0.5^{8} \cdot 0.5^{2} + \binom{10}{9} 0.5^{9} \cdot 0.5^{1} + \binom{10}{10} 0.5^{10} \approx 0.05469 \\
\alpha & = P_{0.5}\left( \sqrt{ \frac{n}{0.25}}(\bar{X}-0.5) >\sqrt{ \frac{n}{0.25}}(0.7-0.5) \right) \\
& \approx P(N(0,1) > 1.26) \approx 0.103
\end{aligned}
$$

$$
\begin{aligned}
\beta &= P(\text{Erro Tipo II}) \stackrel{\text{Sob $h_{1}$}}{=} P_{0.9}\left( \frac{1}{n} \sum^n_{i=1}X_{i}\leq 0.7\right)
= P_{0.9}\left( \sum^{10}_{i=1}X_{i} \leq 7 \right) \\
&= 1-P_{0.9}\left( \sum^{10}_{i=1} X_{i} \geq 8 \right) \\
&= 1-\left(\binom{10}{8} 0.9^{8} \cdot 0.9^{2} + \binom{10}{9} 0.9^{9} \cdot 0.9^{1} + \binom{10}{10} 0.9^{10}\right)
\approx 0.0702 \\
\alpha & = P_{0.9}\left( \sqrt{ \frac{n}{0.09}}(\bar{X}-0.9) \leq \sqrt{ \frac{n}{0.09}}(0.7-0.9) \right) \\
& \approx P(N(0,1) \leq -2.1) \approx 0.018
\end{aligned}
$$

## Poder do teste
Chamamos de poder do teste a probabilidade de rejeitar $h_{0}$ quando este é falso.

No exemplo anterior,
$$
\pi = P_{0.9}(\bar{X} > 0.7) = 1-P_{0.9}(\bar{X}\leq 0.7) = 1 - \beta = 92.92\%
$$

Considere nesse exemplo uma a.a do lançamento de quatro moedas: $(\pmb{x}_{10}) = (1, 0, 1, 0,0,1,1,0,0,0)$.
Como $\bar{x}=0.4 \leq 0.7$, não rejeitamos a hipótese nula $h_{0}$.

Em uma outra amostra, $(\pmb{x}_{10}) = (0,0,1,1,1,1,1,1,1,1)$. Como $\bar{x}=0.8 > 0.7$, rejeitamos a hipótese nula $h_{0}$.

## Um exemplo mais amplo (Diferença)
Seja $(X_{n})$ uma a.a. de $X\sim\mathrm{Ber}(\theta)$, em que $\theta \in (0,1)$. Considere as hipóteses:
$$
\begin{cases}
H_{0}: \theta =0.5 \\
H_{1}: \theta \neq 0.5
\end{cases}
$$

Decisões elaboradas:

1. Se $\bar{x}<0.3$ ou $\bar{x} > 0.7$, rejeitamos $H_{0}$

2. Caso contrário, não rejeitamos $H_0$

Relembrando: $\alpha$ = Probabilidade do Erro Tipo I (Rejeitar um $H_0$ verdadeiro).
$\beta$ Probabilidade do Erro Tipo II (Rejeitar um $H1$ verdadeiro). $\pi =$ Poder do Teste.
Lembre-se que $\sum^n_{i=1}X_{i}\sim \mathrm{Bin}(n,\theta)$. Tome $n =10$.

$$
\begin{aligned}
\alpha &= P_{\theta=0.5}(\bar{X} < 0.3 \text{ ou } \bar{X} > 0.7) = P_{0.5}(\bar{X}<0.3) + P_{0.5}(\bar{X}>0.7) \\
&=P(\mathrm{Bin}(n,0.5) < 3) + P(\mathrm{Bin}(10,0.5)>7) \\ 
&=P(\mathrm{Bin}(n,0.5) \leq 2) + P(\mathrm{Bin}(10,0.5)\geq 8) \\
&= 0.055 +0.055 = 0.11 \\
\end{aligned}
$$

Note que para o Erro Tipo II, não existe uma única probabilidade para o erro sob $H_1$.
Optaremos por tentar calcular seu máximo. $\beta_{\max}$
$$
\begin{aligned}
\beta &= P_{\theta} (0.3\leq \bar{X} \leq 0.7) \theta \in \Theta \setminus \{ 0.5 \} \\
\beta_{\max} &= \sup_{\theta \in \Theta \setminus \{ 0.5 \}} \beta(\theta)
\end{aligned}
$$

Para $n=10$,
$$
\beta(\theta) = P_{\theta}\left( 3\leq \sum^{n=10}_{i=1} X_{i} \leq 7 \right) = P\left( 3\leq \mathrm{Bin}(10,\theta)
\leq 7 \right), \theta \in \Theta \setminus \{ 0.5 \}
$$

Podemos encontrar o valor que maximiza $\beta(\theta)$, $\theta =0.5$ derivando.

## Hipóteses como subconjuntos do espaço paramétrico
Seja $(X_{n})$ uma a.a. de $X\sim\mathrm{Ber}(\theta)$, em que $\theta \in (0,1)$. Considere as hipóteses:
$$
\begin{cases}
H_{0}: \theta \in \Theta_{0} \\
H_{1}: \theta \in \Theta_{1}
\end{cases}
$$

em que $\Theta_{0} \cup \Theta_{1} = \Theta, \Theta_{0},\Theta_{1} \neq \emptyset, \Theta_{0}\cap\Theta_{1}= \emptyset$.

Exemplos de decisões elaboráveis:
$$
\begin{aligned}
&\begin{cases}
\Theta_{0}=\{ 0.5 \} \\
\Theta_{1} = \left( 0, \frac{1}{2} \right) \cup \left( \frac{1}{2}, 1 \right)
\end{cases} \Rightarrow
\begin{cases}
H_{0}: \theta=0.5 \\
H_{1}: \theta \neq 0.5 \\
\end{cases} \text{ Hipótese alternativa bilateral} \\
&\begin{cases}
\Theta_{0}= \left(  0, \frac{1}{2} \right] \\
\Theta_{1} = \left( \frac{1}{2}, 1 \right)
\end{cases} \Rightarrow
\begin{cases}
H_{0}: \theta \leq 0.5\\
H_{1}: \theta > 0.5 \\
\end{cases} \text{ Hipótese alternativa unilateral} \\
&\begin{cases}
\Theta_{0}= \left[ \frac{1}{2}, 1\right)   \\
\Theta_{1} = \left( 0, \frac{1}{2} \right)
\end{cases} \Rightarrow
\begin{cases}
H_{0}: \theta \geq 0.5 \\
H_{1}: \theta < 0.5 \\
\end{cases} \text{ Hipótese alternativa uniteral}
\end{aligned}
$$

## Função poder
No caso geral, calculamos a função poder definida por
$$
\pi (\theta) = P_{\theta}(\{ \text{Rejeitar } H_{0} \}), \theta \in \Theta
$$

em que “Rejeitar $H_0$” é o procedimento de decisão para rejeitar $H_0$.

A partir da função poder conseguimos calcular as probabilidades máximas de cometer os erros tipo I e II.

*Probabilidade Máxima do Erro Tipo I*:
$$
\alpha_{\max} = \sup_{\theta \in \Theta_{0}}(\pi(\theta))
$$

*Probabilidade Máxima do Erro Tipo II:*
$$
\beta_{\max} = \sup_{\theta \in \Theta_{1}}[1-\pi(\theta)]
$$

## Um exemplo do cálculo de erros com hipótese unilateral

Seja $(X_{n})$ uma a.a. de $X\sim\mathrm{Ber}(\theta)$, em que $\theta \in (0,1) = \Theta$. Considere as hipóteses:
$$
\begin{cases}
H_{0}: \theta \geq 0.6\\
H_{1}: \theta < 0.6
\end{cases}
$$

Precisamos de decisões que fazem sentido. Uma delas seria

1. Se $\bar{x}<0.4$, rejeitamos $H_{0}$
2. Se $\bar{x} \geq 0.4$, não rejeitamos $H_{0}$

Vamos calcular as probabilidades máximas dos erros I e II.

Primeiro, encontramos a função poder
$$
\pi(\theta) = P_{\theta}(\bar{X}<0.4)
$$

Como $\sum^n_{i=1}X_{i} \sim \mathrm{Bin}(n,\theta)$, temos que
$$
\pi(\theta) = P_{\theta} \left( \sum^n_{i=1}X_{i} < 0.4 \cdot n \right) = P(\mathrm{Bin}(n,\theta) < 0.4 \cdot n)
$$

Relembrando:
$$
\begin{aligned}
\alpha_{\max} &= \sup_{\theta \in [0.6,1)} \pi(\theta) \\
&= \sup_{\theta \in [0.6,1)} P(\mathrm{Bin}(n, \theta) < 0.4 \cdot n) \\
\beta_{\max} &= \sup_{\theta \in (0,0.6)} (1-\pi(\theta)) \\
&= \sup_{\theta \in (0,0.6)}P(\mathrm{Bin}(n,\theta)\geq 0.4 \cdot n)
\end{aligned}
$$

Para $n = 2$,
$$
\begin{aligned}
\alpha_{\max} &= \sup_{\theta \in [0.6,1)} \pi(\theta) \\
&= \sup_{\theta \in [0.6,1)} P(\mathrm{Bin}(2, \theta) < 0.4 \cdot 2) \\
&= \sup_{\theta \in [0.6,1)} P(\mathrm{Bin(2,\theta)}  = 0)\\
\beta_{\max} &= \sup_{\theta \in (0,0.6)} (1-\pi(\theta)) \\
&= \sup_{\theta \in (0,0.6)}P(\mathrm{Bin}(2,\theta)\geq 0.8 \cdot n) \\
&= \sup_{\theta \in (0,0.6)}P(\mathrm{Bin}(2,\theta) \geq 1) \\
&= \sup_{\theta \in (0,0.6)}[1-P(\mathrm{Bin}(2,\theta) = 0)]
\end{aligned} ~~~ \Rightarrow ~~~~~
\begin{aligned}
\alpha_{\max} &= \sup_{\theta \in [0.6,1]}\left[\binom{2}{0} \theta^0 (1-\theta)^2\right] \\
&=\sup_{\theta \in [0.6,1]} (1-\theta)^2 \\
\beta_{\max} &= \sup_{\theta \in (0,0.6)} (1-(1-\theta)^2)
\end{aligned}
$$

Podemos analisar os gráficos:
```{julia}
#| echo: false
using Plots, Distributions, LaTeXStrings
function poder(theta)
  dist = Binomial(2, theta)
  return cdf(dist, 2 * 0.4)
end
plot(poder, xlims=(0.6, 1), legend=false, ylabel=L"\pi (\theta)", xlabel=L"\theta",
     title=L"\alpha = \pi (\theta) = (1-\theta)^{2}")
```

Como é uma função decrescente, seu supremo está no ponto $0.6$

```{julia}
#| echo: false
using Plots, Distributions, LaTeXStrings
function poder(theta)
  dist = Binomial(2, theta)
  return ccdf(dist, 2 * 0.4)
end
plot(poder, xlims=(0, 0.601), legend=false, ylabel=L"1 - \pi (\theta)", xlabel=L"\theta",
     title=L"\beta  = 1 - \pi (\theta) = (1-\theta)^{2}") #.601 para exibir o último xtick
```

Como é uma função crescente, seu supremo está também no $0.6$
Portanto,
$$
\begin{aligned}
\alpha_{\max}&=(1-0.6)^2 = 0.16\\
\beta_{\max} &= (1-(1-0.6)^2) = 0.84
\end{aligned}
$$

## Teste sob Normalidade
Seja $(x_{n})$ amostra aleatória de $X\sim N(\mu,\sigma^2)$ em que $\sigma^2$ é conhecido.
Considere as hipóteses
$$
\begin{cases}
H_{0}: \mu = \mu_{0}\\
H_{1}: \mu \neq \mu_{0}
\end{cases}
$$
com $\mu_{0} \in \mathbb{R}$ e fixado.

Calcule as probabilidades (máximas) dos erros tipo I e II, para as seguintes decisões

1. Se $\bar{x} < \mu_{0} - 1.96\sqrt{\frac{\sigma^2}{n}}$ ou $\bar{x} > \mu+1.96 \sqrt{ \frac{\sigma^2}{n} }$, então
*rejeitamos* $H_{0}$
2. Caso contrário, não rejeitamos $H_0$

Temos a função poder
$$
\pi(\theta) = P_{\theta}(\mathrm{Rejeitar} H_{0}) = P_{\theta}\left(\bar{X}<\mu_{0} - 1.96 \sqrt{\frac{\sigma^2}{n}}\right)
+P_{\theta}\left( \bar{X} > \mu_{0} + 1.96 \sqrt{  \frac{\sigma^2}{n} } \right)
$$

em que $theta = \mu \in \mathbb{R}$. Portanto,
$$
\alpha_{\max} = \sup_{\theta \in \Theta_{0}} \pi(\theta)
$$

Como $H_{0}=\mu=\mu_{0} \Leftrightarrow H_{0}: \theta \in \Theta$, em que $\Theta_{0}=\{ \mu_{0} \}$, logo,
$\sup_{\theta \in \Theta_{0}} = \mu_{0}$
Portanto, temos que
$$
\alpha_{\max} = \pi(\mu_{0}) = P_{\mu_{0}}\left( \bar{X}<\mu_{0}-1.96\sqrt{ \frac{\sigma^2}{n} } \right) +
P_{\mu_{0}}\left( \bar{X}>\mu_{0} + 1.96 \sqrt{  \frac{\sigma^2}{n} } \right)
$$

Sabemos que, pelo enunciado $\bar{X} \sim N\left( \mu, \frac{\sigma^2}{n} \right) \forall \mu \in \mathbb{R}$ 
sob $H_{0}$, ou seja, quando $\mu= \mu_{0}$ temos que $\bar{X}\sim N\left( \mu_{0}, \frac{\sigma^2}{n} \right)$.
Note que $P_{\mu_{0}}\left( \bar{X}<\mu_{0}-1.96 \sqrt{ \frac{\sigma^2}{n} } \right) =
P_{\mu_{0}}\left(\frac{\bar{X}-\mu_{0}}{\sqrt{ \frac{\sigma^2}{n} }} \right) = 2.5\%$
Pela simetria da distribuição normal, $P_{\mu_{0}}\left( \bar{X}>\mu_{0}+1.96 \sqrt{ \frac{\sigma^2}{n} } \right) = 2.5\%$
Portanto a probabilidade máxima do erro tipo 1 é
$$
\alpha_{\max} = 2.5\% + 2.5\% = 5.0\%
$$

Como $H_{1}: \mu \neq \mu_{0} \Leftrightarrow H_{1}: \theta \in \Theta_{1}$, em que
$\Theta_{1} = \mathbb{R} \setminus \{\mu_{0}\}$, temos que

$$
\begin{aligned}
\beta_{\max} &= \sup_{\theta \in \Theta_{1}}[1-\pi(\theta)] \\
\pi(\theta) &= P_{\theta}\left( \bar{X} < \mu_{0} - 1.96 \sqrt{\frac{\sigma^2}{n}} \right) +
P_{\theta}\left(\bar{X}>\mu_{0}+1.96\sqrt{\frac{\sigma^2}{n}}\right)
\end{aligned}
$$

Sabemos que $\bar{X} \sim \mathrm{N}\left( \mu, \frac{\sigma^2}{n} \right)$ para todo $\mu \in \mathbb{R}$. Assim,
$$
\begin{aligned}
\pi(\theta) &= P_{\theta}\left( \bar{X}<\mu_{0}-1.96 \sqrt{  \frac{\sigma^2}{n} } \right) +
P_{\theta}\left( \bar{X} > \mu_{0} + 1.96 \sqrt{  \frac{\sigma^2}{n} } \right) \\
&= P_{\theta}\left( \frac{\bar{X}-\theta}{\sqrt{ \frac{\sigma^2}{n} }} <
\frac{\mu_{0}-\theta-1.96 \sqrt{  \frac{\sigma^2}{n} } }{\sqrt{  \frac{\sigma^2}{n} }}\right) +
P_{\theta}\left( \frac{\bar{X}-\theta}{\sqrt{ \frac{\sigma^2}{n} }} > 
\frac{\mu_{0}-\theta+1.96 \sqrt{\frac{\sigma^2}{n}}}{\sqrt{\frac{\sigma^2}{n}}}\right)
\end{aligned}
$$

Dessa forma,
$$
\beta_{max} = \sup_{\theta \in \Theta_{1}}[1-\pi(\theta)] = 1 - \inf_{\theta \in \Theta_{1}} \pi(\theta)
$$

Ou seja, o supremo dessa expressão é dado por 1 - o ínfimo da função poder, o que significa que queremos encontrar o
valor de $\theta$ para o qual $P_{\theta}\left( \frac{\bar{X}-\theta}{\sqrt{ \frac{\sigma^2}{n} }} < 
\frac{\mu_{0}-\theta-1.96 \sqrt{\frac{\sigma^2}{n} } }{\sqrt{  \frac{\sigma^2}{n} }}\right) +
P_{\theta}\left(\frac{\bar{X}-\theta}{\sqrt{ \frac{\sigma^2}{n} }} >
\frac{\mu_{0}-\theta+1.96 \sqrt{\frac{\sigma^2}{n}}}{\sqrt{\frac{\sigma^2}{n}}}\right)$ é o menor possível.

Vamos usar $\mu_0 = 7$ e $\sigma^2 = 5$ para visualizarmos o comportamento de $\alpha$ e $\beta$

```{julia}
#| echo : false
using Distributions, Plots, StatsPlots, LaTeXStrings

mu0 = 7
n = 10
sigma2 = 5
function poder(theta)
    dist = Normal(theta, sqrt(sigma2 / n))
    return cdf(dist, (mu0 - 1.96 * sqrt(sigma2 / n))) + (1 - cdf(dist, (mu0 + 1.96 * sqrt(sigma2 / n))))
end

a = plot(poder, xlims=(mu0 - 6 * sqrt(sigma2 / 7), mu0 + 6 * sqrt(sigma2 / 7)), ylims=(0, 1.001), label="", title=L"\alpha = \pi (\theta)",
         xlabel =L"\mu", ylabel = L"\alpha")
vline!([7], label="7")
beta(theta) = 1 - poder(theta)
beta(mu0)
b = plot(beta, xlims=(mu0 - 6 * sqrt(sigma2 / 7), mu0 + 6 * sqrt(sigma2 / 7)), ylims=(-0.01, 1.001), label="", title=L"\beta",
         xlabel=L"\mu", ylabel = L"\beta")
vline!([7], label="7")
display(a)
display(b)
```

### Um outro exemplo

Seja $(X_{n})$ amostra aleatória de $X\sim \mathrm{N}(\mu,\sigma^2)$ com $\sigma^2 = 5, n = 10$ Com as hipóteses
$$
\begin{cases}
H_{0} : \mu = 10 \\
H_{1}: \mu \neq 10
\end{cases}
$$
Com as decisões
1. Rejeitamos $H_{0}$ se $\bar{x} > 10 + 1.96 \sqrt{  \frac{5}{10} }$ ou $\bar{x} < 10 - 1.96 \sqrt{  \frac{5}{10} }$
Foram observados os seguintes valores
$$
\begin{array}{c}
7.1 & 8.9 & 12 & 13 & 11.7 \\
6.1 & 2.5 & 3.1 & 5.2 & 7
\end{array}
$$
Temos então que $\bar{x} = 7.66$ que, como é abaixo de $8.6$, rejeitamos a hipótese nula de que $\mu = 10$

# Procedimento Geral para Testar Hipóteses (Método de Neyman-Pearson)

1. Definimos o Modelo Estatístico:
	1. “Seja $(X_{n})$ amostra aleatória de $x \sim f_{\theta}, \theta \in \Theta$”
2. Definir as hipóteses de interesse:
	1. $H_{0}: \theta \in \Theta_{0} \times H_{1}: \theta \in \Theta_{1}$ em que
  $\Theta_{0} \cap \Theta_{1} = \emptyset, \Theta_{0} \neq \emptyset, \Theta_{1}\neq \emptyset, \Theta_{0}\cup \Theta_{1}=\Theta$
3. A partir da amostra observada, criamos uma regra de decisão para verificar a plausibilidade de $H_{0}$.
4. Definimos os pontos de corte da regra de decisão de forma que a probabilidade máxima do Erro Tipo I não ultrapasse
  um limite prefixado $\alpha \in [0,1]$ (normalmente $5\%$ ou $1\%$). Qualquer valor $\geq \alpha$ é dito ser um
  *nível de significância*.
5. Concluímos o Teste de Hipótese.
	1. Se $H_{0}$ for rejeitado, dizemos que “Há evidências para rejeitar $H_{0}$ a $\alpha \cdot [\mathrm{Valor}]\%$ de
    significância estatística”.
	2. Se $H_{0}$ não for rejeitado, dizemos que “Não há evidências para rejeitar $H_{0}$ a $\alpha \cdot [\mathrm{Valor}]\%$
    de significância estatística”

::: {.callout-note title="Observação"}
Não rejeitar $H_{0}$ *não* indica evidência a favor de $H_{0}$, isto é, não sugere que $H_{0}$ seja
verdadeiro, apenas que aquela amostra não apresentou evidências contrárias.
:::

::: {.callout-note title="Observação"}
Quanto menor o valor de $\alpha$, mais forte será a significância estatística.
:::

## Exemplos

### Eexemplo um
Seja $(X_{n})$ a.a de $X\sim \mathrm{N}(\mu,\sigma^2)$ em que $\sigma^2$ é conhecido.
Considere ($\mu_{0}$ fixado)

$$
\begin{cases}
H_{0} : \mu = \mu_{0} \\
H_{1}: \mu \neq \mu_{0}
\end{cases}
$$

1. Construa uma decisão para rejeitar $H_0$ que produza no máximo $\alpha = 5\%$ (que tenha nível de significância de $5\%$)

	Como a hipótese alternativa é bilateral, $H_{1}:\mu\neq \mu_{0}$ e $\bar{x}$ é a EMV para o parâmetro $\mu$ - a
  esperança da distribuição Normal - definimos a regra:
	Se $\bar{x}< x_{a}$ ou $\bar{x}> x_{b}$, rejeitamos $H_{0}$. Caso contrário, não rejeitamos.
	$$
	\begin{aligned}
		\alpha_{\max} &= \sup_{\theta \in \Theta_{0}} P_{\theta}(\text{Rejeitar }H_{0}), \Theta_{0} = \{ \mu_{0} \} \\
		&= P_{\mu_{0}}(\bar{X}<x_{a}) + P_{\mu_{0}}(\bar{X}>x_{b}) \leq 5\%
	\end{aligned}
	$$

	Note que $\bar{X} \sim \mathrm{N}\left(\mu_{0}, \frac{\sigma^{2}}{n} \right)$, sob $H_{0}$

	Logo,
	$$
	\begin{aligned}
		\alpha_{\max} &= P\left( \mathrm{N}(0,1) < \frac{{x_{a}-\mu_{0}}}{\sqrt{ \frac{\sigma^2}{n} }} \right)+
    P\left( \mathrm{N}(0,1) > \frac{{x_{a}-\mu_{0}}}{\sqrt{ \frac{\sigma^2}{n} }} \right)
	\end{aligned}
	$$

```{julia}
#| echo : false
using Distributions, Plots, StatsPlots, LaTeXStrings

a = plot(Normal(), labels=false)
vline!([-1.96, 1.96], labels=false)
xticks!([-1.96, 1.96], [L"\frac{x_{a}-\mu_{0}}{\sqrt{\frac{\sigma^{2}}{n}}}", L"\frac{x_{b}-\mu_{0}}{\sqrt{\frac{\sigma^{2}}{n}}}"])

display(a)
```

Tomando $\frac{{x_{a}-\mu_{0}}}{\sqrt{\frac{\sigma^2}{n}}} = -1.96$ e $\frac{{x_{b}-\mu_{0}}}{\sqrt{ \frac{\sigma^2}{n}}}
= 1.96$ (tabela normal padrão simétrica), temos que $\alpha_{\max}=5\%$. Assim, resolvendo as equações,
$$
\begin{cases}
     x_{a} = \mu - 1.96 \sqrt{\frac{\sigma^2}{n}} \\
     x_{b} = \mu + 1.96 \sqrt{\frac{\sigma^2}{n}}
\end{cases}
$$

2. Considere $n=100$, $\mu_{0}=1$, $\sigma^2=0.1$ e $\bar{x}=0.99$. Conclua o teste considerando o mesmo nível de significância $\alpha=5\%$.

	 O pontos pontos de corte são $x_{a} = 0.93$ e $x_{b} = 1.069$. Como $0.93\leq 0.99 \leq 1.069$, concluímos que não há
   evidências para rejeitarmos $H_{0}$ a $5\%$ de significância.

3. Refaça considerando $15\%$ de significância estatística.

	Usando os mesmos argumentos do item 1, podemos encontrar novos valores para $x_{a}, x_{b}$ através da tabela da Normal-Padrão:
	$$
	\begin{cases}
       x_{a} = \mu - 1.44 \sqrt{\frac{\sigma^2}{n}} \\
       x_{b} = \mu + 1.44 \sqrt{\frac{\sigma^2}{n}}
	\end{cases}
	$$

	Substituindo esses valores para os fornecidos em 2, temos que $0.95 \leq 0.99 \leq 1.045$. Portanto, continuaríamos a dizer que não há evidências para rejeitarmos $H_{0}$ a $15\%$ de significância.

### 2
Seja $(X_{n})$ a.a de $X\sim \mathrm{N}(\mu,\sigma^2)$ em que $\sigma^2$ é conhecido.

Considere ($\mu_{0}$ fixado)

$$
\begin{cases}
H_{0} : \mu \geq \mu_{0} \\
H_{1}: \mu < \mu_{0}
\end{cases}
$$

1. Construa uma decisão para rejeitar $H_0$ que produza no máximo $\alpha = 5\%$ (que tenha nível de significância de $5\%$)

	Pelos parâmetros e hipóteses envolvidos, $(\mu, \text{unilateral})$, podemos considerar a seguinte decisão:

	Se $\bar{x} < x_{c}$, rejeitamos $H_{0}$. Caso contrário, não rejeitamos.

	$$
	\begin{aligned}
		\alpha_{\max} &= \sup_{\mu \geq \mu_{0}} P_{\theta}(\text{Rejeitar }H_{0}) \\
		&= \sup_{\mu\geq \mu_{0}}P_{\mu}(\bar{X}<x_{c}) \\
		\Rightarrow \alpha_{\max} &=\sup_{\mu\geq \mu_{0}}P_{\mu}\left( \mathrm{N}(0,1)< \frac{{x_{c}-\mu}}
    {\sqrt{\frac{\sigma^2}{n} }} \right)
  \end{aligned}
	$$

	Como essa função (acumulada) é decrescente em $\mu$, temos que
	$$
	\begin{aligned}
		\alpha_{\max} &= P_{\mu_{0}}(\text{Rejeitar }H_{0}) \\
		&= P_{\mu_{0}}(\bar{X}<x_{c}) \\
		\Rightarrow \alpha_{\max} &=P_{\mu_{0}}\left( \mathrm{N}(0,1)< \frac{{x_{c}-\mu_{0}}}
    {\sqrt{\frac{\sigma^2}{n} }} \right) \leq 5\%
  \end{aligned}
	$$

	Logo, para encontrarmos $x_{c}$ tal que $\frac{{x_{c}-\mu_{0}}}{\sqrt{ \frac{\sigma^2}{n} }} = -1.64$
  (da tabela da normal padrão) $\Rightarrow x_{c} = \mu_{0}-1.64 \sqrt{ \frac{\sigma^2}{n} }$
2. Considere $n=100, \mu_{0} = 1, \sigma^2 = 0.1, \bar{x}=0.99$. Conclua o teste anterior a $\alpha = 5\%$ de significância.

	O ponto de corte é $x_{c} = 0.9836$. Como $0.99 \geq 0.9836$, concluímos que não há evidências para rejeitar a
  hipótese nula a $5\%$ de significância. 

### 3
Seja $(X_{n})$ amostra aleatória de $X \sim \mathrm{N}(\mu,\sigma^2)$ em que
$\theta = (\mu, \sigma^2) = \mathrm{R} \times \mathrm{R}^+$, ou seja, ambos parâmetros são desconhecidos.

$$
\begin{cases}
H_{0} : \sigma^2 = \sigma^2_{0} \\
H_{1}: \sigma^2 \neq \sigma^2_{0}
\end{cases} \Rightarrow 
\begin{aligned}
& \text{Decisão com significância } \alpha \\
&\text{Rejeita $H_{0}$ se} \\
&\begin{cases}
s^2 < c_{1c} \\
s^2 > c_{2c} \\
\end{cases} \\
& \text{Em que $c_{1c},c_{2c}$ são tais que} \\
& \sup_{\theta\in\Theta_{0}}P_{\theta}(\text{Erro Tipo I}) = \alpha_{\max} = \alpha \\
&\text{E } s^2 = \frac{1}{n-1} \sum^n_{i=1}(x_{i}-\bar{x})^2
\end{aligned} \\ \\
$$

Sabemos que $\frac{\sum_{i=1}^n(X_{i}-\bar{X})^2}{\sigma^2}\sim \chi^2_{n-1},\forall \mu \in \mathbb{R}, \sigma^2 > 0$.
Em particular, sob $H_{0}$
$$
\begin{aligned}
&\frac{(n-1)s^2(\underset{\sim}{X_{n}})}{\sigma^2_{0}} \sim \chi^2_{n-1}
\\ \Rightarrow& \alpha_{\max} = \sup_{\theta \in \Theta_{0}}\left\{ P_{\theta}(s^2(\underset{\sim}{X_{n}}) < c_{1c}) +
P_{\theta}(s^2(\underset{\sim}{X_{n}})> c_{2c}) \right\} 
\end{aligned}
$$

Além disso, note que $\Theta_{0}=\{ (\mu,\sigma^2)\in \Theta:\sigma^2=\sigma^2_{0} \}$. Portanto, temos que
$$
\begin{aligned}
& \alpha_{\max} = \sup_{\theta \in \Theta_{0}} \underbracket{ \left\{  P\left( \chi^2_{n-1} < 
\frac{c_{1c}(n-1)}{\sigma^2_{0}}\right) + P\left( \chi^2_{n-1} > 
\frac{c_{2c}(n-1)}{\sigma^2_{0}} \right) \right\}}_{\text{Não depende de $\theta$}}\\
\Rightarrow & \alpha_{\max} =  P\left( \chi^2_{n-1} < \frac{c_{1c}(n-1)}{\sigma^2_{0}} \right) + P\left( \chi^2_{n-1} >
\frac{c_{2c}(n-1)}{\sigma^2_{0}} \right) 
\end{aligned}
$$

Fixando $\alpha_{\max}=\alpha$ (significância), encontramos pela tabela os valores de 
$q_{\frac{\alpha}{2},n-1}^{(1)}, q_{\frac{\alpha}{2},n-1}^{(2)}$ tais que dividam a distribuição $\chi^2_{n-1}$ criando
duas seções de $\frac{\alpha}{2}$ de área. Portanto,

$$
\begin{cases}
H_{0} : \sigma^2 = \sigma^2_{0} \\
H_{1}: \sigma^2 \neq \sigma^2_{0}
\end{cases} \Rightarrow 
\begin{aligned}
& \text{Decisão com significância } \alpha \\
&\text{Rejeita $H_{0}$ se} \\
&\begin{cases}
s^2 < q_{\frac{\alpha}{2},n-1}^{(1)} \cdot \frac{\sigma^2_{0}}{(n-1)} \\
s^2 > q_{\frac{\alpha}{2},n-1}^{(2)} \cdot \frac{\sigma^2_{0}}{(n-1)}\\
\end{cases}
\end{aligned}
$$

### Exemplo
Seja $(X_{n})$ amostra aleatória de $X\sim N(\mu,\sigma^2)$ em que $X$ é o peso do pacote de café. Colheu-se uma amostra
de $n=16$ pacotes e observou-se uma variância de $s^2 =169g^2$.
O processo de fabricação diz que a média dos pacotes é $500g$ e desvio-padrão 10 gramas ($\sigma^2_{0}=100g^2$).

Queremos verificar se há alguma evidência de que o processo não esteja sendo cumprido com $\alpha=5\%$ de significância
$$
\begin{cases}
H_{0} : \sigma^2 = 100 \\
H_{1}: \sigma^2 \neq 100
\end{cases} \Rightarrow 
\begin{aligned}
& \text{Decisão com significância } 5\% \\
&\text{Rejeita $H_{0}$ se} \\
&\begin{cases}
s^2 < q_{2.5\%,15}^{(1)} \cdot \frac{100}{15} \\
s^2 > q_{2.5\%,15}^{(2)} \cdot \frac{100}{15}\
\end{cases}
\end{aligned}
$$
Da tabela Qui-quadrado, temos $q_{2.5\%,15}^{(1)} = 6.26$ e $q_{2.5\%,15}^{(2)} =27.49$.

Como $41.73<100<183.26$, concluímos que não há evidências para rejeitar a hipótese nula a $5\%$ de significância

# Fórmulas

## Sob normalidade, variância conhecida

Seja $(X_{n})$ amostra aleatória de $X\sim \mathrm{N}(\mu,\sigma^2)$ em que $\sigma^2$ é conhecido.
$$
\begin{aligned}
&1.
\begin{cases}
H_{0} : \mu = \mu_{0} \\
H_{1}: \mu \neq \mu_{0}
\end{cases} \Rightarrow 
\begin{aligned}
& \text{Decisão com significância } \alpha \\
&\text{Rejeita $H_{0}$ se} \\
&\begin{cases}
\bar{x} < \mu_{0} - z_{\frac{\alpha}{2}} \sqrt{ \frac{\sigma^2}{n} } \\
\bar{x} > \mu_{0} + z_{\frac{\alpha}{2}} \sqrt{ \frac{\sigma^2}{n} }
\end{cases} \\
& \text{Em que $z_{\frac{\alpha}{2}}$ é tal que} \\
& P\left( \mathrm{N(0,1)} < z_{\frac{\alpha}{2}} \right) = \frac{\alpha}{2}
\end{aligned} \\ \\
& 2.
\begin{cases}
H_{0} : \mu \geq \mu_{0} \\
H_{1}: \mu < \mu_{0}
\end{cases} \Rightarrow 
\begin{aligned}
&\text{Rejeita $H_{0}$ se} \\
&\begin{cases}
\bar{x} < \mu_{0} - z_{\alpha} \sqrt{ \frac{\sigma^2}{n} } \\
\end{cases} \\
& \text{Em que $z_{\alpha}$ é tal que} \\
& P\left( \mathrm{N(0,1)} \leq z_{\alpha} \right) = \alpha
\end{aligned} \\ \\
& 3.
\begin{cases}
H_{0} : \mu \leq \mu_{0} \\
H_{1}: \mu > \mu_{0}
\end{cases} \Rightarrow 
\begin{aligned}
&\text{Rejeita $H_{0}$ se} \\
&\begin{cases}
\bar{x} > \mu_{0} + z_{\alpha} \sqrt{ \frac{\sigma^2}{n} } \\
\end{cases} \\
& \text{Em que $z_{\alpha}$ é tal que} \\
& P\left( \mathrm{N(0,1)} \geq z_{\alpha} \right) = \alpha
\end{aligned} \\
\end{aligned}
$$

## Sob normalidade, variância desconhecida {#sec-normalidade-vardesc}

Seja $(X_{n})$ amostra aleatória de $X \sim \mathrm{N}(\mu,\sigma^2)$ em que $\theta = (\mu, \sigma^2) = \mathrm{R}
\times \mathrm{R}^+$, ou seja, ambos parâmetros são desconhecidos.
$$
\begin{aligned}
&1.
\begin{cases}
H_{0} : \mu = \mu_{0}  \\
H_{1} : \mu \neq \mu_{0} \\
\Rightarrow \Theta_{0}=\{ (\mu,\sigma^2) \in \Theta : \mu = \mu_{0} \}\\
\Rightarrow \Theta_{1}=\{ (\mu,\sigma^2) \in \Theta : \mu \neq \mu_{0} \}\\ 
\end{cases} \Rightarrow 
\begin{aligned}
& \text{Decisão com significância } \alpha \\
&\text{Rejeita $H_{0}$ se} \\
&\begin{cases}
\frac{\bar{x}-\mu_{0}}{\sqrt{ \frac{s^2}{n} }} < - t_{\frac{\alpha}{2},n-1} \\
\frac{\bar{x}-\mu_{0}}{\sqrt{ \frac{s^2}{n} }} > t_{\frac{\alpha}{2},n-1} \\
\end{cases} \\
& \text{Em que $t_{\frac{\alpha}{2},n-1}$ é tal que} \\
& P\left( t_{n-1} < - t_{\frac{\alpha}{2},n-1} \right) = \frac{\alpha}{2}
\end{aligned} \\ \\
& 2.
\begin{cases}
H_{0} : \mu \geq \mu_{0} \\
H_{1}: \mu < \mu_{0}
\end{cases} \Rightarrow 
\begin{aligned}
&\text{Rejeita $H_{0}$ se} \\
&\begin{cases}
\frac{\bar{x}-\mu_{0}}{\sqrt{ \frac{s^2}{n} }} < - t_{\alpha,n-1} \\
\end{cases} \\
& \text{Em que $t_{\alpha,n-1}$ é tal que} \\
& P\left( t_{n-1}\leq -t_{\alpha,n-1} \right) = \alpha
\end{aligned} \\ \\
& 3.
\begin{cases}
H_{0} : \mu \leq \mu_{0} \\
H_{1}: \mu > \mu_{0}
\end{cases} \Rightarrow 
\begin{aligned}
&\text{Rejeita $H_{0}$ se} \\
&\begin{cases}
\frac{\bar{x}-\mu_{0}}{\sqrt{ \frac{s^2}{n} }} > - t_{\alpha,n-1} \\
\end{cases} \\
& \text{Em que $t_{\alpha,n-1}$ é tal que} \\
& P\left( t_{n-1} > t_{\alpha,n-1} \right) = \alpha
\end{aligned} \\
\end{aligned}
$$

Em que $s^2 = \frac{1}{n-1} \sum^n_{i=1}(x_{i}-\bar{x})^2$ é a variância amostral (não enviesada)

## Sob normalidade, para a variância.

Seja $(X_{n})$ amostra aleatória de $X \sim \mathrm{N}(\mu,\sigma^2)$ em que
$\theta = (\mu, \sigma^2) = \mathrm{R} \times \mathrm{R}^+$, ou seja, ambos parâmetros são desconhecidos.
$$
\begin{cases}
H_{0} : \sigma^2 = \sigma^2_{0} \\
H_{1}: \sigma^2 \neq \sigma^2_{0}
\end{cases} \Rightarrow 
\begin{aligned}
& \text{Decisão com significância } \alpha \\
&\text{Rejeita $H_{0}$ se} \\
&\begin{cases}
s^2 < q_{\frac{\alpha}{2},n-1}^{(1)} \cdot \frac{\sigma^2_{0}}{(n-1)} \\
s^2 > q_{\frac{\alpha}{2},n-1}^{(2)} \cdot \frac{\sigma^2_{0}}{(n-1)}\\
\end{cases}
\end{aligned}
$$

Em que $s^2 = \frac{1}{n-1} \sum^n_{i=1}(x_{i}-\bar{x})^2$ é a variância amostral (não enviesada) e
$q_{\frac{\alpha}{2},n-1}^{(1)}, q_{\frac{\alpha}{2},n-1}^{(2)}$ tais que dividam a distribuição $\chi^2_{n-1}$
criando duas seções de $\frac{\alpha}{2}$ de área.
