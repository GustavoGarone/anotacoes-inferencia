[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Anotações de Inferência Frequentista",
    "section": "",
    "text": "Prefácio\nEste é um livrete feito na plataforma Quarto para minhas anotações das disciplinas de inferência frequentista ou clássica MAE0225 e MAE0301, ambas ministradas pelo Professor Alexandre Galvão Patriota e de sua autoria.\nDúvidas, sugestões, críticas ou erros por favor contactar em gustavo.garone arroba usp.br",
    "crumbs": [
      "Prefácio"
    ]
  },
  {
    "objectID": "index.html#atribuição-e-licença",
    "href": "index.html#atribuição-e-licença",
    "title": "Anotações de Inferência Frequentista",
    "section": "Atribuição e Licença",
    "text": "Atribuição e Licença\nEste livrete segue a licença pública GPLv3. Reprodução do material deve ser feita respeitando essa licença e com devida atribuição do trabalho original:\nPatriota, A.G., Garone, G.S. (2024, 2025). Notas do curso de inferência clássica (MAE0225, MAE0301) ministradas pelo Prof. A.G. Patriota digitadas por G.S. Garone.",
    "crumbs": [
      "Prefácio"
    ]
  },
  {
    "objectID": "modelo-estatistico.html",
    "href": "modelo-estatistico.html",
    "title": "1  Modelos Estatísticos na abordagem clássica",
    "section": "",
    "text": "1.1 Modelo Estatístico Paramétrico\nEm teoria de probabilidades, conhecemos a medida de probabilidade, logo, fazemos descrições probabilísticas. \\[(\\Omega, \\mathscr{A}, P)\\stackrel{\\text{X}}{\\rightarrow}(\\mathbb{R}, \\mathscr{B}, P_{X})\\] Na prática, contudo, não conhecemos a medida \\(P\\).\nDefinimos então uma família de medidas de probabilidades que possivelmente descrevem o comportamento aleatório dos dados.\nO Modelo Estatístico é definido pela trinca \\[(\\Omega, \\mathscr{A}, \\mathcal{P})\\] em que \\(\\Omega\\) é o espaço amostral (evento certo), \\(\\mathscr{A}\\) é a Sigma álgebra, uma família de subconjuntos ou eventos em \\(\\Omega\\), e \\(\\mathcal{P}\\) é uma família de medidas de probabilidade que possivelmente descrevem o comportamento aleatório dos dados ou eventos sob investigação.\nSe \\(\\mathcal{P} = \\{P_{\\theta} : \\theta \\in \\Theta\\}\\), em que \\(\\Theta \\subseteq \\mathbb{R}^{p}\\) e \\(p \\in \\mathbb{N}\\), então dizemos que \\((\\Omega, \\mathscr{A}, \\mathcal{P})\\) é um modelo estatístico Paramétrico.\nCaso não exista \\(\\Theta \\subseteq \\mathbb{R}^{p}\\) fixo, então dizemos que o modelo é não-paramétrico. Obs: \\(\\Theta\\) é o espaço paramétrico e \\(\\theta\\) é o vetor de parâmetros. \\(\\theta\\) Não* é variável aleatória, apenas indexa as medidas de probabilidade.",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelos Estatísticos na abordagem clássica</span>"
    ]
  },
  {
    "objectID": "modelo-estatistico.html#modelo-estatístico-paramétrico",
    "href": "modelo-estatistico.html#modelo-estatístico-paramétrico",
    "title": "1  Modelos Estatísticos na abordagem clássica",
    "section": "",
    "text": "1.1.1 Exemplos\n\n1.1.1.1 Exemplo de Bernoulli\nConsidere um Ensaio de Bernoulli \\[\\Omega = \\{S,F \\}, \\mathscr{A} = 2^\\Omega\\] Temos algum conhecimento prévio que sugere que as probabilidades de sucesso podem ser \\(0.1, 0.5, 0.9\\)\nNesse caso, \\[\\mathcal{P} = \\{P_{1}, P_{2}, P_{3} \\}\\] em que \\[\\begin{cases}\nP_{1}(\\{S\\}) = 0.1;~ P_{1}(\\{F\\}) = 0.9;~ P_{1}(\\Omega) = 1  \\\\\nP_{2}(\\{S\\}) = 0.5;~ P_{2}(\\{F\\}) = 0.5;~ P_{2}(\\Omega) = 1  \\\\\nP_{3}(\\{S\\}) = 0.9;~ P_{3}(\\{F\\}) = 0.1;~ P_{3}(\\Omega) = 1\n\\end{cases}\\] Observe que \\[\\mathcal{P}=\\{P_{\\theta}: \\theta \\in \\Theta \\}\\] em que \\(\\Theta=\\{1,2,3 \\}\\subseteq \\mathbb{R}\\) Portanto, \\((\\Omega, \\mathscr{A}, \\mathcal{P})\\) é um modelo paramétrico.\nUsando de variáveis aleatórias,\nSeja \\(X : \\Omega \\rightarrow \\mathbb{R}\\) tal que \\[X(\\omega)=\\begin{cases}\n1, \\omega = \\text{S} \\\\\n0, c.c.\n\\end{cases}\\] \\[E_\\theta(X)=\\sum\\limits^{1}_{x=0}xP_\\theta(X=x)\\] \\[\\begin{cases}\n\\theta = 1 \\Rightarrow E_{1}(X)=0.1 \\\\\n\\theta = 2 \\Rightarrow E_{2}(X)=0.5 \\\\\n\\theta = 3 \\Rightarrow E_{3}(X)=0.9\n\\end{cases}\\]\n\n\n1.1.1.2 Exemplo de Exponencial\nSeja \\(\\Omega=(0,\\infty)\\) e \\(\\mathscr{A}\\) uma sigma-álgebra de \\(\\Omega\\) (Sigma-Álgebra de Borel) \\(\\Omega\\) representa o tempo até a ocorrência de um evento (uma reclamação, por exemplo) Temos conhecimento prévio de que as funções densidade de probabilidade que possivelmente descrevem esse evento são:\n\\[\n\\begin{aligned}\nf_{1}(\\omega) &=\n\\begin{cases}\n\\mathrm{e}^{-1\\omega}, \\omega&gt;0 \\\\\n0, c.c.\n\\end{cases}\\\\\nf_{2} (\\omega) &=\n\\begin{cases}\n2\\mathrm{e}^{-2\\omega}, \\omega&gt;0 \\\\\n0, c.c.\n\\end{cases}\\\\\nf_{3} (\\omega) &=\n\\begin{cases}\n\\frac{1}{2}\\mathrm{e}^{-\\frac{1}{2}\\omega}, \\omega&gt;0 \\\\\n0, c.c.\n\\end{cases}\\\\\nf_{4} (\\omega) &=\n\\begin{cases}\n\\frac{1}{10}\\mathrm{e}^{- \\frac{1}{10}\\omega}, \\omega&gt;0 \\\\\n0, c.c.\n\\end{cases}\n\\end{aligned}\n\\]\ne \\(P\\)s, \\[\n\\begin{aligned}\nP_{1}(A)&= \\int_{A} f_{1}(\\omega)d \\omega \\\\\nP_{2}(A)&= \\int_{A} f_{2}(\\omega)d \\omega \\\\\nP_{3}(A)&= \\int_{A} f_{3}(\\omega)d \\omega \\\\\nP_{4}(A)&= \\int_{A} f_{4}(\\omega)d \\omega \\\\\n\\end{aligned}\\] Dessa forma, \\[P_{\\theta}(A)= \\int_{A}f_\\theta(\\omega)d \\omega\\] e \\(\\Omega = \\{1,2,3,4 \\}\\) Seja \\(X: \\Omega \\rightarrow \\mathbb{R}\\) tal que \\[X(\\omega)= \\omega\\] Note que \\[E_\\theta(X)=\\int_{-\\infty}^{\\infty}xf_\\theta(x)dx, \\theta \\in \\{1,2,3,4 \\}\\] \\[E_\\theta(X)=\\begin{cases}\n1,~ \\theta=1 \\\\\n\\frac{1}{2},~ \\theta=2 \\\\\n2,~ \\theta=3 \\\\\n10,~ \\theta=4\n\\end{cases}\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelos Estatísticos na abordagem clássica</span>"
    ]
  },
  {
    "objectID": "modelo-estatistico.html#principais-modelos-estatísticos",
    "href": "modelo-estatistico.html#principais-modelos-estatísticos",
    "title": "1  Modelos Estatísticos na abordagem clássica",
    "section": "1.2 Principais Modelos Estatísticos",
    "text": "1.2 Principais Modelos Estatísticos\n\n1.2.1 Modelo Estatístico de Bernoulli\nDizemos que \\(X\\) é uma variável aleatória com modelo de Bernoulli se, e somente se, \\[\nP_\\theta(X=x)=\\begin{cases}\n\\theta^{x} \\cdot(1-\\theta)^{1-x}, x \\in \\{0,1 \\} \\\\\n0, x \\not \\in \\{0,1 \\}\n\\end{cases}\\] O parâmetro é a probabilidade de sucesso \\[\\begin{cases}\nE_\\theta(X)=\\theta\\\\\n\\mathrm{Var}_{\\theta}(X) = \\theta(1-\\theta) \\\\\nP_\\theta(X=1)=\\theta, P_\\theta(X=0)=1-\\theta\n\\end{cases}\n\\]\nNotação: \\(\\mathrm{Ber}(\\theta)\\) em que \\(\\theta \\in \\Theta=(0,1)\\subseteq \\mathbb{R}\\)\n\n\n1.2.2 Modelo Estatístico Binomial\nDizemos que \\(X\\) é uma variável aleatória com modelo binomial se, e somente se, \\[\nP_\\theta(X=x)=\\begin{cases}\n{n \\choose x} \\cdot \\theta^{x}\\cdot(1-\\theta)^{n-x}, x \\in \\{0,1,\\dots,n \\} \\\\\n0, x \\not \\in \\{0,1,\\dots,n \\}\n\\end{cases}\\] \\[\\begin{cases}\nE_\\theta(X)=n\\theta\\\\\n\\mathrm{Var}_{\\theta}(X) = n\\theta(1-\\theta) \\\\\nP_\\theta(X=0)=(1-\\theta)^{n},\\dots, P_\\theta(X=n)=\\theta^n\n\\end{cases}\\]\nNotação: \\(\\mathrm{Bin}(n,\\theta)\\) em que \\(n\\) é conhecido e fixado previamente, \\(\\theta\\) é a probabilidade de sucesso (parâmetro do modelo) e \\(\\Theta =(0,1)\\) é o espaço paramétrico\n\n\n1.2.3 Modelo Estatístico Geométrico\nDizemos que \\(X\\), representando o número de fracassos até o primeiro sucesso, é uma variável aleatória com modelo estatístico geométrico se, e somente se, \\[P_\\theta(X=x)=\\begin{cases}\n\\theta (1-\\theta)^{x-1}, x \\in \\{1,\\dots\\} \\\\\n0, x \\not \\in \\{1,\\dots\\}\n\\end{cases}\n\\] \\[\n\\begin{cases}\nE_\\theta(X)=\\frac{1}{\\theta}\\\\\n\\mathrm{Var}_\\theta(X) = \\frac{1-\\theta}{\\theta^{2}}\n\\end{cases}\n\\] Notação: \\(\\mathrm{Geom}(\\theta)\\) em que \\(\\theta\\) é o parâmetro do modelo (probabilidade de sucesso) e \\(\\Theta=(0,1)\\) é o espaço paramétrico\n\n\n1.2.4 Modelo de Poisson\nDizemos que \\(X\\) é uma variável aleatória com modelo estatístico Poisson, se, e somente se, \\[\nP_\\theta(X=x)=\\begin{cases}\n\\mathrm{e}^{-\\theta}\\cdot \\frac{\\theta^{x}}{x!}, x \\in \\{0,1,\\dots\\} \\\\\n0, x \\not \\in \\{0,1,\\dots\\}\n\\end{cases}\n\\] \\[\n\\begin{cases}\nE_\\theta(X)=\\theta\\\\\n\\mathrm{Var}_\\theta(X) = \\theta\n\\end{cases}\n\\]\nNotação: \\(\\mathrm{Poisson}(\\theta)\\) em que \\(\\theta\\) é a taxa média de ocorrência do evento (parâmetro do modelo) e \\(\\Theta = (0, \\infty)\\), o espaço paramétrico.\n\n\n1.2.5 Modelo Multinomial\nDizemos que \\(\\pmb{X} = (X_1,\\dots,X_k)\\) é um Vetor Aleatório com modelo estatístico Multinomial se, e somente se a função de probabilidade é \\[\nP_{\\theta}(X_{1}=x_{1},\\dots,X_{k} = x_{k})=\n\\begin{cases}\n\\frac{n!}{x_{1}!x_{2}!\\dots x_{k}!} \\cdot \\theta^{x_{1}}_{1} \\cdot \\cdots \\cdot\\theta^{x_n}_{k} ~~~~ x_{1}+\\dots+x_{k}=n\\\\\n0, c.c\n\\end{cases}\n\\]\n\\[\n\\begin{cases}\nE_\\theta(X_{i})=n\\theta_{i}\\\\\n\\mathrm{Var}_\\theta(X_{i}) = n\\theta_{i} (1-\\theta_{i}) \\\\\n\\mathrm{Cov}(X_{i}X_{j})=-n\\theta_{i}\\theta_{j}\n\\end{cases}\n\\]\nNotação: \\(\\mathrm{Multinomial}(n, \\theta_1,\\theta_2,\\dots,\\theta_k)\\) em que \\(\\theta_{1}+\\dots+\\theta_{k}=1\\) e \\(0\\leq \\theta_{i} \\leq 1\\), \\(\\forall i = 1,2,\\dots,k\\), \\(\\Theta=\\{(\\theta_{1},,\\dots,\\theta_{k}) \\in \\mathbb{R}^{k} : 0\\leq \\theta_{i} \\leq 1, i=1,\\dots,k,\n\\theta_{1}+\\dots+\\theta_{k} = 1 \\}\\)\nEsse modelo tem aplicação em modelos de linguagem como o ChatGPT. (\\(k\\) como tamanho do vocabulário, \\(n=1\\), \\(\\theta_{1}=\\) probabilidade de escolher o primeiro elemento do vocabulário e assim por diante.)\n\n\n1.2.6 Modelo Uniforme contínuo\nDizemos que \\(X\\) é uma variável aleatória contínua com modelo estatístico Uniforme em \\((\\theta_{1}, \\theta_{2}), \\theta_{1}\\leq x \\leq\\theta_{2}\\), se, e somente se, a sua Função Densidade de Probabilidade é \\[\nf_\\theta(x)=\\begin{cases}\n\\frac{1}{\\theta_{2}-\\theta_{1}}, x \\in(\\theta_{1}, \\theta_{2}) \\\\\n0, c.c.\n\\end{cases}\n\\]\n\\[\n\\begin{cases}\nE_\\theta(X)=\\frac{\\theta_2+\\theta_1}{2}\\\\\n\\mathrm{Var}_\\theta(X) = \\frac{(\\theta_2-\\theta_1)^{2}}{12}\\end{cases}\n\\]\nNotação: \\(X \\sim U(\\theta_{1}, \\theta_{2})\\), em que \\(\\theta=(\\theta_{1}, \\theta_{2})\\) é o vetor de parâmetros e \\(\\Theta = \\{\\theta \\in \\mathbb{R}^{2} : \\theta_{2} &gt; \\theta_{1} \\}\\) é o espaço paramétrico.\n\n\n1.2.7 Modelo Exponencial\nDizemos que \\(X\\) é uma variável aleatória contínua com modelo estatístico Exponencial se, e somente se, a sua Função Densidade de Probabilidade é dada por \\[\nf_\\theta(x)=\\begin{cases}\n\\theta \\mathrm{e}^{-\\theta x}, x&gt;0\\\\\n0, c.c.\n\\end{cases}\n\\]\n\\[\n\\begin{cases}\nE_\\theta(X)=\\frac{1}{\\theta}\\\\\n\\mathrm{Var}_\\theta(X) = \\frac{1}{\\theta^{2}}\n\\end{cases}\n\\]\nNotação: \\(X\\sim \\mathrm{Exp}(\\theta), \\theta&gt; 0\\) em que \\(\\theta&gt;0, \\Theta=\\{\\theta \\in \\mathbb{R}: \\theta&gt;0 \\}\\)\n\n\n1.2.8 Modelo Normal\nDizemos que \\(X\\) é uma variável aleatória contínua com modelo estatístico Normal com média \\(\\mu\\) e variância \\(\\sigma^{2}\\) se, e somente se, a sua Função Densidade de Probabilidade é dada por \\[\nf_\\theta(x)=\n\\frac{1}{\\sqrt{2\\pi \\sigma^{2}}}\\cdot \\mathrm{e}^{-\\frac{1}{2\\sigma^{2}}(x-\\mu)^{2}}, x \\in \\mathbb{R}\n\\] em que \\(\\theta= (\\mu, \\sigma^{2}) \\in \\Theta= \\mathbb{R}\\times \\mathbb{R}^{+}\\)\n\\[\n\\begin{cases}\nE_\\theta(X)=\\mu\\\\\n\\mathrm{Var}_\\theta(X) = \\sigma^{2}\n\\end{cases}\n\\]\nNotação: \\(X \\sim N (\\mu, \\sigma^{2})\\), em que \\(\\theta=(\\mu, \\sigma^{2})\\) é o vetor de parâmetros.",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelos Estatísticos na abordagem clássica</span>"
    ]
  },
  {
    "objectID": "populacao-e-amostra.html",
    "href": "populacao-e-amostra.html",
    "title": "2  População e Amostra",
    "section": "",
    "text": "2.1 Variável Populacional\nVeja: Modelo Estatístico para definições dos modelos estatísticos paramétricos.\nPela teoria estatística, população é o conjunto sob investigação de todos os potenciais elementos.\nA Variável Populacional representa os valores numéricos de cada elemento da população: \\[\nX\\sim f_{\\theta,}\\theta \\in \\Theta\n\\] em que \\(f_\\theta\\) é a Função Densidade de Probabilidade da Variável Aleatória populacional. \\(\\theta\\) é o vetor de parâmetros (desconhecido) e \\(\\Theta\\) é o espaço paramétrico",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>População e Amostra</span>"
    ]
  },
  {
    "objectID": "populacao-e-amostra.html#amostra-teórica",
    "href": "populacao-e-amostra.html#amostra-teórica",
    "title": "2  População e Amostra",
    "section": "2.2 Amostra (Teórica)",
    "text": "2.2 Amostra (Teórica)\nNa estatística descritiva, a amostra é definida como um subconjunto da população.\nAlguns livros utilizam o termo “amostra representativa” para representar a amostra confiável de outra amostra que carrega determinados viéses de seleção. O termo representativo é controverso na estatística teórica,\n\n2.2.1 Amostra Aleatória\nNa estatística teórica, dizemos que \\((X_{1},\\dots,X_{n})\\) é uma amostra aleatória de \\(X\\) (v.a. populacional) se \\(X_{1},\\dots,X_{n}\\) forem independentes e identicamente distribuídas de acordo com a distribuição da variável aleatória populacional \\(X\\) Ou seja, \\[\n\\text{Independentes }\\rightarrow\\begin{cases}X_{1}\\sim f_{\\theta,}\\theta \\in \\Theta \\\\\n. \\\\\n. \\\\\n. \\\\\nX_{n} \\sim f_{\\theta}, \\theta \\in \\Theta\n\\end{cases}\n\\]\nSendo assim, \\((X_1, \\dots, X_n)\\) é amostra aleatória de \\(X\\sim f_\\theta, \\theta in \\Theta\\) sempre que \\((X_1, \\dots, X_n)\\) forem independentes para cada \\(P_\\theta, \\theta \\in \\Theta\\) e \\(X_i \\sim f_\\theta, \\theta \\in \\Theta, i = 1, 2, \\dots, n\\)\n\n2.2.1.1 Diagrama\n\n\n\n\n\nflowchart UD\n  A[$X\\sim f_\\theta, \\theta \\in \\Theta, \\g(\\theta)$] --&gt; B($\\X_1, \\sim \\f_\\theta, \\theta \\in \\Theta$)\n  A --&gt; C($\\X_2, \\sim \\f_\\theta, \\theta \\in \\Theta$)\n  A --&gt; D($\\X_n, \\sim \\f_\\theta, \\theta \\in \\Theta$)\n  B --&gt; E(x_1)\n  C --&gt; F(x_2)\n  D --&gt; G(x_n)",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>População e Amostra</span>"
    ]
  },
  {
    "objectID": "populacao-e-amostra.html#sec-ao",
    "href": "populacao-e-amostra.html#sec-ao",
    "title": "2  População e Amostra",
    "section": "2.3 Amostra (Observada)",
    "text": "2.3 Amostra (Observada)\nÉ formada por valores numéricos após utilizar um procedimento de amostragem. \\[\nx_{1},\\dots,x_{n}\n\\]\nem que \\(n\\) é o tamanho amostral.",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>População e Amostra</span>"
    ]
  },
  {
    "objectID": "quantidade-de-interesse.html",
    "href": "quantidade-de-interesse.html",
    "title": "3  Quantidade de Interesse",
    "section": "",
    "text": "É uma quantidade relacionada com a distribuição da variável aleatória populacional, ou seja, qualquer valor em função de \\(\\theta\\). \\[\ng(\\theta)\n\\]\nComo \\(g(\\theta) = E_\\theta(X), g(\\theta)= \\mathrm{Var}_\\theta(X), g(\\theta)=P_\\theta(X\\geq1), g(\\theta)=\\theta\\) em que \\(X\\) é a variável aleatória populacional. Empregando outra variável populacional, como \\(Y\\), conseguimos outras quantidades de interesse, como \\(g(\\theta)=\\mathrm{Cov}_{\\theta}(X,y)\\) ou \\(P_\\theta(X \\in A \\lvert Y \\in B)\\)",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Quantidade de Interesse</span>"
    ]
  },
  {
    "objectID": "dist-amostral.html",
    "href": "dist-amostral.html",
    "title": "4  Distribuição Amostral",
    "section": "",
    "text": "4.1 Exemplos\nSeja \\((X_{1},\\dots,X_{n})\\) amostra aleatória (a.a.) de \\(X\\sim f_{\\theta,}\\theta \\in \\Theta\\)\na Função Densidade de Probabilidade conjunta de \\((X_{1}, X_{2},\\dots,X_{n})\\) é \\(\\forall \\theta \\in \\Theta\\), no caso discreto: \\[\nP_\\theta(X_{1}=k_{1}, X_{2}=k_{2},\\dots,X_{n}= k_{n})\\stackrel{\\text{ind}}{=}\\prod^{n}_{i=1}P_\\theta(X_{i}=k_{i})\n\\stackrel{\\text{id}}{=}\\prod^{n}_{i=1}P_\\theta(X=k_{i})\\stackrel{\\text{def}}{=}\\prod^{n}_{i=1}f_\\theta(k_{i})\n\\]\nno caso contínuo: \\[\nf_\\theta^{(X_1, \\dots, X_n)}(k_{1},k_{2},\\dots,k_{n})\\stackrel{\\text{i.i.d}}{=}\\prod^{n}_{i=1}f_{\\theta}(k_{i})\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Distribuição Amostral</span>"
    ]
  },
  {
    "objectID": "dist-amostral.html#exemplos",
    "href": "dist-amostral.html#exemplos",
    "title": "4  Distribuição Amostral",
    "section": "",
    "text": "4.1.1 Exemplo um\nSeja \\((X_{1},X_{2},X_{3})\\) uma a.a de \\(X\\sim \\text{Ber}(\\theta), \\theta \\in (0,1)\\) 1. Especifique o espaço paramétrico 2. Calcule a função de probabilidade da amostra 3. Encontre as seguintes quantidade de interesses em função de \\(\\theta\\) 1. \\(g(\\theta)=E_\\theta(X)\\) 2. \\(g(\\theta)=P_\\theta(X=0)\\) 3. \\(g(\\theta)=\\mathrm{CV}_\\theta(X)\\) Resolução 1. \\(\\Theta=(0,1)\\) 2. Duas resoluções possíveis 1. Dando valores à amostra \\[\\begin{aligned}\n         &(X_{1}, X_{2},X_{3})  &P(X_{1}= k_{1},X_{2}= k_{2},X_{3}= k_{3})= \\prod ^{3}_{i=1}P_\\theta(X=k_{1})\\\\\n         &(0,0,0) &(1-\\theta)^{3}\\\\\n         &(0,0,1) &(1-\\theta)^{2}\\theta\\\\\n         &(0,1,0) &(1-\\theta)^{2}\\theta\\\\\n         &(1,0,0) &(1-\\theta)^{2}\\theta\\\\\n         &(0,1,1) &(1-\\theta)\\theta^2\\\\\n         &(1,0,1) &(1-\\theta)\\theta^{2}\\\\\n         &(1,1,0) &(1-\\theta)\\theta^{2}\\\\\n         &(1,1,1) &\\theta^{3}\\\\\n         \\end{aligned}\n    \\] 2. Enunciando a função Observe que, se \\(k \\in \\{0,1 \\}\\), \\[\\begin{aligned}\n       &P_\\theta(X=k)=\\theta^{k}(1-\\theta)^{1-k}\\cdot\\mathbb{1}_{\\{0,1 \\}}(k)\\Rightarrow\\\\\n       &P_\\theta(X_{1}=k,X_{2}=k_{2},X_{3}= k_{3})\\stackrel{\\text{i.i.d}}{=}\\prod^{3}_{i=1}\\{\\theta^{k_{i}}\n       (1-\\theta)^{1-k_{i}}\\mathbb{1}_{\\{0,1 \\}}(k_{i}) \\} = \\\\\n       &= \\theta^{\\sum\\limits^{3}_{i=1}k_{i}}(1-\\theta)^{3-\\sum\\limits^{3}_{i=1}k_{i}}\\prod^{3}_{i=1}\\mathbb{1}_{\\{0,1\\}}\n       (k_{i})\n       \\end{aligned}\n    \\] 3. Em função de \\(\\theta\\): 1. \\(g(\\theta)=E_\\theta(X)=\\theta\\) 2. \\(g(\\theta)=P_\\theta(X=0)=1-\\theta\\) 3. \\(g(\\theta)=\\mathrm{CV}_\\theta(X)=\\frac{\\sqrt{\\theta(1-\\theta)}}{\\theta}\\)\n\n\n\n4.1.2 Exemplo dois \nSeja \\((X_{1},\\dots,X_{n})\\) uma a.a de \\(X\\sim\\text{Pois}(\\theta), \\theta \\in(0,\\infty)\\), encontre a f.p. conjunta da amostra. Como esse vetor é uma a.a. (ou seja, variáveis aleatórias independentes e identicamente distribuídas), temos que \\[\nP_\\theta(X_1=k_{1},\\dots,X_{n}=k_{n})\\stackrel{\\text{iid}}{=}\\prod^{n}_{i=1}P_\\theta(X=k_{i})=\\prod^{n}_{i=1}\\{\\mathrm{e}\n^{-\\theta} \\cdot \\frac{\\theta^{k_{i}}}{k_{i}!}\\}\n\\] Sempre que \\(k_{i}\\in\\{0,1,\\dots \\}, \\forall i = 1,\\dots,n\\)\n\\[\n\\Rightarrow P_\\theta(X_1=k_{1},\\dots,X_{n}=k_{n})=\\mathrm{e}^{-n \\theta} \\cdot \\frac{\\theta^{\\sum\\limits^{n}_{i=1}k_{i}}}{\\prod^{n}_{i=1}\n(k_{i})!}\n\\]\n\n\n4.1.3 Exemplo contínuo um\nSeja \\((X_{1},\\dots,X_{n})\\) uma a.a de \\(X\\sim\\text{Exp}(\\theta), \\theta \\in(0,\\infty)\\), encontre a função densidade de probabilidade (f.d.p.) conjunta da amostra. \\[\n\\begin{aligned}\nf_{\\theta}^{(X_1,\\dots,X_{n})}(k_{1},\\dots,k_{n})&\\stackrel{\\text{iid}}{=}\\prod^{n}_{i=1}f_\\theta(k_{i})=\\prod^{n}_{i=1}\n\\{\\theta \\mathrm{e}^{-\\theta k_{i}} \\cdot \\mathbb{1}_{(0,\\infty )}(k_{i}) \\}\n\\\\&\\Rightarrow f_\\theta^{(X_1,\\dots,X_n)}(k_1,\\dots,k_n)=\\theta^{n}\n\\cdot \\mathrm{e}^{-\\theta\\sum\\limits^{n}_{i=1}k_{i}} \\cdot\\prod^{n}_{i=1}\\mathbb{1}_{(0,\\infty)}(k_i)\n\\end{aligned}\n\\]\n\n\n4.1.4 Exemplo contínuo dois\nSeja \\((X_{1},\\dots,X_{n})\\) uma a.a. (i.i.d) de \\(X\\sim N(\\mu,\\sigma^{2})\\) em que \\(\\theta=(\\mu, \\sigma^{2}) \\in \\Theta=\\mathbb{R}\\times \\mathbb{R}_{+}\\). Considere \\(\\stackrel{x}{\\sim}=(x_{1},\\dots,x_{n})\\) a amostra observada.\n\\[\n\\begin{aligned}\nL_{\\stackrel{X}{\\sim}}(\\theta) &\\stackrel{\\text{iid}}{=} \\prod^{n}_{i=1}f_\\theta(x_{i})=\n\\prod^{n}_{i=1}\\left\\{\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} \\mathrm{exp}\\{- \\frac{1}{2\\sigma^{2}}(x_{i}-\\mu)^{2}\\}\\right\\} \\\\\n&= \\frac{1}{(2 \\pi \\sigma^{2})^{\\frac{x}{2}}}\\cdot \\mathrm{exp}\\{- \\frac{1}{2 \\sigma^{2}} \\sum\\limits^{n}_{i=1}(x_{i}-\\mu)^{2}  \\}\n\\end{aligned}\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Distribuição Amostral</span>"
    ]
  },
  {
    "objectID": "funcao-verossimilhanca.html",
    "href": "funcao-verossimilhanca.html",
    "title": "5  Função de Verossimilhança",
    "section": "",
    "text": "5.1 Exemplo\nQuando analisamos a distribuição conjunta da amostra em função de \\(\\theta\\) nos valores da amostra observada, temos a Função de Verossimilhança\n\\[\n\\mathrm{L}_{\\underset{x}{\\sim}}(\\theta)=P_\\theta(X_{1}=x_{1},X_{2}=x_{2},\\dots,X_{n}=x_{n})\n\\]\nem que \\(\\underset{x}{\\sim}=(x_{1},x_{2},\\dots,x_{n})\\) é a amostra observada.\nObs: A função de verossimilhança, no caso discreto, é a probabilidade de observar a amostra observada.\nConsidere \\((X_{1},X_{2},X_{3},X_{4})\\) a.a de \\(X\\sim \\text{Ber}(\\theta), \\theta \\in \\{0.1, 0.5, 0.9 \\}\\). Note que o espaço paramétrico é \\(\\Theta=\\{0.1,0.5,0.9 \\}\\). Considere, ainda, que a amostra observada foi \\((0,1,1,1)\\). Encontre a função de verossimilhança.\n\\[\n\\begin{aligned}\nL_\\underset{X}{\\sim}(\\theta)&=P_\\theta(X_{1}=x_{1},X_{2}=x_{2},X_{3}=x_{3},X_{4}=x_{4}) \\\\\n&= \\theta^{\\sum\\limits^{4}_{i=1}x_{i}}\n(1-\\theta)^{4-\\sum\\limits^{4}_{i=1}x_{i}}\\cdot \\cancelto{1}{\\prod^{4}_{i=1}\\mathbb{1}_{\\{0,1 \\}}(x_{i})} =\n\\theta^{3}(1-\\theta)\n\\end{aligned}\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Função de Verossimilhança</span>"
    ]
  },
  {
    "objectID": "estatisticas.html",
    "href": "estatisticas.html",
    "title": "6  Estatísticas",
    "section": "",
    "text": "6.1 Exemplo\nFunções da amostra que não dependem de \\(\\theta \\in \\Theta\\)\nSeja \\((X_{1}, \\dots, X_{n})\\) a.a. de \\(X\\sim f_{\\theta,}\\theta \\in \\Theta\\). São estatísticas: 1. \\(T_{1}(X_{1},\\dots,X_{n})=X_{1}+\\dots+X_{n}\\) 2. \\(T_{2}=\\bar{X}= \\frac{X_{1}+\\dots+X_{n}}{n}\\) 3. \\(T_{3}=\\max\\{X_{1},\\dots,X_{n}\\}=X_{(n)}\\) 4. \\(T_{4}=\\min\\{X_{1},\\dots,X_{n}\\}=X_{(1)}\\) 5. \\(T_{5}(X_{1},\\dots,X_{n})=X_{(n)}-X_{(1)}\\) 6. \\(T_{6}(X_{1},\\dots,X_{n})=X_{i}, \\text{para algum }i=1,\\dots,n\\) 7. \\(T_{7}(X_{1},\\dots,X_{n})=\\frac{1}{n}\\sum\\limits^{n}_{i=1}(X_{i}-\\bar{X})^{2}\\) 8. \\(T_{8}(X_{1},\\dots,X_{n})=\\frac{1}{n-1}\\sum\\limits^{n}_{i=1}(X_{i}-\\bar{X})^{2}\\) 9. \\(T_{9}(X_{1},\\dots,X_{n})=\\frac{1}{n}\\sum\\limits^{n}_{i=1}|X_{i}-\\bar{X}|\\) 10. \\(T_{10}(X_{1},\\dots,X_{n})=\\sqrt{\\frac{1}{n}\\sum\\limits^{n}_{i=1}(X_{i}-\\bar{X})^{2}}\\) etc.\nObservação: As estatísticas são variáveis aleatórias.\n\\[\n\\underset{\\sim}{X_n}=(X_{1},\\dots,X_{n}):\\Omega\\rightarrow \\mathbb{R}^{n}\n\\] \\[\nT(\\underset{\\sim}{X})=T \\circ \\underset{\\sim}{X}:\\Omega\\rightarrow \\mathbb{R}\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Estatísticas</span>"
    ]
  },
  {
    "objectID": "estimadores.html",
    "href": "estimadores.html",
    "title": "7  Estimadores",
    "section": "",
    "text": "7.1 Estimativas\nSão estatísticas cujo objetivo é estimar uma quantidade de interesse. Portanto, estimadores são também variáveis aleatórias.\nSão os valores observados a partir da amostra observada dos estimadores. Portanto, estimativas são valores numéricos\nExemplos: 1. \\(\\bar{X}\\) é uma estatística, \\(\\bar{X}\\) é um estimador para \\(g(\\theta)=E_\\theta(X)\\) 2. Observando \\(\\bar{x}=\\frac{1}{n} \\sum\\limits^{n}_{i=1}x_{i}\\) é uma estimativa Seja \\((X_{1},X_{2})\\) a.a de \\(X\\sim \\text{Ber}(\\theta), \\theta \\in (0,1)\\). Considere as estatísticas e suas funções de probabilidade: \\[\n\\begin{aligned}\nT_{1}(X_{1},X_{2}) &= X_{1}\\\\\nT_{2}(X_{1},X_{2}) &= X_{2}\\\\\nT_{3}(X_{1},X_{2}) &= X_{1}+X_{2}\\\\\nT_{4}(X_{1},X_{2}) &= \\max\\{X_{1},X_{2}\\}\\\\\nT_{5}(X_{1},X_{2}) &= \\min\\{X_{1},X_{2}\\}\\\\\nT_{6}(X_{1},X_{2}) &= \\frac{1}{2}[(X_{1}-\\bar{X})^{2}+(X_{2}-\\bar{X})^{2}]=S^{2}_{n}=\\frac{1}{2}S_{n-1}^{2}\n\\end{aligned}\n\\]\n\\[\n\\begin{array}{cccccc}\n(X_{1},X_{2}) & P_{\\theta X_{1},X_{2}} & X_{1} & X_{2} & X_{1}+X_{2} & T_4 & T_5 & \\bar{X}& S^{2}_{n} & S^{2}_{n-1}\\\\\n\\hline\n(0,0) & (1-\\theta)^{2} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n(1,0) & \\theta(1-\\theta) & 1 & 0 & 1 & 1 & 0 & 0.5 & 0.25 & 0.5\\\\\n(0,1) & \\theta(1-\\theta) & 0 & 1 & 1 & 1 & 0 & 0.5 & 0.25 & 0.5\\\\\n(1,1) & \\theta^{2} & 1 & 1 & 2 & 1 & 1 & 1 & 0 & 0\\\\\n\\hline\n\\end{array}\n\\]\nCalcule para \\(T \\in \\{T_{1},T_{2},T_{3},T_{4},T_{5},T_{6} \\}\\)\na-) \\(E_\\theta(T)\\)\n\\(E_\\theta(T_{1}(X_{1},X_{2}))=E_\\theta(X_{1})= (1-\\theta)^{2}+1 \\cdot\\theta(1-\\theta)+0 \\theta(1-\\theta)+1\n\\theta^{2}= \\theta\\)\nO mesmo vale para \\(T_2\\)\n\\(E_\\theta(T_{3}(X_{1},X_{2}))=E_\\theta(X_{1}+X_{2})=2 \\theta\\) \\(E_\\theta(T_{4}(X_{1},X_{2}))=E_\\theta(\\max\\{X_{1},X_{2}\\})=0 \\cdot (1-\\theta)^{2}+ 1\\cdot[2\\cdot \\theta(1-\\theta)+\n\\theta^{2}] = 2\\theta-\\theta^{2}\\) \\(E_{\\theta}(T_{5}(X_{1},X_{2}))=E_\\theta(\\min\\{X_{1},X_{2} \\})=\\theta^{2}\\) \\(E_\\theta(T_{6}(X_{1},X_{2}))=2 \\theta(1-\\theta)\\)\nb-) \\(\\mathrm{Var}_{\\theta}(T)\\)\nTermine com os mesmos raciocínios\nc-) \\(P_\\theta(T=0)\\)\nTermine com os mesmos raciocínios\nAlguns resultados importantes:\n\\[\n\\begin{aligned}\nE_\\theta(\\bar{X})&=E_\\theta(\\frac{X_{1}+X_{2}}{2})=\\theta, \\theta \\in(0,1)\\\\\nE_{\\theta}(\\bar{X}^{2}) &= 0^{2}(1-\\theta)^{2} + \\frac{2}{4}  \\theta (1-\\theta) + 1^{2}\\theta^{2}=\\frac{1}{2}\\theta +\n\\frac{1}{2} \\theta^{2}, \\theta \\in(0,1)\\\\\n\\Rightarrow \\mathrm{Var}_\\theta(\\bar{X}) &= \\frac{\\theta+\\theta^{2}}{2} - \\theta^{2} =\\frac{\\theta (1-\\theta)}{2},\n\\theta \\in(0,1)\n\\end{aligned}\n\\]\nAplicando em nossa tabela (\\(S_{n-1}^{2}\\)):\n\\[\n\\begin{aligned}\nE_\\theta([(X_{1}-\\bar{X})^{2}+(X_{2}-\\bar{X})^{2}])&= \\theta(1-\\theta)\\\\\nE_\\theta([(X_{1}-\\bar{X})^{2}+(X_{2}-\\bar{X})^{2}]^{2}) &= \\frac{\\theta(1-\\theta)}{2}\\\\\n\\Rightarrow\\mathrm{Var}_{\\theta}([(X_{1}-\\bar{X})^{2}+(X_{2}-\\bar{X})^{2}]) &= \\frac{1}{2}\\theta(1-\\theta)\n[1-2 \\cdot \\theta(1-\\theta)]\n\\end{aligned}\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Estimadores</span>"
    ]
  },
  {
    "objectID": "estimadores.html#propriedades-dos-estimadores-para-quantidades-de-interesse",
    "href": "estimadores.html#propriedades-dos-estimadores-para-quantidades-de-interesse",
    "title": "7  Estimadores",
    "section": "7.2 Propriedades dos estimadores para quantidades de interesse",
    "text": "7.2 Propriedades dos estimadores para quantidades de interesse\n\n7.2.1 Estimados não viciados ou enviesados\nSeja \\((X_{1},\\dots,X_{n})\\) a.a. de \\(X\\sim f_{\\theta,}\\theta \\in \\Theta\\) e considere \\(T(X_{1},\\dots,X_{n})=\\hat{\\theta}_n\\) um estimador para \\(\\theta\\). Dizemos que \\(\\hat{\\theta}_n\\) é não-enviesado para \\(\\theta \\Leftrightarrow\\) \\[\nE_\\theta(\\hat \\theta_{n}) = \\theta, \\forall \\theta \\in \\Theta\n\\] De forma geral, \\(T(X_{1},\\dots,X_{n})\\) é um estimador não-viciado para \\(g(\\theta) \\Leftrightarrow\\) \\[\nE_\\theta(T(X_{1},\\dots,X_{n}))=g(\\theta), \\forall \\theta \\in \\Theta\n\\] Caso contrário, dizemos que \\(T(X_{1},\\dots,X_{n})\\) é viciado ou enviesado para \\(g(\\theta)\\) Dizemos que \\(\\hat\\theta_{n}\\) é fracamente consistente para \\(\\theta \\Leftrightarrow\\) \\[\n\\lim_{n\\to \\infty}{P_\\theta(|\\hat \\theta_{n}- \\theta| &gt; \\epsilon)=0, \\forall \\theta \\in \\Theta}\n\\] e para cada \\(\\epsilon&gt;0\\) fixado.\n\n7.2.1.1 Estimadores não viciados assintoticamente\nDizemos que \\(T(X_{1},\\dots,X_{n})\\) é um estimador assintoticamente não viciado para \\(g(\\theta) \\Leftrightarrow\\) \\[\n\\lim_{n\\to \\infty}{E_\\theta(T(X_{1},\\dots,X_{n}))} = g(\\theta), \\forall \\theta \\in \\Theta\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Estimadores</span>"
    ]
  },
  {
    "objectID": "eqm-e-vies.html",
    "href": "eqm-e-vies.html",
    "title": "8  Erro quadrático médio (EQM)",
    "section": "",
    "text": "8.1 Propriedades do EQM\nO erro quadrático médio (EQM) do estimador \\(T(X_{1},\\dots,X_{n})\\) com respeito a \\(g(\\theta)\\) é definido por \\[\n\\mathrm{EQM}(T,g(\\theta))=E_{\\theta}((T(X_{1},\\dots,X_{n})-g(\\theta))^{2})\n\\] Obs.:\nSe \\(T(X_{1},\\dots,X_{n})\\) for não viciado para \\(g(\\theta)\\), então \\(\\mathrm{EQM}(T,g(\\theta))=\\mathrm{Var}_{\\theta}(T(X_{1},\\dots,X_{n})) \\forall \\theta \\in \\Theta\\)\nSeja \\(T(X_{1},\\dots,X_{n})\\) um estimador para \\(g(\\theta)\\), seja \\(\\mu_{t} = E_\\theta(T(X_{1},\\dots,X_{n}))\\) \\[\n\\begin{aligned}\n&\\mathrm{EQM}(T,g(\\theta))\\\\\n&=E_\\theta[(T(X_{1},\\dots,X_{n})-\\mu_{t}+\\mu_{t}-g(\\theta))^{2}] \\\\ \\\\\n&= E_\\theta[((T(X_{1},\\dots,X_{n})- \\mu_{t})+ (\\mu_{t}-g(\\theta)))^{2}] \\\\\n& = E_\\theta[(T(X_{1},\\dots,X_{n})-\\mu_{t})^{2}+2(T(X_{1},\\dots,X_{n})-\\mu_{t})(\\mu_{t}g(\\theta))+(\\mu_{t}- g(\\theta))^{2}] \\\\\n&= \\overbrace{E_\\theta[(T(X_{1},\\dots,X_{n})-\\mu_{t})^{2}]}^{\\mathrm{Var}_{\\theta}(T(X_{1},\\dots,X_{n}))} + 2(\\mu_{t} -\ng(\\theta))\\cancelto{0}{E_\\theta(T(X_{1},\\dots,X_{n})-\\mu_{t})} + (\\mu_{t}-g(\\theta))^{2} \\\\\n&=\\mathrm{Var}_\\theta(T(X_{1},\\dots,X_{n})) + (\\mu_{t}-g(\\theta))^{2}\n\\end{aligned}\n\\]\nPortanto, \\[\n\\mathrm{EQM}(T,g(\\theta)) = \\mathrm{Var}_\\theta(T(X_{1},\\dots,X_{n})) + (\\mu_{t}-g(\\theta))^{2}\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Erro quadrático médio (EQM)</span>"
    ]
  },
  {
    "objectID": "eqm-e-vies.html#viés",
    "href": "eqm-e-vies.html#viés",
    "title": "8  Erro quadrático médio (EQM)",
    "section": "8.2 Viés",
    "text": "8.2 Viés\nDenotamos de viés de \\(T(X_{1},\\dots,X_{n})\\) com respeito a \\(g(\\theta)\\) por \\[\n\\mathrm{Viés}(T,g(\\theta)) = E_\\theta(T(X_{1},\\dots,X_{n}))-g(\\theta),\\forall \\theta \\in \\Theta\n\\]\nDessa forma, temos que \\[\n\\mathrm{EQM}(T,g(\\theta)) = \\mathrm{Var}_\\theta(T(X_{1},\\dots,X_{n})) + [\\mathrm{Viés}(T(X_1,\\dots,X_n),g(\\theta))]^{2}\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Erro quadrático médio (EQM)</span>"
    ]
  },
  {
    "objectID": "eqm-e-vies.html#exemplo",
    "href": "eqm-e-vies.html#exemplo",
    "title": "8  Erro quadrático médio (EQM)",
    "section": "8.3 Exemplo",
    "text": "8.3 Exemplo\nSeja \\((X_{1},\\dots,X_{n})\\) uma amostra aleatória, ou seja, independentes e identicamente distribuídas (i.i.d.), de \\(X\\sim \\mathrm{Ber}(\\theta)\\) em que \\(\\theta \\in \\Theta = (0,1)\\). Calcule o viés e o EQM de \\(\\bar{X}_{n}\\) com respeito a \\(g(\\theta)=P_\\theta(X=1)\\)\nO estimador é, então, \\(T(X_{1},\\dots,X_{n})=\\bar{X}_{n}= \\frac{X_{1}+\\dots+X_{n}}{n}\\) para \\(g(\\theta)=P_\\theta(X=1)=\\theta\\) (pelo modelo de Bernoulli). \\[\n\\begin{aligned}\nE_\\theta(\\bar{X}_{n}) &= E_\\theta\\left(\\frac{1}{n}\\sum\\limits^{n}_{i=1}X_{i}\\right)=\n\\frac{1}{n}\\sum\\limits^{n}_{i=1}E_\\theta(X_{i}) \\stackrel{id. dist.}{\\Rightarrow} \\\\\nE_\\theta(\\bar{X}_{n}) &= \\frac{1}{n} \\sum\\limits^{n}_{i=1} E_\\theta(X) \\\\\n& = \\frac{n}{n} \\theta = \\theta, \\forall \\theta \\in \\Theta\n\\end{aligned}\n\\]\nPortanto, \\(\\bar{X}_{\\theta, n}\\) é não enviesado para \\(g(\\theta) = \\theta\\). \\[\n\\Rightarrow \\mathrm{Viés}(\\bar{X}_{n}, g(\\theta)) = 0, \\forall \\theta \\in \\Theta\n\\]\nPara o EQM, \\[\n\\begin{aligned}\n\\mathrm{EQM}(\\bar{X}_{n},g(\\theta)) &= \\mathrm{Var}_\\theta(\\bar{X}_{n}) - 0^{2} = \\mathrm{Var}_\\theta\n\\left(\\frac{1}{n}\\sum\\limits^{n}_{i=1}X_{i}\\right)= \\frac{1}{n^{2}}\\mathrm{Var}_\\theta\\left(\\sum\\limits^{n}_{i=1}X_{i}\\right)\\\\\n& \\stackrel{\\text{ind}}{=} \\frac{1}{n^{2}}\\sum\\limits ^{n}_{i=1}\\mathrm{Var}_\\theta(X_{i})\n\\stackrel{\\text{ind. dist.}}{=} \\frac{1}{n^{2}} \\sum\\limits^{n}_{i=1}\\mathrm{Var}_{\\theta}(X), \\\\\n&= \\frac{n \\theta(1-\\theta)}{n^{2}} = \\frac{\\theta(1-\\theta)}{n}, \\forall \\theta \\in \\Theta\n\\end{aligned}\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Erro quadrático médio (EQM)</span>"
    ]
  },
  {
    "objectID": "monte-carlo.html",
    "href": "monte-carlo.html",
    "title": "9  Simulações de Monte Carlo",
    "section": "",
    "text": "9.1 Método\nTem como objetivo replicar artificialmente os dados de um modelo estatístico para estudar o comportamento de estatísticas e estimadores (ou qualquer procedimento estatístico)",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Simulações de Monte Carlo</span>"
    ]
  },
  {
    "objectID": "monte-carlo.html#método",
    "href": "monte-carlo.html#método",
    "title": "9  Simulações de Monte Carlo",
    "section": "",
    "text": "Defina o modelo estatístico: “Seja \\((X_{1},\\dots,X_{n})\\)” a.a. de \\(X\\sim f_{\\theta,}\\theta \\in \\Theta\\).\nEscolha \\(\\theta_{0} \\in \\Theta\\) e considere-o fixado daqui em diante.\nPara \\(n\\) fixado, gere (\\(x_{1}, x_{2},\\dots,x_{n}\\)) a amostra observada de \\(X\\sim f_{\\theta_{0}}\\)\nArmazene a amostra observada\nRepita 3. e 4. \\(M=10000\\) vezes (ou quantas vezes desejar, a depender do caso)",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Simulações de Monte Carlo</span>"
    ]
  },
  {
    "objectID": "emv.html",
    "href": "emv.html",
    "title": "10  Estimação via máxima verossimilhança (EMV)",
    "section": "",
    "text": "10.1 Exemplo\nO valor numérico \\(\\hat\\theta_{n}\\) que maximiza a função de verossimilhança, ou seja, \\(L_{\\stackrel{x}{\\sim}}(\\hat\\theta_{n}) \\geq L_{\\stackrel{x}{\\sim}}(\\theta)\\forall \\theta \\in \\Theta\\) é dito ser uma estimativa de máxima verossimilhança (MV) para \\(\\theta\\). Observe que \\(\\hat\\theta_{n}\\) depende da amostra observada e portanto: \\(\\hat\\theta_{n} = \\hat\\theta(x_{1},x_{2},\\dots,x_{n})\\).\nO estimador de máxima verossimilhança é obtido substituindo \\((x_{1},\\dots,x_{n})\\) por \\((X_{1},\\dots,X_{n})\\), ou seja, \\(\\hat\\theta(X_{1},\\dots,X_{n})\\) é o Estimador de Máxima Verossimilhança (EMV)\nSeja \\((X_{1},\\dots,X_{n})\\) amostra aleatória (a.a.) de \\(X\\sim f_{\\theta}, \\theta \\in \\{\\frac{1}{3}, \\frac{1}{2} \\}\\) em que \\(f_\\theta\\) é uma função de probabilidade que satisfaz:\n\\[\n\\begin{array}{|c|c|c|c|}\n\\hline\nX=x: & 0 & 1 & 2\\\\\n\\hline\nf_{\\theta}(x): & \\theta & \\theta^{2} & 1-\\theta-\\theta^{2}\\\\\n\\hline\n\\end{array}\n\\] Considere que a amostra observada é \\(\\stackrel{x}{\\sim}=(0,0,1)\\).\na-) Encontre a estimativa da máxima verossimilhança\nSabemos que \\[\nf_{\\theta}(x)= \\theta^{\\mathbb{1}_{\\{0\\}}(x)} \\cdot (\\theta^{2})^{\\mathbb{1}_{\\{1\\}}(x)} \\cdot\n(1-\\theta-\\theta^{2})^{\\mathbb{1}_{\\{2\\}}(x)} \\forall \\theta \\in \\Theta\n\\] portanto,\n\\[\nL_{\\stackrel{x}{\\sim}}(\\theta)\\stackrel{\\text{iid}}{=}\\prod^{n}_{i=1}f_{\\theta}(x_{i})  =\n\\theta^{\\sum\\limits^{n}_{i=1}\\mathbb{1}_{\\{0\\}}(x_{i})} \\cdot (\\theta^{2})^{\\sum\\limits^{n}_{i=1}\\mathbb{1}_{\\{1\\}}(x_{i})}\n\\cdot (1-\\theta-\\theta^{2})^{\\sum\\limits^{n}_{i=1}\\mathbb{1}_{\\{2\\}}(x_{i})} \\forall \\theta \\in \\Theta\n\\]\nPara \\(\\stackrel{x}{\\sim}=(0,0,1)\\), \\[\nL_{\\stackrel{x}{\\sim}}(\\theta) = \\theta^{2} \\cdot \\theta^{2} \\cdot (1-\\theta -\\theta^{2})^{0} = \\theta^{4}\n\\forall \\theta \\in \\Theta\n\\]\nSubstituindo \\(\\forall \\theta \\in \\Theta\\): \\[\n\\theta = \\frac{1}{2}\\Rightarrow L_{\\stackrel{x}{\\sim}}\\left(\\frac{1}{2}\\right)=\\frac{1}{16} ~~~~ \\theta =\n\\frac{1}{3}\\Rightarrow L_{\\stackrel{x}{\\sim}}\\left(\\frac{1}{3}\\right)=\\frac{1}{81}\n\\]\nPortanto, \\(\\hat\\theta_{n}=\\frac{1}{2}\\) é a estimativa de máxima verossimilhança.",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Estimação via máxima verossimilhança (EMV)</span>"
    ]
  },
  {
    "objectID": "emv.html#exemplo",
    "href": "emv.html#exemplo",
    "title": "10  Estimação via máxima verossimilhança (EMV)",
    "section": "",
    "text": "10.1.1 Invariância dos EMVs\nTeorema. Se \\(\\hat\\theta_{(X_{1},\\dots,X_{n})}\\) for EMV para \\(\\theta\\), então \\(g(\\hat\\theta_{(X_{1},\\dots,X_{n})})\\) é o EMV para \\(g(\\theta)\\), ou seja, \\(g(\\hat\\theta_n)\\) é a estimativa de máxima verossimilhança para \\(g(\\theta)\\)\nMais um exemplo:\nSeja \\((X_{1},\\dots,X_{n})\\) a.a. de \\(X\\sim N(\\mu, \\sigma^2)\\) em que \\(\\theta = (\\mu, \\sigma^{2}) \\in \\Theta=\\mathbb{R}\\times \\mathbb{R}^{+}\\) Assuma que \\(\\stackrel{x}{\\sim} = (x_{1},\\dots,x_{n})\\) é a amostra observada. Lembrando que estaremos chamando \\(\\theta=(\\mu, \\sigma^{2})\\), mas estes são parâmetros genéricos. Poderíamos, por exemplo, chamá-los de \\(\\theta=(\\theta_{1},\\theta_{2})\\), o que pode facilitar a visualizar algumas derivadas.\na-) Encontre as estimativas de máxima verossimilhança para \\(\\theta = (\\mu, \\sigma^{2})\\):\nA Função de Verossimilhança é: \\[\n\\begin{aligned}\nL_{\\underset{\\sim}{x}}(\\theta)&\\stackrel{\\text{iid}}{=}\\prod^{n}_{i=1}f_\\theta(x_{i}) = \\prod^{n}_{i=1}\n\\left\\{ \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}}\\cdot \\exp\\left\\{\\frac{-1}{2}\\cdot\\frac{(x_{i}-\\mu)^{2}}{\\sigma^{2}} \\right\\} \\right\\}\\\\\n&= \\frac{1}{(2\\pi \\sigma^{2})^{\\frac{n}{2}}} \\cdot \\exp\\left\\{ \\frac{-1}{2\\sigma^{2}}\\sum\\limits^{n}_{i=1}(x_{i}-\\mu)^{2}\\right\\}\n\\end{aligned}\n\\]\nPodemos derivar para encontrar o máximo da FMV. Para isso, derivaremos e igualamos a zero primeiro em relação a \\(\\mu\\), então a \\(\\sigma^{2}\\) (podemos aplicar o logaritmo para facilitar as operações.)\n\\[\n\\begin{aligned}\n\\frac{\\partial\\ln(L_{\\underset{\\sim}{x}})}{\\partial \\mu} &= \\frac{1}{\\sigma^{2}}\\sum\\limits^{n}_{i=1}(x_{i}-\\mu) =0\n\\Rightarrow \\hat \\mu =\\frac{1}{n} \\sum\\limits^{n}_{i=1}x_{i} \\\\\n\\frac{\\partial \\ln(L_{\\underset{\\sim}{x}})}{\\partial \\sigma^{2}} &= - \\frac{n}{2\\sigma^{2}} + \\frac{1}{2\\sigma^{4}}\n\\sum\\limits^{n}_{i=1}(x_{i}-\\mu)^{2} =0 \\\\\n\\therefore \\\\\n\\mathrm{Estimativas~ MV} & =\n\\begin{cases}\n\\mu = \\bar{x} \\\\\n-\\frac{n}{2\\sigma^{2}} + \\frac{1}{2 \\sigma^{4}} \\sum\\limits^{n}_{i=1}(x_{i}-\\mu)^{2}=0 \\\\\n\\end{cases} \\\\\n&\\Leftrightarrow \\begin{cases}\n\\hat{\\mu}=\\bar{x} \\\\\n\\hat{\\sigma}^{2}= \\frac{1}{n}\\sum\\limits^{n}_{i=1}(x_{i}-\\bar{x})^{2}\n\\end{cases}\n\\end{aligned}\n\\]\nEstes são os pontos que maximizam a Função de Máxima Verossimilhança. (Provados em cálculo), ou seja, são as estimativas de máxima verossimilhança para \\(\\mu, \\sigma^{2}\\) respectivamente, e \\(\\hat{\\mu}(X_{1},\\dots,X_{n})=\\bar{X}, \\sigma^{2}(X_{1},\\dots,X_{n})=\\frac{1}{n}\\sum\\limits^{n}_{i=1}(X_{i}-\\bar{X})^{2}\\) são os estimadores de máxima verossimilhança.\nPela propriedade de invariância podemos encontrar o EMV para \\(g(\\theta)= \\frac{\\sqrt{\\mathrm{Var}_\\theta(X)}}{E_{\\theta(X)}}\\): \\[\n\\widehat{g(\\theta)} = \\frac{\\sqrt{\\frac{1}{n}\\sum\\limits^{n}_{i=1}(X_{i}-\\bar{X})}}{\\bar{X}}\n\\]\nObservação: Seja \\((X_{1},\\dots,X_{n})\\) a.a. de \\(X\\sim N(\\mu, \\sigma^{2})\\). Então, 1. \\(\\bar{X} \\underset{\\text{Exata!}}{\\sim}N\\left(\\mu, \\frac{\\sigma^{2}}{n}\\right)\\forall \\mu, \\sigma \\in \\mathbb{R} : \\sigma^{2}&gt;0 \\text{ e } n\\geq 1\\) 2. \\(\\sum\\limits^{n}_{i=1} \\frac{(x_{1}-\\bar{X})^{2}}{\\sigma^{2}}\\underset{\\text{Exata!}}{\\sim}\\chi^{2}_{(n-1)}\\) em que \\(\\chi^{2}_{k}\\) representa a Distribuição Qui-Quadrado com \\(k\\) grau de liberdade, cuja função densidade de probabilidade é: \\[\nf(x) = \\frac{1}{\\Gamma(\\frac{k}{2})2^{\\frac{k}{2}}} \\cdot x^{\\frac{k}{2}-1} \\cdot \\exp\\left\\{\\frac{-x}{2}\\right\\} \\cdot \\mathbb{1}_{(0, \\infty)}(x)\n\\]\nPara qualquer outra distribuição, existe um resultado aproximado pelo Teorema do Limite Central",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Estimação via máxima verossimilhança (EMV)</span>"
    ]
  },
  {
    "objectID": "qui-quadrado.html",
    "href": "qui-quadrado.html",
    "title": "11  Distribuição Qui-Quadrado",
    "section": "",
    "text": "Se \\(Z\\sim N(0,1)\\), então \\(Z^{2}\\sim \\chi^2\\), como provado por transformação de variáveis aleatórias\nSe \\(W\\sim \\chi^{2}_{k}\\), então sua função densidade de probabilidade é dada por \\[\n\\begin{cases}\n\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)\\cdot 2 ^{\\frac{k}{2}}}\\cdot w^{\\frac{k}{2} -1} \\cdot e^{-\\frac{1}{2}w}, ~ ~~ w &gt; 0 \\\\\n0, cc\n\\end{cases}\n\\] Sendo assim, \\[\n\\begin{cases}\nE(W) = k \\\\\n\\mathrm{Var}(W) = 2k\n\\end{cases}\n\\] Se \\(Z_{1},Z_{2,}\\dots,Z_{N}\\stackrel{\\text{iid}}{\\sim} N(0,1)\\), então \\[Z_{1}^{2}+\\dots+Z_{n}^{2}\\sim \\chi^{2}_{n}\\] Prova por função característica\nSe \\(X_{1},\\dots,X_{n} \\stackrel{\\text{iid}}{\\sim}N(\\mu,\\sigma^{2}),\\) então \\[\n\\frac{(X_{1}-\\mu)^{2}+\\dots+(X_{n}-\\mu)^{2}}{\\sigma^{2}} \\sim \\chi^{2}_{n}\n\\] Se \\(X_{1},\\dots,X_{n} \\stackrel{\\text{iid}}{\\sim}N(\\mu,\\sigma^{2}),\\) então \\[\n\\frac{(X_{1}-\\bar{X})^{2}+\\dots+(X_{n}-\\bar{X})^{2}}{\\sigma^{2}} \\sim \\chi^{2}_{n-1}\n\\]\nAdemais, se \\(Y\\sim \\chi^2_{\\nu}\\), então \\[\n\\frac{Y-\\nu}{\\sqrt{2\\nu}} \\stackrel{a}{\\approx} N(0,1)\n\\] para \\(\\nu &gt; 30\\)",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Distribuição Qui-Quadrado</span>"
    ]
  },
  {
    "objectID": "tlc.html",
    "href": "tlc.html",
    "title": "12  Teorema do Limite Central",
    "section": "",
    "text": "12.1 Exemplo\nTambém conhecido como Teorema Central do Limite, é fundamental para a teoria da probabilidade e a estatística.\nSeja \\(X_{1},\\dots,X_{n}\\) a.a de \\(X\\sim f_{\\theta},\\theta \\in \\Theta : E_{\\theta}(X^{2}) &lt; \\infty\\), então: \\[\n\\bar{X} \\stackrel{\\text{Aproximadamente}}{\\sim }N\\left(E_\\theta(X), \\frac{Var_{\\theta}{X}}{n}\\right)\n\\]\nFormalmente, temos o enunciado do Teorema do Limite Central: \\[\n\\frac{\\sqrt{n}(\\bar{X}-E_\\theta(x))}{\\sqrt{\\mathrm{Var}_\\theta(X)}} \\underset{n\\rightarrow \\infty}{\\stackrel{\\text{Distribuição}}{\\rightarrow}} N\\sim (0,1) \\forall \\theta \\in \\Theta\n\\]\nSe \\(X\\sim N(\\mu, \\sigma)\\), então a distribuição é exata.\nAdemais, seja \\(g\\) uma função contínua e diferenciável tal que \\(g'(\\theta)\\neq0\\). Então, \\[\ng(\\bar{X}) \\stackrel{\\text{Aproximadamente}}{\\sim}N\\left(g(E_\\theta(X)), \\frac{g'(E_\\theta(X))^{2}\\mathrm{Var}_\\theta(X)}{n}\\right)\n\\]\nSeja \\((X_{1},\\dots,X_{n})\\) a.a de \\(X\\sim Ber(\\theta), \\theta \\in (0,1)\\). Já vimos que EMV \\(p/ \\theta\\) é \\[\n\\bar{\\theta}(X_{1},\\dots,X_{n}) = \\bar{X}\n\\]\ne o EMV p/ \\(g(\\theta) = \\mathrm{Var}_{\\theta(x)}= \\theta(1-\\theta)\\) é: \\[\n\\widehat{g(\\theta)} = \\bar{X}(1-\\bar{X}).\n\\]\nAgora, \\[\n\\begin{aligned}\n\\bar{X} &\\stackrel{\\text{approx.}}{\\sim} N\\left(\\theta, \\frac{\\theta(1-\\theta)}{n}\\right)\\\\\ng(\\bar{X}) = \\bar{X}(1-\\bar{X}) & \\stackrel{\\text{approx}}{\\sim } N\\left(\\theta(1-\\theta),\n\\frac{[g'(\\theta)]^{2} \\theta(1-\\theta)}{n}\\right) \\\\\n&~~~\\Rightarrow~N\\left(\\theta(1-\\theta), \\frac{(1-2\\theta)^{2}\\theta(1-\\theta)}{n}\\right)\n\\end{aligned}\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Teorema do Limite Central</span>"
    ]
  },
  {
    "objectID": "estimador-intervalar.html",
    "href": "estimador-intervalar.html",
    "title": "13  Intervalo de Confiança ou Estimador Intervalar",
    "section": "",
    "text": "13.1 IC sob normalidade para \\(\\mu\\) com \\(\\sigma^2\\) conhecido\nSeja \\((X_1,\\ldots, X_n)\\) a.a. de \\(X\\sim f_\\theta, \\theta \\in \\Theta \\subseteq \\mathbb{R}\\).\nUm intervalo de confiança (IC) com coeficiente de confiança \\(\\gamma\\) é um intervalo aleatório que satisfaz: \\[\nP_{\\theta} (I_{1}(\\pmb{X}_{n}) \\leq \\theta \\leq I_{2} (\\pmb{X}_{n})) = \\gamma \\forall \\theta \\in \\Theta\n\\]\nem que \\(\\pmb{X}_{n} = (X_{1}\\ldots,X_{n})\\)\nObservação:\nSe o intervalo aleatório \\([I_{1} (\\pmb{X}_{n}),I_{2}(\\pmb{X}_{n})\\) satisfaz \\[\nP_{\\theta} (I_{1} (\\pmb{X}_{n}) \\leq \\theta \\leq I_{2} (\\pmb{X}_{n})) \\geq \\gamma \\forall \\theta \\in \\Theta\n\\]\nEntão o intervalo aleatório é um intervalo de confiança com pelo menos coeficiente de confiança \\(\\gamma\\)\nObservação: \\(I_{1} (\\pmb{X}_{n})\\) e \\(I_{2} (\\pmb{X}_{n})\\) são estatísticas e são tais que \\(I_{1} (\\pmb{X}_{n}) \\leq I_{2} (\\pmb{X}_{n})\\)\nObservação: Quando substituímos \\((\\pmb{X}_{n})\\) pela amostra observada \\((\\pmb{x}_{n})\\) temos que\n\\[\nP_{\\theta}(I_{1} (\\pmb{x}_{n}) \\leq \\theta \\leq I_{2} (\\pmb{x}_{n})) = \\begin{cases}\n1,  \\text{ se } \\theta \\in [I_{1}(\\pmb{x}_{n}), I_{2} (\\pmb{x}_{n})] \\\\\n0, \\text{ caso contrário }\n\\end{cases}\n\\]\nEm uma distribuição normal \\((\\theta, \\sigma^2)\\), por exemplo, conseguimos de forma genérica para qualquer \\(\\gamma \\in [0,1]\\) encontrar pela tabela um valor de \\(c_{\\gamma}\\) que satisfaça \\(P(-c_{\\gamma} \\leq Z \\leq c_{\\gamma})\\). Assim, \\[\n-c_{\\gamma} \\leq Z \\leq c_{\\gamma} \\Leftrightarrow -c_{\\gamma} \\leq \\frac{\\sqrt{n}(\\bar{X}-\\theta)}{\\sqrt{\\sigma^{2}}}\n\\leq c_{\\gamma} \\Leftrightarrow \\bar{X} - c_{\\gamma} \\frac{\\sqrt{\\sigma^2}}{\\sqrt{n}} \\leq \\theta \\leq \\bar{X} + c_{\\gamma}\n\\frac{\\sqrt{\\sigma^2}}{\\sqrt{n}}\n\\]\nPortanto, \\[\nP_{\\theta}\\left(\\bar{X} - c_{\\gamma} \\frac{\\sqrt{\\sigma^2}}{\\sqrt{n}} \\leq \\theta \\leq \\bar{X} + c_{\\gamma}\n\\frac{\\sqrt{\\sigma^2}}{\\sqrt{n}}\\right) = \\gamma \\forall \\theta \\in \\Theta\n\\]\nDessa forma, \\(\\bar{X} - c_{\\gamma} \\frac{\\sqrt{\\sigma^2}}{\\sqrt{n}} \\leq \\theta \\leq \\bar{X} + c_{\\gamma}\n\\frac{\\sqrt{\\sigma^2}}{\\sqrt{n}}\\) é um intervalo de confiança cujo coeficiente de confiança é \\(\\gamma\\).",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intervalo de Confiança ou Estimador Intervalar</span>"
    ]
  },
  {
    "objectID": "estimador-intervalar.html#ic-sob-normalidade-para-mu-com-sigma2-conhecido",
    "href": "estimador-intervalar.html#ic-sob-normalidade-para-mu-com-sigma2-conhecido",
    "title": "13  Intervalo de Confiança ou Estimador Intervalar",
    "section": "",
    "text": "13.1.1 Exemplo\nConsidere que a amostra observada foi \\(1, 2.2, 3, 3.5\\) de uma distribuição \\(N(\\theta, 3)\\).\nEncontre o IC observado com coeficiente de confiança de \\(99\\%\\).\nPrimeiro encontramos a média: \\(\\bar{x} = 2.42\\).\nDessa forma, \\(c_{99\\%} = 2.58\\) e nosso intervalo de confiança é \\[\n\\left[2.42 - 2.58 \\frac{\\sqrt{3}}{\\sqrt{4}}, 2.42 + 2.58\\frac{\\sqrt{3}}{\\sqrt{4}}\\right] = [0.18, 4.65]\n\\]\nDessa forma, se repetirmos o experimento \\(N\\) vezes, esperamos que \\(\\gamma = 99\\%\\) dos ICs observados contenham a quantidade de interesse.\n\n13.1.1.1 Notação\n\\[\n\\mathrm{IC}(\\theta, \\gamma) = [I_{1} (\\pmb{X}_{n}), I_{2} (\\pmb{X}_{n})]\n\\] Denotará o intervalo de confiança teórico\n\\[\n\\mathrm{IC}_{\\mathrm{Obs}}(\\theta, \\gamma) = [I_{1} (\\pmb{x}_{n}), I_{2} (\\pmb{x}_{n})]\n\\] ## IC sob normalidade para \\(\\mu\\) quando \\(\\sigma^2\\) é desconhecido\nSeja uma a.a de \\(X\\sim N(\\mu, \\sigma^2)\\) em que \\(\\mu, \\sigma^2\\) são desconhecidos. Então, o IC para \\(\\mu\\) com coeficiente de confiança \\(\\gamma\\) é dado por\n\\[\n\\mathrm{IC}(\\mu,\\gamma) = \\left[\\bar{X} - t_{\\gamma, n-1} \\sqrt{\\frac{s^2 (\\pmb{X}_{n})}{n}}, \\bar{X} +t_{\\gamma, n-1}\n\\sqrt{\\frac{s^2 (\\pmb{X}_{n})}{n}}\\right]\n\\]\nEm que \\(s^2 (\\pmb{x}_{n}) = \\frac{1}{n-1}\\sum^{k}_{i=1}(X_i - \\bar{x})^2\\) e \\(t_{y,(n-1)}\\) deve ser calculado da tabela \\(P(-t_{\\gamma, n-1} \\leq T_{n-1} \\leq t_{\\gamma, n-1}) = \\gamma\\), \\(T_{n-1} \\sim \\mathrm{t-Student}(n-1)\\)\nSe a variância for desconhecida, substituímos \\(\\sigma^2\\) pelo estimador \\[\ns^2 = \\frac{1}{n-1}\\sum\\limits^{n}_{i=1} (X_{i} - \\bar{X})^2\n\\] e o valor de \\(c_{\\gamma}\\) obtido de uma t-Student com \\(n-1\\) graus de liberdade.\nJustificativa:\nSe uma a.a. de \\(X\\sim N(\\mu,\\sigma^2)\\), \\(\\theta = (\\mu, \\sigma^2) \\in \\mathbb{R} \\times\\mathbb{R_{+}}\\).\n\n\\(\\bar{X} \\sim N (\\mu, \\frac{\\sigma^2}{n})\\)\n\\(\\sqrt{n}\\frac{\\bar{X} - \\mu}{\\sqrt{\\sigma^2}}\\sim N(0,1)\\)\n\\(\\sum\\limits^{n}_{i=1}\\frac{(X_{i} - \\bar{X})^2}{\\sqrt{\\sigma^2}} \\sim \\chi^2_{n-1}\\)\n\\(\\sqrt{n}\\frac{\\bar{X} - \\mu}{\\sqrt{s^2}} = \\sqrt{n}\\frac{\\bar{X} - \\mu}{\\sqrt{s^2}} \\cdot\n\\frac{\\sqrt{\\sigma^2}}{\\sqrt{\\sigma^2}}  = \\frac{\\sqrt{n}\\frac{\\bar{X} - \\mu}{\\sqrt{\\sigma^2}}}\n{\\sqrt{\\frac{\\sum(X_i - \\bar{X})^2}{\\sigma^2} \\cdot \\frac{1}{n-1}}}\\)\nSe \\(Z \\sim N(0,1)\\) e \\(W \\sim \\chi^{2}_{k}\\), então \\(t = \\frac{Z}{\\sqrt{\\frac{W}{k}}} \\sim t_{k}\\)\n\\(\\sqrt{n} \\frac{(\\bar{X} - \\mu)}{\\sqrt{s^2}} \\sim t_{(n-1)}\\)\n\n\n\n\n13.1.2 Exercício\nConsidere uma a.a de \\(X\\sim N(\\mu, \\sigma^2)\\), ambos parâmetros desconhecidos, em que \\(X\\) é o retorno de um ativo específico. Considere que a seguinte amostra foi observada: \\[\n(-1.3\\%, 0.4\\%, -1.7\\%, 3.2\\%, 0.7\\%, -1.6\\%, 1.0\\%, 1.5\\%, 1.2\\%, -0.6\\%)\n\\]\nEncontre o IC para \\(\\mu\\), com coeficiente de confiança \\(\\gamma = 99\\%\\)\nTemos que \\(s^2(\\pmb{x}_{n}) = 2.48\\) e \\(\\bar{x} = 0.28\\). Da tabela, \\(t_{99\\%, 9} = 3.25\\)\nSendo assim,\n\\[\n\\mathrm{IC}_{\\mathrm{Obs}}(\\mu,99\\%) = \\left[0.28 - 3.25 \\sqrt{\\frac{2.48}{10}}, 0.28 + 3.25\\sqrt{\\frac{2.48}{10}}\\right] = [-1.34, 1.9]\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intervalo de Confiança ou Estimador Intervalar</span>"
    ]
  },
  {
    "objectID": "estimador-intervalar.html#ic-sob-normalidade-para-sigma2-com-mu-desconhecido",
    "href": "estimador-intervalar.html#ic-sob-normalidade-para-sigma2-com-mu-desconhecido",
    "title": "13  Intervalo de Confiança ou Estimador Intervalar",
    "section": "13.2 IC sob normalidade para \\(\\sigma^2\\) com \\(\\mu\\) desconhecido",
    "text": "13.2 IC sob normalidade para \\(\\sigma^2\\) com \\(\\mu\\) desconhecido\nSeja uma a.a. de \\(X \\sim N(\\mu, \\sigma^2)\\) em que \\(\\mu, \\sigma^2\\) são desconhecidos.\nO intervalo de confiança para \\(\\sigma^2\\) com coeficiente de confiança \\(\\gamma\\) é dado por:\n\\[\n\\begin{aligned}\n\\mathrm{IC} (\\sigma^2, \\gamma) = \\left [\\frac{(n-1)s^2(\\pmb{X}_{n})}{q^{(2)}_{\\gamma, n-1}},\n\\frac{(n-1)s^2(\\pmb{X}_{n})}{q^{(1)}_{\\gamma, n-1}}\\right] \\\\\n\\mathrm{IC}_{\\mathrm{Obs}} (\\sigma^2, \\gamma) = \\left [\\frac{(n-1)s^2(\\pmb{x}_{n})}{q^{(2)}_{\\gamma, n-1}},\n\\frac{(n-1)s^2(\\pmb{x}_{n})}{q^{(1)}_{\\gamma, n-1}}\\right]\n\\end{aligned}\n\\]\nem que \\(s^2(\\pmb{X}_{n}) = \\frac{1}{n-1} \\sum^{n}_{i=1} (X_i - \\bar{X})^2\\) e \\(s^2(\\pmb{x}_{n}) =\n\\frac{1}{n-1} \\sum^{n}_{i=1} (x_i - \\bar{x})^2\\) E \\(q^{(1)}_{\\gamma, n-1}, q^{(2)}_{\\gamma, n-1}\\) são obtidos calculando \\(P(q^{(1)}_{\\gamma, n-1} \\leq W \\leq q^{(2)}_{\\gamma, n-1}) = \\gamma\\) no qual \\(W\\sim \\chi^{2}_{n-1}\\) e \\(P(\\chi^2_{n-1} \\leq q^{(1)}_{\\gamma, n-1}) = \\frac{1-\\gamma}{2} = P( \\chi^2_{n-1} \\geq q^{(2)}_{\\gamma,n-1})\\)\nDemonstração:\nVamos construir um IC para a variância.\nNote que \\[\\sum\\limits^{n}_{i=1}\\frac{(X_{i} - \\bar{X})^2}{\\sqrt{\\sigma^2}} \\sim \\chi^2_{n-1}\\]\nNote ainda que \\(W \\sim \\chi^{2}_{n-1}\\), ou seja, \\(P(q^{(1)}_{\\gamma, n-1} \\leq W \\leq q^{(2)}_{\\gamma, n-1}) = \\gamma\\). Assim, \\(W = \\frac{(n-1)s^2}{\\sigma^2}\\).\nDessa forma,\n\\[\n\\frac{1}{q^{(2)}_{\\gamma, n-1}} \\leq \\frac{\\sigma^2}{(n-1) s^2} \\leq \\frac{1}{q^{(1)}_{\\gamma, n-1}} \\Leftrightarrow\n\\frac{(n-1)s^2}{q^{(2)}_{\\gamma, n-1}} \\leq \\sigma^2 \\leq \\frac{(n-1)s^2}{q^{(1)}_{\\gamma, n-1}}\n\\]\nem que \\(s^2 = \\frac{1}{n-1} \\sum^{n}_{i=1} (X_i - \\bar{X})^2\\)\nPortanto, \\[ P_{\\theta}\\left (\\frac{(n-1)s^2}{q^{(2)}_{\\gamma, n-1}} \\leq \\sigma^2 \\leq \\frac{(n-1)s^2}{q^{(1)}_{\\gamma, n-1}}\\right)\n= \\gamma \\forall \\theta \\in \\Theta\\]\nExercício:\nConsidere uma a.a de \\(X\\sim N(0, \\theta), \\theta &gt; 0\\) em que \\(X\\) é o retorno de um ativo específico. Considere que a seguinte amostra foi observada: \\[\n(-1.3\\%, 0.4\\%, -1.7\\%, 3.2\\%, 0.7\\%, -1.6\\%, 1.0\\%, 1.5\\%, 1.2\\%, -0.6\\%)\n\\]\nConstrua um intervalo de confiança para a variância (populacional) \\(\\theta\\) com coeficiente de confiança \\(\\gamma = 95\\%\\).\nO IC para a variância é \\[\n\\mathrm{IC}(\\theta, \\gamma) = \\left[\\frac{(n-1)s^2 (\\pmb{x}_{n})}{q^2_{\\gamma,n-1}},\\frac{(n-1)s^2 (\\pmb{x}_{n})}{q^1_{\\gamma,n-1}}\\right]\n\\]\nem que \\(s^2 (\\pmb{x}_{n}) = \\frac{1}{n-1}\\sum^{k}_{i=1}(X_i - \\bar{x})^2\\) e \\(q^{1}_{\\gamma, n-1}, q^{2}_{\\gamma, n-1}\\) satisfazem as fronteiras que delimitam uma área de \\(\\gamma\\) em torno da média. Nesse caso, como \\(n = 10\\), \\(q^{1}_{9} = 2.7, q^{2}_{9} = 19.023\\). Dessa forma, \\(\\bar{x} = 0.28, \\sum_{i}x_{i}^2=2.308 \\Rightarrow s^2(\\pmb{x}_{n}) = \\frac{10}{9}(2.308 - 0.28^2) = 2.48\\)\nSendo assim \\[\n\\mathrm{IC}_{\\mathrm{Obs}}(\\theta, 95\\%) = \\left[\\frac{9 \\cdot 2.48}{19.023}, \\frac{9 \\cdot 2.48}{2.7}\\right] = [1.17, 8.27]\n\\]\nPodemos concluir que, com uma confiança de \\(95\\%\\), a variância está nesse intervalo.",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intervalo de Confiança ou Estimador Intervalar</span>"
    ]
  },
  {
    "objectID": "estimador-intervalar.html#intervalo-de-confiança-para-a-proporção",
    "href": "estimador-intervalar.html#intervalo-de-confiança-para-a-proporção",
    "title": "13  Intervalo de Confiança ou Estimador Intervalar",
    "section": "13.3 Intervalo de Confiança para a proporção",
    "text": "13.3 Intervalo de Confiança para a proporção\nSeja \\((\\pmb{x}_{n})\\)uma amostra aleatória de \\(X\\sim Ber(\\theta)\\) em que \\(\\theta \\in [0,1]\\). Sabemos que, pelo Teorema do Limite Central\n\\[\n\\bar{X} \\stackrel{a}{\\approx} N\\left( \\theta, \\frac{\\theta (1-\\theta)}{n} \\right)\n\\]\nDessa forma, \\[\n\\frac{\\sqrt{ n }(\\bar{X}-\\theta)}{\\sqrt{ \\theta(1-\\theta) }} \\stackrel{a}{\\approx} N(0,1)\n\\]\nAlém disso, pelo Teorema de Slutsky \\[\n\\frac{\\sqrt{ n }(\\bar{X}-\\theta)}{\\sqrt{ \\bar{X}(1-\\bar{X}) }} \\stackrel{a}{\\approx} N(0,1)\n\\]\nObserve que, se \\(Z \\sim N(0,1)\\) então \\(-c_{\\gamma}\\leq Z \\leq c_{\\gamma} \\Leftrightarrow -c_{\\gamma }\n\\leq \\frac{\\sqrt{ n }(\\bar{X}-\\theta)}{\\sqrt{ \\bar{X}(1-\\bar{X}) }}\\leq c_{\\gamma}\\) \\[\n\\Leftrightarrow \\bar{X} - c_{\\gamma} \\sqrt{ \\frac{\\bar{X} (1-\\bar{X})}{n} } \\leq \\theta \\leq \\bar{X} + c_{\\gamma}\n\\sqrt{\\frac{\\bar{X} (1-\\bar{X})}{n}}\n\\]\nEntão, seja \\(c_{\\gamma}\\) tal que \\[\n\\begin{aligned}\n&P (-c_{\\gamma}\\leq N(0,1)\\leq c_{\\gamma }) = \\gamma \\\\\n\\Rightarrow &P_{\\theta}\\left(\\bar{X} - c_{\\gamma} \\sqrt{ \\frac{\\bar{X} (1-\\bar{X})}{n} } \\leq \\theta \\leq\n\\bar{X} + c_{\\gamma} \\sqrt{ \\frac{\\bar{X} (1-\\bar{X})}{n} }\\right) \\approx \\gamma~ \\forall \\theta \\in \\Theta = [0,1]\n\\end{aligned}\n\\]\nque melhora conforme \\(n \\rightarrow \\infty\\)\nLogo, um IC aproximado com coeficiente de confiança \\(\\gamma\\) é dado por \\[\nIC(\\theta, \\gamma) = \\left[\\bar{X} - c_{\\gamma} \\sqrt{ \\frac{\\bar{X} (1-\\bar{X})}{n} }, \\bar{X} + c_{\\gamma}\n\\sqrt{ \\frac{\\bar{X} (1-\\bar{X})}{n} }\\right] \\cap \\Theta\n\\]\n\n13.3.1 Exemplo\nSeja \\((\\pmb x_{n })\\) uma amostra aleatória de \\(X\\sim \\mathrm{Ber} (\\theta)\\) em que \\(\\theta \\in [0,1]\\). \\[\nX(w) = \\begin{cases}\n1, \\text{ se w disser que vota no candidato} \\\\ \\\\\n0, c.c\n\\end{cases}\n\\] A amostra observada foi \\((0,0,0,1,0,0,0,1)\\). Encontre o IC para a proporção de intenção de votos no candidato considerando \\(\\gamma = 99\\%\\)\n\\(\\bar{x} = \\frac{1}{4 }, \\bar{x}(1-\\bar{x}) = \\frac{3}{16}, n = 8, c_{\\gamma} = 2.58\\)\n\\[\n\\begin{aligned}\n\\mathrm{IC}_{\\mathrm{Obs}}(\\theta, 99\\%) &= \\left[0.25- 2.58 \\frac{\\sqrt{3}}{4\\sqrt{8}}, 0.25 - 2.58\n\\frac{\\sqrt{3}}{4 \\sqrt{8}}\\right] \\cap [0,1] \\\\\n&= [0.25 - 0.39, 0.25 + 0.39] \\cap [0,1]  \\\\\n&= [0,0.64]\n\\end{aligned}\n\\]\n\n\n13.3.2 Intervalos conservadores e otimistas\nUm intervalo de confiança de proporção é dito ser conservador quando o calculamos tomando \\(\\theta\\) Na variância (\\(\\theta(1-\\theta)\\)) como o valor mais alto possível. No exemplo Bernoulli, o valor máximo para \\(\\theta\\) (derivando \\(\\theta(1-\\theta)\\) e igualando a 0, \\(1-2 \\theta = 0\\)) é \\(\\theta=0.5\\). Dessa forma, o IC conservador é calculado usando \\(\\theta=0.5\\).\nPor sua vez, um IC otimista é calculado usando o valor de \\(\\theta\\) obtido através do EMV para \\(\\theta\\), no caso Bernoulli, usaríamos essa variância como \\(\\bar{X}(1-\\bar{X})\\)",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intervalo de Confiança ou Estimador Intervalar</span>"
    ]
  },
  {
    "objectID": "estimador-intervalar.html#como-interpretar-intervalos-de-confiança",
    "href": "estimador-intervalar.html#como-interpretar-intervalos-de-confiança",
    "title": "13  Intervalo de Confiança ou Estimador Intervalar",
    "section": "13.4 Como interpretar intervalos de confiança",
    "text": "13.4 Como interpretar intervalos de confiança\nImportante:\nNa estatística clássica (frequentista), devemos interpretar um intervalo de confiança \\([a,b]\\) com \\(\\gamma=0.95\\) da seguinte forma:\n“Com \\(95\\%\\) de confiança, o intervalo \\([a,b]\\) conterá o valor da quantidade de interesse”.\nIsso é importante para diferenciar a interpretação frequentista (Theta do espaço paramétrico) da Bayesiana (Theta como variável aleatória). Dessa forma, estaria incorreto na estatística clássica dizer que\n“O intervalo \\([a,b]\\) conterá a quantidade de interesse com probabilidade \\(95\\%\\)” ou “O intervalo \\([a,b]\\) conterá a quantidade de interesse \\(95\\%\\) das vezes”",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intervalo de Confiança ou Estimador Intervalar</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html",
    "href": "teste-hipotese.html",
    "title": "14  Teste de Hipótese simples",
    "section": "",
    "text": "14.1 Etapas de um teste de hipótese\nUm dos principais objetivos da estática é testar hipóteses. Veja algumas dessas hipóteses potenciais: 1. A moeda é honesta? 2. O medicamento proposto é melhor que o vendido no mercado? 3. O número médio de acidentes aumentou em relação ao ano passado? 4. A altura interfere na performance num determinado esporte? 5. Um suspeito é culpado? 6. O mercado financeiro está em equilíbrio? 7. Dona Maria terá dinheiro para comprar o pão do próximo mês?",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#etapas-de-um-teste-de-hipótese",
    "href": "teste-hipotese.html#etapas-de-um-teste-de-hipótese",
    "title": "14  Teste de Hipótese simples",
    "section": "",
    "text": "Formular as hipóteses de interesse;\n\nNa estatística clássica, pela abordagem Fisheriana (uma hipótese) ou Neyman-Pearson (mais de uma hipótese).\n\nObservar dados experimentais do estudo relacionado ao problema;\nElaborar uma conclusão utilizando um procedimento estatístico.",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#exemplo-da-hipótese-5.",
    "href": "teste-hipotese.html#exemplo-da-hipótese-5.",
    "title": "14  Teste de Hipótese simples",
    "section": "14.2 Exemplo da hipótese 5.",
    "text": "14.2 Exemplo da hipótese 5.\nConsidere uma pessoa que está sendo acusada de ter cometido um crime.\nAs duas hipóteses envolvidas aqui são (abordagem de Neyman-Pearson): 1. “O suspeito não é culpado” -&gt; \\(h_{0}\\) Hipótese Nula ou de não-efeito; 2. “O suspeito é culpado” -&gt; \\(h_{1}\\) Hipótese alternativa ou hipótese que contém o efeito.\n\nApós coletar as evidências, dizemos que, se houver evidências de que o suspeito cometeu o crime, a pessoa é culpada.\nSe não, concluímos que não é culpado.\n\nContudo, devemos nos atentar aos erros de decisão\n\n14.2.1 Erros de decisão\n\\[\n\\begin{array}{c|cc}\n& H_{0} & H_{1} \\\\\n\\hline\n\\text{Decisão}  & \\text{Não cometeu o crime}  & \\text{Cometeu o crime}\\\\\n\\hline\n\\text{Inocente} & \\text{Acerto} & \\text{Erro Tipo II}\\\\\n\\text{Culpado} & \\text{Erro Tipo I} & \\text{Acerto}\\\\\n\\hline\n\\end{array}\n\\] - Erro Tipo I: Decidir que o acusado é culpado quando na verdade é inocente (Rejeitar \\(H_{0}\\)). - Erro Tipo II: Decidir que o acusado é inocente quando na verdade é culpado (Rejeitar \\(H_{1}\\)).",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#exemplo-da-hipótese-1.",
    "href": "teste-hipotese.html#exemplo-da-hipótese-1.",
    "title": "14  Teste de Hipótese simples",
    "section": "14.3 Exemplo da hipótese 1.",
    "text": "14.3 Exemplo da hipótese 1.\nEstamos interessados em verificar se uma moeda é honesta. Executaremos \\(n\\) experimentos de Bernoulli e verificaremos se a face voltada para cima após o lançamento é cara. Dessa forma, sendo \\(X\\) o resultado dum lançamento, teremos a a.a \\((\\pmb{X}_{n})\\) de \\(X\\sim \\mathrm{Ber}(\\theta), \\theta \\in \\Theta = [0,1]\\). Suspeitamos que a moeda é honesta ou que \\(\\theta = 0.9\\).\nNossas hipóteses são 1. \\(H_{0}\\) -&gt; \\(\\theta = 0.5\\) 2. \\(H_{1}\\) -&gt; \\(\\theta = 0.9\\)\n\n14.3.1 Erros Tipo I e II\nNote que \\(\\bar{X}\\) é um estimador para \\(\\theta\\) e \\(\\bar{x}\\) é uma estimativa. - Se \\(\\bar{x}&gt;0.7\\), rejeitaremos a hipótese nula \\(h_{0}\\), a moeda não seria honesta e haveria um viés agindo sobre seus lançamentos. - Se \\(\\bar{x}\\leq 0.7\\), concluiremos que a moeda é honesta. \\[\n\\begin{array}{c|cc}\n& H_{0} & H_{1} \\\\\n\\hline\n\\text{Decisão}  & \\text{Honesta} & \\text{Viesada}\\\\\n\\hline\n\\bar{x} &lt; 0.7 & \\text{Acerto} & \\text{Erro Tipo II}\\\\\n\\bar{x} \\geq 0.7& \\text{Erro Tipo I} & \\text{Acerto}\\\\\n\\hline\n\\end{array}\n\\]\n\nErro Tipo I: Rejeitar que a moeda é honesta (rejeitar \\(h_{0}\\)) quando na verdade é.\nErro Tipo II: Rejeitar que a moeda é enviesada (rejeitar \\(h_{1}\\)) quando na verdade é.\n\nCalculando a probabilidade dos erros \\[\n\\begin{aligned}\nP(\\text{Erro Tipo I}) &= P(\\text{Probabilidade de rejeitar $h_{0}$}|\\text{$h_{0}$ é verdadeiro}) \\\\\n\\text{Incorreto na Estatística Clássica} &= P(\\bar{X} &gt; 0.7 | \\theta = 0.5) \\\\\n\\text{Correto na Estatística Clássica} & = P_{0.5}(\\bar{X} &gt; 0.7)\n\\end{aligned}\n\\]\nNa segunda notação, correta na estatística frequentista, \\(P\\) está sob a hipótese nula \\(h_{0}\\) verdadeira. Dessa forma \\[\nP(\\text{Erro Tipo II}) = P_{0.9}(\\bar{X}\\leq 0.7)\n\\]\nCalcule as probabilidades dos erros considerando \\(n=10\\) e a aproximação pela distribuição normal.\n\n14.3.1.1 Calculando Exato e pela aproximação do Teorema do Limite Central\nSabemos que \\(\\sum ^{n}_{i=1}X_{i}\\sim \\mathrm{Bin}(n, \\theta)\\), logo \\[\n\\begin{aligned}\n\\alpha &= P(\\text{Erro Tipo I}) \\stackrel{\\text{Sob $h_{0}$}}{=} P_{0.5}\\left( \\frac{1}{n} \\sum^n_{i=1}X_{i}&gt; 0.7\\right)\n= P_{0.5}\\left( \\sum^{10}_{i=1}X_{i} &gt; 7 \\right) \\\\\n&= P_{0.5}\\left( \\sum^{10}_{i=1} X_{i} \\geq 8 \\right) \\\\\n&= \\binom{10}{8} 0.5^{8} \\cdot 0.5^{2} + \\binom{10}{9} 0.5^{9} \\cdot 0.5^{1} + \\binom{10}{10} 0.5^{10} \\approx 0.05469 \\\\\n\\alpha & = P_{0.5}\\left( \\sqrt{ \\frac{n}{0.25}}(\\bar{X}-0.5) &gt;\\sqrt{ \\frac{n}{0.25}}(0.7-0.5) \\right) \\\\\n& \\approx P(N(0,1) &gt; 1.26) \\approx 0.103\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\beta &= P(\\text{Erro Tipo II}) \\stackrel{\\text{Sob $h_{1}$}}{=} P_{0.9}\\left( \\frac{1}{n} \\sum^n_{i=1}X_{i}\\leq 0.7\\right)\n= P_{0.9}\\left( \\sum^{10}_{i=1}X_{i} \\leq 7 \\right) \\\\\n&= 1-P_{0.9}\\left( \\sum^{10}_{i=1} X_{i} \\geq 8 \\right) \\\\\n&= 1-\\left(\\binom{10}{8} 0.9^{8} \\cdot 0.9^{2} + \\binom{10}{9} 0.9^{9} \\cdot 0.9^{1} + \\binom{10}{10} 0.9^{10}\\right)\n\\approx 0.0702 \\\\\n\\alpha & = P_{0.9}\\left( \\sqrt{ \\frac{n}{0.09}}(\\bar{X}-0.9) \\leq \\sqrt{ \\frac{n}{0.09}}(0.7-0.9) \\right) \\\\\n& \\approx P(N(0,1) \\leq -2.1) \\approx 0.018\n\\end{aligned}\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#poder-do-teste",
    "href": "teste-hipotese.html#poder-do-teste",
    "title": "14  Teste de Hipótese simples",
    "section": "14.4 Poder do teste",
    "text": "14.4 Poder do teste\nChamamos de poder do teste a probabilidade de rejeitar \\(h_{0}\\) quando este é falso.\nNo exemplo anterior, \\[\n\\pi = P_{0.9}(\\bar{X} &gt; 0.7) = 1-P_{0.9}(\\bar{X}\\leq 0.7) = 1 - \\beta = 92.92\\%\n\\]\nConsidere nesse exemplo uma a.a do lançamento de quatro moedas: \\((\\pmb{x}_{10}) = (1, 0, 1, 0,0,1,1,0,0,0)\\). Como \\(\\bar{x}=0.4 \\leq 0.7\\), não rejeitamos a hipótese nula \\(h_{0}\\).\nEm uma outra amostra, \\((\\pmb{x}_{10}) = (0,0,1,1,1,1,1,1,1,1)\\). Como \\(\\bar{x}=0.8 &gt; 0.7\\), rejeitamos a hipótese nula \\(h_{0}\\).",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#um-exemplo-mais-amplo-diferença",
    "href": "teste-hipotese.html#um-exemplo-mais-amplo-diferença",
    "title": "14  Teste de Hipótese simples",
    "section": "14.5 Um exemplo mais amplo (Diferença)",
    "text": "14.5 Um exemplo mais amplo (Diferença)\nSeja \\((X_{n})\\) uma a.a. de \\(X\\sim\\mathrm{Ber}(\\theta)\\), em que \\(\\theta \\in (0,1)\\). Considere as hipóteses: \\[\n\\begin{cases}\nH_{0}: \\theta =0.5 \\\\\nH_{1}: \\theta \\neq 0.5\n\\end{cases}\n\\]\nDecisões elaboradas:\n\nSe \\(\\bar{x}&lt;0.3\\) ou \\(\\bar{x} &gt; 0.7\\), rejeitamos \\(H_{0}\\)\nCaso contrário, não rejeitamos \\(H_0\\)\n\nRelembrando: \\(\\alpha\\) = Probabilidade do Erro Tipo I (Rejeitar um \\(H_0\\) verdadeiro). \\(\\beta\\) Probabilidade do Erro Tipo II (Rejeitar um \\(H1\\) verdadeiro). \\(\\pi =\\) Poder do Teste. Lembre-se que \\(\\sum^n_{i=1}X_{i}\\sim \\mathrm{Bin}(n,\\theta)\\). Tome \\(n =10\\).\n\\[\n\\begin{aligned}\n\\alpha &= P_{\\theta=0.5}(\\bar{X} &lt; 0.3 \\text{ ou } \\bar{X} &gt; 0.7) = P_{0.5}(\\bar{X}&lt;0.3) + P_{0.5}(\\bar{X}&gt;0.7) \\\\\n&=P(\\mathrm{Bin}(n,0.5) &lt; 3) + P(\\mathrm{Bin}(10,0.5)&gt;7) \\\\\n&=P(\\mathrm{Bin}(n,0.5) \\leq 2) + P(\\mathrm{Bin}(10,0.5)\\geq 8) \\\\\n&= 0.055 +0.055 = 0.11 \\\\\n\\end{aligned}\n\\]\nNote que para o Erro Tipo II, não existe uma única probabilidade para o erro sob \\(H_1\\). Optaremos por tentar calcular seu máximo. \\(\\beta_{\\max}\\) \\[\n\\begin{aligned}\n\\beta &= P_{\\theta} (0.3\\leq \\bar{X} \\leq 0.7) \\theta \\in \\Theta \\setminus \\{ 0.5 \\} \\\\\n\\beta_{\\max} &= \\sup_{\\theta \\in \\Theta \\setminus \\{ 0.5 \\}} \\beta(\\theta)\n\\end{aligned}\n\\]\nPara \\(n=10\\), \\[\n\\beta(\\theta) = P_{\\theta}\\left( 3\\leq \\sum^{n=10}_{i=1} X_{i} \\leq 7 \\right) = P\\left( 3\\leq \\mathrm{Bin}(10,\\theta)\n\\leq 7 \\right), \\theta \\in \\Theta \\setminus \\{ 0.5 \\}\n\\]\nPodemos encontrar o valor que maximiza \\(\\beta(\\theta)\\), \\(\\theta =0.5\\) derivando.",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#hipóteses-como-subconjuntos-do-espaço-paramétrico",
    "href": "teste-hipotese.html#hipóteses-como-subconjuntos-do-espaço-paramétrico",
    "title": "14  Teste de Hipótese simples",
    "section": "14.6 Hipóteses como subconjuntos do espaço paramétrico",
    "text": "14.6 Hipóteses como subconjuntos do espaço paramétrico\nSeja \\((X_{n})\\) uma a.a. de \\(X\\sim\\mathrm{Ber}(\\theta)\\), em que \\(\\theta \\in (0,1)\\). Considere as hipóteses: \\[\n\\begin{cases}\nH_{0}: \\theta \\in \\Theta_{0} \\\\\nH_{1}: \\theta \\in \\Theta_{1}\n\\end{cases}\n\\]\nem que \\(\\Theta_{0} \\cup \\Theta_{1} = \\Theta, \\Theta_{0},\\Theta_{1} \\neq \\emptyset, \\Theta_{0}\\cap\\Theta_{1}= \\emptyset\\).\nExemplos de decisões elaboráveis: \\[\n\\begin{aligned}\n&\\begin{cases}\n\\Theta_{0}=\\{ 0.5 \\} \\\\\n\\Theta_{1} = \\left( 0, \\frac{1}{2} \\right) \\cup \\left( \\frac{1}{2}, 1 \\right)\n\\end{cases} \\Rightarrow\n\\begin{cases}\nH_{0}: \\theta=0.5 \\\\\nH_{1}: \\theta \\neq 0.5 \\\\\n\\end{cases} \\text{ Hipótese alternativa bilateral} \\\\\n&\\begin{cases}\n\\Theta_{0}= \\left(  0, \\frac{1}{2} \\right] \\\\\n\\Theta_{1} = \\left( \\frac{1}{2}, 1 \\right)\n\\end{cases} \\Rightarrow\n\\begin{cases}\nH_{0}: \\theta \\leq 0.5\\\\\nH_{1}: \\theta &gt; 0.5 \\\\\n\\end{cases} \\text{ Hipótese alternativa unilateral} \\\\\n&\\begin{cases}\n\\Theta_{0}= \\left[ \\frac{1}{2}, 1\\right)   \\\\\n\\Theta_{1} = \\left( 0, \\frac{1}{2} \\right)\n\\end{cases} \\Rightarrow\n\\begin{cases}\nH_{0}: \\theta \\geq 0.5 \\\\\nH_{1}: \\theta &lt; 0.5 \\\\\n\\end{cases} \\text{ Hipótese alternativa uniteral}\n\\end{aligned}\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#função-poder",
    "href": "teste-hipotese.html#função-poder",
    "title": "14  Teste de Hipótese simples",
    "section": "14.7 Função poder",
    "text": "14.7 Função poder\nNo caso geral, calculamos a função poder definida por \\[\n\\pi (\\theta) = P_{\\theta}(\\{ \\text{Rejeitar } H_{0} \\}), \\theta \\in \\Theta\n\\]\nem que “Rejeitar \\(H_0\\)” é o procedimento de decisão para rejeitar \\(H_0\\).\nA partir da função poder conseguimos calcular as probabilidades máximas de cometer os erros tipo I e II.\nProbabilidade Máxima do Erro Tipo I: \\[\n\\alpha_{\\max} = \\sup_{\\theta \\in \\Theta_{0}}(\\pi(\\theta))\n\\]\nProbabilidade Máxima do Erro Tipo II: \\[\n\\beta_{\\max} = \\sup_{\\theta \\in \\Theta_{1}}[1-\\pi(\\theta)]\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#um-exemplo-do-cálculo-de-erros-com-hipótese-unilateral",
    "href": "teste-hipotese.html#um-exemplo-do-cálculo-de-erros-com-hipótese-unilateral",
    "title": "14  Teste de Hipótese simples",
    "section": "14.8 Um exemplo do cálculo de erros com hipótese unilateral",
    "text": "14.8 Um exemplo do cálculo de erros com hipótese unilateral\nSeja \\((X_{n})\\) uma a.a. de \\(X\\sim\\mathrm{Ber}(\\theta)\\), em que \\(\\theta \\in (0,1) = \\Theta\\). Considere as hipóteses: \\[\n\\begin{cases}\nH_{0}: \\theta \\geq 0.6\\\\\nH_{1}: \\theta &lt; 0.6\n\\end{cases}\n\\]\nPrecisamos de decisões que fazem sentido. Uma delas seria\n\nSe \\(\\bar{x}&lt;0.4\\), rejeitamos \\(H_{0}\\)\nSe \\(\\bar{x} \\geq 0.4\\), não rejeitamos \\(H_{0}\\)\n\nVamos calcular as probabilidades máximas dos erros I e II.\nPrimeiro, encontramos a função poder \\[\n\\pi(\\theta) = P_{\\theta}(\\bar{X}&lt;0.4)\n\\]\nComo \\(\\sum^n_{i=1}X_{i} \\sim \\mathrm{Bin}(n,\\theta)\\), temos que \\[\n\\pi(\\theta) = P_{\\theta} \\left( \\sum^n_{i=1}X_{i} &lt; 0.4 \\cdot n \\right) = P(\\mathrm{Bin}(n,\\theta) &lt; 0.4 \\cdot n)\n\\]\nRelembrando: \\[\n\\begin{aligned}\n\\alpha_{\\max} &= \\sup_{\\theta \\in [0.6,1)} \\pi(\\theta) \\\\\n&= \\sup_{\\theta \\in [0.6,1)} P(\\mathrm{Bin}(n, \\theta) &lt; 0.4 \\cdot n) \\\\\n\\beta_{\\max} &= \\sup_{\\theta \\in (0,0.6)} (1-\\pi(\\theta)) \\\\\n&= \\sup_{\\theta \\in (0,0.6)}P(\\mathrm{Bin}(n,\\theta)\\geq 0.4 \\cdot n)\n\\end{aligned}\n\\]\nPara \\(n = 2\\), \\[\n\\begin{aligned}\n\\alpha_{\\max} &= \\sup_{\\theta \\in [0.6,1)} \\pi(\\theta) \\\\\n&= \\sup_{\\theta \\in [0.6,1)} P(\\mathrm{Bin}(2, \\theta) &lt; 0.4 \\cdot 2) \\\\\n&= \\sup_{\\theta \\in [0.6,1)} P(\\mathrm{Bin(2,\\theta)}  = 0)\\\\\n\\beta_{\\max} &= \\sup_{\\theta \\in (0,0.6)} (1-\\pi(\\theta)) \\\\\n&= \\sup_{\\theta \\in (0,0.6)}P(\\mathrm{Bin}(2,\\theta)\\geq 0.8 \\cdot n) \\\\\n&= \\sup_{\\theta \\in (0,0.6)}P(\\mathrm{Bin}(2,\\theta) \\geq 1) \\\\\n&= \\sup_{\\theta \\in (0,0.6)}[1-P(\\mathrm{Bin}(2,\\theta) = 0)]\n\\end{aligned} ~~~ \\Rightarrow ~~~~~\n\\begin{aligned}\n\\alpha_{\\max} &= \\sup_{\\theta \\in [0.6,1]}\\left[\\binom{2}{0} \\theta^0 (1-\\theta)^2\\right] \\\\\n&=\\sup_{\\theta \\in [0.6,1]} (1-\\theta)^2 \\\\\n\\beta_{\\max} &= \\sup_{\\theta \\in (0,0.6)} (1-(1-\\theta)^2)\n\\end{aligned}\n\\]\nPodemos analisar os gráficos:\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComo é uma função decrescente, seu supremo está no ponto \\(0.6\\)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComo é uma função crescente, seu supremo está também no \\(0.6\\) Portanto, \\[\n\\begin{aligned}\n\\alpha_{\\max}&=(1-0.6)^2 = 0.16\\\\\n\\beta_{\\max} &= (1-(1-0.6)^2) = 0.84\n\\end{aligned}\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#teste-sob-normalidade",
    "href": "teste-hipotese.html#teste-sob-normalidade",
    "title": "14  Teste de Hipótese simples",
    "section": "14.9 Teste sob Normalidade",
    "text": "14.9 Teste sob Normalidade\nSeja \\((x_{n})\\) amostra aleatória de \\(X\\sim N(\\mu,\\sigma^2)\\) em que \\(\\sigma^2\\) é conhecido. Considere as hipóteses \\[\n\\begin{cases}\nH_{0}: \\mu = \\mu_{0}\\\\\nH_{1}: \\mu \\neq \\mu_{0}\n\\end{cases}\n\\] com \\(\\mu_{0} \\in \\mathbb{R}\\) e fixado.\nCalcule as probabilidades (máximas) dos erros tipo I e II, para as seguintes decisões\n\nSe \\(\\bar{x} &lt; \\mu_{0} - 1.96\\sqrt{\\frac{\\sigma^2}{n}}\\) ou \\(\\bar{x} &gt; \\mu+1.96 \\sqrt{ \\frac{\\sigma^2}{n} }\\), então rejeitamos \\(H_{0}\\)\nCaso contrário, não rejeitamos \\(H_0\\)\n\nTemos a função poder \\[\n\\pi(\\theta) = P_{\\theta}(\\mathrm{Rejeitar} H_{0}) = P_{\\theta}\\left(\\bar{X}&lt;\\mu_{0} - 1.96 \\sqrt{\\frac{\\sigma^2}{n}}\\right)\n+P_{\\theta}\\left( \\bar{X} &gt; \\mu_{0} + 1.96 \\sqrt{  \\frac{\\sigma^2}{n} } \\right)\n\\]\nem que \\(theta = \\mu \\in \\mathbb{R}\\). Portanto, \\[\n\\alpha_{\\max} = \\sup_{\\theta \\in \\Theta_{0}} \\pi(\\theta)\n\\]\nComo \\(H_{0}=\\mu=\\mu_{0} \\Leftrightarrow H_{0}: \\theta \\in \\Theta\\), em que \\(\\Theta_{0}=\\{ \\mu_{0} \\}\\), logo, \\(\\sup_{\\theta \\in \\Theta_{0}} = \\mu_{0}\\) Portanto, temos que \\[\n\\alpha_{\\max} = \\pi(\\mu_{0}) = P_{\\mu_{0}}\\left( \\bar{X}&lt;\\mu_{0}-1.96\\sqrt{ \\frac{\\sigma^2}{n} } \\right) +\nP_{\\mu_{0}}\\left( \\bar{X}&gt;\\mu_{0} + 1.96 \\sqrt{  \\frac{\\sigma^2}{n} } \\right)\n\\]\nSabemos que, pelo enunciado \\(\\bar{X} \\sim N\\left( \\mu, \\frac{\\sigma^2}{n} \\right) \\forall \\mu \\in \\mathbb{R}\\) sob \\(H_{0}\\), ou seja, quando \\(\\mu= \\mu_{0}\\) temos que \\(\\bar{X}\\sim N\\left( \\mu_{0}, \\frac{\\sigma^2}{n} \\right)\\). Note que \\(P_{\\mu_{0}}\\left( \\bar{X}&lt;\\mu_{0}-1.96 \\sqrt{ \\frac{\\sigma^2}{n} } \\right) =\nP_{\\mu_{0}}\\left(\\frac{\\bar{X}-\\mu_{0}}{\\sqrt{ \\frac{\\sigma^2}{n} }} \\right) = 2.5\\%\\) Pela simetria da distribuição normal, \\(P_{\\mu_{0}}\\left( \\bar{X}&gt;\\mu_{0}+1.96 \\sqrt{ \\frac{\\sigma^2}{n} } \\right) = 2.5\\%\\) Portanto a probabilidade máxima do erro tipo 1 é \\[\n\\alpha_{\\max} = 2.5\\% + 2.5\\% = 5.0\\%\n\\]\nComo \\(H_{1}: \\mu \\neq \\mu_{0} \\Leftrightarrow H_{1}: \\theta \\in \\Theta_{1}\\), em que \\(\\Theta_{1} = \\mathbb{R} \\setminus \\{\\mu_{0}\\}\\), temos que\n\\[\n\\begin{aligned}\n\\beta_{\\max} &= \\sup_{\\theta \\in \\Theta_{1}}[1-\\pi(\\theta)] \\\\\n\\pi(\\theta) &= P_{\\theta}\\left( \\bar{X} &lt; \\mu_{0} - 1.96 \\sqrt{\\frac{\\sigma^2}{n}} \\right) +\nP_{\\theta}\\left(\\bar{X}&gt;\\mu_{0}+1.96\\sqrt{\\frac{\\sigma^2}{n}}\\right)\n\\end{aligned}\n\\]\nSabemos que \\(\\bar{X} \\sim \\mathrm{N}\\left( \\mu, \\frac{\\sigma^2}{n} \\right)\\) para todo \\(\\mu \\in \\mathbb{R}\\). Assim, \\[\n\\begin{aligned}\n\\pi(\\theta) &= P_{\\theta}\\left( \\bar{X}&lt;\\mu_{0}-1.96 \\sqrt{  \\frac{\\sigma^2}{n} } \\right) +\nP_{\\theta}\\left( \\bar{X} &gt; \\mu_{0} + 1.96 \\sqrt{  \\frac{\\sigma^2}{n} } \\right) \\\\\n&= P_{\\theta}\\left( \\frac{\\bar{X}-\\theta}{\\sqrt{ \\frac{\\sigma^2}{n} }} &lt;\n\\frac{\\mu_{0}-\\theta-1.96 \\sqrt{  \\frac{\\sigma^2}{n} } }{\\sqrt{  \\frac{\\sigma^2}{n} }}\\right) +\nP_{\\theta}\\left( \\frac{\\bar{X}-\\theta}{\\sqrt{ \\frac{\\sigma^2}{n} }} &gt;\n\\frac{\\mu_{0}-\\theta+1.96 \\sqrt{\\frac{\\sigma^2}{n}}}{\\sqrt{\\frac{\\sigma^2}{n}}}\\right)\n\\end{aligned}\n\\]\nDessa forma, \\[\n\\beta_{max} = \\sup_{\\theta \\in \\Theta_{1}}[1-\\pi(\\theta)] = 1 - \\inf_{\\theta \\in \\Theta_{1}} \\pi(\\theta)\n\\]\nOu seja, o supremo dessa expressão é dado por 1 - o ínfimo da função poder, o que significa que queremos encontrar o valor de \\(\\theta\\) para o qual \\(P_{\\theta}\\left( \\frac{\\bar{X}-\\theta}{\\sqrt{ \\frac{\\sigma^2}{n} }} &lt;\n\\frac{\\mu_{0}-\\theta-1.96 \\sqrt{\\frac{\\sigma^2}{n} } }{\\sqrt{  \\frac{\\sigma^2}{n} }}\\right) +\nP_{\\theta}\\left(\\frac{\\bar{X}-\\theta}{\\sqrt{ \\frac{\\sigma^2}{n} }} &gt;\n\\frac{\\mu_{0}-\\theta+1.96 \\sqrt{\\frac{\\sigma^2}{n}}}{\\sqrt{\\frac{\\sigma^2}{n}}}\\right)\\) é o menor possível.\nVamos usar \\(\\mu_0 = 7\\) e \\(\\sigma^2 = 5\\) para visualizarmos o comportamento de \\(\\alpha\\) e \\(\\beta\\)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n14.9.1 Um outro exemplo\nSeja \\((X_{n})\\) amostra aleatória de \\(X\\sim \\mathrm{N}(\\mu,\\sigma^2)\\) com \\(\\sigma^2 = 5, n = 10\\) Com as hipóteses \\[\n\\begin{cases}\nH_{0} : \\mu = 10 \\\\\nH_{1}: \\mu \\neq 10\n\\end{cases}\n\\] Com as decisões 1. Rejeitamos \\(H_{0}\\) se \\(\\bar{x} &gt; 10 + 1.96 \\sqrt{  \\frac{5}{10} }\\) ou \\(\\bar{x} &lt; 10 - 1.96 \\sqrt{  \\frac{5}{10} }\\) Foram observados os seguintes valores \\[\n\\begin{array}{c}\n7.1 & 8.9 & 12 & 13 & 11.7 \\\\\n6.1 & 2.5 & 3.1 & 5.2 & 7\n\\end{array}\n\\] Temos então que \\(\\bar{x} = 7.66\\) que, como é abaixo de \\(8.6\\), rejeitamos a hipótese nula de que \\(\\mu = 10\\)",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#exemplos",
    "href": "teste-hipotese.html#exemplos",
    "title": "14  Teste de Hipótese simples",
    "section": "15.1 Exemplos",
    "text": "15.1 Exemplos\n\n15.1.1 Eexemplo um\nSeja \\((X_{n})\\) a.a de \\(X\\sim \\mathrm{N}(\\mu,\\sigma^2)\\) em que \\(\\sigma^2\\) é conhecido. Considere (\\(\\mu_{0}\\) fixado)\n\\[\n\\begin{cases}\nH_{0} : \\mu = \\mu_{0} \\\\\nH_{1}: \\mu \\neq \\mu_{0}\n\\end{cases}\n\\]\n\nConstrua uma decisão para rejeitar \\(H_0\\) que produza no máximo \\(\\alpha = 5\\%\\) (que tenha nível de significância de \\(5\\%\\))\nComo a hipótese alternativa é bilateral, \\(H_{1}:\\mu\\neq \\mu_{0}\\) e \\(\\bar{x}\\) é a EMV para o parâmetro \\(\\mu\\) - a esperança da distribuição Normal - definimos a regra: Se \\(\\bar{x}&lt; x_{a}\\) ou \\(\\bar{x}&gt; x_{b}\\), rejeitamos \\(H_{0}\\). Caso contrário, não rejeitamos. \\[\n\\begin{aligned}\n     \\alpha_{\\max} &= \\sup_{\\theta \\in \\Theta_{0}} P_{\\theta}(\\text{Rejeitar }H_{0}), \\Theta_{0} = \\{ \\mu_{0} \\} \\\\\n     &= P_{\\mu_{0}}(\\bar{X}&lt;x_{a}) + P_{\\mu_{0}}(\\bar{X}&gt;x_{b}) \\leq 5\\%\n\\end{aligned}\n\\]\nNote que \\(\\bar{X} \\sim \\mathrm{N}\\left(\\mu_{0}, \\frac{\\sigma^{2}}{n} \\right)\\), sob \\(H_{0}\\)\nLogo, \\[\n\\begin{aligned}\n     \\alpha_{\\max} &= P\\left( \\mathrm{N}(0,1) &lt; \\frac{{x_{a}-\\mu_{0}}}{\\sqrt{ \\frac{\\sigma^2}{n} }} \\right)+\nP\\left( \\mathrm{N}(0,1) &gt; \\frac{{x_{a}-\\mu_{0}}}{\\sqrt{ \\frac{\\sigma^2}{n} }} \\right)\n\\end{aligned}\n\\]\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTomando \\(\\frac{{x_{a}-\\mu_{0}}}{\\sqrt{\\frac{\\sigma^2}{n}}} = -1.96\\) e \\(\\frac{{x_{b}-\\mu_{0}}}{\\sqrt{ \\frac{\\sigma^2}{n}}}\n= 1.96\\) (tabela normal padrão simétrica), temos que \\(\\alpha_{\\max}=5\\%\\). Assim, resolvendo as equações, \\[\n\\begin{cases}\n     x_{a} = \\mu - 1.96 \\sqrt{\\frac{\\sigma^2}{n}} \\\\\n     x_{b} = \\mu + 1.96 \\sqrt{\\frac{\\sigma^2}{n}}\n\\end{cases}\n\\]\n\nConsidere \\(n=100\\), \\(\\mu_{0}=1\\), \\(\\sigma^2=0.1\\) e \\(\\bar{x}=0.99\\). Conclua o teste considerando o mesmo nível de significância \\(\\alpha=5\\%\\).\nO pontos pontos de corte são \\(x_{a} = 0.93\\) e \\(x_{b} = 1.069\\). Como \\(0.93\\leq 0.99 \\leq 1.069\\), concluímos que não há evidências para rejeitarmos \\(H_{0}\\) a \\(5\\%\\) de significância.\nRefaça considerando \\(15\\%\\) de significância estatística.\nUsando os mesmos argumentos do item 1, podemos encontrar novos valores para \\(x_{a}, x_{b}\\) através da tabela da Normal-Padrão: \\[\n\\begin{cases}\n    x_{a} = \\mu - 1.44 \\sqrt{\\frac{\\sigma^2}{n}} \\\\\n    x_{b} = \\mu + 1.44 \\sqrt{\\frac{\\sigma^2}{n}}\n\\end{cases}\n\\]\nSubstituindo esses valores para os fornecidos em 2, temos que \\(0.95 \\leq 0.99 \\leq 1.045\\). Portanto, continuaríamos a dizer que não há evidências para rejeitarmos \\(H_{0}\\) a \\(15\\%\\) de significância.\n\n\n\n15.1.2 2\nSeja \\((X_{n})\\) a.a de \\(X\\sim \\mathrm{N}(\\mu,\\sigma^2)\\) em que \\(\\sigma^2\\) é conhecido.\nConsidere (\\(\\mu_{0}\\) fixado)\n\\[\n\\begin{cases}\nH_{0} : \\mu \\geq \\mu_{0} \\\\\nH_{1}: \\mu &lt; \\mu_{0}\n\\end{cases}\n\\]\n\nConstrua uma decisão para rejeitar \\(H_0\\) que produza no máximo \\(\\alpha = 5\\%\\) (que tenha nível de significância de \\(5\\%\\))\nPelos parâmetros e hipóteses envolvidos, \\((\\mu, \\text{unilateral})\\), podemos considerar a seguinte decisão:\nSe \\(\\bar{x} &lt; x_{c}\\), rejeitamos \\(H_{0}\\). Caso contrário, não rejeitamos.\n\\[\n\\begin{aligned}\n     \\alpha_{\\max} &= \\sup_{\\mu \\geq \\mu_{0}} P_{\\theta}(\\text{Rejeitar }H_{0}) \\\\\n     &= \\sup_{\\mu\\geq \\mu_{0}}P_{\\mu}(\\bar{X}&lt;x_{c}) \\\\\n     \\Rightarrow \\alpha_{\\max} &=\\sup_{\\mu\\geq \\mu_{0}}P_{\\mu}\\left( \\mathrm{N}(0,1)&lt; \\frac{{x_{c}-\\mu}}\n{\\sqrt{\\frac{\\sigma^2}{n} }} \\right)\n  \\end{aligned}\n\\]\nComo essa função (acumulada) é decrescente em \\(\\mu\\), temos que \\[\n\\begin{aligned}\n     \\alpha_{\\max} &= P_{\\mu_{0}}(\\text{Rejeitar }H_{0}) \\\\\n     &= P_{\\mu_{0}}(\\bar{X}&lt;x_{c}) \\\\\n     \\Rightarrow \\alpha_{\\max} &=P_{\\mu_{0}}\\left( \\mathrm{N}(0,1)&lt; \\frac{{x_{c}-\\mu_{0}}}\n{\\sqrt{\\frac{\\sigma^2}{n} }} \\right) \\leq 5\\%\n  \\end{aligned}\n\\]\nLogo, para encontrarmos \\(x_{c}\\) tal que \\(\\frac{{x_{c}-\\mu_{0}}}{\\sqrt{ \\frac{\\sigma^2}{n} }} = -1.64\\) (da tabela da normal padrão) \\(\\Rightarrow x_{c} = \\mu_{0}-1.64 \\sqrt{ \\frac{\\sigma^2}{n} }\\)\nConsidere \\(n=100, \\mu_{0} = 1, \\sigma^2 = 0.1, \\bar{x}=0.99\\). Conclua o teste anterior a \\(\\alpha = 5\\%\\) de significância.\nO ponto de corte é \\(x_{c} = 0.9836\\). Como \\(0.99 \\geq 0.9836\\), concluímos que não há evidências para rejeitar a hipótese nula a \\(5\\%\\) de significância.\n\n\n\n15.1.3 3\nSeja \\((X_{n})\\) amostra aleatória de \\(X \\sim \\mathrm{N}(\\mu,\\sigma^2)\\) em que \\(\\theta = (\\mu, \\sigma^2) = \\mathrm{R} \\times \\mathrm{R}^+\\), ou seja, ambos parâmetros são desconhecidos.\n\\[\n\\begin{cases}\nH_{0} : \\sigma^2 = \\sigma^2_{0} \\\\\nH_{1}: \\sigma^2 \\neq \\sigma^2_{0}\n\\end{cases} \\Rightarrow\n\\begin{aligned}\n& \\text{Decisão com significância } \\alpha \\\\\n&\\text{Rejeita $H_{0}$ se} \\\\\n&\\begin{cases}\ns^2 &lt; c_{1c} \\\\\ns^2 &gt; c_{2c} \\\\\n\\end{cases} \\\\\n& \\text{Em que $c_{1c},c_{2c}$ são tais que} \\\\\n& \\sup_{\\theta\\in\\Theta_{0}}P_{\\theta}(\\text{Erro Tipo I}) = \\alpha_{\\max} = \\alpha \\\\\n&\\text{E } s^2 = \\frac{1}{n-1} \\sum^n_{i=1}(x_{i}-\\bar{x})^2\n\\end{aligned} \\\\ \\\\\n\\]\nSabemos que \\(\\frac{\\sum_{i=1}^n(X_{i}-\\bar{X})^2}{\\sigma^2}\\sim \\chi^2_{n-1},\\forall \\mu \\in \\mathbb{R}, \\sigma^2 &gt; 0\\). Em particular, sob \\(H_{0}\\) \\[\n\\begin{aligned}\n&\\frac{(n-1)s^2(\\underset{\\sim}{X_{n}})}{\\sigma^2_{0}} \\sim \\chi^2_{n-1}\n\\\\ \\Rightarrow& \\alpha_{\\max} = \\sup_{\\theta \\in \\Theta_{0}}\\left\\{ P_{\\theta}(s^2(\\underset{\\sim}{X_{n}}) &lt; c_{1c}) +\nP_{\\theta}(s^2(\\underset{\\sim}{X_{n}})&gt; c_{2c}) \\right\\}\n\\end{aligned}\n\\]\nAlém disso, note que \\(\\Theta_{0}=\\{ (\\mu,\\sigma^2)\\in \\Theta:\\sigma^2=\\sigma^2_{0} \\}\\). Portanto, temos que \\[\n\\begin{aligned}\n& \\alpha_{\\max} = \\sup_{\\theta \\in \\Theta_{0}} \\underbracket{ \\left\\{  P\\left( \\chi^2_{n-1} &lt;\n\\frac{c_{1c}(n-1)}{\\sigma^2_{0}}\\right) + P\\left( \\chi^2_{n-1} &gt;\n\\frac{c_{2c}(n-1)}{\\sigma^2_{0}} \\right) \\right\\}}_{\\text{Não depende de $\\theta$}}\\\\\n\\Rightarrow & \\alpha_{\\max} =  P\\left( \\chi^2_{n-1} &lt; \\frac{c_{1c}(n-1)}{\\sigma^2_{0}} \\right) + P\\left( \\chi^2_{n-1} &gt;\n\\frac{c_{2c}(n-1)}{\\sigma^2_{0}} \\right)\n\\end{aligned}\n\\]\nFixando \\(\\alpha_{\\max}=\\alpha\\) (significância), encontramos pela tabela os valores de \\(q_{\\frac{\\alpha}{2},n-1}^{(1)}, q_{\\frac{\\alpha}{2},n-1}^{(2)}\\) tais que dividam a distribuição \\(\\chi^2_{n-1}\\) criando duas seções de \\(\\frac{\\alpha}{2}\\) de área. Portanto,\n\\[\n\\begin{cases}\nH_{0} : \\sigma^2 = \\sigma^2_{0} \\\\\nH_{1}: \\sigma^2 \\neq \\sigma^2_{0}\n\\end{cases} \\Rightarrow\n\\begin{aligned}\n& \\text{Decisão com significância } \\alpha \\\\\n&\\text{Rejeita $H_{0}$ se} \\\\\n&\\begin{cases}\ns^2 &lt; q_{\\frac{\\alpha}{2},n-1}^{(1)} \\cdot \\frac{\\sigma^2_{0}}{(n-1)} \\\\\ns^2 &gt; q_{\\frac{\\alpha}{2},n-1}^{(2)} \\cdot \\frac{\\sigma^2_{0}}{(n-1)}\\\\\n\\end{cases}\n\\end{aligned}\n\\]\n\n\n15.1.4 Exemplo\nSeja \\((X_{n})\\) amostra aleatória de \\(X\\sim N(\\mu,\\sigma^2)\\) em que \\(X\\) é o peso do pacote de café. Colheu-se uma amostra de \\(n=16\\) pacotes e observou-se uma variância de \\(s^2 =169g^2\\). O processo de fabricação diz que a média dos pacotes é \\(500g\\) e desvio-padrão 10 gramas (\\(\\sigma^2_{0}=100g^2\\)).\nQueremos verificar se há alguma evidência de que o processo não esteja sendo cumprido com \\(\\alpha=5\\%\\) de significância \\[\n\\begin{cases}\nH_{0} : \\sigma^2 = 100 \\\\\nH_{1}: \\sigma^2 \\neq 100\n\\end{cases} \\Rightarrow\n\\begin{aligned}\n& \\text{Decisão com significância } 5\\% \\\\\n&\\text{Rejeita $H_{0}$ se} \\\\\n&\\begin{cases}\ns^2 &lt; q_{2.5\\%,15}^{(1)} \\cdot \\frac{100}{15} \\\\\ns^2 &gt; q_{2.5\\%,15}^{(2)} \\cdot \\frac{100}{15}\\\n\\end{cases}\n\\end{aligned}\n\\] Da tabela Qui-quadrado, temos \\(q_{2.5\\%,15}^{(1)} = 6.26\\) e \\(q_{2.5\\%,15}^{(2)} =27.49\\).\nComo \\(41.73&lt;100&lt;183.26\\), concluímos que não há evidências para rejeitar a hipótese nula a \\(5\\%\\) de significância",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#sob-normalidade-variância-conhecida",
    "href": "teste-hipotese.html#sob-normalidade-variância-conhecida",
    "title": "14  Teste de Hipótese simples",
    "section": "16.1 Sob normalidade, variância conhecida",
    "text": "16.1 Sob normalidade, variância conhecida\nSeja \\((X_{n})\\) amostra aleatória de \\(X\\sim \\mathrm{N}(\\mu,\\sigma^2)\\) em que \\(\\sigma^2\\) é conhecido. \\[\n\\begin{aligned}\n&1.\n\\begin{cases}\nH_{0} : \\mu = \\mu_{0} \\\\\nH_{1}: \\mu \\neq \\mu_{0}\n\\end{cases} \\Rightarrow\n\\begin{aligned}\n& \\text{Decisão com significância } \\alpha \\\\\n&\\text{Rejeita $H_{0}$ se} \\\\\n&\\begin{cases}\n\\bar{x} &lt; \\mu_{0} - z_{\\frac{\\alpha}{2}} \\sqrt{ \\frac{\\sigma^2}{n} } \\\\\n\\bar{x} &gt; \\mu_{0} + z_{\\frac{\\alpha}{2}} \\sqrt{ \\frac{\\sigma^2}{n} }\n\\end{cases} \\\\\n& \\text{Em que $z_{\\frac{\\alpha}{2}}$ é tal que} \\\\\n& P\\left( \\mathrm{N(0,1)} &lt; z_{\\frac{\\alpha}{2}} \\right) = \\frac{\\alpha}{2}\n\\end{aligned} \\\\ \\\\\n& 2.\n\\begin{cases}\nH_{0} : \\mu \\geq \\mu_{0} \\\\\nH_{1}: \\mu &lt; \\mu_{0}\n\\end{cases} \\Rightarrow\n\\begin{aligned}\n&\\text{Rejeita $H_{0}$ se} \\\\\n&\\begin{cases}\n\\bar{x} &lt; \\mu_{0} - z_{\\alpha} \\sqrt{ \\frac{\\sigma^2}{n} } \\\\\n\\end{cases} \\\\\n& \\text{Em que $z_{\\alpha}$ é tal que} \\\\\n& P\\left( \\mathrm{N(0,1)} \\leq z_{\\alpha} \\right) = \\alpha\n\\end{aligned} \\\\ \\\\\n& 3.\n\\begin{cases}\nH_{0} : \\mu \\leq \\mu_{0} \\\\\nH_{1}: \\mu &gt; \\mu_{0}\n\\end{cases} \\Rightarrow\n\\begin{aligned}\n&\\text{Rejeita $H_{0}$ se} \\\\\n&\\begin{cases}\n\\bar{x} &gt; \\mu_{0} + z_{\\alpha} \\sqrt{ \\frac{\\sigma^2}{n} } \\\\\n\\end{cases} \\\\\n& \\text{Em que $z_{\\alpha}$ é tal que} \\\\\n& P\\left( \\mathrm{N(0,1)} \\geq z_{\\alpha} \\right) = \\alpha\n\\end{aligned} \\\\\n\\end{aligned}\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#sec-normalidade-vardesc",
    "href": "teste-hipotese.html#sec-normalidade-vardesc",
    "title": "14  Teste de Hipótese simples",
    "section": "16.2 Sob normalidade, variância desconhecida",
    "text": "16.2 Sob normalidade, variância desconhecida\nSeja \\((X_{n})\\) amostra aleatória de \\(X \\sim \\mathrm{N}(\\mu,\\sigma^2)\\) em que \\(\\theta = (\\mu, \\sigma^2) = \\mathrm{R}\n\\times \\mathrm{R}^+\\), ou seja, ambos parâmetros são desconhecidos. \\[\n\\begin{aligned}\n&1.\n\\begin{cases}\nH_{0} : \\mu = \\mu_{0}  \\\\\nH_{1} : \\mu \\neq \\mu_{0} \\\\\n\\Rightarrow \\Theta_{0}=\\{ (\\mu,\\sigma^2) \\in \\Theta : \\mu = \\mu_{0} \\}\\\\\n\\Rightarrow \\Theta_{1}=\\{ (\\mu,\\sigma^2) \\in \\Theta : \\mu \\neq \\mu_{0} \\}\\\\\n\\end{cases} \\Rightarrow\n\\begin{aligned}\n& \\text{Decisão com significância } \\alpha \\\\\n&\\text{Rejeita $H_{0}$ se} \\\\\n&\\begin{cases}\n\\frac{\\bar{x}-\\mu_{0}}{\\sqrt{ \\frac{s^2}{n} }} &lt; - t_{\\frac{\\alpha}{2},n-1} \\\\\n\\frac{\\bar{x}-\\mu_{0}}{\\sqrt{ \\frac{s^2}{n} }} &gt; t_{\\frac{\\alpha}{2},n-1} \\\\\n\\end{cases} \\\\\n& \\text{Em que $t_{\\frac{\\alpha}{2},n-1}$ é tal que} \\\\\n& P\\left( t_{n-1} &lt; - t_{\\frac{\\alpha}{2},n-1} \\right) = \\frac{\\alpha}{2}\n\\end{aligned} \\\\ \\\\\n& 2.\n\\begin{cases}\nH_{0} : \\mu \\geq \\mu_{0} \\\\\nH_{1}: \\mu &lt; \\mu_{0}\n\\end{cases} \\Rightarrow\n\\begin{aligned}\n&\\text{Rejeita $H_{0}$ se} \\\\\n&\\begin{cases}\n\\frac{\\bar{x}-\\mu_{0}}{\\sqrt{ \\frac{s^2}{n} }} &lt; - t_{\\alpha,n-1} \\\\\n\\end{cases} \\\\\n& \\text{Em que $t_{\\alpha,n-1}$ é tal que} \\\\\n& P\\left( t_{n-1}\\leq -t_{\\alpha,n-1} \\right) = \\alpha\n\\end{aligned} \\\\ \\\\\n& 3.\n\\begin{cases}\nH_{0} : \\mu \\leq \\mu_{0} \\\\\nH_{1}: \\mu &gt; \\mu_{0}\n\\end{cases} \\Rightarrow\n\\begin{aligned}\n&\\text{Rejeita $H_{0}$ se} \\\\\n&\\begin{cases}\n\\frac{\\bar{x}-\\mu_{0}}{\\sqrt{ \\frac{s^2}{n} }} &gt; - t_{\\alpha,n-1} \\\\\n\\end{cases} \\\\\n& \\text{Em que $t_{\\alpha,n-1}$ é tal que} \\\\\n& P\\left( t_{n-1} &gt; t_{\\alpha,n-1} \\right) = \\alpha\n\\end{aligned} \\\\\n\\end{aligned}\n\\]\nEm que \\(s^2 = \\frac{1}{n-1} \\sum^n_{i=1}(x_{i}-\\bar{x})^2\\) é a variância amostral (não enviesada)",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#sob-normalidade-para-a-variância.",
    "href": "teste-hipotese.html#sob-normalidade-para-a-variância.",
    "title": "14  Teste de Hipótese simples",
    "section": "16.3 Sob normalidade, para a variância.",
    "text": "16.3 Sob normalidade, para a variância.\nSeja \\((X_{n})\\) amostra aleatória de \\(X \\sim \\mathrm{N}(\\mu,\\sigma^2)\\) em que \\(\\theta = (\\mu, \\sigma^2) = \\mathrm{R} \\times \\mathrm{R}^+\\), ou seja, ambos parâmetros são desconhecidos. \\[\n\\begin{cases}\nH_{0} : \\sigma^2 = \\sigma^2_{0} \\\\\nH_{1}: \\sigma^2 \\neq \\sigma^2_{0}\n\\end{cases} \\Rightarrow\n\\begin{aligned}\n& \\text{Decisão com significância } \\alpha \\\\\n&\\text{Rejeita $H_{0}$ se} \\\\\n&\\begin{cases}\ns^2 &lt; q_{\\frac{\\alpha}{2},n-1}^{(1)} \\cdot \\frac{\\sigma^2_{0}}{(n-1)} \\\\\ns^2 &gt; q_{\\frac{\\alpha}{2},n-1}^{(2)} \\cdot \\frac{\\sigma^2_{0}}{(n-1)}\\\\\n\\end{cases}\n\\end{aligned}\n\\]\nEm que \\(s^2 = \\frac{1}{n-1} \\sum^n_{i=1}(x_{i}-\\bar{x})^2\\) é a variância amostral (não enviesada) e \\(q_{\\frac{\\alpha}{2},n-1}^{(1)}, q_{\\frac{\\alpha}{2},n-1}^{(2)}\\) tais que dividam a distribuição \\(\\chi^2_{n-1}\\) criando duas seções de \\(\\frac{\\alpha}{2}\\) de área.",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese-duas-pops.html",
    "href": "teste-hipotese-duas-pops.html",
    "title": "15  Teste de Hipótese para duas populações",
    "section": "",
    "text": "15.1 Dados independentes e dependentes\nSejam \\(X, Y\\) duas variáveis de interesse representando duas sub-populações. Estamos interessados em verificar se a média populacional de \\(X\\) é menor, maior ou igual à de \\(Y\\). Sendo assim, precisamos considerar os casos em que \\(X\\) é independente de \\(Y\\) e o caso em que não são independentes (pareados).\nUm pesquisador propôs um novo método de investimento para aumentar o rendimento mensal. Selecionou 20 investidores aleatoriamente de um universo de investidores cadastrados. Em um primeiro momento, o pesquisador deixou os investidores investirem do jeito que sabem e ao final verificou a renda obtida. \\(X\\) é o rendimento dos investidores sem ter o conhecimento do método.\nEntão, o pesquisador ensinou seu método aos investidores, onde \\(Y\\) passou a ser o rendimento dos investidores após a aplicação do método ensinado.\nClaramente, \\(X,Y\\) são dependentes.\nO mesmo pesquisador testará o mesmo método de forma diferente. Para testar o seu método, o pesquisador selecionou 20 indivíduos com características similares do universo de investidores, dos quais\nNesse caso, as variáveis \\(X,Y\\) são independentes.\nEm ambas abordagens, temos as mesmas hipóteses de interesse: \\[\n1.\n\\begin{cases}\nH_{0}:\\mu_{X}\\geq \\mu_{y} \\\\\nH_{1}: \\mu_{X} &lt; \\mu_{Y}\n\\end{cases} ~~~2.\n\\begin{cases}\nH_{0}:\\mu_{X}\\leq \\mu_{Y} \\\\\nH_{1}: \\mu_{X} &gt; \\mu_{Y}\n\\end{cases} ~~~3.\n\\begin{cases}\nH_{0}:\\mu_{X}= \\mu_{Y} \\\\\nH_{1}: \\mu_{X} \\neq \\mu_{Y}\n\\end{cases}\n\\]\nPodemos definir \\(\\mu_{D}=\\mu_{X}-\\mu_{Y}\\) e reescrever as hipóteses \\[\n1.\n\\begin{cases}\nH_{0}:\\mu_{D} \\geq 0 \\\\\nH_{1}: \\mu_{D} &lt; 0\n\\end{cases} ~~~2.\n\\begin{cases}\nH_{0}:\\mu_{D} \\leq 0 \\\\\nH_{1}: \\mu_{D} &gt; 0\n\\end{cases} ~~~3.\n\\begin{cases}\nH_{0}:\\mu_{D} = 0 \\\\\nH_{1}: \\mu_{D} \\neq 0\n\\end{cases}\n\\]\nPara os próximos exemplos, assumiremos normalidade para \\(X,Y\\)",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Teste de Hipótese para duas populações</span>"
    ]
  },
  {
    "objectID": "teste-hipotese-duas-pops.html#dados-independentes-e-dependentes",
    "href": "teste-hipotese-duas-pops.html#dados-independentes-e-dependentes",
    "title": "15  Teste de Hipótese para duas populações",
    "section": "",
    "text": "10 foram designados aleatoriamente a não receber o método \\(X:\\) rendimento de um indivíduo que não recebeu o método\n10 foram designados aleatoriamente a receberem o método \\(Y:\\) rendimento de um indivíduo que recebeu o método\n\n\n\n\n\n\n15.1.1 Caso pareado (variáveis dependentes)\nNo caso em que \\(X,Y\\) são dependentes, as amostras são \\((X_{n})\\) amostras aleatórias de \\(X\\) e \\((Y_{n})\\) a.a de \\(Y\\) tais que \\(X_{i},Y_{i}\\) são dependentes.\nPara este caso, fazemos \\(D_{i}=X_{i}-Y_{i},i=1,2,\\dots, n\\). Temos que \\((D_{n})\\) é uma amostra aleatória de \\(D=X-Y \\sim N(\\mu_{D},\\sigma^2_{D})=N(\\mu_{X}-\\mu_{Y},\\sigma^2_{X}+\\sigma^2_{Y}-2 \\rho\\sigma_{X}\\sigma _{Y})\\)\nObserve ainda que \\[\n\\bar{D}_{\\mathrm{Par}}=\\sum^{n}_{i=1} \\frac{D_{i}}{n} \\sim N\\left( \\mu_{D}, \\frac{\\sigma^2_{D}}{n} \\right)\n\\] Note ainda que as variância e covariância de \\(X,Y\\) estão embutidas em \\(\\sigma^2_{D}\\). Podemos usar \\[\ns^2_{D}(\\underset{\\sim}{D})=\\frac{1}{n-1}\\sum^n_{i=1}(D_{i}-\\bar{D}_{\\mathrm{Par}})^2\n\\] Para estimar \\(\\sigma^2_{D}\\) Podemos construir as decisões como já vimos anteriormente em testes sob normalidade com variância desconhecida.\n\n\n15.1.2 Exemplo\nForam coletados os rendimentos (em mil reais) antes e após a aplicação o método para 12 investidores. Queremos verificar se o método aumentou o rendimento médio. Chamaremos de \\(X\\) o rendimento anterior ao treinamento e \\(Y\\) o rendimento após. Isto é, queremos verificar se \\(\\mu_{D}=\\mu_{X}-\\mu_{Y}\\leq 0\\). Portanto, nossa hipótese nula é de que o treinamento não tem efeito positivo no rendimento:\n\\[\n\\begin{cases}\nH_{0}:\\mu_{D} \\geq 0 \\\\\nH_{1}: \\mu_{D} &lt; 0\n\\end{cases}\n\\]\n\\[\n\\begin{array}{c|c|c|c}\n\\mathrm{Indíce}  & \\mathrm{Antes}(X)  &  \\mathrm{Depois}(Y) & \\mathrm{Dif}(D) & \\mathrm{Dif^2}(D^2)\\\\\n\\hline\n1 & 2.4 &  4.3 & -1.9 & 3.61\\\\\n2 & 2.8  & 3.4  & -0.6 & 0.36\\\\\n3 & 4.6  & 3.2  & 1.4 & 1.96\\\\\n4 & 3.1  & 3.3 & -0.2 & 0.04\\\\\n5 & 3.1  & 3.3 & -0.2 & 0,04\\\\\n6 & 4.7 &  5.8 & -1.1 & 1.21\\\\\n7 & 3.5  & 3.8 & -0.3 & 0.09\\\\\n8 & 1.7  & 3.5 & -1.8 & 3.24\\\\\n9 & 2.3  & 3.2 & -0.9 & 0.81\\\\\n10 & 2.6  & 3.9 & -1.3 & 1.69\\\\\n11 & 4.2  & 3.6 & 0.6 & 0.36\\\\\n12 & 3.4  & 4.3  & -0.9  & 0.81\\\\\n\\hline\n\\mathrm{Média}  & 3.2  & 3.8  & -0.6 \\\\\ns^2 & 0.87  & 0.55  &  0.9\n\\end{array}\n\\]\nTemos que \\(s^2(\\underset{\\sim}{D})=0.9\\). (Podemos calcular diretamente ou usando a coluna \\(D^2\\) e substituindo no somatório \\(s^2_{D}=\\left( \\frac{\\sum^n_{i=1}d_{i}^2}{n}-\\bar{d}^2 \\right) \\frac{n}{n-1}\\) ) Rejeitamos \\(H_{0}\\) se \\(\\frac{\\bar{d}_{\\mathrm{Par}}}{\\sqrt{ \\frac{s^2_{D}}{n}}}&lt;-t_{\\alpha,n-1}\\) onde \\(t_{\\alpha,n-1}\\) é tal que \\(P(t_{n-1}&lt;-t_{\\alpha,n-1})=\\alpha\\), onde \\(t_{n-1}\\) é a distribuição T de Student com \\(n-1\\) graus de liberdade.\nTemos que \\(\\frac{\\bar{d}_{\\mathrm{Par}}}{\\sqrt{ \\frac{s^2_{D}}{n}}} =-2.19\\) e, a \\(\\alpha=0.05\\), \\(-t_{0.05,11}=-1.796\\). Como \\(-2.19&lt; -1.796\\), podemos dizer que há evidências de que o método aumenta o rendimento médio dos investidores a \\(5\\%\\) de significância. Por outro lado, com \\(\\alpha=0.01, -t_{0.01,11}=-2.718\\) e, por \\(-2.19\\geq -2.718\\), dizemos que não há evidências para rejeitar a hipótese de que o método não aumenta o rendimento (rejeitar a hipótese nula) a 1% de significância estatística.",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Teste de Hipótese para duas populações</span>"
    ]
  },
  {
    "objectID": "teste-hipotese-duas-pops.html#caso-de-independência",
    "href": "teste-hipotese-duas-pops.html#caso-de-independência",
    "title": "15  Teste de Hipótese para duas populações",
    "section": "15.2 Caso de independência",
    "text": "15.2 Caso de independência\nSejam \\(X, Y\\) variáveis aleatórias independentes tais que \\[ \\begin{cases}\nX\\sim N(\\mu_{X},\\sigma^2_{X}) \\\\\nY\\sim (\\mu_{Y},\\sigma^2_{Y})\n\\end{cases} \\] Considere \\((X_{n})\\) amostra aleatória de \\(X\\) e \\((Y_{m})\\). Estamos interessados em testar as hipóteses: \\[\n1.\n\\begin{cases}\nH_{0}:\\mu_{X}\\geq \\mu_{Y} \\\\\nH_{1}: \\mu_{X} &lt; \\mu_{Y}\n\\end{cases} ~~~2.\n\\begin{cases}\nH_{0}:\\mu_{X}\\leq \\mu_{Y} \\\\\nH_{1}: \\mu_{X} &gt; \\mu_{Y}\n\\end{cases} ~~~3.\n\\begin{cases}\nH_{0}:\\mu_{X}= \\mu_{Y} \\\\\nH_{1}: \\mu_{X} \\neq \\mu_{Y}\n\\end{cases}\n\\] Podemos definir \\(\\mu_{D}=\\mu_{X}-\\mu_{Y}\\) e obter a equivalência dessas hipóteses \\[\n1.\n\\begin{cases}\nH_{0}:\\mu_{D} \\geq 0 \\\\\nH_{1}: \\mu_{D} &lt; 0\n\\end{cases} ~~~2.\n\\begin{cases}\nH_{0}:\\mu_{D} \\leq 0 \\\\\nH_{1}: \\mu_{D} &gt; 0\n\\end{cases} ~~~3.\n\\begin{cases}\nH_{0}:\\mu_{D} = 0 \\\\\nH_{1}: \\mu_{D} \\neq 0\n\\end{cases}\n\\] Considere \\[\n\\bar{D}_{\\mathrm{NPar}}=\\bar{X}-\\bar{Y}\n\\]\nComo \\(\\bar{X}\\sim N\\left( \\mu_{X}, \\frac{\\sigma^2_{X}}{n} \\right), \\bar{Y} \\sim N\\left( \\mu_{Y}, \\frac{\\sigma^2_{Y}}{m} \\right)\\), temos que \\(\\bar{D}_{\\mathrm{NPar}} \\sim N\\left( \\mu_{D}, \\frac{\\sigma^2_{X}}{n} + \\frac{\\sigma^2_{Y}}{m} \\right)\\)\n\n15.2.1 Ambas variâncias conhecidas\nPodemos substituir o valor numérico das variâncias na distribuição de \\(\\bar{D}_{\\mathrm{NPar}}\\), obtendo uma distribuição normal com pontos de corte para as decisões:\n\\[\n\\begin{aligned}\n1. &\\text{ Rejeita $H_{0}$ se } \\bar{d}_{\\mathrm{NPar}} &lt; -z_{\\alpha} \\underbrace{\\sqrt{ \\frac{\\sigma^2}{n} +\n\\frac{\\sigma^2}{m} }}_{\\mathrm{Var}(\\bar{D}_{\\mathrm{NPar}})}, \\sigma_{2}=\\sigma^2_{X}=\\sigma^2_{Y} \\\\\n2. & \\text{ Rejeita $H_{0}$ se } \\bar{d}_{\\mathrm{NPar}} &gt; z_{\\alpha} \\sqrt{ \\frac{\\sigma^2}{n} + \\frac{\\sigma^2}{m} } \\\\\n3. & \\text{ Rejeita $H_{0}$ se } \\bar{d}_{\\mathrm{NPar}} &lt; -z_{\\frac{\\alpha}{2}} \\sqrt{ \\frac{\\sigma^2}{n} +\n\\frac{\\sigma^2}{m} } \\text{ ou } \\bar{d}_{NPar} &gt; z_{\\alpha} \\sqrt{ \\frac{\\sigma^2}{n} + \\frac{\\sigma^2}{m} }\n\\end{aligned}\n\\]\n\n\n15.2.2 Variâncias desconhecidas e iguais\nTemos que \\(\\sigma^2_{X}=\\sigma^2_{Y}=\\sigma^2\\) é conhecido.\n\n15.2.2.1 Estimando via t-Student\nAtravés da distribuição t-Student com \\(n+m-2\\) graus de liberdade, podemos estimar os pontos de corte. Temos nossas decisões: \\[\n\\begin{aligned}\n1. &\\text{ Rejeita $H_{0}$ se } \\bar{d}_{\\mathrm{NPar}} &lt; -t_{n+m-2,\\alpha} \\sqrt{ \\frac{s_{p}^2}{n} + \\frac{s_{p}^2}{m} } \\\\\n2. & \\text{ Rejeita $H_{0}$ se } \\bar{d}_{\\mathrm{NPar}} &gt; t_{n+m-2,\\alpha} \\sqrt{ \\frac{s^2_{p}}{n} + \\frac{s^2_{p}}{m} } \\\\\n3. & \\text{ Rejeita $H_{0}$ se } \\bar{d}_{\\mathrm{NPar}} &lt; -t_{n+m-2,\\frac{\\alpha}{2}} \\sqrt{ \\frac{s^2_{p}}{n} +\n\\frac{s^2_{p}}{m} } \\text{ ou } \\bar{d}_{NPar} &gt; t_{n+m-2, \\frac{\\alpha}{2}} \\sqrt{ \\frac{s^2_{p}}{n} + \\frac{s^2_{p}}{m} }\n\\end{aligned}\n\\] Onde \\(s^2_{p}= \\frac{(n-1)s^2_{X}+(m-1)s^2_{Y}}{n+m-2}\\), com \\(s^2_{X}, s^2_{Y}\\) sendo os estimadores não enviesados para as variâncias de \\(X\\) e \\(Y\\), respectivamente. (Ponderamos os estimadores com base no tamanho de sua amostra, assim favorecendo os estimadores mais precisos)\n\n\n\n15.2.3 Variâncias desconhecidas e diferentes (caso geral)\nTemos que \\(\\sigma^2_{X},\\sigma^2_{Y}\\) são desconhecidos e diferentes.\n\n15.2.3.1 Estimando via t-Student\nusaremos a distribuição t-Student com \\(n'\\) graus de liberdade. Temos nossas decisões:\n\\[\n\\begin{aligned}\n1. &\\text{ Rejeita $H_{0}$ se } \\bar{d}_{\\mathrm{NPar}} &lt; -t_{n',\\alpha} \\sqrt{ \\frac{s_{p}^2}{n} + \\frac{s_{p}^2}{m} } \\\\\n2. & \\text{ Rejeita $H_{0}$ se } \\bar{d}_{\\mathrm{NPar}} &gt; t_{n',\\alpha} \\sqrt{ \\frac{s^2_{p}}{n} + \\frac{s^2_{p}}{m} } \\\\\n3. & \\text{ Rejeita $H_{0}$ se } \\bar{d}_{\\mathrm{NPar}} &lt; -t_{n',\\frac{\\alpha}{2}} \\sqrt{\\frac{s^2_{p}}{n} + \\frac{s^2_{p}}{m}}\n\\text{ ou } \\bar{d}_{NPar} &gt; t_{n', \\frac{\\alpha}{2}} \\sqrt{ \\frac{s^2_{p}}{n} + \\frac{s^2_{p}}{m} }\n\\end{aligned}\n\\]\nOnde \\(s^2_{p}= \\frac{(n-1)s^2_{X}+(m-1)s^2_{Y}}{n+m-2}\\).\n\n15.2.3.1.1 Encontrando \\(n’\\)\nNa fórmula acima, temos os graus de liberdade da t-Student dado por \\[\nn' \\approx \\frac{\\left(\\frac{s^2_{X}}{n}+\\frac{s^2_{Y}}{m}\\right)^2}{\\frac{\\left(\\frac{s^2_{X}}{n}\\right)^2}\n{n-1}+\\frac{\\left(\\frac{s^2_{Y}}{m}\\right)^2}{m-1}}\n\\] Esse valor, caso não inteiro, deverá ser arredondado.\n\n\n\n15.2.3.2 Exemplo (Importante)\nQueremos testar a resistência de dois tipos de viga de aço, \\(A\\) e \\(B\\). Tomando-se \\(n=15\\) vigas do tipo \\(A\\) e \\(m=20\\) vigas do tipo \\(B\\). de um teste \\(f\\), conseguimos com \\(10\\%\\) de significância que as variâncias não são iguais. Obtemos os valores da tabela: \\[\n\\begin{array}{c|c}\n\\text{Tipo}  & \\text{Média}  & \\text{Variância} (s^2) \\\\\n\\hline\nA & 70.5 & 81.6 \\\\\nB  & 84.3  & 210.8 \\\\\n\\bar{d}_{\\mathrm{NPar}}  & -13.8 & --\n\\end{array}\n\\] Teste a hipótese \\[\n\\begin{cases}\nH_{0} : \\mu_{X} = \\mu_{Y} \\\\\nH_{1}:\\mu_{X} \\neq \\mu_{Y}\n\\end{cases}\n\\] Com significância \\(\\alpha = 0.05\\) para os casos\n\n15.2.3.2.1 Caso 1. Variâncias conhecidas\nTemos do produtor que \\(\\sigma^2_{X}=81, \\sigma^2_{Y}=209\\) \\(\\bar{d}_{\\mathrm{NPar}}=70.5-84.3 = -13.8\\). Logo, \\(z_{\\frac{\\alpha}{2}}\\sqrt{ \\frac{81}{15} + \\frac{209}{20}}=7.8\\). Como \\(-13.8 &lt; -7.8\\), concluímos que há evidências para rejeitarmos a hipótese nula de que as resistências médias das vigas \\(A,B\\) são iguais a \\(\\alpha = 5\\%\\) de significância estatística\n\n\n15.2.3.2.2 Caso 2. Variâncias desconhecidas e iguais.\nPara \\(\\alpha=0.05\\), \\(t_{33,0.025}=2.03\\). Encontrando \\(s^2_{P} = 155.988\\). Finalmente, \\(t_{33,0.025}\n\\sqrt{  \\frac{s^2_{P}}{n} + \\frac{s^2_{P}}{m} }=8.65\\). Como \\(-13.8 &lt; -8.65\\), concluímos que há evidências para rejeitarmos a hipótese nula a \\(\\alpha=5\\%\\) de significância estatística.\n\n\n15.2.3.2.3 Caso 3. Variâncias desconhecidas e diferentes\nPrimeiro calculamos \\(n'= 32.08 \\stackrel{\\text{Arrendonda}}{=}32\\). Assim, \\(t_{32,0.025} = 2.037\\). Portanto, \\(t_{32,0.025} \\sqrt{  \\frac{s^2_{X}}{n} + \\frac{s^2_{Y}}{m} }=8.14\\). Como \\(-13.8 &lt; -8.15\\), concluímos que há evidências para rejeitarmos a hipótese nula a \\(\\alpha=5\\%\\) de significância estatística.",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Teste de Hipótese para duas populações</span>"
    ]
  },
  {
    "objectID": "tabela-frequencias.html",
    "href": "tabela-frequencias.html",
    "title": "16  Tabela de frequências",
    "section": "",
    "text": "Sejam \\(X,Y\\) variáveis aleatórias cujos valores observados são \\(B_{1},B_{2},\\dots,B_{l}\\) e \\(A_{1},A_{2}, A_{k}\\), respectivamente. Observam-se os seguintes dados \\[\n\\begin{array}{ccc}\n\\mathrm{ind.}  & X & Y \\\\\n1  & B_{2} & A_{1} \\\\\n2 & B_{7} & A_{3} \\\\\n\\vdots & \\vdots  & \\vdots \\\\\nn  & B_{1} & A_{5}\n\\end{array}\n\\] Colocamos nossos dados numa tabela de frequências absolutas observadas \\[\n\\begin{array}{c|cccc|c}\nX\\setminus Y  & A_{1} & A_{2} & \\dots & A_{k} & \\mathrm{Total}~X \\\\\n\\hline\nB_{1} & O_{11} & O_{12} & \\dots & O_{1k} & O_{1\\cdot} \\\\\nB_{2} & O_{11} & O_{12} & \\dots & O_{1k} & O_{2\\cdot} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\nB_{l} & O_{l1} & O_{l2} & \\dots & O_{lk} & O_{l\\cdot} \\\\\n\\hline\n\\mathrm{Total}~Y  & O_{\\cdot_{1}} & O_{\\cdot_{2}}  & \\dots  & O_{\\cdot k}  & n\n\\end{array}\n\\]\nTemos nossa tabela de frequências esperadas [[Teste de Hipótese|sob]] \\(H_{0}\\) (Independência) \\[\n\\begin{array}{c|cccc|c}\nX\\setminus Y  & A_{1} & A_{2} & \\dots & A_{k} & \\mathrm{Total}~X \\\\\n\\hline\nB_{1} & E_{11} & E_{12} & \\dots & E_{1k} & O_{1\\cdot} \\\\\nB_{2} & E_{11} & E_{12} & \\dots & E_{1k} & O_{2\\cdot} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\nB_{l} & E_{l1} & E_{l2} & \\dots & E_{lk} & O_{l\\cdot} \\\\\n\\hline\n\\mathrm{Total}~Y  & O_{\\cdot_{1}} & O_{\\cdot_{2}}  & \\dots  & O_{\\cdot k}  & n\n\\end{array}\n\\] Em que \\[\nE_{ij} = \\frac{O_{i \\cdot} \\cdot O_{\\cdot j}}{n}\n\\] Note que, sob independência \\[\n\\begin{aligned}\nP(B_{i}\\cap A_{j}) &= P(B_{i}) \\cdot P(A_{j}) \\\\\nE_{ij} &= n \\cdot P(B_{i}\\cap A_{j}) \\stackrel{\\mathrm{ind.}}{=} n P(B_{i}) \\cdot P_{A_{j}}\n\\end{aligned}\n\\] Estimando \\(P(B_{i}), P(A_{j})\\) temos \\[\n\\widehat{P(B_{i})} = \\frac{O_{i\\cdot}}{n}, \\widehat{P(A_{j})}= \\frac{O_{\\cdot j}}{n}\n\\] Logo, o valor esperado estimado é \\[\n\\widehat{E_{ij}}=n \\cdot \\widehat{P(B_{i})}\\cdot\\widehat{P(A_{j})} = \\frac{O_{i \\cdot} \\cdot O_{\\cdot j}}{n}\n\\]\nEm ambos testes, usaremos a seguinte estatística para testar suas hipóteses (independência e homogeneidade) \\[\n\\chi^2 = \\sum^k_{i=1}\\sum^l_{j=1} \\frac{(O_{ij}-E_{ij})^2}{E_{ij}}\n\\] Sob \\(H_{0}\\), ou seja, \\[\n\\chi^2_{obs}\\sim \\chi^2_{(k-1)(l-1)}\n\\] Dessa forma, rejeitamos a hipótese \\(H_{0}\\) a \\(\\alpha\\) graus de liberdade se \\[\n\\chi^2_{obs} &gt; c_{p}\n\\] em que \\(c_{p}\\) satisfaz \\(P(\\chi^2_{(k-1)(l-1)} &gt; c_{p})=\\alpha\\).\nObservação Essa aproximação com a \\(\\chi^2\\) só funciona de modo razoável quando cada \\(E_{ij}&gt;5\\)",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Tabela de frequências</span>"
    ]
  },
  {
    "objectID": "analise-aderencia.html",
    "href": "analise-aderencia.html",
    "title": "17  Teste Qui-Quadrado e análise de aderência",
    "section": "",
    "text": "17.1 Exemplo\nA análise de aderência testa a distribuição dos dados: \\[\n\\begin{cases}\nH_{0}: P= P_{0} \\\\\nH_{1}: P \\neq P_{0}\n\\end{cases}\n\\] Em que \\(P_{0}\\) é a medida de probabilidade especificada que governaria (sob \\(H_0\\)) os eventos observados.\nNeste teste co,paramos a frequência observada com a frequência esperada em \\(k\\) eventos disjuntos e distintos observáveis. \\[\n\\begin{array}{c|cc}\n\\text{Eventos}  & 1  & 2 & \\dots & k \\\\\n\\hline\nP_{0} & P_{01} & P_{02} & \\dots & P_{0k} \\\\\nE_{i}  & E_{1} & E_{2} & \\dots & E_{k} \\\\\nO_{i} & O_{1} & O_{2} & \\dots & O_{k}\n\\end{array}\n\\]\nEm que observou-se uma amostra de tamanho \\(n\\). Temos também que \\(E_{i}\\) é o valor esperado do número de eventos \\(i\\) sob \\(H_{0}\\) \\[\n\\mathrm{Freq. Esperada} = E_{i} = P_{0i} \\cdot n\n\\]\ne \\(\\mathrm{Freq. Observada} = O_{i}\\) é o numero real de eventos \\(i\\) observados na amostra. A estatística para testar \\(H_{0}\\) é \\[\n\\chi^2 = \\sum^k_{i=1} \\frac{(E_{i}-O_{i})^2}{E_{i}}\n\\]\nque, sob \\(H_0\\) - ou seja, sob a hipótese de que \\(P_{0}\\) é de fato a medida de probabilidade que governa o comportamento probabilístico do evento - é aproximadamente \\[\n\\underbracket{\\chi^2 \\sim \\chi^2_{(k-1)}}_{\\mathrm{Sob}~H_{0}}\n\\]\n*Esse procedimento é confiável sempre que \\(E_{i}&gt;5 \\forall i \\in \\{ 1,\\dots,k \\}\\)\nConsidere que queremos verificar se os números sorteados nos concursos da Mega Sena são de fato uniformemente distribuídos. Nesse caso, analisaremos 60 eventos, cuja probabilidade de cada um seria, caso uniformemente distribuídos, \\(\\frac{1}{60}\\). \\[\n\\begin{cases}\nH_{0}: P = P_{0} \\\\\nH_{1}: P \\neq P_{0}\n\\end{cases}\n\\]\nEm que \\(P_{0}(\\{ i \\}) = \\frac{1}{60} \\forall i \\in \\{ 1,2,\\dots 60 \\}\\)\nVamos criar a tabela para as frequências. Consideraremos a primeira bola de todos os \\(2800\\) sorteios da Mega. \\[\n\\begin{array}{c|ccc}\n\\mathrm{Eventos}  &  1 & 2 & \\dots & 60\\\\\n\\hline\nP_{0} & \\frac{1}{60} & \\frac{1}{60} & \\dots & \\frac{1}{60} \\\\\nE_{i} & \\frac{2800}{60}  & \\frac{2800}{60}  & \\dots  & \\frac{2800}{60} \\\\\nO_{i} & 42 & 48 & \\dots  & 55\n\\end{array}\n\\] Portanto, \\[\n\\chi^2 = \\sum^{60}_{i} \\frac{(46.7 - O_{i})^2}{46.7} \\stackrel{a}{\\sim} \\chi^2_{59}\n\\] Considerando um nível de significância de \\(\\alpha=5\\%\\), calculamos o ponto crítico \\(c\\) tal que \\[\nP(\\chi^2_{59}&gt;c) = 0.05\n\\]\nPelo computador, encontramos \\(c = 77.93\\) Logo, como \\(\\chi^2=56.68 &lt; 77.93\\), concluímos que, sob \\(H_{0}\\), não há evidências de que o modelo não seja equiprovável a \\(5\\%\\) de significância de estatística.",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Teste Qui-Quadrado e análise de aderência</span>"
    ]
  },
  {
    "objectID": "analise-aderencia.html#k-grupos",
    "href": "analise-aderencia.html#k-grupos",
    "title": "17  Teste Qui-Quadrado e análise de aderência",
    "section": "17.2 K-Grupos",
    "text": "17.2 K-Grupos\n\n(Morettin, Pag.404 E.7) Considere os \\(n=30\\) dados abaixo que supostamente seguem uma distribuição normal \\(N(10,25)\\). (usando os dados do livro já em ordem) \\[\n\\begin{array}{}\n1.01 & 1.73 & 3.93 & 4.44 & 6.37 & 6.51 \\\\\n\\vdots  & \\vdots  & \\vdots & \\vdots  & \\vdots & \\vdots \\\\\n14.11 & 14.6 & 14.64 & 14.75 & 16.68 & 22.14\n\\end{array}\n\\] Queremos testar se os dados de fato se distribuem de acordo com \\(N(10,25)\\). \\[\n\\begin{cases}\nH_{0}:P=N(10,25) \\\\\nH_{1}:P\\neq N(10,25)\n\\end{cases}\n\\] Sob \\(H_{0}\\), podemos dividir a distribuição normal em \\(k\\) blocos. Escolheremos \\(k=4\\) delimitado pelos quartis teóricos dessa distribuição normal. (Primeiro padronizamos, encontramos os valores pela tabela, então voltamos para nossa normal) \\[\n\\begin{cases}\nq_{1} = 6.63 \\\\\nq_{2} = 10 \\\\\nq_{3} = 13.3\n\\end{cases} \\stackrel{\\mathrm{Intervalos}}{\\Rightarrow}\n\\begin{cases}\n1.(-\\infty, q_{1}) \\\\\n2.[q_{1},q_{2}] \\\\\n3.(q_{2},q_{3}] \\\\\n4.(q_{3},\\infty)\n\\end{cases}\n\\] Podemos produzir uma tabela com as frequências por intervalo \\[\n\\begin{array}{c|cc}\n\\mathrm{Eventos}  &  1.  & 2. & 3. & 4.\\\\\n\\hline\nE_{i} & 0.25 \\cdot 30=7.5  & 7.5 & 7.5 & 7.5 \\\\\nO_{i} & 6  & 9 & 9 & 6 \\\\\n\\end{array}\n\\] \\[\n\\chi^2 = \\sum^4_{i=1} \\frac{(7.5 - O_{i})^2}{7.5} = 1.2\n\\] Na \\(\\chi^2_{3}\\) (número de nichos), com nível de significância \\(\\alpha=0.10\\), \\(c = 6.25\\). Como \\(\\chi^2=1.2&lt;6.25\\), concluímos que não há evidências de que a distribuição dos dados difere de uma \\(N(10,25)\\) a \\(\\alpha=10\\%\\) de significância estatística",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Teste Qui-Quadrado e análise de aderência</span>"
    ]
  },
  {
    "objectID": "testes-ind-homo.html",
    "href": "testes-ind-homo.html",
    "title": "18  Testes de Independência e Homogeneidade",
    "section": "",
    "text": "18.1 Teste de Homogeneidade\nCom a ajuda da tabela de frequências, conseguimos testar independência entre eventos e homogeneidade em distribuição de eventos. Por mais que utilizem o mesmo mecanismo, os dois testes são interpretados de forma diferentes e, portanto, também apresentados individualmente nesta seção.\nUsamos esse teste para verificar se as medidas de probabilidade de vários grupos diferentes são iguais (seguem uma mesma distribuição).\nOs totais marginais para cada grupo devem ser fixados antes de executarmos o experimento. \\[\n\\begin{cases}\nH_{0}: \\text{Os grupos são independentes} \\\\\nH_{1} : \\text{Pelo menos um dos grupos não é indepndente}\n\\end{cases}\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Testes de Independência e Homogeneidade</span>"
    ]
  },
  {
    "objectID": "testes-ind-homo.html#teste-de-independência",
    "href": "testes-ind-homo.html#teste-de-independência",
    "title": "18  Testes de Independência e Homogeneidade",
    "section": "18.2 Teste de independência",
    "text": "18.2 Teste de independência\nUsamos esse teste para verificar se os eventos são independentes.\nAqui, apenas o tamanho amostral (total dos totais) é fixado.\n\\[\n\\begin{cases}\nH_{0}: \\text{Os grupos se distribuem de forma equivalente} \\\\\nH_{1} : \\text{Pelo menos um dos grupos não se distribui de forma equivalente}\n\\end{cases}\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Testes de Independência e Homogeneidade</span>"
    ]
  },
  {
    "objectID": "testes-ind-homo.html#exemplos",
    "href": "testes-ind-homo.html#exemplos",
    "title": "18  Testes de Independência e Homogeneidade",
    "section": "18.3 Exemplos",
    "text": "18.3 Exemplos\n\n18.3.1 Primeiro exemplo (homogeniedade)\n510 segurados foram amostrados, sendo 200 de São Paulo, 100 do Ceará e 210 de Pernambuco. O objetivo é verificar se o número de acidentes se distribui igualmente entre os estados. \\[\n\\begin{array}{ccc}\n\\mathrm{Indivíduos}  & \\mathrm{Estado}  & \\mathrm{Sinistralidade}\\\\\n1 & \\mathrm{SP} & 1 \\\\\n\\vdots  & \\vdots & \\vdots \\\\\n200  &  \\mathrm{SP}  & 0 \\\\\n1 & \\mathrm{CE} & 1 \\\\\n\\vdots  & \\vdots & \\vdots \\\\\n100  &  \\mathrm{CE}  & 0 \\\\\n1 & \\mathrm{PE} & 1 \\\\\n\\vdots  & \\vdots & \\vdots \\\\\n210  &  \\mathrm{PE}  & 0\n\\end{array}\n\\] Tabela Observada \\[\n\\begin{array}{c|cc|c}\n&     \\mathrm{Sinistralidade}   &  \\\\\n\\mathrm{Estado}  & 1 & 0 &  \\mathrm{Total} \\\\\n\\hline\n\\mathrm{SP} & 60 & 140 & 200 \\\\\n\\mathrm{CE} & 10 & 90 & 100 \\\\\n\\mathrm{PE} & 50 & 160 & 210 \\\\\n\\hline\n\\mathrm{Total} & 120 & 390 & 510\n\\end{array}\n\\] Tabela esperada \\[\n\\begin{array}{c|cc|c}\n&     \\mathrm{Sinistralidade}   &  \\\\\n\\mathrm{Estado}  & 1 & 0 &  \\mathrm{Total} \\\\\n\\hline\n\\mathrm{SP} & 47 & 153 & 200 \\\\\n\\mathrm{CE} & 24 & 76 & 100 \\\\\n\\mathrm{PE} & 49 & 161 & 210 \\\\\n\\hline\n\\mathrm{Total} & 120 & 390 & 510\n\\end{array}\n\\] Temos nossa estatística qui-quadrado \\[\n\\begin{aligned}\n\\chi^2_{obs} &= \\frac{(60-47)^2}{47} + \\frac{(140-153)^2}{153} + \\frac{(10-24)^2}{24} + \\frac{(90-76)^2}{76} \\\\\n&+ \\frac{(50-49)^2}{49} + \\frac{(160-161)^2}{161} = 15.47\n\\end{aligned}\n\\] Concluiremos o teste tomando \\(\\alpha = 1\\%\\) de significância estatística Sabemos que, sob \\(H_{0}\\), \\[\n\\chi^2_{obs} \\sim \\chi^2_{(3-1)(2-1)}\n\\] Logo, devemos encontrar \\(c_{p}\\) tal que \\[\nP(\\chi^2_{2}&gt; c_{p}) = 1\\%\n\\] Pela tabela, \\(c_{p}=9.21\\) Como \\(15.47 &gt; 9.21\\), concluímos que, a sinistralidade não se distribui de forma homogênea entre os estados de SP, CE e PE a \\(1\\%\\) de significância.\n\n\n18.3.2 Outro Exemplo (independência)\nTemos nossa tabela de valores observados: \\[\n\\begin{array}{c|ccc|c}\n\\mathrm{Opinião}  & \\mathrm{1ª~Tent}  & \\mathrm{2ª~Tent}  & \\mathrm{3ª~Tent}  & \\mathrm{Total} \\\\\n\\hline\n\\mathrm{Excelente}  &  62  & 36 & 12 & 110\\\\\n\\mathrm{Satisfatório}  & 84 & 42 & 14 & 140\\\\\n\\mathrm{Insatisfatório}  & 24 & 22 & 24 & 70 \\\\\n\\hline\n\\mathrm{Total}  & 170 & 100 & 50 & 320\n\\end{array}\n\\] Nossa tabela de valores esperados (arredondados): \\[\n\\begin{array}{c|ccc|c}\n\\mathrm{Opinião}  & \\mathrm{1ª~Tent}  & \\mathrm{2ª~Tent}  & \\mathrm{3ª~Tent}  & \\mathrm{Total} \\\\\n\\hline\n\\mathrm{Excelente}  &  58 & 34 & 17 & 110 \\\\\n\\mathrm{Satisfatório}  & 74 & 44 & 22 & 140\\\\\n\\mathrm{Insatisfatório}  & 37 & 22 & 11 & 70 \\\\\n\\hline\n\\mathrm{Total}  & 170 & 100 & 50 & 320\n\\end{array}\n\\] \\[\n\\begin{aligned}\n\\chi^2_{obs} &= \\frac{(62-58)^2}{58} + \\frac{(36-34)^2}{34} + \\frac{(12-17)^2}{17} + \\frac{(84-74)^2}{74} +\n\\frac{(42-44)^2}{44} \\\\\n&+ \\frac{(14-22)^2}{22} + \\frac{(24-37)^2}{37} + \\frac{(22-22)^2}{22} + \\frac{(24-11)^2}{11} = 26.14\n\\end{aligned}\n\\] Considerando \\(\\alpha= 5\\%\\), precisamos encontrar \\(c_{p}\\) tal que \\[\nP(\\chi^2_{4}&gt;c_{p})=5\\% \\Rightarrow c_{p} = 9.49\n\\] Como \\(26.14 &gt; 9.49\\), podemos concluir que, a \\(5\\%\\) de significância estatística, existem evidências que o número da tentativa tem influência sobre a opinião do cliente.",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Testes de Independência e Homogeneidade</span>"
    ]
  },
  {
    "objectID": "familia-exponencial.html",
    "href": "familia-exponencial.html",
    "title": "19  Família Exponencial (FE)",
    "section": "",
    "text": "19.1 Família Exponencial unidimensional\nPodemos representar a função \\(f_\\theta\\) genericamente, incluindo funções de probabilidade e funções densidade de probabilidade, por meio da família exponencial.\nDizemos que \\(f_\\theta\\) pertence à família exponencial unidimensional se, e somente se,\n\\[\nf_\\theta(x) = \\left \\{\\begin{array}{lr}\n\\mathrm{e}^{c(\\theta)\\cdot T(x) + d(\\theta) + S(x)}, & x \\in \\mathfrak{X} \\\\\n0, & x \\not \\in \\mathfrak{X}\n\\end{array} \\right.\n\\]\nem que \\(c(\\cdot)\\) e \\(d(\\cdot)\\) são funções de “\\(\\theta\\)” cujas formas são conhecidas e \\(T(\\cdot)\\) e \\(S(\\cdot)\\) são funções de \\(x\\) com formas conhecidas, em que \\(\\mathfrak{X} = \\{x: f_\\theta(x) &gt; 0\\}\\) não depende de “\\(\\theta\\)”. \\(\\mathfrak{X}\\) é dito ser o suporte de \\(f_\\theta\\)",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Família Exponencial (FE)</span>"
    ]
  },
  {
    "objectID": "familia-exponencial.html#família-exponencial-unidimensional",
    "href": "familia-exponencial.html#família-exponencial-unidimensional",
    "title": "19  Família Exponencial (FE)",
    "section": "",
    "text": "19.1.1 Exemplos\n\n19.1.1.1 Exemplo 1 - Exponencial\nconsidere \\(X \\sim f_\\theta, \\theta \\in (0,\\infty)\\) e \\[\nf_\\theta(x) = \\left\\{\\begin{array}{lr}\n\\theta \\cdot \\mathrm{e}^{-\\theta x}, & x &gt; 0 \\\\\n0, & \\mathrm{c.c}\n\\end{array}\\right.\n\\]\nOu seja, \\(X\\sim \\mathrm{Exp}(\\theta), \\theta &gt; 0\\). Mostre que \\(f_\\theta\\) pertence à família exponencial.\n\n19.1.1.1.1 Resposta\nNote que \\(\\mathfrak{X}=\\{x:f_\\theta(x)&gt;0\\} = (0, \\infty)\\) não depende de “\\(\\theta\\)”. Além disso, como \\(\\theta &gt; 0\\), temos que \\(\\theta = \\mathrm{e}^{\\ln(\\theta)}\\). Portanto, para \\(x&gt;0\\), temos que \\[\nf_\\theta(x) = \\mathrm{e}^{-\\theta x + \\ln \\theta}\n\\] Assim, \\[\n\\begin{array}{cc}\nc(\\theta) = -\\theta, & d(\\theta) = \\ln \\theta \\\\\nT(x) = x, & S(x) = 0\n\\end{array}\n\\]\n\n\n\n19.1.1.2 Exemplo 2 - Bernoulli\nconsidere \\(X\\sim \\mathrm{Ber}(\\theta), \\theta \\in \\Theta = (0,1)\\). Mostre que a sua função de probabilidade pertence à família exponencial.\n\n19.1.1.2.1 Resposta\nObserve que \\[\nf_\\theta(x) = \\left\\{\\begin{array}{lr}\n\\theta^x \\cdot (1-\\theta)^{1-x}, & x \\in \\{0,1\\}\n0, & x \\not \\in \\{0,1\\}\n\\end{array}\\right.\n\\] O suporte é \\(\\mathfrak{x} = \\{ x : f_\\theta(x)&gt;0\\} = \\{0,1\\}\\) não depende de “\\(\\theta\\)”.\nAlém disso, como \\(\\theta^x (1-\\theta)^{1-x} &gt; 0\\), temos que \\[\n\\begin{aligned}\n\\theta^x (1-\\theta)^{1-x} &= \\mathrm{e}^{\\ln(\\theta^x(1-\\theta)^{1-x})}\\\\\n&= \\mathrm{e}^{x\\ln\\theta + (1-x) \\ln (1-\\theta)}\\\\\n&=\\mathrm{e}^{x\\ln\\theta + \\ln (1-\\theta) - x \\ln(1-\\theta)} \\\\\n&= \\mathrm{e}^{x\\left(\\ln \\theta - \\ln(1-\\theta)\\right) + \\ln(\\1-\\theta)}\\\\\n&=\\mathrm{e}^{\\ln\\left(\\frac{\\theta}{1-\\theta}\\right)x+\\ln(1-\\theta)}\n\\end{aligned}\n\\]\nlogo, \\(f_\\theta(x) = \\mathrm{e}^{c(\\theta)T(x)+d(\\theta)+S(x)}, x \\in \\{0,1\\}\\), em que \\[\n\\begin{array}{cc}\nc(\\theta) = \\ln(\\frac{\\theta}{1-\\theta}), & T(x) = x \\\\\nd(\\theta) = \\ln(1-\\theta), & S(x) = 0\n\\end{array}\n\\]\n\n\n\n19.1.1.3 Exemplo 3 - Normal (Média)\nconsidere que \\(X\\sim N(\\theta,1), \\theta \\in \\mathbb{R}\\). Mostre que a sua função densidade de probabilidade pertence a FE unidimensional.\n\n\n19.1.1.4 Resposta\nNote que \\[\nf_\\theta(x) = \\frac{1}{\\sqrt{2\\pi}} \\mathrm{e} ^{-\\frac{1}{2} (x-\\theta)^2}, x \\in \\mathbb{R}\n\\]\nO suporte é \\(\\mathfrak{X} = \\{x: f_\\theta(x) &gt; 0\\} = (-\\infty, \\infty)\\) e não depende de “\\(\\theta\\)”. Além disso, temos que \\[\n\\begin{aligned}\nf_\\theta(x) & = \\mathrm{e}^{-\\frac{1}{2}(x-\\theta)^2-\\ln\\sqrt{2\\pi}} \\\\\n&= \\mathrm{e}^{-\\frac{1}{2}(x^2 - 2x\\theta + \\theta^2) - \\ln\\sqrt{2\\pi}} \\\\\n&= \\mathrm{e}^{\\theta x - \\frac{1}{2}\\theta^2 - \\frac{1}{2}x^2 - \\ln\\sqrt{2\\pi}}\n\\end{aligned}\n\\]\nPortanto, \\(f_\\theta\\) pertence à FE: \\[\n\\begin{array}{cc}\nc(\\theta) = \\theta, & T(x) = x \\\\\nd(\\theta) = -\\frac{1}{2}\\theta^2, & S(x) = -\\frac{1}{2}x^2-\\ln\\sqrt{2\\pi}\n\\end{array}\n\\]\n\n\n19.1.1.5 Exempo 4 - Normal (Variância)\nconsidere que \\(X\\sim N(0,\\theta), \\theta \\in (0,\\infty)\\). Mostre que a sua função densidade de probabilidade pertence a FE unidimensional.\n\n19.1.1.5.1 Resposta\nObserve que \\[\nf_\\theta(x) = \\frac{1}{\\sqrt{2\\pi\\theta}} \\mathrm{e}^{-\\frac{1}{2\\theta}x^2}, x \\in \\mathbb{R}\n\\]\nO suporte \\(\\mathfrak{X} = \\mathbb{R}\\) não depende de “\\(\\theta\\)”. Além disso, \\[\n\\begin{aligned}\nf_\\theta(x) &= \\mathrm{e}^{-\\frac{1}{2\\theta}x^2 - \\frac{1}{2} \\ln(2\\pi\\theta)} \\\\\n&= \\mathrm{e}^{c(\\theta)T(x)+d(\\theta)+S(x)}\n\\end{aligned}\n\\]\nem que \\[\n\\begin{array}{cc}\nc(\\theta) = -\\frac{1}{2\\theta}, & T(x) = x^2 \\\\\nd(\\theta) = -\\frac{1}{2}\\ln(2\\pi\\theta), & S(x) = 0\n\\end{array}\n\\]\nPortanto, \\(f_\\theta\\) pertence à FE.\n\n\n\n19.1.1.6 Exemplo 5 - Normal (Média = Variância)\nSeja \\(X\\sim N(\\theta, \\theta)\\), em que \\(\\theta \\in (0, \\infty)\\). Mostre que a sua função densidade de probabilidade pertence à FE de dimensão 1.\n\n19.1.1.6.1 Resposta\nA função densidade de probabilidade de \\(X\\) é \\[\n\\begin{aligned}\nf_\\theta(x) &= \\frac{1}{\\sqrt{2\\pi\\theta}} \\mathrm{e}^{-\\frac{1}{2\\theta}(x-\\theta)^2} \\\\\n&= \\mathrm{e}^{-\\frac{1}{2\\theta}(x^2-2\\theta x+ \\theta^2) -\\frac{1}{2}\\ln(2\\pi\\theta)} \\\\\n&= \\mathrm{e}^{-\\frac{x^2}{2\\theta}+x-\\frac{1}{2}\\theta-\\frac{1}{2}\\ln(2\\pi\\theta)} \\\\\n&= \\mathrm{e}^{c(\\theta)T(x)+d(\\theta)+S(x)}\n\\end{aligned}\n\\] em que \\[\n\\begin{array}{cc}\nc(\\theta) = -\\frac{1}{2\\theta}, & T(x) = x^2 \\\\\nd(\\theta) = -\\frac{1}{2}\\theta-\\frac{1}{2}\\ln(2\\pi\\theta), & S(x) = x\n\\end{array}\n\\]\n\n\n\n19.1.1.7 Exemplo 6 - contraexemplo uniforme\nSe \\(X\\sim U(0,\\theta), \\theta \\in (0,\\infty)\\), então a sua função densidade de probabilidade não pertence à FE, pois o seu suporte depende de “\\(\\theta\\)”. \\[\nf_\\theta(x) = \\left\\{\\begin{array}{lr}\n\\frac{1}{\\theta}, & x \\in (0, \\theta)\n0, & x \\not \\in (0, \\theta)\n\\end{array}\\right.\n\\]\nDessa forma, \\(\\mathfrak{X} = \\{ x : f_\\theta(x)&gt; 0\\} = (0,\\theta)\\)\n\n\n19.1.1.8 Exemplo 7 - contraexemplo Normal (Média e Variância)\nconsidere que \\(X \\sim N(\\mu,\\sigma^2)\\), em que \\(\\theta = (\\mu, \\sigma^2) \\in \\mathbb{R}\\times\\mathbb{R}^+\\). Então, pode-se mostrar que a sua função densidade de probabilidade não pertence à FE unidimensional.\nObserve que \\[\n\\begin{aligned}\nf_\\theta(x) &= \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\mathrm{e}^{-\\frac{1}{2\\sigma^2}(x-\\mu)^2} \\\\\n&= \\mathrm{e}^{-\\frac{1}{2\\sigma^2}(x^2-2x\\mu+\\mu^2) - \\frac{1}{2} \\ln (2\\pi\\sigma^2)} \\\\\n&= \\mathrm{e}^{-\\frac{1}{2\\sigma^2}x^2+\\frac{x\\mu}{\\sigma^2}-\\frac{1}{2\\sigma^2}\\mu^2-\\frac{1}{2}\\ln(2\\pi\\sigma^2)}\n\\end{aligned}\n\\]\nportanto, não é possível definir \\(c(\\theta),T(x),d(\\theta)\\) e \\(S(x)\\) tais que \\(c(\\cdot), T(\\cdot)\\) representem \\(-\\frac{1}{2\\sigma^2}x^2 + \\frac{x\\mu}{\\sigma^2}\\)\n\n\n\n19.1.2 Propriedades na integração\ncomo \\(f_\\theta\\) é uma função (densidade) de probabilidade, temos que \\[\n\\int_{-\\infty}^{\\infty} f_\\theta(x) dx = 1 \\mathrm{(caso \\ contínuo)}, \\forall \\theta \\in \\Theta\n\\]\nSe \\(f_\\theta\\) pertence à FE unidimensional, então\n\\[\n\\begin{aligned}\n&\\int_{\\mathfrak{X}} \\mathrm{e}^{c(\\theta)T(x) +d(\\theta) + S(x)} dx = 1 \\forall \\theta \\in \\Theta \\\\\n\\Rightarrow& \\int_{\\mathfrak{X}} \\mathrm{e}^{c(\\theta)T(x) + S(x)} dx = \\mathrm{e}^{-d(\\theta)} \\forall \\theta \\in \\Theta \\\\\n\\end{aligned}\n\\]",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Família Exponencial (FE)</span>"
    ]
  },
  {
    "objectID": "familia-exponencial.html#sec-fek",
    "href": "familia-exponencial.html#sec-fek",
    "title": "19  Família Exponencial (FE)",
    "section": "19.2 Família Exponencial k-dimensional",
    "text": "19.2 Família Exponencial k-dimensional\nDizemos que a função (densidade) de probabilidade \\(f_\\theta\\) pertence à FE k-dimensional se, e somente se, \\[\nf_\\theta(x) = \\left\\{\\begin{array}{lr}\n\\mathrm{e}^{\\sum^k_{j=1} c_j(\\theta)T_j(x)+d(\\theta)+S(x)}, &x \\in \\mathfrak{X} \\\\\n0, & x \\not \\in \\mathfrak{X}\n\\end{array}\\right.\n\\] em que 1. \\(\\mathfrak{X} = \\{ x: f_\\theta (x) &gt; 0 \\}\\) não depende de “\\(\\theta\\)” 2. As funções \\(c_1(\\cdot),\\dots,c_k(\\cdot)\\) e \\(d(\\cdot)\\) dependem apenas de “\\(\\theta\\)” (formas conhecidas) e 3. As funções \\(T_1(\\cdot),\\dots,T_l(\\cdot)\\) e \\(S(\\cdot)\\) dependem apenas de \\(x\\) (formas conhecidas)\ncomo \\(f_\\theta\\) é uma função (densidade) de probabilidade, temos que (para o caso contínuo) \\[\n\\int_{-\\infty}^{\\infty} f_\\theta(x)dx=1 \\forall \\theta \\in \\Theta\n\\]\nSe \\(f_\\theta\\) pertence à FE k-dimensional, então \\[\n\\begin{aligned}\n&\\int_{\\mathfrak{X}} \\mathrm{e}^{c(\\theta)T(x) +d(\\theta) + S(x)} dx = 1 \\forall \\theta \\in \\Theta \\\\\n\\Rightarrow &\\int_{\\mathfrak{X}} \\mathrm{e}^{c(\\theta)T(x) + S(x)} dx = \\mathrm{e}^{-d(\\theta)} \\forall \\theta \\in \\Theta \\\\\n\\end{aligned}\n\\]\n\n19.2.1 Exemplos\n\n19.2.1.1 Exemplo 1 - Normal (Média e Variância)\nSeja \\(X\\sim N(\\mu, \\sigma^2)\\), em que \\(\\theta = (\\mu, \\sigma^2) \\in \\mathbb{R}\\times\\mathbb{R}^+\\). Mostre que a sua função densidade de probabilidade pertence à FE de dimensão 2.\n\n19.2.1.1.1 Resposta\nNote que \\[\n\\begin{aligned}\nf_\\theta(x) &= \\mathrm{e}^{-\\frac{1}{2\\sigma^2}x^2+\\frac{x\\mu}{\\sigma^2}-\\frac{1}{2\\sigma^2}\\mu^2-\\frac{1}{2}\\ln(2\\pi\\sigma^2)}\n&= \\mathrm{e}^{c_1(\\theta)T_1(x) + c_2(\\theta)T_2(x) + d(\\theta) + S(x)}\n\\end{aligned}\n\\]\nem que \\[\n\\begin{array}{cc}\nc_1(\\theta) = -\\frac{1}{2\\sigma^2}, & T_1(x) = x^2 \\\\\nc_2(\\theta) = \\frac{\\mu}{\\sigma^2}, & T_2(x) = x \\\\\nd(\\theta) = -\\frac{1}{2}\\frac{\\mu^2}{\\sigma^2}-\\frac{1}{2}\\ln(2\\pi\\sigma^2), & S(x) = 0\n\\end{array}\n\\]\n\n\n\n\n19.2.2 Exercício\nRefaça o exemplo 5 do caso univarido com \\(N(\\theta, \\theta^2)\\) e mostre que pertence à FE de dimensão 2.",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Família Exponencial (FE)</span>"
    ]
  },
  {
    "objectID": "distribuicao-amostral.html",
    "href": "distribuicao-amostral.html",
    "title": "20  Distribuição Amostral - Aprofundamento",
    "section": "",
    "text": "20.1 Relação com a FE\nSeja \\((Y_1,\\dots,Y_n)\\) uma amostra aleatória de \\(Y\\sim f_\\theta, \\theta \\in \\Theta\\). A função (densidade) de probabilidade da amostra aleatória é dada por \\[\nf_\\theta^{(n)}(y_1,\\dots,y_n) \\stackrel{=}{\\mathrm{iid}} = \\prod_{i=1}^n f_\\theta (y_i), \\forall \\theta \\in \\Theta\n\\] e \\(X_i \\in \\mathbb{R}, i = 1,\\dots,n\\)",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Distribuição Amostral - Aprofundamento</span>"
    ]
  },
  {
    "objectID": "distribuicao-amostral.html#relação-com-a-fe",
    "href": "distribuicao-amostral.html#relação-com-a-fe",
    "title": "20  Distribuição Amostral - Aprofundamento",
    "section": "",
    "text": "20.1.1 Unidimensional\nSe \\(f_\\theta\\) pertence à Família Exponencial Unidimensional, então \\[\n\\begin{aligned}\n&f_\\theta^{(n)}(y_1,\\dots,y_n) =\n\\left\\{ \\begin{array}{lr}\n\\prod^n_{i=1} \\mathrm{e}^{c(\\theta)T(y_i) +d(\\theta)+S(y_i)}, & \\mathrm{se}\\ y_1,\\dots,y_n \\in \\mathfrak{X} \\\\\n0, & \\mathrm{c.c}\n\\end{array}\\right. \\\\\n\\Rightarrow\n&f_\\theta^{(n)}(\\underbracket{y_1,\\dots,y_n}_{\\pmb{y}_n}) =\n\\left\\{ \\begin{array}{lr}\n\\mathrm{e}^{c(\\theta)\\sum^n_{i=1}T(y_i) +nd(\\theta)+\\sum^n_{i=1}S(y_i)}, & \\mathrm{se}\\ \\pmb{y}_n\\in \\mathfrak{X}^n \\\\\n0, & \\mathrm{c.c}\n\\end{array}\\right.\n\\end{aligned}\n\\]\nEm que \\(\\mathfrak{X}^n = \\mathfrak{X} \\times \\dots \\times \\mathfrak{X}\\)\nPortanto, \\(f_\\theta^{(n)}\\) pertence à FE unidimensional.\n\n\n20.1.2 k-dimensional\nSe \\(f_\\theta\\) pertence à FE k-dimensional, então \\[\n\\begin{aligned}\n&f_\\theta^{(n)}(\\pmb{y}_n) =\n\\left\\{ \\begin{array}{lr}\n\\prod^n_{i=1} \\mathrm{e}^{\\sum^k_{j=1}c_j(\\theta)T_j(y_i) +d(\\theta)+S(y_i)}, & \\mathrm{se}\\ \\pmb{y}_n \\in \\mathfrak{X}^n \\\\\n0, & \\mathrm{c.c}\n\\end{array}\\right. \\\\\n\\Rightarrow\n&f_\\theta^{(n)}(\\pmb{y}_n) =\n\\left\\{ \\begin{array}{lr}\n\\mathrm{e}^{\\sum^k_{j=1}c_j(\\theta)\\sum^n_{i=1}T_j(y_i) +nd(\\theta)+\\sum^n_{i=1}S(y_i)}, & \\mathrm{se}\\ \\pmb{y}_n \\in \\mathfrak{X}^n \\\\\n0, & \\mathrm{c.c}\n\\end{array}\\right.\n\\end{aligned}\n\\]\nTome \\(T_j^*(\\pmb{y}_n) = \\sum^n_{i=1}T_j(y_i)\\), \\(d^*(\\theta)=nd(\\theta)\\), \\(S^*(\\pmb{y}_n) \\sum^n_{i=1}S(y_i)\\) \\[\n\\Rightarrow f_\\theta^{(n)}(\\pmb{y}_n) =\n\\left\\{ \\begin{array}{lr}\n\\mathrm{e}^{\\sum^k_{j=1}c_j(\\theta)T^*_j(\\pmb{y}_n) +d^*(\\theta)+S^*(\\pmb{y}_n)}, & \\mathrm{se}\\ \\pmb{y}_n \\in \\mathfrak{X}^n \\\\\n0, & \\mathrm{c.c}\n\\end{array}\\right.\n\\]\nPortanto, \\(f_\\theta^{(n)}\\) pertence à FE k-dimensional.\n\n\n20.1.3 Exemplos\n\n20.1.3.1 Bernoulli\nSe \\(X\\sim\\mathrm{Ber}(\\theta), \\theta \\in (0,1)\\), então \\[\n\\begin{aligned}\n&f_\\theta(y) = \\left\\{\\begin{array}{ll}\n\\theta^y \\cdot (1-\\theta)^{1-y}, & y \\in \\{0,1\\} \\\\\n0, & \\mathrm{c.c.}\n\\end{array} \\right. \\\\\n\\Rightarrow\n&f_\\theta(y) = \\left\\{\\begin{array}{ll}\n\\mathrm{e}^{\\ln\\left(\\frac{\\theta}{(1-\\theta)}\\right)y+\\ln(1-\\theta)}, & y \\in \\{0,1\\} \\\\\n0, & \\mathrm{c.c.}\n\\end{array} \\right.\n\\end{aligned}\n\\]\nA função probabilidade da amostra é \\[\nf_\\theta(\\pmb{y}_n) = \\left\\{\\begin{array}{ll}\n\\mathrm{e}^{\\ln\\left(\\frac{\\theta}{(1-\\theta)}\\right)\\sum_{i=1}^n y_i+n\\ln(1-\\theta)}, & \\pmb{y}_n \\in \\{0,1\\}^n \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right.\n\\]\n\n\n20.1.3.2 Binomial\nSe \\(X\\sim\\mathrm{Bin}(m, \\theta), \\theta \\in (0,1)\\) e \\(m\\) fixado, então \\[\nf_\\theta(y) = \\left\\{\\begin{array}{ll}\n\\binom{m}{y}\\theta^y \\cdot (1-\\theta)^{m-y}, & y \\in \\{0,1,\\dots,m\\} \\\\\n0, & \\mathrm{c.c.}\n\\end{array} \\right.\n\\]\nNote que \\[\n\\begin{aligned}\n\\binom{m}{y} \\theta^y(1-\\theta)^{m-y} &= \\mathrm{e}^{\\ln\\binom{m}{y} + y \\ln\\theta + (m-y)\\ln(1-\\theta)} \\\\\n&= \\mathrm{e}^{\\ln\\binom{m}{y} + y\\ln(\\theta) + m\\ln(1-\\theta) - y\\ln(1-\\theta)}\n\\end{aligned}\n\\]\nPortanto, \\[\nf_\\theta(y) = \\left\\{\\begin{array}{ll}\n\\mathrm{e}^{\\ln\\left(\\frac{\\theta}{1-\\theta}\\right) y + m \\ln(1-\\theta) + \\ln\\binom{m}{y}}, & y \\in \\{0,1\\dots,m\\} \\\\\n0, & \\mathrm{c.c.}\n\\end{array} \\right.\n\\]\nA função probabilidade da amostra é \\[\nf_\\theta(\\pmb{y}_n) = \\left\\{\\begin{array}{ll}\n\\mathrm{e}^{\\ln\\left(\\frac{\\theta}{1-\\theta}\\right)\\sum^n_{i=1}y_i + nm \\ln(1-\\theta) + \\sum_{i=1}^n\\ln\\binom{m}{y_i}}, & \\pmb{y}_n \\in \\mathfrak{X}^n \\\\\n0, & \\mathrm{c.c.}\n\\end{array} \\right.\n\\]\nem que \\(\\mathfrak{X} = \\{0,1,\\dots,m\\}\\)\n\n\n20.1.3.3 Dirichlet\n\nSe \\(X\\sim\\mathrm{Dirichlet}(\\alpha_1,\\dots,\\alpha_k)\\), então \\(X=(Y_1,\\dots,Y_k)^T\\) é um vetor (coluna) aleatório k-dimensional cuja função densidade de probabilidade é dada por \\[\nf_\\theta(y) = \\left\\{\\begin{array}{ll}\n\\frac{\\Gamma(\\sum^k_{j=1} \\alpha_j)}{\\prod^k_{j=1}\\Gamma(\\alpha_j)} \\cdot \\prod^k_{j=1} y_j^{\\alpha_j-1}, & y_j \\in \\mathfrak{X} \\\\\n0, & \\mathrm{c.c.}\n\\end{array} \\right.\n\\]\nem que \\(\\mathfrak{X} = \\{(y_1,\\dots,y_k) \\in (0,1)^k : \\sum^k_{j=1} y_j = 1\\}\\) e o vetor de parâmetros é \\(\\theta=(\\alpha_1,\\dots,\\alpha_k) \\in \\mathbb{R}^k_+\\).\nA amostra aleatória é \\[\nX_1 = \\left(\n\\begin{array}{c}\nY_{11}\\\\\n\\vdots \\\\\nY_{k1}\n\\end{array}\n\\right),\nX_2 = \\left(\n\\begin{array}{c}\nY_{12} \\\\\n\\vdots \\\\\nY_{k2}\n\\end{array}\n\\right), \\dots,\nX_n = \\left(\n\\begin{array}{c}\nY_{1n} \\\\\n\\vdots \\\\\nY_{kn}\n\\end{array}\n\\right)\n\\]\ne sua função densidade de probabilidade é dada por \\[\nf_\\theta(\\pmb{y}_n) \\stackrel{\\mathrm{iid}}{=}\\left\\{\\begin{array}{ll}\n\\left[\\frac{\\Gamma(\\sum^k_{j=1} \\alpha_j)}{\\prod^k_{j=1}\\Gamma(\\alpha_j)}\\right]^n \\cdot \\prod_{i=1}^n\\left(\\prod^k_{j=1} y_{ij}^{\\alpha_j-1}\\right), & y_i \\in \\mathfrak{X}, \\forall i=1,\\dots,n \\\\\n0, & \\mathrm{c.c.}\n\\end{array} \\right.\n\\]\nTome \\[\ng(\\theta) = \\left[\n\\frac{\\Gamma(\\sum^k_{j=1}\\alpha_j)}{\\prod^k_{j=1}\\Gamma(\\alpha_j}\n\\right]^n\n\\]\n\\[\n\\Rightarrow\nf_\\theta^{(n)}(\\pmb{y}_n) = \\left\\{ \\begin{array}{ll}\ng(\\theta) \\cdot \\prod^n_{i=1}\\prod^k_{j=1} y_{ij}^{\\alpha_j-1}, & y_1,\\dots,y_n \\in \\mathfrak{X} \\\\\n0,  & \\mathrm{c.c.}\n\\end{array}\\right.\n\\]\nNote que \\[\n\\begin{aligned}\ng(\\theta) \\cdot \\prod^n_{i=1} \\prod^k_{j=1} y_{ij}^{\\alpha_j-1} &= \\mathrm{e}^\n{\\ln g(\\theta) + \\sum^n_{i=1} \\sum^k_{j=1}(\\alpha_j-1)\\ln y_{ij}} \\\\\n&= \\mathrm{e}^{\\ln g(\\theta) + \\sum^k_{j=1}(\\alpha_j -1) \\sum^n_{i=1} \\ln y_{ij}}\n\\end{aligned}\n\\]\n\n\\[\n\\Rightarrow\nf_\\theta^{(n)}(\\pmb{y}_n) = \\left\\{ \\begin{array}{ll}\n\\mathrm{e}^{\\sum^k_j=1} c_j^*(\\theta)T_j^*(\\pmb{y}_n)+d^*(\\theta) = S^*(\\pmb{y}_n), & \\pmb{y}_n \\in \\mathfrak{X}^n \\\\\n0,  & \\mathrm{c.c.}\n\\end{array}\\right.\n\\]\nPortanto, pertence à FE k-dimensional\n\n\n\n20.1.4 Observação\nA partir de \\(f_\\theta^{(n)}\\) conseguimos fazer inferência sobre a quantidade de interesse Podemos encontrar a distribuição de estatísticas e estimadores.",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Distribuição Amostral - Aprofundamento</span>"
    ]
  },
  {
    "objectID": "estatisticas-suficientes.html",
    "href": "estatisticas-suficientes.html",
    "title": "21  Estatísticas Suficientes",
    "section": "",
    "text": "21.1 Caso discreto\nSeja \\(\\pmb{X}_n(x_1,\\dots,x_n)\\) uma amostra aleatória de \\(X\\sim f_\\theta,\n\\theta \\in \\Theta\\). Dizemos que uma estatística \\(T(\\pmb{X}_n\\) é suficinete para o modelo estatístico se, e somente se, a distribuição da amosta dado que \\(T(\\pmb{X}_n = t\\) não depende de “\\(\\theta\\)”. Ou seja, \\[\n\\begin{aligned}\nP_\\theta(X_1\\leq y_1,\\dots,X_n\\leq y_n \\lvert T(\\pmb{X}_n)=t)\\\n\\text{Não depende de}\\ \\theta, \\forall y_1,\\dots,y_n \\in \\mathbb{R} \\\\\n\\text{E para todo valor de}\\ t \\ \\text{para o quais a distribuição de}\\ T(\\pmb{X}_n)\\ \\text{exista}\n\\end{aligned}\n\\]\nEm outras palavras, a informação probabilística sobre “\\(\\theta\\)” da amostra aleatória está inteiramente contida no modelo induzido pela estatística.\nNo caso discreto, basta mostrar que \\(P_\\theta(X_1=y_1,\\dots,X_n=y_n\\lvert T(\\pmb{X}_n)=t)\\) não depende de “\\(\\theta\\)” para todo \\(y_1,\\dots,y_n \\in \\mathbb{R}\\) e valores de \\(t\\) para os quais a distribuição de \\(T(\\pmb{X}_n)\\) exista.\nNo caso contínuo, basta mostrar que \\(f_\\theta^{(n)}(y_1,\\dots,y_n\\lvert t)\\) não depende de \\(\\theta\\) para todo \\(y_1,\\dots,y_n \\in \\mathbb{R}\\) e valores de t para os quais a função densidade de probabilidade de \\(T(\\pmb{X}_n)\\) exista.\nComo discutiremos adiante, podemos substituir a restrição \\(\\in \\mathbb{R}\\) pelo termo “quase certamente” (\\(\\mathrm{q.c.}\\), isto é, para todos exceto um conjunto enumerável (de medida de probabilidade nula).\nTambém, \\(f_\\theta^{(n)}\\) será reescrita por \\(f_\\theta^{\\pmb{X}_n}\\) para diferenciar a função (densidade) da amostra, da estatística e das condicionais.",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Estatísticas Suficientes</span>"
    ]
  },
  {
    "objectID": "estatisticas-suficientes.html#caso-discreto",
    "href": "estatisticas-suficientes.html#caso-discreto",
    "title": "21  Estatísticas Suficientes",
    "section": "",
    "text": "21.1.1 Exemplos\n\n21.1.1.1 Exemplo 1\nSeja \\(\\pmb{X}_n = (X_1, \\dots, X_n)\\) uma a.a. de \\(X\\sim\\mathrm{Ber}(\\theta), \\theta \\in \\Theta = (0,1)\\). Verifique se \\(T(\\pmb{X}_n)=\\sum^n_{i=1}X_i\\) é suficiente para o modelo estatístico.\n\n21.1.1.1.1 Resposta\nPor definição, \\[\nP_\\theta(X_1=y_1,\\dots,X_n=y_n\\lvert T(\\pmb{X}_n) = t) = \\frac{P_\\theta(X_1=y_1,\\dots,X_n=y_n,T(\\pmb{X}_n)=t)}\n{P_\\theta(T(\\pmb{X}_n)=t)}\n\\]\nSabemos que \\(T(\\pmb{X}_n)=\\sum^n_{i=1}X_i \\sim \\mathrm{Bin}(n,\\theta)\\) (por função geradora de momentos). Portanto, \\[\nP_\\theta(T(\\pmb{X}_n)=t) = \\left\\{ \\begin{array}{ll}\n\\binom{n}{t} \\cdot \\theta^t\\cdot(1-\\theta)^{n-t},&\\text{Se}\\ t \\in \\{0,1,\\dots,n\\} \\\\\n0, & \\mathrm{c.c.}\n\\end{array} \\right.\n\\]\nLogo, \\(P_\\theta(X_1=y_1,\\dots,X_n=y_n\\lvert T(\\pmb{X}_n) = t)\\) só está bem definida se \\(t \\in \\{0,1,\\dots,n\\}\\)\n(Numerador) \\[\n\\begin{aligned}\n&P_\\theta(X_1=y_1,\\dots,X_n=y_n,T(\\pmb{X}_n)=t) \\\\\n&\\stackrel{\\mathrm{TPT}}{=}\n\\overbracket{P_\\theta(X_1=y_1,\\dots,X_n=y_n)}^{A} \\cdot \\overbracket{P_\\theta(T(\\pmb{X}_n)=t\\lvert X_1=y_1,\\dots,X_n=y_n)}^B\n\\end{aligned}\n\\]\nObserve que \\(A\\) é a função probabilidade da amostra e \\[\nB = \\left \\{ \\begin{array}{ll}\n1, & \\text{Se}\\ t = \\sum^n_{i=1} y_i \\\\\n0, & \\mathrm{c.c}\n\\end{array}\\right.\n\\]\nPara \\(t \\in \\{0,1,\\dots,n\\}\\)\n\\[\nP_\\theta(X_1=y_1,\\dots,X_n=y_n\\lvert T(\\pmb{X}_n) = t) =\n\\left \\{ \\begin{array}{ll}\n\\frac{\\prod^n_{i=1}\\theta^{y_i}(1-\\theta)^{1-y_i}}{\\binom{n}{t}\\cdot\\theta^t\\cdot(1-\\theta)^{n-t}}, & \\text{Se}\\ t = \\sum^n_{i=1}y_i\\\\\n0, & \\mathrm{c.c}\n\\end{array}\\right. \\forall \\theta \\in \\Theta\n\\]\nPortanto, \\[\nP_\\theta(X_1=y_1,\\dots,X_n=y_n\\lvert T(\\pmb{X}_n) = t) =\n\\left \\{ \\begin{array}{ll}\n\\frac{\\theta^{\\sum^n_{i=1}y_i}(1-\\theta)^{\\sum^n_{i=1}1-y_i}}{\\binom{n}{t}\\cdot\\theta^t\\cdot(1-\\theta)^{n-t}}, & \\text{Se}\\ t = \\sum^n_{i=1}y_i \\\\\n0, & \\mathrm{c.c}\n\\end{array}\\right. \\forall \\theta \\in \\Theta\n\\]\nConcluímos que \\[\n\\begin{aligned}\n&P_\\theta(X_1=y_1,\\dots,X_n=y_n\\lvert T(\\pmb{X}_n) = t) =\n\\left \\{ \\begin{array}{ll}\n\\frac{\\prod_{i=1}^n \\mathbb{1}_{\\{0,1\\}}(y_i)}{\\binom{n}{t}}, & \\text{Se}\\ t = \\sum^n_{i=1}y_i \\\\\n0, & \\mathrm{c.c}\n\\end{array}\\right.\\\\\n&\\forall \\theta \\in \\Theta, t \\in \\{0,1,\\dots,n\\}, \\forall y_1,\\dots,y_n \\in \\mathbb{R}\n\\end{aligned}\n\\]\nA estatística \\(T(\\pmb{X}_n)\\) é suficiente para o modelo estatístico Bernoulli\n\n\n\n21.1.1.2 Exemplo 2\nSeja \\(\\pmb{X}_n=(X_1,\\dots,X_n)\\) uma amostra aleatória de \\(X\\sim\\mathrm{Pois}(\\theta), \\theta \\in \\Theta = (0,\\infty)\\). Verifique se \\(T(\\pmb{X}_n)= \\frac{1}{n}\\sum^n_{i=1}X_i\\) é uma estatística suficiente para o modelo estatístico.\n\n21.1.1.2.1 Resposta\n\\[\nP_\\theta(X_1=y_1,\\dots,X_n=y_n\\lvert T(\\pmb{X}_n) = t) =\n\\frac{P_\\theta(X_1=y_1,\\dots,X_n=y_n,T(\\pmb{X}_n) = t)}\n{P_\\theta(T(\\pmb{X}_n)=t)}\n\\]\n(Numerador) \\[\n\\begin{aligned}\n&P_\\theta(X_1=y_1,\\dots,X_n=y_n,T(\\pmb{X}_n)=t) \\\\\n&\\stackrel{\\mathrm{TPT}}{=}\n\\overbracket{P_\\theta(X_1=y_1,\\dots,X_n=y_n)}^{A} \\cdot \\overbracket{P_\\theta(T(\\pmb{X}_n)=t\\lvert X_1=y_1,\\dots,X_n=y_n)}^B\n\\end{aligned}\n\\]\nObserve que \\(A\\) é a função probabilidade da amostra e \\[\nB = \\left \\{ \\begin{array}{ll}\n1, & \\text{Se}\\ t =\\frac{1}{n} \\sum^n_{i=1} y_i \\\\\n0, & \\mathrm{c.c}\n\\end{array}\\right.\n\\]\nJá sabemos que \\(\\sum^n_{i=1}X_i\\sim \\mathrm{Pois}(n\\cdot\\theta)\\) (por função geradora de momentos)\n\\[\nP_\\theta\\left(\\frac{1}{n}\\sum^n_{i=1}X_i=\\frac{k}{n}\\right) = \\left\\{ \\begin{array}{ll}\n\\mathrm{e}^{-n\\theta}\\cdot \\frac{(n\\theta)^k}{k!}, & k \\in \\{0,1,2,\\dots\\} \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right.\n\\]\nTome \\(t = \\frac{k}{n}\\), então \\(k=nt\\) \\[\nP_\\theta\\left(\\sum^n_{i=1}X_i=k\\right) = \\left\\{ \\begin{array}{ll}\n\\mathrm{e}^{-n\\theta}\\cdot \\frac{(n\\theta)^{nt}}{(nt)!}, & t \\in \\{0,\\frac{1}{n},\\frac{2}{n},\\dots\\} \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right.\n\\]\nPortanto, para \\(t \\in \\{0, \\frac{1}{n}, \\frac{2}{n},\\dots\\}\\), temos que \\[\n\\begin{aligned}\n&P_\\theta(X_1=y_1,\\dots,X_n=y_n\\lvert T(\\pmb{X}_n)=t) = \\\\ &=\n\\left\\{ \\begin{array}{ll}\n\\frac{\\prod^n_{i=1}\\left\\{\\mathrm{e}^{-\\theta}\\cdot\\frac{\\theta^{y_i}}{y_i!}\\cdot \\mathbb{1}_{\\{0,1,\\dots\\}}(y_i)\\right\\}}\n{\\mathrm{e}^{-n\\theta}\\cdot \\frac{(n\\theta)^{nt}}{(nt)!}}, & t=\\frac{1}{n} \\sum^n_{i=1} y_i \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right. \\\\\n&= \\left\\{\\begin{array}{ll}\n\\frac{\\mathrm{e}^{-n\\theta}\\cdot\\theta^{\\sum^n_{i=1}y_i}\\cdot \\prod^n_{i=1}\\mathbb{1}_{\\{0,1,\\dots\\}}(y_i)(nt!)}\n{\\prod^n_{i=1}(y_i!)\\cdot\\mathrm{e}^{-n\\theta}\\cdot (n\\theta)^{nt}}, & t=\\frac{1}{n} \\sum^n_{i=1} y_i \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right. \\\\\n&= \\left\\{\\begin{array}{ll}\n\\frac{\\prod^n_{i=1}\\mathbb{1}_{\\{0,1,\\dots\\}}(y_i)(nt!)}\n{\\prod^n_{i=1}(y_i!)(n)^{nt}}, & t=\\frac{1}{n} \\sum^n_{i=1} y_i \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right. \\\\\n\\forall \\theta \\in \\Theta\n\\end{aligned}\n\\]\nNão depende de “\\(\\theta\\)” para todo \\(y_1,\\dots,y_n \\in \\mathbb{R}\\) e \\(t \\in \\{0,\\frac{1}{n},\\frac{2}{n},\\dots\\}\\)",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Estatísticas Suficientes</span>"
    ]
  },
  {
    "objectID": "estatisticas-suficientes.html#caso-contínuo",
    "href": "estatisticas-suficientes.html#caso-contínuo",
    "title": "21  Estatísticas Suficientes",
    "section": "21.2 Caso Contínuo",
    "text": "21.2 Caso Contínuo\n\n21.2.1 Exemplo (Normal)\nSeja \\(\\pmb{X}_n = (X_1,\\dots,X_n)\\) a.a. de \\(X\\sim N(\\theta,1), \\theta \\in \\mathbb{R}\\). Verifique se \\(T(\\pmb{X}_n) = \\sum^n_{i=1}X_i\\) é suficiente para o modelo estatístico.\n\n21.2.1.1 Resposta\nPor definição \\[\nf_\\theta^{\\pmb{X}_n\\lvert T(\\pmb{X}_n) = t}(y_1,\\dots,y_n) = \\frac{f_\\theta^{\\pmb{X}_n, T(\\pmb{X}_n)}(y_1,\\dots,y_n,t)}\n{f_\\theta^{T(\\pmb{X}_n)}(t)}\n\\]\n(Denominador) Já sabemos que \\(T(\\pmb{X}_n) = \\sum^n_{i=1} X_i \\sim N(n\\theta, n)\\) \\[\n\\Rightarrow f_\\theta^{T(\\pmb{X}_n)}(t) = \\frac{1}{\\sqrt{2\\pi n}} \\cdot \\mathrm{e}^{-\\frac{1}{2n}\\cdot (t-n\\theta)^2}, t \\in \\mathbb{R}\n\\] (Numerador) \\[\nf_\\theta^{\\pmb{X}_n,T(\\pmb{X}_n)}(y_1,\\dots,y_n,t) =\nf_\\theta^{\\pmb{X}_n}(y_1,\\dots,y_n) \\cdot f_\\theta^{T(\\pmb{X}_n)\\lvert \\pmb{X}_n=(y_1,\\dots,y_n)}(t)\n\\]\nNote que \\[\n\\begin{aligned}\n&f_\\theta^{T(\\pmb{X}_n)\\lvert \\pmb{X}_n = (y_1,\\dots,y_n} = \\left\\{ \\begin{array}{ll}\n1, & t = \\sum^n_{i=1} y_1 \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right. \\\\\n\\Rightarrow&\nf_\\theta^{\\pmb{X}_n\\lvert T(\\pmb{X}_n) = t}(y_1,\\dots,y_n) = \\left\\{ \\begin{array}{ll}\n\\frac{f_\\theta^{\\pmb{X}_n}(y_1,\\dots,y_n)}\n{f_\\theta^{T(\\pmb{X}_n)}(t)},& t = \\sum^n_{i=1}y_i \\\\\n0, & \\mathrm{c.c.}\n\\end{array} \\right.\n\\end{aligned}\n\\]\nLogo \\[\nf_\\theta^{T(\\pmb{X}_n)\\lvert \\pmb{X}_n = (y_1,\\dots,y_n)} = \\left\\{ \\begin{array}{ll}\n\\frac{\\frac{1}{\\sqrt{2\\pi\\cdot1}^n} \\cdot \\mathrm{exp}\\left\\{-\\frac{1}{2} \\sum^n_i=1 (y_i - \\theta)^2\\right\\}}\n{\\frac{1}{\\sqrt{2\\pi\\cdot n}}\\cdot \\mathrm{exp}\\left\\{-\\frac{1}{2n}(t-n\\theta)^2\\right\\}}, & t = \\sum y_i \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right.\n\\]\nNote que \\(-\\frac{1}{2}\\sum(y_i-\\theta)^2=-\\frac{1}{2}\\left(\\frac{t^2}{n}-2t\\theta+n\\theta^2\\right)\\) Logo \\(f_\\theta^{\\pmb{X}_n\\lvert T(\\pmb{X}_n)=t}\\) não depende de \\(\\theta\\) e \\(\\sum X_i\\) é suficiente para o modelo estatístico.\n\n\n\n21.2.2 Problema das funções densidade de probabilidade\nA função densidade de probabilidade não é única. Entretanto, é única para quase todo ponto (quase certamente).\nPor exemplo, \\(X\\sim\\mathrm{Exp}(\\theta), \\theta \\in (0,\\infty)\\) \\[\n\\begin{aligned}\nf_\\theta(x) &= \\left\\{\\begin{array}{ll}\n\\theta \\cdot \\mathrm{e}^{-\\theta x},& x \\in (0, \\infty) \\\\\n0, & x \\not \\in (0,\\infty)\n\\end{array}\\right.\\ \\ \\theta \\in \\Theta \\\\\nP_\\theta(X&gt;2)&=\\int^\\infty_2 \\theta \\cdot \\mathrm{e}^{-\\theta x} dx \\\\\n&\\text{Se $A$ é enumerável e defina} \\\\\nf_\\theta(x)^A &= \\left\\{\\begin{array}{ll}\n\\theta \\cdot \\mathrm{e}^{-\\theta x},& x \\in (0, \\infty) \\setminus A \\\\\n10, & x \\in \\mathrm{A} \\\\\n0, & x \\not \\in (0,\\infty)\n\\end{array}\\right. \\ \\ \\theta \\in \\Theta \\\\\n\\end{aligned}\n\\]\nTemos que \\[\n\\begin{aligned}\nf_\\theta(x) &= f_\\theta^A(x), \\forall x \\in \\mathbb{R} \\setminus A, \\forall \\theta \\in \\Theta \\\\\n\\text{e} \\\\\nf_\\theta(x) &\\neq f_\\theta^A(x), \\forall x \\in A, \\forall \\theta \\in \\Theta\n\\end{aligned}\n\\]\nNote que \\(f_\\theta\\) e \\(f_\\theta^A\\) são diferentes, mas produzem as mesmas probabilidades. Dizemos portanto que \\(f_\\theta\\) e \\(f_\\theta^A\\) são iguais quase certamente, ou seja, \\[\nP_\\theta\\left(f_\\theta(x)=f_\\theta^A(x)\\right) = 1, \\forall \\theta \\in \\Theta\n\\] Ou, de outra forma, \\[\nP_\\theta\\left(f_\\theta(x)\\neq f_\\theta^A(x)\\right) = 0, \\forall \\theta \\in \\Theta\n\\]\nNotação: \\[\nf_\\theta(x) = f_\\theta^A(x)\\ \\mathrm{q.c.}\\ \\forall \\theta \\in \\Theta\n\\]\nNo caso contínuo, portanto, a estatística \\(T(\\pmb{X}_n)\\) será suficiente mesmo se \\(f_\\theta^{\\pmb{X}_n\\lvert T(\\pmb{X}_n)=t}(y_1,\\dots,y_n)\\) depende de “\\(\\theta\\)” para \\((y_1,\\dots,y_n) \\in A\\), DESDE QUE \\(P_\\theta(X \\in A) = 0, \\forall \\theta \\in \\Theta\\). Ou seja, pode depender de “\\(\\theta\\)” em um conjunto com probabilidade zero.",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Estatísticas Suficientes</span>"
    ]
  },
  {
    "objectID": "estatisticas-suficientes.html#sec-crit-fat",
    "href": "estatisticas-suficientes.html#sec-crit-fat",
    "title": "21  Estatísticas Suficientes",
    "section": "21.3 Critério da Fatoração de Neyman-Fisher (Caso Simples)",
    "text": "21.3 Critério da Fatoração de Neyman-Fisher (Caso Simples)\nSeja \\(\\pmb{X}_n=(x_1,\\dots,x_n)\\) uma amostra aleatória de \\(X \\sim f_\\theta, \\theta \\in \\Theta \\subseteq \\mathbb{R}^p\\) (sendo as \\(f_\\theta, \\theta \\in \\Theta\\) do mesmo “tipo”, formalmente, dominadas pela mesma medida), em que \\(p \\in \\{1,2,\\dots\\}\\). Uma estatística \\(T(\\pmb{X}_n)\\) é suficiente para o modelo estatístico se, e somente se, existirem funções \\(h(\\cdot): \\mathbb{R}^n\\rightarrow \\mathbb{R}, m(\\cdot,\\cdot): \\mathrm{Im}(T)\\times\\theta\\rightarrow\\mathbb{R}\\) (mensuráveis) tais que: \\[\nf_\\theta^{\\pmb{X}_n}(y_1,\\dots,y_n) = h(y_1,\\dots,y_n)\\cdot m\\left(T(y_1,\\dots,y_n),\\theta\\right), \\forall \\theta \\in \\Theta\\; \\mathrm{q.c.}\n\\]\nObs:\n\n\\(h\\) não depende de “\\(\\theta\\)”;\n\\(m\\) depende de valores amostrais por meio da estatística \\(T(\\pmb{X}_n)\\);\n\\(f_\\theta^{\\pmb{X}_n} = f_\\theta^{(n)}\\) é a função (densidade) de probabilidade da amostra aleatória.\nNote que a função de verossimilhança é obtida calculando \\(f_\\theta^{\\pmb{X}_n}\\) na amostra observada, ou seja, \\[\nL_{\\pmb{X}_n}(\\theta) = f_\\theta^{\\pmb{X}_n}\\underbracket{(x_1,\\dots,x_n)}^{\\pmb{X}_n}\n\\]\nAlguns livros usam a função de verossimilhança no critério da fatoração \\[\nL_{\\pmb{X}_n}(\\theta) =  h(\\pmb{X}_n) \\cdot m(T(\\pmb{X}_n),\\theta)\\; \\mathrm{q.c.} \\forall \\theta \\in \\Theta\n\\] em que \\(\\pmb{X}_n = (x_1,\\dots,x_n)\\) da amostra observada\n\n\n21.3.1 Prova (caso discreto)\n\n21.3.1.1 \\(\\Rightarrow\\)\nAssuma que \\(T(\\pmb{X}_n)\\) seja suficiente. Por definição, \\(P_\\theta(X_1=y_1,\\dots,X_n=y_n\\lvert T(\\pmb{X}_n) = t)\\) não depende de “\\(\\theta\\)”. Logo, podemos escrever: \\[\nP_\\theta(X_1=y_1,\\dots,X_n=y_n\\lvert T(\\pmb{X}_n)=t) = h^*(y_1,\\dots,y_n,t) \\forall \\theta \\in \\Theta\n\\tag{21.1}\\] Note também que, \\[\nP_\\theta(X_1=y_1,\\dots,X_n=y_n\\lvert T(\\pmb{X}_n)=t) =\n\\frac{P_\\theta(X_1=y_1,\\dots,X_n=y_n, T(\\pmb{X}_n)=t)}\n{P_\\theta(T(\\pmb{X}_n)=t)}\n\\] para valores de \\(t\\) em que a probabilidade condicional exista.\n\\[\n\\begin{aligned}\n&P_\\theta(X_1=y_1,\\dots,X_n=y_n, T(\\pmb{X}_n)=t) \\\\\n&=\nP_\\theta(X_1=y_1,\\dots,X_n=y_n)\\cdot P_\\theta(T(\\pmb{X}_n)=t\\lvert X_1=y_1,\\dots,X_n=y_n)\n\\end{aligned}\n\\]\nComo, com \\(\\pmb{y}_n = y_1,\\dots,y_n\\), \\[\nP_\\theta(T(\\pmb{X}_n)=t\\lvert X_1=y_1,\\dots,X_n=y_n) = \\left\\{\\begin{array}{ll}\n1, & T(\\pmb{y}_n) = t \\\\\n0, & \\mathrm{cc}\n\\end{array}\\right.\n\\] então \\[\nP_\\theta(X_1=y_1,\\dots,X_n=y_n\\lvert T(\\pmb{X}_n)=t) =\n\\frac{P_\\theta(X_1=y_1,\\dots,X_n=y_n)\\cdot \\mathbb{1}(T(\\pmb{y}_n)=t)}\n{P_\\theta(T(\\pmb{X}_n) =t)}\n\\tag{21.2}\\]\nPor (21.1) e (21.2), temos que \\[\nh^*(y_1,\\dots,y_n, t) \\cdot P_\\theta(T(\\pmb{X}_n)=t) = P_\\theta(X_1=y_1,\\dots,X_n=y_n\\lvert \\mathbb{1}(T(\\pmb{X}_n)=t))\n\\]\nPara \\(T(\\pmb{X}_n)=t\\), temos que \\[\nP_\\theta(X_1=y_1,\\dots,X_n=y_n) = h^*(y_1,\\dots,y_n, T(\\pmb{y}_n)) \\cdot P_\\theta(T(\\pmb{X}_n)=T(\\pmb{y}_n))\n\\]\n\n\n21.3.1.2 \\(\\Leftarrow\\)\nAssuma que existam \\(h, m\\) tais que \\[\nP_\\theta(X_1=y_1,\\dots,X_n=y_n) = h(\\mathrm{y}_n) \\cdot m(T(\\pmb{y}_n,\\theta)\n\\] Note que \\[\nP_\\theta(X_1=y_1,\\dots,X_n=y_n\\lvert T(\\pmb{X}_n = t) = \\left\\{ \\begin{array}{ll}\n\\frac{P_\\theta(X_1=y_1,\\dots,X_n=y_n)}{P_\\theta(T(\\pmb{X}_n)=t)}, & T(\\pmb{y}_n)=t \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right.\n\\]\nObserve que \\[\nP_\\theta(T(\\pmb{X}_n) = t) = \\sum_{(y_1,\\dots,y_n) : T(\\pmb{y})_n=t} P_\\theta(X_1=y_1,\\dots,X_n=y_n)\n\\] Por suposição \\[\n\\begin{aligned}\nP_\\theta(T(\\pmb{X}_n) =t) &= \\sum_{\\pmb{y}_n : T(\\pmb{y}_n)=t} h(\\pmb{y}_n) \\cdot m (T(\\pmb{y}_n), \\theta) \\\\\n&= \\sum_{\\pmb{y}_n : T(\\pmb{y}_n)=t} h(\\pmb{y}_n) \\cdot m(t,\\theta) \\\\\n&= m(t,\\theta) \\cdot \\sum_{\\pmb{y}_n : T(\\pmb{y}_n)=t} h(\\pmb{y}_n)\n\\end{aligned}\n\\] Portanto \\[\nP_\\theta(X_1=y_1,\\dots,X_n=y_n\\lvert T(\\pmb{X}_n)=t) = \\left\\{\\begin{array}{ll}\n\\frac{h(\\pmb{y}_n)\\cdot m(T(\\pmb{y}_n),\\theta)}{m(t,\\theta) \\cdot \\sum_{\\pmb{y}_n: T(\\pmb{y}_n) = t} h(\\pmb{y}_n)}, & T(\\pmb{y}_n) = t \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right.\n\\]\nLogo, \\[\nP_\\theta(X_1=y_1,\\dots,X_n=y_n\\lvert T(\\pmb{X}_n)=t) = \\left\\{\\begin{array}{ll}\n\\frac{h(\\pmb{y}_n)}{\\sum_{\\pmb{y}_n: T(\\pmb{y}_n) = t} h(\\pmb{y}_n)}, & T(\\pmb{y}_n) = t \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right.\n\\]\n\n\n\n21.3.2 Exemplo (1 do caso discreto)\nSeja \\(\\pmb{X}_n=(X_1,\\dots,X_n)\\) amostra aleatória de \\(X\\sim\\mathrm{Ber}(\\theta), \\theta \\in \\Theta = (0,1)\\). Verifique se \\(T(\\pmb{X}_n)=\\sum^n_{i=1}X_i\\) é suficiente para o modelo estatístico.\n\n21.3.2.1 Resposta\nObserve que \\[\n\\begin{aligned}\nf_\\theta^{\\pmb{X}_n}(y_1,\\dots,y_n) &= \\left\\{ \\begin{array}{ll}\n\\prod^n_{i=1}\\left\\{\\theta^{y_i}\\cdot(1-\\theta)^{1-y_i}\\right\\}, & y_i \\in \\{0,1\\}, \\forall i = 1,\\dots,n \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right. \\\\\n\\Rightarrow\nf_\\theta^{\\pmb{X}_n}(y_1,\\dots,y_n) &= \\theta^{\\sum y_i} \\cdot (1-\\theta)^{n-\\sum y_i} \\cdot \\prod^n_{i=1}\\mathbb{1}_{\\{0,1\\}} (y_1)\n\\end{aligned}\n\\]\nTome \\(h(y_1,\\dots,y_n) = \\prod^n_{i=1}\\mathbb{1}_{\\{0,1\\}}(y_1)\\) e \\(m(T(y_1,\\dots,y_n),\\theta) = \\theta^{\\sum y_i} \\cdot (1-\\theta)^{n-\\sum y_i}\\) em que \\(T(y_1,\\dots,y_n) = \\sum^n_{i=1}y_i\\). Temos que \\[\nf_\\theta^{\\pmb{X}_n}(y_1,\\dots,y_n) = h(y_1,\\dots,y_n) \\cdot m(T(y_1,\\dots,y_n),\\theta),\\forall \\theta \\in \\Theta\n\\]\nPelo critério da fatoração, \\(T(\\pmb{X}_n) = \\sum^n_{i=1}X_i\\) é suficiente para o modelo de Bernoulli.\n\n\n\n21.3.3 Mais Exemplos\n\n21.3.3.1 Exemplo a\nSeja \\(\\pmb{X}_n\\) amostra aleatória de \\(X\\sim\\mathrm{Beta}(a,b), \\theta = (a,b) \\in \\Theta = \\mathbb{R}^2_+\\). Encontre uma estatística suficiente para o modelo.\n\n21.3.3.1.1 Resposta\nA função densidade de probabilidade da amostra aleatória é\n\\[\nf^{\\pmb{X}_n}_\\theta(y_1,\\dots,y_n) \\stackrel{a.a}{=}\n\\prod^n_{i=1} f_\\theta(y_i) \\stackrel{\\mathrm{Beta}}{=}\n\\prod^n_{i=1}\n\\left\\{\n\\frac{1}{\\beta(a,b)}y_{i}^{a-1}\\cdot(1-y_i)^{b-1}\n\\right\\}\n\\]\nEm que \\[\n\\begin{aligned}\n\\beta(a,b) &= \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)} \\\\\n\\Gamma(a) &= \\int^\\infty_0 x^{a-1}\\cdot\\mathrm{e}^{-x}dx\n\\end{aligned}\n\\]\n\\[\n\\Rightarrow f_\\theta^{\\pmb{X}_n}(\\pmb{y}_n) = \\left\\{\\begin{array}{ll}\n\\frac{1}{\\beta(a,b)^n}\n\\left(\\prod^n_{i=1}y_1\\right)^{a-1} \\cdot \\left( \\prod^n_{i=1} (1-y_1)\\right)^{b-1}, & \\pmb{y}_n \\in (0,1)^n \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right.\n\\]\nTome \\(h(\\pmb{y}_n) = \\prod^n_{i=1}\\mathbb{1}(y_i)_{(0,1)}\\) e \\[\nm(t,\\theta) = \\frac{1}{\\beta(a,b)^n} \\cdot t_1^{a-1} \\cdot t_2^{b-1}\n\\] em que \\(t=(t_1,t_2)\\) e \\(t_1 = \\prod^n_{i=1}y_i, t_2 = \\prod^n_{i=1}(1-y_i)\\). Ou seja, \\[\nT(\\pmb{X}_n) = \\left(\\prod^n_{i=1}X_i, \\prod^n_{i=1}(1-X_i)\\right)\n\\] é uma estatística suficiente para o modelo pois \\[\nf_\\theta^{\\pmb{X}_n}(\\pmb{y}_n) \\cdot m(T(\\pmb{y}_n),\\theta)\\ \\mathrm{q.c.}\\ \\forall \\theta \\in \\Theta\n\\]\n\n\n\n21.3.3.2 Exemplo b\nSeja \\(\\pmb{X}_n\\) amostra aleatória de \\(X\\sim\\mathrm{N}(\\mu,\\sigma^2), \\theta = (\\mu,\\sigma^2) \\in \\Theta = \\mathbb{R}\\times\\mathbb{R}_+\\). Encontre uma estatística suficiente para o modelo.\n\n21.3.3.2.1 Resposta\nA função densidade de probabilidade da amostra aleatória é \\[\n\\begin{aligned}\nf_\\theta^{\\pmb{X}_n} (y_1,\\dots,y_n) &\\stackrel{\\mathrm{a.a.}}{=} \\prod_{i=1}^n f_\\theta(y_i), \\forall \\pmb{y}_n \\in \\mathbb{R}^n \\\\\n&\\stackrel{N}{=} \\prod_{i=1}^N \\left\\{\n\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\cdot \\mathrm{exp}\\left\\{-\\frac{1}{2\\sigma^2}(y_i-\\mu)^2\\right\\}\n\\right\\} \\\\\n&= \\frac{1}{(2\\pi\\sigma^2)^{\\frac{n}{2}}} \\cdot \\mathrm{exp} \\left\\{-\\frac{1}{2\\sigma^2}\\sum^n_{i=1}(y_1-\\mu)^2\\right\\}\n\\end{aligned}\n\\]\nNote que \\((y_1-\\mu)^2 =y_i^2 -2y_i\\mu + \\mu^2\\), logo,\n\\[\nf_\\theta^{\\pmb{X}_n} = \\frac{1}{(2\\pi\\sigma^2)^{\\frac{n}{2}}} \\cdot \\mathrm{e} \\left\\{\n-\\frac{1}{\\sigma^2}\\left(\n\\sum^n_{i=1} y_i^2-2\\mu\\sum^n_{i=1}y_i + n\\mu^2\n\\right)\n\\right\\}\n\\]\nTome \\(h(\\pmb{y}_n) = \\frac{1}{(2\\pi\\sigma^2)}\\) e \\[\nm(t,\\theta) = \\frac{1}{(\\sigma^2)^{\\frac{n}{2}}}\\cdot \\mathrm{exp}\\left\\{\n-\\frac{1}{2\\sigma^2} \\left(t_2-2\\mu t_1-\\mu^2\\right)\n\\right\\}\n\\]\nEm que \\(t=(t_1,t_2)\\) e \\(t_1 = \\sum^n_{y_1}, t_2 = \\sum^n_{i=1}y^2_i\\)\nOu seja, \\(T(\\pmb{X}_n) = \\left(\\sum^n_{i=1}X_i,\\sum^n_{i=1}X^2\\right)\\) é uma estatística suficiente para o modelo pois \\[\nf_\\theta^{\\pmb{X}_n}(\\pmb{y}_n) \\cdot m(T(\\pmb{X}_n),\\theta)\\ \\mathrm{q.c.}\\ \\forall \\theta \\in \\Theta\n\\]\n\n\n\n21.3.3.3 Exemplo c\nSeja \\(\\pmb{X}_n\\) amostra aleatória de \\(X\\sim\\mathrm{Unif}(0,\\theta), \\theta = (a,b) \\in \\Theta = \\mathbb{R}_+\\). Encontre uma estatística suficiente para o modelo.\n\n21.3.3.3.1 Respostas\nA função densidade de probabilidade da amostra aleatória\n\\[\nf_\\theta^{\\pmb{X}_n}(y_1,\\dots,y_n) \\stackrel{a.a.}{=}\n\\prod^n_{i=1} f_\\theta(y_1) \\forall \\pmb{y}_n \\in \\mathbb{R}^n\n\\]\nNote que \\[\nf_\\theta (x)= \\left\\{\\begin{array}{ll}\n\\frac{1}{\\theta}, & x \\in (0,\\theta] \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right. = \\frac{1}{\\theta}\\mathbb{1}(x)_{(0,\\theta])}\n\\]\nLogo, \\[\nf_\\theta^{\\pmb{X}_n}(\\pmb{y}_n) \\stackrel{\\mathrm{Unif}}{=}\n\\frac{1}{\\theta} \\mathbb{1}_{(0,\\theta]}(y_i) \\\\\n= \\frac{1}{\\theta^n} \\cdot \\prod^n_{i=1} \\mathbb{1}_{(0,\\theta]}(y_1)\n\\]\nNote que \\[\n\\prod^n_{i=1} \\mathbb{1}_{(0,\\theta]}(y_i) \\Leftrightarrow\n\\left\\{\\begin{array}{ll}\n0 &lt; y_1 \\leq \\theta \\\\\n\\vdots \\\\\n0 &lt; y_n \\leq \\theta\n\\end{array}\\right. \\Leftrightarrow\n\\left\\{\\begin{array}{ll}\n\\min(\\pmb{y}_n) &gt; 0 \\\\\n\\max(\\pmb{y}_n) \\leq \\theta\n\\end{array}\\right.\n\\]\nLogo \\[\n\\prod^n_{i=1}\\mathbb{1}(y_i)_{(0,\\theta]} = 1 \\Leftrightarrow\n\\mathbb{1}_{(0,\\infty)}(\\min(\\pmb{y}_n)) \\cdot\n\\mathbb{1}_{(0,\\theta]}(\\max(\\pmb{y}_n))\n\\]\ne\n\\[\nf_\\theta^{\\pmb{X}_n}(\\pmb{y}_n) = \\frac{1}{\\theta^n}\n\\mathbb{1}_{(0,\\infty)}(\\min(\\pmb{y}_n)) \\cdot\n\\mathbb{1}_{(0,\\theta]}(\\max(\\pmb{y}_n))\n\\]\nTome \\(h(\\pmb{y}_n) = \\mathbb{1}(\\min(\\pmb{y}_n))\\) e \\(m(t,\\theta) = \\frac{1}{\\theta^n} \\mathbb{1}_{(0,\\theta]}(t)\\), em que \\(t=\\max(\\pmb{y}_n)\\)\nPortanto, pelo critério da fatoração, \\(T(\\pmb{X}_n) = \\max(x_1,\\dots,x_n)\\) é uma estatística suficiente para o modelo em questão.",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Estatísticas Suficientes</span>"
    ]
  },
  {
    "objectID": "estatisticas-suficientes.html#teorema-invariância-da-estatística-suficiente",
    "href": "estatisticas-suficientes.html#teorema-invariância-da-estatística-suficiente",
    "title": "21  Estatísticas Suficientes",
    "section": "21.4 Teorema (“Invariância” da estatística suficiente)",
    "text": "21.4 Teorema (“Invariância” da estatística suficiente)\nSeja \\(T(\\pmb{X}_n)\\) uma estatística suficiente para o modelo estatístico. Então \\[\nG(\\pmb{X}_n) = s(T(\\pmb{X}_n))\n\\] é uma estatistica suficiente se \\(s(\\cdot)\\) for bijetora (só precisa ser injetora)\n\n21.4.1 Prova\nComo \\(T(\\pmb{X}_n)\\) é suficiente para o modelo temos, pelo critério da fatoração, que \\[\nf_\\theta^{\\pmb{X}_n}(\\pmb{y}_n) = h(\\pmb{y}_n) \\cdot m(T(\\pmb{y}_n),\\theta)\\ \\mathrm{q.c.}\\ \\forall \\theta \\in \\Theta\n\\]\nComo \\(s(\\cdot)\\) é bijetora, temos que sua inversa existe: \\[\nT(\\pmb{X}_n) = s^{-1}(G(\\pmb{X}_n)) \\Rightarrow\nT(\\pmb{y}_n) = s^{-1}(G(\\pmb{y}_n))\n\\]\nPortanto, \\[\nf_\\theta^{\\pmb{X}_n} (\\pmb{y}_n)= h(\\pmb{y}_n) \\cdot m(s^{-1}(G(\\pmb{X}_n)), \\theta)\\ \\mathrm{q.c.}\\ \\forall \\theta \\in \\Theta\n\\]\nTome \\(m^*(\\cdot,\\theta) = m(s^{-1}(\\cdot),\\theta)\\). Substituindo, temos que\n\\[\nf_\\theta^{\\pmb{X}_n} (\\pmb{y}_n)= h(\\pmb{y}_n) \\cdot m^*((G(\\pmb{X}_n)), \\theta)\\ \\mathrm{q.c.}\\ \\forall \\theta \\in \\Theta\n\\]\nLogo, pelo critério da fatoração, \\(G(\\pmb{X}_n)\\) é suficiente para o modelo estatístico.\n\n\n21.4.2 Exemplos\n\nSe \\(T(\\pmb{X}_n) = \\sum^n_{i=1}X_i\\) é suficiente para o modelo, então:\n\n\n\\(G(\\pmb{X}_n)=\\frac{1}{n}\\sum^n_{i=1}X_i\\) é suficiente para o modelo;\n\\(G(\\pmb{X}_n)=\\mathrm{e}^{\\sum^n_{i=1}X_i}\\) é suficiente para o modelo;\nPara \\(\\sum X_i \\neq 0\\ \\mathrm{q.c.}\\), \\(G(\\pmb{X}_n)=\\frac{1}{\\sum^n_{i=1}X_i}\\) é suficiente para o modelo;\n\\(G(\\pmb{X}_n)=\\left(\\sum^n_{i=1}X_i\\right)^2\\) não é necessariamente suficiente para o modelo uma vez que \\(f(x) = x^2\\) não é injetora.\n\n\nSe \\(T(\\pmb{X}_n)\\) for suficiente para o modelo estatístico, então\n\n\n\\(G(\\pmb{X}_n) = (T(\\pmb{X}_n), T(\\pmb{X}_n))\\) é suficiente para o modelo;\n\\(G(\\pmb{X}_n) = (T(\\pmb{X}_n), X_1)\\) é suficiente para o modelo;\n\\(G(\\pmb{X}_n) = (T(\\pmb{X}_n)^2, \\pmb{X}_n)\\) é suficiente para o modelo pois \\(\\pmb{X}_n\\) é suficiente para o modelo;\n\\(G(\\pmb{X}_n) = (X_1\\dots,X_n)\\) “quase” nunca será suficiente para o modelo;\n\\(G(\\pmb{X}_n) = (\\pmb{X}_n,X_1)\\) é suficiente para o modelo.\nA amostra ordenada \\(X_{(1)} \\leq \\dots \\leq X_{(n)}\\), denotada por \\(T^*(\\pmb{X}_n)=(X_{(1)},\\dots,X_{(n)})\\) é uma estatística suficiente para o modelo pelo critério da fatoração, no caso de variáveis aleatórias independentes e identicamente distribuídas. \\(T^*(\\pmb{X}_n)\\) é dita ser a estatística de ordem;\nSe \\(f_\\theta\\) pertencer à família exponencial k-dimensional, então \\(T(\\pmb{X}_n) = \\left(\\sum^n_{i=1}T_1(X_i),\\dots,\\sum^n_{i=1}T_k(X_i)\\right)\\) é uma estatística suficiente para o modelo. Prova: \\[\n\\begin{aligned}\nf_\\theta^{\\pmb{X}_n}(\\pmb{y}_n) &\\stackrel{\\mathrm{iid}}{=}\n\\prod^n_{i=1} f_\\theta(y_i) = \\prod^n_{i=1}\\left\\{\n\\mathrm{exp}\\left\\{\n\\sum_{j=1}^n c_j(\\theta)T_j(y_i)+d(\\theta)+S(y_i)\\cdot\\mathbb{1}_{\\mathfrak{X}}(y_i)\n\\right\\}\n\\right\\} \\\\\n&= \\mathrm{exp}\\left\\{\n\\sum^n_{j=1} c_j(\\theta) \\cdot \\sum^n_{i}T_j(y_i) + nd(\\theta) + \\sum^n_{i=1}S(y_i) \\cdot \\prod^n_{i=1}\\mathbb{1}_{\\mathfrak{X}}(y_i)\n\\right\\}\n\\end{aligned}\n\\]\n\nTome \\(h(\\pmb{y}_n) = \\prod \\mathbb{1}_{\\mathfrak{X}}(y_i)\\cdot \\mathrm{e}^{\\sum S(y_i)}\\), \\(m(t,\\theta) = \\mathrm{exp}\\left\\{c_1(\\theta)t_1+\\dots+c_k(\\theta)t_k +nd(\\theta)\\right\\}\\), em que \\(t=(t_1,\\dots,t_k)\\) e \\[\n\\begin{aligned}\nt_1 &= \\sum T_1(y_i) \\\\\n\\vdots \\\\\nt_k & = \\sum T_k(y_i)\n\\end{aligned}\n\\]\nPelo critério da fatoração, \\[\nT(\\pmb{X}_n) = \\left(\\sum^n_{i=1} T_1(X_i),\\dots,\\sum^n_{i=1}T_k(X_i)\\right)\n\\]",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Estatísticas Suficientes</span>"
    ]
  },
  {
    "objectID": "estatisticas-suficientes.html#estatísticas-suficientes-minimais-sm",
    "href": "estatisticas-suficientes.html#estatísticas-suficientes-minimais-sm",
    "title": "21  Estatísticas Suficientes",
    "section": "21.5 Estatísticas Suficientes Minimais (SM)",
    "text": "21.5 Estatísticas Suficientes Minimais (SM)\nDizemos que \\(T(\\pmb{X}_n)\\) é uma estatística suficiente minimal para o modelo se, e somente se:\n\n\\(T(\\pmb{X}_n)\\) é suficiente para o modelo\nPara qualquer outra estatística suficiente \\(U(\\pmb{X}_n)\\), existe uma função \\(H\\) tal que \\[\nT(\\pmb{X}_n) = H(U(\\pmb{X}_n)),\\ \\mathrm{q.c.}\n\\]\n\nObs: A \\(\\sigma\\)-álgebra associada à estatística suficiente minimal é a menor \\(\\sigma\\)-álgebra dentre aquelas associadas às estatísticas suficientes.\n\n21.5.1 Teorema (1 das estatísticas SM)\nSeja \\(\\pmb{X}_n = (X_1,\\dots,X_n)\\) de \\(X\\sim f_\\theta, \\theta \\in \\Theta_0=\\{\\theta_0,\\theta_1,\\dots,\\theta_p\\}\\) em que \\(\\mathfrak{X} = \\{x:f_\\theta(x)&gt;0\\}\\) não depende de “\\(\\theta\\)”. Então, \\[\nT(\\pmb{x}) = \\left(\n\\frac{f_{\\theta_1}^{\\pmb{X}}(\\pmb{x})}{f_{\\theta_0}^{\\pmb{X}_n}(\\pmb{x})}, \\dots,\n\\frac{f_{\\theta_p}^{\\pmb{X}}(\\pmb{x})}{f_{\\theta_0}^{\\pmb{X}_n}(\\pmb{x})}\n\\right)\n\\] em que \\(T:\\mathfrak{X} \\rightarrow \\mathbb{R}^p\\) é uma estatística suficiente minimal (\\(T(\\pmb{X}_n)\\)) para o modelo estatístico.\nIsso trata de razões entre funções verossimilhança.\n\n21.5.1.1 Prova\nNote que, \\(\\forall \\pmb{y}_n \\in \\mathfrak{X}\\), temos que \\[\nf_{\\theta_j}^{\\pmb{X}_n}(\\pmb{y}_n) =\nf_{\\theta_0}^{\\pmb{X}_n}(\\pmb{y}_n) \\cdot\n\\frac{f_{\\theta_j}^{\\pmb{X}_n}(\\pmb{y}_n)}{f_{\\theta_0}^{\\pmb{X}_n}(\\pmb{y}_n)}\n\\] Tome \\(h(\\pmb{y}_n) = f_{\\theta_0}^{\\pmb{X}_n}(\\pmb{y}_n)\\), não depende dos diferentes valores de “\\(\\theta\\)” e\n\\[\nm(T(\\pmb{X}_n), \\theta) =\n\\left\\{\\begin{array}{ll}\nT_1(\\pmb{y}_n), & \\theta = \\theta_1 \\\\\nT_2(\\pmb{y}_n), & \\theta = \\theta_2 \\\\\n\\vdots \\\\\nT_p(\\pmb{y}_n), & \\theta = \\theta_p \\\\\n\\end{array}\\right.\n\\]\nEm que \\(T(\\pmb{x}) = (T_1(\\pmb{x}), \\dots, T_p(\\pmb{x})\\) e \\(T_j = \\frac{f_{\\theta_j}^{\\pmb{X}_n}(\\pmb{y}_n)}{f_{\\theta_0}^{\\pmb{X}_n}(\\pmb{y}_n)}\\).\nLogo, \\(T(\\pmb{X}_n)\\) é suficiente para o modelo pelo critério da fatoração.\nSeja \\(U(\\pmb{X}_n)\\) uma estatística suficiente para o modelo. Entãom pelo critério da fatoração,\n\\[\nf_\\theta^{\\pmb{X}_n}(pmb{y}_n) = h'(\\pmb{y}_n) \\cdot m'(U(\\pmb{y}_n),\\theta)\\ \\mathrm{q.c.}\\ \\forall \\theta \\in \\Theta_0\n\\]\nObserve que \\(\\forall \\pmb{y}_n \\in \\mathbb{X}\\), \\[\n\\frac{f_{\\theta_j}^{\\pmb{X}_n}(\\pmb{y}_n)}{f_{\\theta_0}^{\\pmb{X}_n}(\\pmb{y}_n)} =\n\\frac{h'(\\pmb{y}_n)\\cdot m'(U(\\pmb{y}_n),\\theta_j)}{h'(\\pmb{y}_n)\\cdot m'(U(\\pmb{y}_n),\\theta_0)}\\ \\mathrm{q.c.}\\ \\forall \\theta \\in \\Theta_0\n\\]\nLogo,\n\\[\nT_j(\\pmb{y}_n) =\n\\frac{f_{\\theta_j}^{\\pmb{X}_n}(\\pmb{y}_n)}{f_{\\theta_0}^{\\pmb{X}_n}(\\pmb{y}_n)} =\n\\frac{m'(U(\\pmb{y}_n),\\theta_j)}{m'(U(\\pmb{y}_n),\\theta_0)},\\ j=1,\\dots,p\n\\]\nPortanto, existe \\(H\\) tal que \\[\nT_j(\\pmb{X}_n) = H(U(\\pmb{X}_n))\n\\] basta tomar \\[\nH(u) = \\left(\n\\frac{m'(u,\\theta_1)}{m'(u,\\theta_0)}, \\dots,\n\\frac{m'(u\\theta_p)}{m'(u,\\theta_0)}\n\\right)\n\\]\n\n\n21.5.1.2 Exemplo (Bernoulli)\nSeja \\(\\pmb{X}_n = (X_1,\\dots,X_n)\\) amostra aleatória de \\(X\\sim\\mathrm{Ber}(\\theta), \\theta \\in \\Theta = \\{0.1,0,5\\}\\). Encontre uma estatística suficiente minimal.\n\n21.5.1.2.1 Resposta\nPelo teorema anterior, \\[\nT(\\pmb{y}_n) = \\frac{f_{\\theta_1}^{\\pmb{X}_n}(\\pmb{y}_n)}{f_{\\theta_0}^{\\pmb{X}_n}(\\pmb{y}_n)}\n\\] é suficiente minimal, em que \\(\\theta_0=0.1,\\theta_1=0.5\\) e\n\\[\n\\begin{aligned}\nf_{0.1}^{\\pmb{X}_n} (\\pmb{y}_n) &= 0.1^{\\sum y_i} \\cdot 0.9^{n-\\sum y_i} \\\\\nf_{0.5}^{\\pmb{X}_n} (\\pmb{y}_n) &= 0.5^{\\sum y_i} \\cdot 0.5^{n-\\sum y_i} \\\\\n&\\forall \\pmb{y}_n \\in \\mathfrak{X} = \\{0,1\\}^n\n\\end{aligned}\n\\] Logo \\[\n\\begin{aligned}\nT(\\pmb{y}_n) &= \\frac{0.5^n}{0.1^{\\sum y_i} \\cdot 0.9^{n-\\sum y_i}} \\\\\n&= \\frac{0.5^n}{\\left(\\frac{0.1}{0.9}\\right)^{\\sum y_i} \\cdot 0.9^{\\sum y_i}} \\\\\n&= 9^{\\sum y_i} \\cdot \\left(\\frac{5}{9}\\right)^n\n\\end{aligned}\n\\]\nNote que \\(T(\\pmb{y}_n)\\) é função 1:1 de \\(T'(\\pmb{y}_n) = \\sum^n_{i=1} y_i\\). Logo, \\[\nT'(\\pmb{X}_n) = \\sum^n_{i=1} X_i\n\\] é também uma estatística suficiente minimal para o modelo.\n\n\n\n21.5.1.3 Exemplo (Normal)\nSeja \\(\\pmb{X}_n = (X_1,\\dots,X_n)\\) amostra aleatória de \\(X\\sim\\mathrm{N}(\\mu,\\sigma^2), \\theta = (\\mu,\\sigma^2) \\in \\Theta = \\{(0,1),(1,1),(0,2)\\}\\). Encontre uma estatística suficiente minimal.\n\n21.5.1.3.1 Resposta\nPelo teorema, \\[\nT(\\pmb{y}_n) = \\left(\n\\frac{f_{\\theta_1}^{\\pmb{X}_n}(\\pmb{y}_n)}{f_{\\theta_0}^{\\pmb{X}_n}(\\pmb{y}_n)},\n\\frac{f_{\\theta_2}^{\\pmb{X}_n}(\\pmb{y}_n)}{f_{\\theta_0}^{\\pmb{X}_n}(\\pmb{y}_n)}\n\\right)\n\\]\nTomando \\(\\theta_0 = (0,1), (1,1), (0,2)\\), temos\n\\[\n\\begin{aligned}\nf_{\\theta_0}^{\\pmb{X}_n}(\\pmb{y}_n) &=\n\\frac{\\mathrm{exp}\\left\\{-\\frac{1}{2}\\sum y^2_i\\right\\}}{(\\sqrt{2\\pi})^n} \\\\\nf_{\\theta_1}^{\\pmb{X}_n}(\\pmb{y}_n) &=\n\\frac{\\mathrm{exp}\\left\\{-\\frac{1}{2}(\\sum y^2_i - 2\\sum y_i + n)\\right\\}}{(\\sqrt{2\\pi})^n} \\\\\nf_{\\theta_2}^{\\pmb{X}_n}(\\pmb{y}_n) &=\n\\frac{\\mathrm{exp}\\left\\{-\\frac{1}{4}\\sum y^2_i \\right\\}}{(\\sqrt{4\\pi})^n} \\\\\n\\end{aligned}\n\\]\nPortanto, \\[\n\\begin{aligned}\nT(\\pmb{y}_n) &= \\left(\n\\mathrm{exp}\\left\\{-\\frac{1}{2}(n-2\\sum y_i)\\right\\},\n\\frac{\\mathrm{exp}\\left\\{-\\frac{1}{4}\\sum y_i^2 + \\frac{1}{2}\\sum y_i^2)\\right\\}}{2^{\\frac{n}{2}}},\n\\right)\n\\\\\n&= \\left(\n\\mathrm{exp}\\left\\{-\\frac{1}{2}(n-2\\sum y_i)\\right\\},\n\\frac{\\mathrm{exp}\\left\\{\\frac{1}{4}\\sum y_i^2)\\right\\}}{2^{\\frac{n}{2}}},\n\\right)\n\\end{aligned}\n\\] dessa forma, \\(T(\\pmb{X}_n)\\) é SM para o modelo.\nNote que \\(T(\\pmb{y}_n)\\) é função 1:1 de \\((\\sum y_i, \\sum y_i^2)\\). Portanto, \\[\nT'(\\pmb{X}_n) = \\left(\n\\sum^n_{i=1} X_i, \\sum^n_{i=1} X_i^2\n\\right)\n\\] é também SM para o modelo.\n\n\n\n\n21.5.2 Teorema (2 das estatísticas SM)\nSeja \\(\\pmb{X}_n=(X_1,\\dots,X_n)\\) amostra aleatória de \\(X\\sim f_\\theta, \\theta \\in \\Theta\\). Considere \\(\\Theta_0 \\subseteq \\Theta\\) não vazio. Então, se \\(T(\\pmb{X}_n)\\) for SM para o modelo reduzido a \\(\\Theta_0\\) e suficiente para \\(\\Theta\\), então será suficiente minimal para \\(\\Theta\\).\n\n21.5.2.1 Prova\nComo \\(T(\\pmb{X}_n)\\) é SM para o modelo restrito a \\(\\Theta_0 \\subseteq \\Theta\\), para qualquer estatística \\(U(\\pmb{X}_n)\\) suficiente para o modelo restrito a \\(\\Theta_0\\), existe \\(H\\) tal que\n\\[\nT(\\pmb{X}_n) = H(U(\\pmb{X}_n))\\ \\mathrm{q.c.}\n\\]\nObserve que todas as estatísticas suficientes para o modelo completo \\(\\Theta\\) são também suficientes para os modelos restritos. Como \\(T(\\pmb{X}_n)\\) também é, por hipótese, suficiente para o modelo completo, então é função de qualquer estatística suficiente minimal para o modelo completo.\n\n\n21.5.2.2 Exemplo (Bernoulli)\nSeja \\(\\pmb{X}_n = (X_1,\\dots,X_n)\\) amostra aleatória de \\(X\\sim\\mathrm{Ber}(\\theta), \\theta \\in \\Theta = (0,1)\\). Mostre que \\(T(\\pmb{X}_n) = \\sum X_i\\) é uma estatística suficiente minimal para o modelo.\n\n21.5.2.2.1 Resposta\nTome \\(\\Theta_0 = \\{0.1,0.5\\} \\subseteq \\Theta\\). Já mostramos que \\(T'(\\pmb{X}_n) = \\sum^n_{i=1} X_i\\) é suficiente minimal para o modelo reduzido a \\(\\Theta_0\\). Além disso, \\(T'(\\pmb{X}_n) = \\sum X_i\\) é suficiente para o modelo completo. Logo, pelo Teorema 2 para estatísticas suficientes minimais, concluímos que \\(T'(\\pmb{X}_n) = \\sum X_i\\) é também SM para o modelo completo.\n\n\n\n21.5.2.3 Exemplo (Normal)\nSeja \\(\\pmb{X}_n = (X_1,\\dots,X_n)\\) amostra aleatória de \\(X\\sim\\mathrm{N}(\\mu,\\sigma^2), \\theta = (\\mu,\\sigma^2) \\in \\Theta = \\mathbb{R}\\times\\mathbb{R}^+\\). Mostre que \\(T(\\pmb{X}_n) = (\\sum X_i, \\sum X_i^2)\\) é suficiente minimal para o modelo.\n\n21.5.2.3.1 Resposta correta\nTome \\(\\Theta_0 = \\{(0,1),(1,1),(0,2)\\}\\). Já mostramos que essa estatística é suficiente minimal para o modelo reduzido a \\(\\Theta_0\\). Além disso, já mostramos (lista) que é suficiente para o modelo completo. Logo, pelo teorema 2, é SM para o modelo completo.\n\n\n21.5.2.3.2 Tentativa alternativa (frustrada)\nTome \\(\\Theta_0 = \\{(0,1),(1,1)\\}\\). Pelo teorema 1, temos que \\[\nT(\\pmb{X}_n) = \\frac{f_{\\theta_1}^{\\pmb{X}_n}(\\pmb{X}_n)}{f_{\\theta_0}^{\\pmb{X}_n}(\\pmb{X}_n)}\n\\] é SM para o modelo reduzido.\n\\[\nT(\\pmb{X}_n) = \\frac{\\frac{1}{(2\\pi)^{\\frac{n}{2}}}\\mathrm{exp}\\left\\{-\\frac{1}{2} (\\sum X_i^2 -2\\sum X_i + n)\\right\\}}\n{\\frac{1}{(2\\pi)^{\\frac{n}{2}}}\\mathrm{exp}\\left\\{-\\frac{1}{2} \\sum X_i^2\\right\\}} = \\mathrm{e}^{\\sum X_i -\\frac{1}{2} n}\n\\]\nComo \\(\\sum X_i\\) é uma função 1:1 da estatística, temos que \\(\\sum X_i\\) é também SM para o modelo reduzido. Entretanto, \\(\\sum X_i\\) não é suficiente para o modelo completo.\n\n\n21.5.2.3.3 Tentativa 2\nTome \\(\\Theta_0 = \\{(0,1),(1,1), (2,1)\\}\\). Então, pelo teorema 1, temos que\n\\[\nT(\\pmb{X}_n) = \\left(\\frac{f_{\\theta_1}^{\\pmb{X}_n}(\\pmb{X}_n)}{f_{\\theta_0}^{\\pmb{X}_n}(\\pmb{X}_n)},\n\\frac{f_{\\theta_2}^{\\pmb{X}_n}(\\pmb{X}_n)}{f_{\\theta_0}^{\\pmb{X}_n}(\\pmb{X}_n)}\\right) \\stackrel{\\text{Tent. Ant.}}{=}\n\\left(\n\\mathrm{e}^{\\sum X_i - \\frac{1}{2}n},\n\\frac{f_{\\theta_2}^{\\pmb{X}_n}(\\pmb{X}_n)}{f_{\\theta_0}^{\\pmb{X}_n}(\\pmb{X}_n)}\\right)\n\\]\nTemos que\n\\[\n\\frac{f_{\\theta_2}^{\\pmb{X}_n}(\\pmb{X}_n)}{f_{\\theta_0}^{\\pmb{X}_n}(\\pmb{X}_n)} =\n\\frac{\\frac{1}{(2\\pi)^{\\frac{n}{2}}}\\mathrm{exp}\\left\\{-\\frac{1}{2} (\\sum X_i^2 -4\\sum X_i + 4n)\\right\\}}\n{\\frac{1}{(2\\pi)^{\\frac{n}{2}}}\\mathrm{exp}\\left\\{-\\frac{1}{2} \\sum X_i^2\\right\\}} = \\mathrm{e}^{2\\sum X_i -2 n}\n\\] Logo, \\[\nT(\\pmb{X}_n) =\n\\left(\n\\mathrm{e}^{\\sum X_i - \\frac{1}{2}n},\n\\mathrm{e}^{2\\sum X_i -2 n}\\right)\n\\]\nComo \\(\\sum X_i\\) é função 1:1 de \\(T(\\pmb{X}_n)\\), temos que é SM para o modelo reduzido a \\(\\Theta_0\\). Entretanto, não é suficiente para o modelo completo.\nObserve que o espaço paramétrico do modelo completo é \\(\\mathbb{R}\\times\\mathbb{R}^+\\)\n\n\n\n21.5.2.4 Exemplo (Uniforme)\nSeja \\(\\pmb{X}_n=(X_1,\\dots,X_n)\\) amostra aleatória de \\(X\\sim U(0,\\theta), \\theta \\in \\Theta = (0,\\infty)\\). Verifique se \\(\\pmb{X}_{(n)} = \\max\\{X_1,\\dots,X_n\\}\\) é SM para o modelo.\n\n21.5.2.4.1 Resposta\nJá sabemos que é uma estatística suficiente para o modelo. Com o suporte \\(\\mathfrak{X}_\\theta = (0,\\theta]\\) depende de “\\(\\theta\\)”, não podemos usar o teorema anterior. Precisamos mostras que, para qualquer estatística suficiente para o modelo \\(U(\\pmb{X}_n)\\) existe \\(H(\\cdot)\\) tal que\n\\[\nX_{(n)} = H(U(\\pmb{X}_n))\\ \\mathrm{q.c.}\n\\]\nSeja \\(f_\\theta^{\\pmb{X}_n}\\) a função densidade probabilidade da amostra aleatória \\[\n\\begin{aligned}\nf_\\theta^{\\pmb{X}_n}(\\pmb{y}_n) &= \\frac{1}{\\theta^n} \\prod \\mathbb{1}_{(0,\\theta]}(y_i)  \\\\\n&= \\frac{1}{\\theta^n} \\mathbb{1}_{(0,\\infty)} (\\min\\{y_1,\\dots,y_n\\}) \\cdot \\mathbb{1}_{(0,\\theta]}(\\max\\{y_1,\\dots,y_n\\})\n\\end{aligned}\n\\]\n\nNote que podemos escrever \\[\n\\max\\{y_1,\\dots,y_n\\} = \\inf\\left\\{ \\theta \\in (0,\\infty) : f_\\theta^{\\pmb{X}_n}(\\pmb{y}_n) &gt; 0\\right\\}\n\\]\nLogo, a estatística \\(X_{(n)}\\) pode ser reescrita por \\[\nX_{(n)} = \\inf\\left\\{\\theta \\in (0,\\infty) : f^{\\pmb{X}_n}_\\theta (\\pmb{X}_n) &gt; 0\\right\\}\n\\]\nSeja \\(U(\\pmb{X}_n)\\) uma estatística suficiente qualquer. Então, pelo CF, temos que existem \\(h'\\) e \\(m'\\) tais que \\[\nf_\\theta{\\pmb{y}_n}^{\\pmb{X}_n} = h'(\\pmb{y}_n) \\cdot m'(U(\\pmb{y}_n), \\theta),\\ \\mathrm{q.c.}\\ \\forall \\theta &gt;0\n\\]\nLogo, \\[\nX_{(n)} = \\inf\\left\\{\\theta \\in (0,\\infty) : h'(\\pmb{X}_n) \\cdot m'(U(\\pmb{X}_n),\\theta) &gt; 0\\right\\} \\mathrm{q.c.}\n\\]\nComo \\(h'(\\pmb{y}_n)&gt; 0\\ \\mathrm{q.c.}\\) temos que\n\\[\nX_{(n)} = \\inf\\left\\{\\theta \\in (0,\\infty) : m'(U(\\pmb{X}_n),\\theta)&gt;0 \\right\\} \\mathrm{q.c.}\n\\]\nPortanto, \\(X_{(n)}\\) é suficiente minimal para o modelo, pois existe \\(H(\\cdot)\\) tal que \\[\nX_{(n)} = H(U(\\pmb{X}_n))\\ \\mathrm{q.c.}\n\\]\nBasta tomar \\[\nH(u) = \\inf\\left\\{\n\\theta \\in \\Theta : m'(u,\\theta)&gt; )\n\\right\\}\n\\]\n\n\n\n21.5.2.5 Observação\n\\[\nf_\\theta(x) = \\mathrm{e}^{-\\theta} \\cdot \\mathbb{1}_{(0,\\theta)}(x)\n\\]\nDefina \\(g:\\mathbb{R}^+\\rightarrow\\mathbb{R}\\) \\[\n\\begin{aligned}\ng(u) &= \\inf\\left\\{\\theta \\in (0,\\infty) : f_\\theta(u) &gt; 0\\right\\}\\\\\n&= \\inf\\left\\{\\theta \\in (0,\\infty) : \\mathrm{e}^{-\\theta}\\cdot \\mathbb{1}_{(0,\\theta]}(u) &gt; 0\\right\\}\\\\\n\\Rightarrow g(u) &= u\n\\end{aligned}\n\\]\n\n\n21.5.2.6 Exemplo (Normal Curvada)\nSeja \\(\\pmb{X}_n = (X_1,\\dots,X_n)\\) amostra aleatória de \\(X\\sim\\mathrm{N}(\\theta,\\theta^2), \\theta \\in \\Theta = \\mathbb{R}^+\\). Encontre uma estatística SM para o modelo.",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Estatísticas Suficientes</span>"
    ]
  }
]