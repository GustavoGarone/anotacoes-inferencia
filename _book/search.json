[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Anotações de Inferência Frequentista",
    "section": "",
    "text": "Prefácio\nEste é um livrete feito na plataforma Quarto para minhas anotações das disciplinas de inferência frequentista ou clássica MAE0225 e MAE0301, ambas ministradas pelo Professor Alexandre Galvão Patriota e de sua autoria.\nDúvidas, sugestões, críticas ou erros por favor contactar em gustavo.garone arroba usp.br",
    "crumbs": [
      "Prefácio"
    ]
  },
  {
    "objectID": "index.html#atribuição-e-licença",
    "href": "index.html#atribuição-e-licença",
    "title": "Anotações de Inferência Frequentista",
    "section": "Atribuição e Licença",
    "text": "Atribuição e Licença\nEste livrete segue a licença pública GPLv3. Reprodução do material deve ser feita respeitando essa licença e com devida atribuição do trabalho original:\nPatriota, A.G., Garone, G.S. (2024, 2025). Notas do curso de inferência clássica (MAE0225, MAE0301) ministradas pelo Prof. A.G. Patriota digitadas por G.S. Garone.",
    "crumbs": [
      "Prefácio"
    ]
  },
  {
    "objectID": "index.html#agradecimentos",
    "href": "index.html#agradecimentos",
    "title": "Anotações de Inferência Frequentista",
    "section": "Agradecimentos",
    "text": "Agradecimentos\nAo Andrey Sarmento por suas contribuições em revisar e melhorar o projeto.",
    "crumbs": [
      "Prefácio"
    ]
  },
  {
    "objectID": "modelo-estatistico.html",
    "href": "modelo-estatistico.html",
    "title": "1  Modelos Estatísticos na abordagem clássica",
    "section": "",
    "text": "1.1 Modelo Estatístico Paramétrico\nEm teoria de probabilidades, conhecemos a medida de probabilidade, logo, fazemos descrições probabilísticas. \\[(\\Omega, \\mathscr{A}, P)\\stackrel{\\text{X}}{\\rightarrow}(\\mathbb{R}, \\mathscr{B}, P_{X})\\] Na prática, contudo, não conhecemos a medida \\(P\\).\nDefinimos então uma família de medidas de probabilidades que possivelmente descrevem o comportamento aleatório dos dados.\nO Modelo Estatístico é definido pela trinca \\[(\\Omega, \\mathscr{A}, \\mathcal{P})\\] em que \\(\\Omega\\) é o espaço amostral (evento certo), \\(\\mathscr{A}\\) é a Sigma álgebra, uma família de subconjuntos ou eventos em \\(\\Omega\\), e \\(\\mathcal{P}\\) é uma família de medidas de probabilidade que possivelmente descrevem o comportamento aleatório dos dados ou eventos sob investigação.\nSe \\(\\mathcal{P} = \\{P_{\\theta} : \\theta \\in \\Theta\\}\\), em que \\(\\Theta \\subseteq \\mathbb{R}^{p}\\) e \\(p \\in \\mathbb{N}\\), então dizemos que \\((\\Omega, \\mathscr{A}, \\mathcal{P})\\) é um modelo estatístico Paramétrico.\nCaso não exista \\(\\Theta \\subseteq \\mathbb{R}^{p}\\) fixo, então dizemos que o modelo é não-paramétrico.",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelos Estatísticos na abordagem clássica</span>"
    ]
  },
  {
    "objectID": "modelo-estatistico.html#modelo-estatístico-paramétrico",
    "href": "modelo-estatistico.html#modelo-estatístico-paramétrico",
    "title": "1  Modelos Estatísticos na abordagem clássica",
    "section": "",
    "text": "Observação\n\n\n\n\\(\\Theta\\) é o espaço paramétrico e \\(\\theta\\) é o vetor de parâmetros. \\(\\theta\\) Não é variável aleatória, apenas indexa as medidas de probabilidade.\n\n\n\n1.1.1 Exemplos\n\n1.1.1.1 Exemplo de Bernoulli\nConsidere um Ensaio de Bernoulli \\[\\Omega = \\{S,F \\}, \\mathscr{A} = 2^\\Omega\\] Temos algum conhecimento prévio que sugere que as probabilidades de sucesso podem ser \\(0.1, 0.5, 0.9\\)\nNesse caso, \\[\\mathcal{P} = \\{P_{1}, P_{2}, P_{3} \\}\\] em que \\[\\begin{cases}\nP_{1}(\\{S\\}) = 0.1;~ P_{1}(\\{F\\}) = 0.9;~ P_{1}(\\Omega) = 1  \\\\\nP_{2}(\\{S\\}) = 0.5;~ P_{2}(\\{F\\}) = 0.5;~ P_{2}(\\Omega) = 1  \\\\\nP_{3}(\\{S\\}) = 0.9;~ P_{3}(\\{F\\}) = 0.1;~ P_{3}(\\Omega) = 1\n\\end{cases}\\] Observe que \\[\\mathcal{P}=\\{P_{\\theta}: \\theta \\in \\Theta \\}\\] em que \\(\\Theta=\\{1,2,3 \\}\\subseteq \\mathbb{R}\\) Portanto, \\((\\Omega, \\mathscr{A}, \\mathcal{P})\\) é um modelo paramétrico.\nUsando de variáveis aleatórias,\nSeja \\(X : \\Omega \\rightarrow \\mathbb{R}\\) tal que \\[X(\\omega)=\\begin{cases}\n1, \\omega = \\text{S} \\\\\n0, c.c.\n\\end{cases}\\] \\[E_\\theta(X)=\\sum\\limits^{1}_{x=0}xP_\\theta(X=x)\\] \\[\\begin{cases}\n\\theta = 1 \\Rightarrow E_{1}(X)=0.1 \\\\\n\\theta = 2 \\Rightarrow E_{2}(X)=0.5 \\\\\n\\theta = 3 \\Rightarrow E_{3}(X)=0.9\n\\end{cases}\\]\n\n\n1.1.1.2 Exemplo de Exponencial\nSeja \\(\\Omega=(0,\\infty)\\) e \\(\\mathscr{A}\\) uma sigma-álgebra de \\(\\Omega\\) (Sigma-Álgebra de Borel) \\(\\Omega\\) representa o tempo até a ocorrência de um evento (uma reclamação, por exemplo) Temos conhecimento prévio de que as funções densidade de probabilidade que possivelmente descrevem esse evento são:\n\\[\n\\begin{aligned}\nf_{1}(\\omega) &=\n\\begin{cases}\n\\mathrm{e}^{-1\\omega}, \\omega&gt;0 \\\\\n0, c.c.\n\\end{cases}\\\\\nf_{2} (\\omega) &=\n\\begin{cases}\n2\\mathrm{e}^{-2\\omega}, \\omega&gt;0 \\\\\n0, c.c.\n\\end{cases}\\\\\nf_{3} (\\omega) &=\n\\begin{cases}\n\\frac{1}{2}\\mathrm{e}^{-\\frac{1}{2}\\omega}, \\omega&gt;0 \\\\\n0, c.c.\n\\end{cases}\\\\\nf_{4} (\\omega) &=\n\\begin{cases}\n\\frac{1}{10}\\mathrm{e}^{- \\frac{1}{10}\\omega}, \\omega&gt;0 \\\\\n0, c.c.\n\\end{cases}\n\\end{aligned}\n\\]\ne \\(P\\)s, \\[\n\\begin{aligned}\nP_{1}(A)&= \\int_{A} f_{1}(\\omega)d \\omega \\\\\nP_{2}(A)&= \\int_{A} f_{2}(\\omega)d \\omega \\\\\nP_{3}(A)&= \\int_{A} f_{3}(\\omega)d \\omega \\\\\nP_{4}(A)&= \\int_{A} f_{4}(\\omega)d \\omega \\\\\n\\end{aligned}\\] Dessa forma, \\[P_{\\theta}(A)= \\int_{A}f_\\theta(\\omega)d \\omega\\] e \\(\\Omega = \\{1,2,3,4 \\}\\) Seja \\(X: \\Omega \\rightarrow \\mathbb{R}\\) tal que \\[X(\\omega)= \\omega\\] Note que \\[E_\\theta(X)=\\int_{-\\infty}^{\\infty}xf_\\theta(x)dx, \\theta \\in \\{1,2,3,4 \\}\\] \\[E_\\theta(X)=\\begin{cases}\n1,~ \\theta=1 \\\\\n\\frac{1}{2},~ \\theta=2 \\\\\n2,~ \\theta=3 \\\\\n10,~ \\theta=4\n\\end{cases}\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelos Estatísticos na abordagem clássica</span>"
    ]
  },
  {
    "objectID": "modelo-estatistico.html#principais-modelos-estatísticos",
    "href": "modelo-estatistico.html#principais-modelos-estatísticos",
    "title": "1  Modelos Estatísticos na abordagem clássica",
    "section": "1.2 Principais Modelos Estatísticos",
    "text": "1.2 Principais Modelos Estatísticos\n\n1.2.1 Modelo Estatístico de Bernoulli\nDizemos que \\(X\\) é uma variável aleatória com modelo de Bernoulli se, e somente se, \\[\nP_\\theta(X=x)=\\begin{cases}\n\\theta^{x} \\cdot(1-\\theta)^{1-x}, x \\in \\{0,1 \\} \\\\\n0, x \\not \\in \\{0,1 \\}\n\\end{cases}\\] O parâmetro é a probabilidade de sucesso \\[\\begin{cases}\nE_\\theta(X)=\\theta\\\\\n\\mathrm{Var}_{\\theta}(X) = \\theta(1-\\theta) \\\\\nP_\\theta(X=1)=\\theta, P_\\theta(X=0)=1-\\theta\n\\end{cases}\n\\]\nNotação: \\(\\mathrm{Ber}(\\theta)\\) em que \\(\\theta \\in \\Theta=(0,1)\\subseteq \\mathbb{R}\\)\n\n\n1.2.2 Modelo Estatístico Binomial\nDizemos que \\(X\\) é uma variável aleatória com modelo binomial se, e somente se, \\[\nP_\\theta(X=x)=\\begin{cases}\n{n \\choose x} \\cdot \\theta^{x}\\cdot(1-\\theta)^{n-x}, x \\in \\{0,1,\\dots,n \\} \\\\\n0, x \\not \\in \\{0,1,\\dots,n \\}\n\\end{cases}\\] \\[\\begin{cases}\nE_\\theta(X)=n\\theta\\\\\n\\mathrm{Var}_{\\theta}(X) = n\\theta(1-\\theta) \\\\\nP_\\theta(X=0)=(1-\\theta)^{n},\\dots, P_\\theta(X=n)=\\theta^n\n\\end{cases}\\]\nNotação: \\(\\mathrm{Bin}(n,\\theta)\\) em que \\(n\\) é conhecido e fixado previamente, \\(\\theta\\) é a probabilidade de sucesso (parâmetro do modelo) e \\(\\Theta =(0,1)\\) é o espaço paramétrico\n\n\n1.2.3 Modelo Estatístico Geométrico\nDizemos que \\(X\\), representando o número de fracassos até o primeiro sucesso, é uma variável aleatória com modelo estatístico geométrico se, e somente se, \\[P_\\theta(X=x)=\\begin{cases}\n\\theta (1-\\theta)^{x-1}, x \\in \\{1,\\dots\\} \\\\\n0, x \\not \\in \\{1,\\dots\\}\n\\end{cases}\n\\] \\[\n\\begin{cases}\nE_\\theta(X)=\\frac{1}{\\theta}\\\\\n\\mathrm{Var}_\\theta(X) = \\frac{1-\\theta}{\\theta^{2}}\n\\end{cases}\n\\] Notação: \\(\\mathrm{Geom}(\\theta)\\) em que \\(\\theta\\) é o parâmetro do modelo (probabilidade de sucesso) e \\(\\Theta=(0,1)\\) é o espaço paramétrico\n\n\n1.2.4 Modelo de Poisson\nDizemos que \\(X\\) é uma variável aleatória com modelo estatístico Poisson, se, e somente se, \\[\nP_\\theta(X=x)=\\begin{cases}\n\\mathrm{e}^{-\\theta}\\cdot \\frac{\\theta^{x}}{x!}, x \\in \\{0,1,\\dots\\} \\\\\n0, x \\not \\in \\{0,1,\\dots\\}\n\\end{cases}\n\\] \\[\n\\begin{cases}\nE_\\theta(X)=\\theta\\\\\n\\mathrm{Var}_\\theta(X) = \\theta\n\\end{cases}\n\\]\nNotação: \\(\\mathrm{Poisson}(\\theta)\\) em que \\(\\theta\\) é a taxa média de ocorrência do evento (parâmetro do modelo) e \\(\\Theta = (0, \\infty)\\), o espaço paramétrico.\n\n\n1.2.5 Modelo Multinomial\nDizemos que \\(\\pmb{X} = (X_1,\\dots,X_k)\\) é um Vetor Aleatório com modelo estatístico Multinomial se, e somente se a função de probabilidade é \\[\nP_{\\theta}(X_{1}=x_{1},\\dots,X_{k} = x_{k})=\n\\begin{cases}\n\\frac{n!}{x_{1}!x_{2}!\\dots x_{k}!} \\cdot \\theta^{x_{1}}_{1} \\cdot \\cdots \\cdot\\theta^{x_n}_{k} ~~~~ x_{1}+\\dots+x_{k}=n\\\\\n0, c.c\n\\end{cases}\n\\]\n\\[\n\\begin{cases}\nE_\\theta(X_{i})=n\\theta_{i}\\\\\n\\mathrm{Var}_\\theta(X_{i}) = n\\theta_{i} (1-\\theta_{i}) \\\\\n\\mathrm{Cov}(X_{i}X_{j})=-n\\theta_{i}\\theta_{j}\n\\end{cases}\n\\]\nNotação: \\(\\mathrm{Multinomial}(n, \\theta_1,\\theta_2,\\dots,\\theta_k)\\) em que \\(\\theta_{1}+\\dots+\\theta_{k}=1\\) e \\(0\\leq \\theta_{i} \\leq 1\\), \\(\\forall i = 1,2,\\dots,k\\), \\(\\Theta=\\{(\\theta_{1},,\\dots,\\theta_{k}) \\in \\mathbb{R}^{k} : 0\\leq \\theta_{i} \\leq 1, i=1,\\dots,k,\n\\theta_{1}+\\dots+\\theta_{k} = 1 \\}\\)\nEsse modelo tem aplicação em modelos de linguagem como o ChatGPT. (\\(k\\) como tamanho do vocabulário, \\(n=1\\), \\(\\theta_{1}=\\) probabilidade de escolher o primeiro elemento do vocabulário e assim por diante.)\n\n\n1.2.6 Modelo Uniforme contínuo\nDizemos que \\(X\\) é uma variável aleatória contínua com modelo estatístico Uniforme em \\((\\theta_{1}, \\theta_{2}), \\theta_{1}\\leq x \\leq\\theta_{2}\\), se, e somente se, a sua Função Densidade de Probabilidade é \\[\nf_\\theta(x)=\\begin{cases}\n\\frac{1}{\\theta_{2}-\\theta_{1}}, x \\in(\\theta_{1}, \\theta_{2}) \\\\\n0, c.c.\n\\end{cases}\n\\]\n\\[\n\\begin{cases}\nE_\\theta(X)=\\frac{\\theta_2+\\theta_1}{2}\\\\\n\\mathrm{Var}_\\theta(X) = \\frac{(\\theta_2-\\theta_1)^{2}}{12}\\end{cases}\n\\]\nNotação: \\(X \\sim U(\\theta_{1}, \\theta_{2})\\), em que \\(\\theta=(\\theta_{1}, \\theta_{2})\\) é o vetor de parâmetros e \\(\\Theta = \\{\\theta \\in \\mathbb{R}^{2} : \\theta_{2} &gt; \\theta_{1} \\}\\) é o espaço paramétrico.\n\n\n1.2.7 Modelo Exponencial\nDizemos que \\(X\\) é uma variável aleatória contínua com modelo estatístico Exponencial se, e somente se, a sua Função Densidade de Probabilidade é dada por \\[\nf_\\theta(x)=\\begin{cases}\n\\theta \\mathrm{e}^{-\\theta x}, x&gt;0\\\\\n0, c.c.\n\\end{cases}\n\\]\n\\[\n\\begin{cases}\nE_\\theta(X)=\\frac{1}{\\theta}\\\\\n\\mathrm{Var}_\\theta(X) = \\frac{1}{\\theta^{2}}\n\\end{cases}\n\\]\nNotação: \\(X\\sim \\mathrm{Exp}(\\theta), \\theta&gt; 0\\) em que \\(\\theta&gt;0, \\Theta=\\{\\theta \\in \\mathbb{R}: \\theta&gt;0 \\}\\)\n\n\n1.2.8 Modelo Normal\nDizemos que \\(X\\) é uma variável aleatória contínua com modelo estatístico Normal com média \\(\\mu\\) e variância \\(\\sigma^{2}\\) se, e somente se, a sua Função Densidade de Probabilidade é dada por \\[\nf_\\theta(x)=\n\\frac{1}{\\sqrt{2\\pi \\sigma^{2}}}\\cdot \\mathrm{e}^{-\\frac{1}{2\\sigma^{2}}(x-\\mu)^{2}}, x \\in \\mathbb{R}\n\\] em que \\(\\theta= (\\mu, \\sigma^{2}) \\in \\Theta= \\mathbb{R}\\times \\mathbb{R}^{+}\\)\n\\[\n\\begin{cases}\nE_\\theta(X)=\\mu\\\\\n\\mathrm{Var}_\\theta(X) = \\sigma^{2}\n\\end{cases}\n\\]\nNotação: \\(X \\sim N (\\mu, \\sigma^{2})\\), em que \\(\\theta=(\\mu, \\sigma^{2})\\) é o vetor de parâmetros.",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelos Estatísticos na abordagem clássica</span>"
    ]
  },
  {
    "objectID": "populacao-e-amostra.html",
    "href": "populacao-e-amostra.html",
    "title": "2  População e Amostra",
    "section": "",
    "text": "2.1 Variável Populacional\nVeja: Modelo Estatístico para definições dos modelos estatísticos paramétricos.\nPela teoria estatística, população é o conjunto sob investigação de todos os potenciais elementos.\nA Variável Populacional representa os valores numéricos de cada elemento da população: \\[\nX\\sim f_{\\theta,}\\theta \\in \\Theta\n\\] em que \\(f_\\theta\\) é a Função Densidade de Probabilidade da Variável Aleatória populacional. \\(\\theta\\) é o vetor de parâmetros (desconhecido) e \\(\\Theta\\) é o espaço paramétrico",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>População e Amostra</span>"
    ]
  },
  {
    "objectID": "populacao-e-amostra.html#amostra-teórica",
    "href": "populacao-e-amostra.html#amostra-teórica",
    "title": "2  População e Amostra",
    "section": "2.2 Amostra (Teórica)",
    "text": "2.2 Amostra (Teórica)\nNa estatística descritiva, a amostra é definida como um subconjunto da população.\nAlguns livros utilizam o termo “amostra representativa” para representar a amostra confiável de outra amostra que carrega determinados viéses de seleção. O termo representativo é controverso na estatística teórica,\n\n2.2.1 Amostra Aleatória\nNa estatística teórica, dizemos que \\((X_{1},\\dots,X_{n})\\) é uma amostra aleatória de \\(X\\) (v.a. populacional) se \\(X_{1},\\dots,X_{n}\\) forem independentes e identicamente distribuídas de acordo com a distribuição da variável aleatória populacional \\(X\\) Ou seja, \\[\n\\text{Independentes }\\rightarrow\\begin{cases}X_{1}\\sim f_{\\theta,}\\theta \\in \\Theta \\\\\n. \\\\\n. \\\\\n. \\\\\nX_{n} \\sim f_{\\theta}, \\theta \\in \\Theta\n\\end{cases}\n\\]\nSendo assim, \\((X_1, \\dots, X_n)\\) é amostra aleatória de \\(X\\sim f_\\theta, \\theta in \\Theta\\) sempre que \\((X_1, \\dots, X_n)\\) forem independentes para cada \\(P_\\theta, \\theta \\in \\Theta\\) e \\(X_i \\sim f_\\theta, \\theta \\in \\Theta, i = 1, 2, \\dots, n\\)\n\n2.2.1.1 Diagrama\n\n\n\n\n\nflowchart UD\n  A[$X\\sim f_\\theta, \\theta \\in \\Theta, \\g(\\theta)$] --&gt; B($\\X_1, \\sim \\f_\\theta, \\theta \\in \\Theta$)\n  A --&gt; C($\\X_2, \\sim \\f_\\theta, \\theta \\in \\Theta$)\n  A --&gt; D($\\X_n, \\sim \\f_\\theta, \\theta \\in \\Theta$)\n  B --&gt; E(x_1)\n  C --&gt; F(x_2)\n  D --&gt; G(x_n)",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>População e Amostra</span>"
    ]
  },
  {
    "objectID": "populacao-e-amostra.html#sec-ao",
    "href": "populacao-e-amostra.html#sec-ao",
    "title": "2  População e Amostra",
    "section": "2.3 Amostra (Observada)",
    "text": "2.3 Amostra (Observada)\nÉ formada por valores numéricos após utilizar um procedimento de amostragem. \\[\nx_{1},\\dots,x_{n}\n\\]\nem que \\(n\\) é o tamanho amostral.",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>População e Amostra</span>"
    ]
  },
  {
    "objectID": "quantidade-de-interesse.html",
    "href": "quantidade-de-interesse.html",
    "title": "3  Quantidade de Interesse",
    "section": "",
    "text": "É uma quantidade relacionada com a distribuição da variável aleatória populacional, ou seja, qualquer valor em função de \\(\\theta\\). \\[\ng(\\theta)\n\\]\nComo \\(g(\\theta) = E_\\theta(X), g(\\theta)= \\mathrm{Var}_\\theta(X), g(\\theta)=P_\\theta(X\\geq1), g(\\theta)=\\theta\\) em que \\(X\\) é a variável aleatória populacional. Empregando outra variável populacional, como \\(Y\\), conseguimos outras quantidades de interesse, como \\(g(\\theta)=\\mathrm{Cov}_{\\theta}(X,y)\\) ou \\(P_\\theta(X \\in A \\lvert Y \\in B)\\)",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Quantidade de Interesse</span>"
    ]
  },
  {
    "objectID": "dist-amostral.html",
    "href": "dist-amostral.html",
    "title": "4  Distribuição Amostral",
    "section": "",
    "text": "4.1 Exemplos\nSeja \\((X_{1},\\dots,X_{n})\\) amostra aleatória (a.a.) de \\(X\\sim f_{\\theta,}\\theta \\in \\Theta\\)\na Função Densidade de Probabilidade conjunta de \\((X_{1}, X_{2},\\dots,X_{n})\\) é \\(\\forall \\theta \\in \\Theta\\), no caso discreto: \\[\nP_\\theta(X_{1}=k_{1}, X_{2}=k_{2},\\dots,X_{n}= k_{n})\\stackrel{\\text{ind}}{=}\\prod^{n}_{i=1}P_\\theta(X_{i}=k_{i})\n\\stackrel{\\text{id}}{=}\\prod^{n}_{i=1}P_\\theta(X=k_{i})\\stackrel{\\text{def}}{=}\\prod^{n}_{i=1}f_\\theta(k_{i})\n\\]\nno caso contínuo: \\[\nf_\\theta^{(X_1, \\dots, X_n)}(k_{1},k_{2},\\dots,k_{n})\\stackrel{\\text{i.i.d}}{=}\\prod^{n}_{i=1}f_{\\theta}(k_{i})\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Distribuição Amostral</span>"
    ]
  },
  {
    "objectID": "dist-amostral.html#exemplos",
    "href": "dist-amostral.html#exemplos",
    "title": "4  Distribuição Amostral",
    "section": "",
    "text": "4.1.1 Exemplo um\nSeja \\((X_{1},X_{2},X_{3})\\) uma a.a de \\(X\\sim \\text{Ber}(\\theta), \\theta \\in (0,1)\\) 1. Especifique o espaço paramétrico 2. Calcule a função de probabilidade da amostra 3. Encontre as seguintes quantidade de interesses em função de \\(\\theta\\) 1. \\(g(\\theta)=E_\\theta(X)\\) 2. \\(g(\\theta)=P_\\theta(X=0)\\) 3. \\(g(\\theta)=\\mathrm{CV}_\\theta(X)\\) Resolução 1. \\(\\Theta=(0,1)\\) 2. Duas resoluções possíveis 1. Dando valores à amostra \\[\\begin{aligned}\n         &(X_{1}, X_{2},X_{3})  &P(X_{1}= k_{1},X_{2}= k_{2},X_{3}= k_{3})= \\prod ^{3}_{i=1}P_\\theta(X=k_{1})\\\\\n         &(0,0,0) &(1-\\theta)^{3}\\\\\n         &(0,0,1) &(1-\\theta)^{2}\\theta\\\\\n         &(0,1,0) &(1-\\theta)^{2}\\theta\\\\\n         &(1,0,0) &(1-\\theta)^{2}\\theta\\\\\n         &(0,1,1) &(1-\\theta)\\theta^2\\\\\n         &(1,0,1) &(1-\\theta)\\theta^{2}\\\\\n         &(1,1,0) &(1-\\theta)\\theta^{2}\\\\\n         &(1,1,1) &\\theta^{3}\\\\\n         \\end{aligned}\n    \\] 2. Enunciando a função Observe que, se \\(k \\in \\{0,1 \\}\\), \\[\\begin{aligned}\n       &P_\\theta(X=k)=\\theta^{k}(1-\\theta)^{1-k}\\cdot\\mathbb{1}_{\\{0,1 \\}}(k)\\Rightarrow\\\\\n       &P_\\theta(X_{1}=k,X_{2}=k_{2},X_{3}= k_{3})\\stackrel{\\text{i.i.d}}{=}\\prod^{3}_{i=1}\\{\\theta^{k_{i}}\n       (1-\\theta)^{1-k_{i}}\\mathbb{1}_{\\{0,1 \\}}(k_{i}) \\} = \\\\\n       &= \\theta^{\\sum\\limits^{3}_{i=1}k_{i}}(1-\\theta)^{3-\\sum\\limits^{3}_{i=1}k_{i}}\\prod^{3}_{i=1}\\mathbb{1}_{\\{0,1\\}}\n       (k_{i})\n       \\end{aligned}\n    \\] 3. Em função de \\(\\theta\\): 1. \\(g(\\theta)=E_\\theta(X)=\\theta\\) 2. \\(g(\\theta)=P_\\theta(X=0)=1-\\theta\\) 3. \\(g(\\theta)=\\mathrm{CV}_\\theta(X)=\\frac{\\sqrt{\\theta(1-\\theta)}}{\\theta}\\)\n\n\n\n4.1.2 Exemplo dois \nSeja \\((X_{1},\\dots,X_{n})\\) uma a.a de \\(X\\sim\\text{Pois}(\\theta), \\theta \\in(0,\\infty)\\), encontre a f.p. conjunta da amostra. Como esse vetor é uma a.a. (ou seja, variáveis aleatórias independentes e identicamente distribuídas), temos que \\[\nP_\\theta(X_1=k_{1},\\dots,X_{n}=k_{n})\\stackrel{\\text{iid}}{=}\\prod^{n}_{i=1}P_\\theta(X=k_{i})=\\prod^{n}_{i=1}\\{\\mathrm{e}\n^{-\\theta} \\cdot \\frac{\\theta^{k_{i}}}{k_{i}!}\\}\n\\] Sempre que \\(k_{i}\\in\\{0,1,\\dots \\}, \\forall i = 1,\\dots,n\\)\n\\[\n\\Rightarrow P_\\theta(X_1=k_{1},\\dots,X_{n}=k_{n})=\\mathrm{e}^{-n \\theta} \\cdot \\frac{\\theta^{\\sum\\limits^{n}_{i=1}k_{i}}}{\\prod^{n}_{i=1}\n(k_{i})!}\n\\]\n\n\n4.1.3 Exemplo contínuo um\nSeja \\((X_{1},\\dots,X_{n})\\) uma a.a de \\(X\\sim\\text{Exp}(\\theta), \\theta \\in(0,\\infty)\\), encontre a função densidade de probabilidade (f.d.p.) conjunta da amostra. \\[\n\\begin{aligned}\nf_{\\theta}^{(X_1,\\dots,X_{n})}(k_{1},\\dots,k_{n})&\\stackrel{\\text{iid}}{=}\\prod^{n}_{i=1}f_\\theta(k_{i})=\\prod^{n}_{i=1}\n\\{\\theta \\mathrm{e}^{-\\theta k_{i}} \\cdot \\mathbb{1}_{(0,\\infty )}(k_{i}) \\}\n\\\\&\\Rightarrow f_\\theta^{(X_1,\\dots,X_n)}(k_1,\\dots,k_n)=\\theta^{n}\n\\cdot \\mathrm{e}^{-\\theta\\sum\\limits^{n}_{i=1}k_{i}} \\cdot\\prod^{n}_{i=1}\\mathbb{1}_{(0,\\infty)}(k_i)\n\\end{aligned}\n\\]\n\n\n4.1.4 Exemplo contínuo dois\nSeja \\((X_{1},\\dots,X_{n})\\) uma a.a. (i.i.d) de \\(X\\sim N(\\mu,\\sigma^{2})\\) em que \\(\\theta=(\\mu, \\sigma^{2}) \\in \\Theta=\\mathbb{R}\\times \\mathbb{R}_{+}\\). Considere \\(\\stackrel{x}{\\sim}=(x_{1},\\dots,x_{n})\\) a amostra observada.\n\\[\n\\begin{aligned}\nL_{\\stackrel{X}{\\sim}}(\\theta) &\\stackrel{\\text{iid}}{=} \\prod^{n}_{i=1}f_\\theta(x_{i})=\n\\prod^{n}_{i=1}\\left\\{\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} \\mathrm{exp}\\{- \\frac{1}{2\\sigma^{2}}(x_{i}-\\mu)^{2}\\}\\right\\} \\\\\n&= \\frac{1}{(2 \\pi \\sigma^{2})^{\\frac{x}{2}}}\\cdot \\mathrm{exp}\\{- \\frac{1}{2 \\sigma^{2}} \\sum\\limits^{n}_{i=1}(x_{i}-\\mu)^{2}  \\}\n\\end{aligned}\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Distribuição Amostral</span>"
    ]
  },
  {
    "objectID": "funcao-verossimilhanca.html",
    "href": "funcao-verossimilhanca.html",
    "title": "5  Função de Verossimilhança",
    "section": "",
    "text": "5.1 Exemplo\nQuando analisamos a distribuição conjunta da amostra em função de \\(\\theta\\) nos valores da amostra observada, temos a Função de Verossimilhança\n\\[\n\\mathrm{L}_{\\boldsymbol{x}}(\\theta)=P_\\theta(X_{1}=x_{1},X_{2}=x_{2},\\dots,X_{n}=x_{n})\n\\]\nem que \\(\\boldsymbol{x}=(x_{1},x_{2},\\dots,x_{n})\\) é a amostra observada.\nConsidere \\((X_{1},X_{2},X_{3},X_{4})\\) a.a de \\(X\\sim \\text{Ber}(\\theta), \\theta \\in \\{0.1, 0.5, 0.9 \\}\\). Note que o espaço paramétrico é \\(\\Theta=\\{0.1,0.5,0.9 \\}\\). Considere, ainda, que a amostra observada foi \\((0,1,1,1)\\). Encontre a função de verossimilhança.\n\\[\n\\begin{aligned}\nL_{\\boldsymbol{x}_n}(\\theta)&=P_\\theta(X_{1}=x_{1},X_{2}=x_{2},X_{3}=x_{3},X_{4}=x_{4}) \\\\\n&= \\theta^{\\sum\\limits^{4}_{i=1}x_{i}}\n(1-\\theta)^{4-\\sum\\limits^{4}_{i=1}x_{i}}\\cdot \\cancelto{1}{\\prod^{4}_{i=1}\\mathbb{1}_{\\{0,1 \\}}(x_{i})} =\n\\theta^{3}(1-\\theta)\n\\end{aligned}\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Função de Verossimilhança</span>"
    ]
  },
  {
    "objectID": "estatisticas.html",
    "href": "estatisticas.html",
    "title": "6  Estatísticas",
    "section": "",
    "text": "6.1 Exemplo\nFunções da amostra que não dependem de \\(\\theta \\in \\Theta\\)\nSeja \\((X_{1}, \\dots, X_{n})\\) a.a. de \\(X\\sim f_{\\theta,}\\theta \\in \\Theta\\). São estatísticas: 1. \\(T_{1}(X_{1},\\dots,X_{n})=X_{1}+\\dots+X_{n}\\) 2. \\(T_{2}=\\bar{X}= \\frac{X_{1}+\\dots+X_{n}}{n}\\) 3. \\(T_{3}=\\max\\{X_{1},\\dots,X_{n}\\}=X_{(n)}\\) 4. \\(T_{4}=\\min\\{X_{1},\\dots,X_{n}\\}=X_{(1)}\\) 5. \\(T_{5}(X_{1},\\dots,X_{n})=X_{(n)}-X_{(1)}\\) 6. \\(T_{6}(X_{1},\\dots,X_{n})=X_{i}, \\text{para algum }i=1,\\dots,n\\) 7. \\(T_{7}(X_{1},\\dots,X_{n})=\\frac{1}{n}\\sum\\limits^{n}_{i=1}(X_{i}-\\bar{X})^{2}\\) 8. \\(T_{8}(X_{1},\\dots,X_{n})=\\frac{1}{n-1}\\sum\\limits^{n}_{i=1}(X_{i}-\\bar{X})^{2}\\) 9. \\(T_{9}(X_{1},\\dots,X_{n})=\\frac{1}{n}\\sum\\limits^{n}_{i=1}|X_{i}-\\bar{X}|\\) 10. \\(T_{10}(X_{1},\\dots,X_{n})=\\sqrt{\\frac{1}{n}\\sum\\limits^{n}_{i=1}(X_{i}-\\bar{X})^{2}}\\) etc.",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Estatísticas</span>"
    ]
  },
  {
    "objectID": "estatisticas.html#exemplo",
    "href": "estatisticas.html#exemplo",
    "title": "6  Estatísticas",
    "section": "",
    "text": "Observação\n\n\n\nAs estatísticas são variáveis aleatórias: \\[\n\\begin{aligned}\n\\pmb{X}_n&=(X_{1},\\dots,X_{n}):\\Omega\\rightarrow \\mathbb{R}^{n} \\\\\nT(\\pmb{X}_n)&=T \\circ \\pmb{X}_n:\\Omega\\rightarrow \\mathbb{R}\n\\end{aligned}\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Estatísticas</span>"
    ]
  },
  {
    "objectID": "estimadores.html",
    "href": "estimadores.html",
    "title": "7  Estimadores",
    "section": "",
    "text": "7.1 Estimativas\nSão estatísticas cujo objetivo é estimar uma quantidade de interesse. Portanto, estimadores são também variáveis aleatórias.\nSão os valores observados a partir da amostra observada dos estimadores. Portanto, estimativas são valores numéricos\nExemplos: 1. \\(\\bar{X}\\) é uma estatística, \\(\\bar{X}\\) é um estimador para \\(g(\\theta)=E_\\theta(X)\\) 2. Observando \\(\\bar{x}=\\frac{1}{n} \\sum\\limits^{n}_{i=1}x_{i}\\) é uma estimativa Seja \\((X_{1},X_{2})\\) a.a de \\(X\\sim \\text{Ber}(\\theta), \\theta \\in (0,1)\\). Considere as estatísticas e suas funções de probabilidade: \\[\n\\begin{aligned}\nT_{1}(X_{1},X_{2}) &= X_{1}\\\\\nT_{2}(X_{1},X_{2}) &= X_{2}\\\\\nT_{3}(X_{1},X_{2}) &= X_{1}+X_{2}\\\\\nT_{4}(X_{1},X_{2}) &= \\max\\{X_{1},X_{2}\\}\\\\\nT_{5}(X_{1},X_{2}) &= \\min\\{X_{1},X_{2}\\}\\\\\nT_{6}(X_{1},X_{2}) &= \\frac{1}{2}[(X_{1}-\\bar{X})^{2}+(X_{2}-\\bar{X})^{2}]=S^{2}_{n}=\\frac{1}{2}S_{n-1}^{2}\n\\end{aligned}\n\\]\n\\[\n\\begin{array}{cccccccccc}\n(X_{1},X_{2}) & P_{\\theta X_{1},X_{2}} & X_{1} & X_{2} & X_{1}+X_{2} & T_4 & T_5 & \\bar{X}& S^{2}_{n} & S^{2}_{n-1} \\\\\n\\hline\n(0,0) & (1-\\theta)^{2} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n(1,0) & \\theta(1-\\theta) & 1 & 0 & 1 & 1 & 0 & 0.5 & 0.25 & 0.5\\\\\n(0,1) & \\theta(1-\\theta) & 0 & 1 & 1 & 1 & 0 & 0.5 & 0.25 & 0.5\\\\\n(1,1) & \\theta^{2} & 1 & 1 & 2 & 1 & 1 & 1 & 0 & 0\\\\\n\\hline\n\\end{array}\n\\]\nCalcule para \\(T \\in \\{T_{1},T_{2},T_{3},T_{4},T_{5},T_{6} \\}\\)\na-) \\(E_\\theta(T)\\)\n\\(E_\\theta(T_{1}(X_{1},X_{2}))=E_\\theta(X_{1})= (1-\\theta)^{2}+1 \\cdot\\theta(1-\\theta)+0 \\theta(1-\\theta)+1\n\\theta^{2}= \\theta\\)\nO mesmo vale para \\(T_2\\)\n\\(E_\\theta(T_{3}(X_{1},X_{2}))=E_\\theta(X_{1}+X_{2})=2 \\theta\\) \\(E_\\theta(T_{4}(X_{1},X_{2}))=E_\\theta(\\max\\{X_{1},X_{2}\\})=0 \\cdot (1-\\theta)^{2}+ 1\\cdot[2\\cdot \\theta(1-\\theta)+\n\\theta^{2}] = 2\\theta-\\theta^{2}\\) \\(E_{\\theta}(T_{5}(X_{1},X_{2}))=E_\\theta(\\min\\{X_{1},X_{2} \\})=\\theta^{2}\\) \\(E_\\theta(T_{6}(X_{1},X_{2}))=2 \\theta(1-\\theta)\\)\nb-) \\(\\mathrm{Var}_{\\theta}(T)\\)\nTermine com os mesmos raciocínios\nc-) \\(P_\\theta(T=0)\\)\nTermine com os mesmos raciocínios\nAlguns resultados importantes:\n\\[\n\\begin{aligned}\nE_\\theta(\\bar{X})&=E_\\theta(\\frac{X_{1}+X_{2}}{2})=\\theta, \\theta \\in(0,1)\\\\\nE_{\\theta}(\\bar{X}^{2}) &= 0^{2}(1-\\theta)^{2} + \\frac{2}{4}  \\theta (1-\\theta) + 1^{2}\\theta^{2}=\\frac{1}{2}\\theta +\n\\frac{1}{2} \\theta^{2}, \\theta \\in(0,1)\\\\\n\\Rightarrow \\mathrm{Var}_\\theta(\\bar{X}) &= \\frac{\\theta+\\theta^{2}}{2} - \\theta^{2} =\\frac{\\theta (1-\\theta)}{2},\n\\theta \\in(0,1)\n\\end{aligned}\n\\]\nAplicando em nossa tabela (\\(S_{n-1}^{2}\\)):\n\\[\n\\begin{aligned}\nE_\\theta([(X_{1}-\\bar{X})^{2}+(X_{2}-\\bar{X})^{2}])&= \\theta(1-\\theta)\\\\\nE_\\theta([(X_{1}-\\bar{X})^{2}+(X_{2}-\\bar{X})^{2}]^{2}) &= \\frac{\\theta(1-\\theta)}{2}\\\\\n\\Rightarrow\\mathrm{Var}_{\\theta}([(X_{1}-\\bar{X})^{2}+(X_{2}-\\bar{X})^{2}]) &= \\frac{1}{2}\\theta(1-\\theta)\n[1-2 \\cdot \\theta(1-\\theta)]\n\\end{aligned}\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Estimadores</span>"
    ]
  },
  {
    "objectID": "estimadores.html#propriedades-dos-estimadores-para-quantidades-de-interesse",
    "href": "estimadores.html#propriedades-dos-estimadores-para-quantidades-de-interesse",
    "title": "7  Estimadores",
    "section": "7.2 Propriedades dos estimadores para quantidades de interesse",
    "text": "7.2 Propriedades dos estimadores para quantidades de interesse\n\n7.2.1 Estimados não-viciados ou não-enviesados\nSeja \\((X_{1},\\dots,X_{n})\\) a.a. de \\(X\\sim f_{\\theta,}\\theta \\in \\Theta\\) e considere \\(T(X_{1},\\dots,X_{n})=\\hat{\\theta}_n\\) um estimador para \\(\\theta\\). Dizemos que \\(\\hat{\\theta}_n\\) é não-enviesado para \\(\\theta \\Leftrightarrow\\) \\[\nE_\\theta(\\hat \\theta_{n}) = \\theta, \\forall \\theta \\in \\Theta\n\\] De forma geral, \\(T(X_{1},\\dots,X_{n})\\) é um estimador não-viciado para \\(g(\\theta) \\Leftrightarrow\\) \\[\nE_\\theta(T(X_{1},\\dots,X_{n}))=g(\\theta), \\forall \\theta \\in \\Theta\n\\] Caso contrário, dizemos que \\(T(X_{1},\\dots,X_{n})\\) é viciado ou enviesado para \\(g(\\theta)\\).\nDizemos que \\(\\hat\\theta_{n}\\) é fracamente consistente para \\(\\theta \\Leftrightarrow\\) \\[\n\\lim_{n\\to \\infty}{P_\\theta(|\\hat \\theta_{n}- \\theta| &gt; \\epsilon)=0, \\forall \\theta \\in \\Theta}\n\\] e para cada \\(\\epsilon&gt;0\\) fixado.\n\n7.2.1.1 Estimadores não viciados assintoticamente\nDizemos que \\(T(X_{1},\\dots,X_{n})\\) é um estimador assintoticamente não viciado para \\(g(\\theta) \\Leftrightarrow\\) \\[\n\\lim_{n\\to \\infty}{E_\\theta(T(X_{1},\\dots,X_{n}))} = g(\\theta), \\forall \\theta \\in \\Theta\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Estimadores</span>"
    ]
  },
  {
    "objectID": "eqm-e-vies.html",
    "href": "eqm-e-vies.html",
    "title": "8  Erro quadrático médio (EQM)",
    "section": "",
    "text": "8.1 Propriedades do EQM\nO erro quadrático médio (EQM) do estimador \\(T(X_{1},\\dots,X_{n})\\) com respeito a \\(g(\\theta)\\) é definido por \\[\n\\mathrm{EQM}(T,g(\\theta))=E_{\\theta}((T(X_{1},\\dots,X_{n})-g(\\theta))^{2})\n\\]\nSeja \\(T(X_{1},\\dots,X_{n})\\) um estimador para \\(g(\\theta)\\), seja \\(\\mu_{t} = E_\\theta(T(X_{1},\\dots,X_{n}))\\) \\[\n\\begin{aligned}\n&\\mathrm{EQM}(T,g(\\theta))\\\\\n&=E_\\theta[(T(X_{1},\\dots,X_{n})-\\mu_{t}+\\mu_{t}-g(\\theta))^{2}] \\\\ \\\\\n&= E_\\theta[((T(X_{1},\\dots,X_{n})- \\mu_{t})+ (\\mu_{t}-g(\\theta)))^{2}] \\\\\n& = E_\\theta[(T(X_{1},\\dots,X_{n})-\\mu_{t})^{2}+2(T(X_{1},\\dots,X_{n})-\\mu_{t})(\\mu_{t}g(\\theta))+(\\mu_{t}- g(\\theta))^{2}] \\\\\n&= \\overbrace{E_\\theta[(T(X_{1},\\dots,X_{n})-\\mu_{t})^{2}]}^{\\mathrm{Var}_{\\theta}(T(X_{1},\\dots,X_{n}))} + 2(\\mu_{t} -\ng(\\theta))\\cancelto{0}{E_\\theta(T(X_{1},\\dots,X_{n})-\\mu_{t})} + (\\mu_{t}-g(\\theta))^{2} \\\\\n&=\\mathrm{Var}_\\theta(T(X_{1},\\dots,X_{n})) + (\\mu_{t}-g(\\theta))^{2}\n\\end{aligned}\n\\]\nPortanto, \\[\n\\mathrm{EQM}(T,g(\\theta)) = \\mathrm{Var}_\\theta(T(X_{1},\\dots,X_{n})) + (\\mu_{t}-g(\\theta))^{2}\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Erro quadrático médio (EQM)</span>"
    ]
  },
  {
    "objectID": "eqm-e-vies.html#viés",
    "href": "eqm-e-vies.html#viés",
    "title": "8  Erro quadrático médio (EQM)",
    "section": "8.2 Viés",
    "text": "8.2 Viés\nDenotamos de viés de \\(T(X_{1},\\dots,X_{n})\\) com respeito a \\(g(\\theta)\\) por \\[\n\\mathrm{Viés}(T,g(\\theta)) = E_\\theta(T(X_{1},\\dots,X_{n}))-g(\\theta),\\forall \\theta \\in \\Theta\n\\]\nDessa forma, temos que \\[\n\\mathrm{EQM}(T,g(\\theta)) = \\mathrm{Var}_\\theta(T(X_{1},\\dots,X_{n})) + [\\mathrm{Viés}(T(X_1,\\dots,X_n),g(\\theta))]^{2}\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Erro quadrático médio (EQM)</span>"
    ]
  },
  {
    "objectID": "eqm-e-vies.html#exemplo",
    "href": "eqm-e-vies.html#exemplo",
    "title": "8  Erro quadrático médio (EQM)",
    "section": "8.3 Exemplo",
    "text": "8.3 Exemplo\nSeja \\((X_{1},\\dots,X_{n})\\) uma amostra aleatória, ou seja, independentes e identicamente distribuídas (i.i.d.), de \\(X\\sim \\mathrm{Ber}(\\theta)\\) em que \\(\\theta \\in \\Theta = (0,1)\\). Calcule o viés e o EQM de \\(\\bar{X}_{n}\\) com respeito a \\(g(\\theta)=P_\\theta(X=1)\\)\nO estimador é, então, \\(T(X_{1},\\dots,X_{n})=\\bar{X}_{n}= \\frac{X_{1}+\\dots+X_{n}}{n}\\) para \\(g(\\theta)=P_\\theta(X=1)=\\theta\\) (pelo modelo de Bernoulli). \\[\n\\begin{aligned}\nE_\\theta(\\bar{X}_{n}) &= E_\\theta\\left(\\frac{1}{n}\\sum\\limits^{n}_{i=1}X_{i}\\right)=\n\\frac{1}{n}\\sum\\limits^{n}_{i=1}E_\\theta(X_{i}) \\stackrel{id. dist.}{\\Rightarrow} \\\\\nE_\\theta(\\bar{X}_{n}) &= \\frac{1}{n} \\sum\\limits^{n}_{i=1} E_\\theta(X) \\\\\n& = \\frac{n}{n} \\theta = \\theta, \\forall \\theta \\in \\Theta\n\\end{aligned}\n\\]\nPortanto, \\(\\bar{X}_{\\theta, n}\\) é não enviesado para \\(g(\\theta) = \\theta\\). \\[\n\\Rightarrow \\mathrm{Viés}(\\bar{X}_{n}, g(\\theta)) = 0, \\forall \\theta \\in \\Theta\n\\]\nPara o EQM, \\[\n\\begin{aligned}\n\\mathrm{EQM}(\\bar{X}_{n},g(\\theta)) &= \\mathrm{Var}_\\theta(\\bar{X}_{n}) - 0^{2} = \\mathrm{Var}_\\theta\n\\left(\\frac{1}{n}\\sum\\limits^{n}_{i=1}X_{i}\\right)= \\frac{1}{n^{2}}\\mathrm{Var}_\\theta\\left(\\sum\\limits^{n}_{i=1}X_{i}\\right)\\\\\n& \\stackrel{\\text{ind}}{=} \\frac{1}{n^{2}}\\sum\\limits ^{n}_{i=1}\\mathrm{Var}_\\theta(X_{i})\n\\stackrel{\\text{ind. dist.}}{=} \\frac{1}{n^{2}} \\sum\\limits^{n}_{i=1}\\mathrm{Var}_{\\theta}(X), \\\\\n&= \\frac{n \\theta(1-\\theta)}{n^{2}} = \\frac{\\theta(1-\\theta)}{n}, \\forall \\theta \\in \\Theta\n\\end{aligned}\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Erro quadrático médio (EQM)</span>"
    ]
  },
  {
    "objectID": "monte-carlo.html",
    "href": "monte-carlo.html",
    "title": "9  Simulações de Monte Carlo",
    "section": "",
    "text": "9.1 Método\nTem como objetivo replicar artificialmente os dados de um modelo estatístico para estudar o comportamento de estatísticas e estimadores (ou qualquer procedimento estatístico)",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Simulações de Monte Carlo</span>"
    ]
  },
  {
    "objectID": "monte-carlo.html#método",
    "href": "monte-carlo.html#método",
    "title": "9  Simulações de Monte Carlo",
    "section": "",
    "text": "Defina o modelo estatístico: “Seja \\((X_{1},\\dots,X_{n})\\)” a.a. de \\(X\\sim f_{\\theta,}\\theta \\in \\Theta\\).\nEscolha \\(\\theta_{0} \\in \\Theta\\) e considere-o fixado daqui em diante.\nPara \\(n\\) fixado, gere (\\(x_{1}, x_{2},\\dots,x_{n}\\)) a amostra observada de \\(X\\sim f_{\\theta_{0}}\\)\nArmazene a amostra observada\nRepita 3. e 4. \\(M=10000\\) vezes (ou quantas vezes desejar, a depender do caso)",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Simulações de Monte Carlo</span>"
    ]
  },
  {
    "objectID": "emv.html",
    "href": "emv.html",
    "title": "10  Estimação via máxima verossimilhança (EMV)",
    "section": "",
    "text": "10.1 Exemplo\nO valor numérico \\(\\hat\\theta_{n}\\) que maximiza a função de verossimilhança, ou seja, \\(L_{\\stackrel{x}{\\sim}}(\\hat\\theta_{n}) \\geq L_{\\stackrel{x}{\\sim}}(\\theta)\\forall \\theta \\in \\Theta\\) é dito ser uma estimativa de máxima verossimilhança (MV) para \\(\\theta\\). Observe que \\(\\hat\\theta_{n}\\) depende da amostra observada e portanto: \\(\\hat\\theta_{n} = \\hat\\theta(x_{1},x_{2},\\dots,x_{n})\\).\nO estimador de máxima verossimilhança é obtido substituindo \\((x_{1},\\dots,x_{n})\\) por \\((X_{1},\\dots,X_{n})\\), ou seja, \\(\\hat\\theta(X_{1},\\dots,X_{n})\\) é o Estimador de Máxima Verossimilhança (EMV)\nSeja \\((X_{1},\\dots,X_{n})\\) amostra aleatória (a.a.) de \\(X\\sim f_{\\theta}, \\theta \\in \\{\\frac{1}{3}, \\frac{1}{2} \\}\\) em que \\(f_\\theta\\) é uma função de probabilidade que satisfaz:\n\\[\n\\begin{array}{|c|c|c|c|}\n\\hline\nX=x: & 0 & 1 & 2\\\\\n\\hline\nf_{\\theta}(x): & \\theta & \\theta^{2} & 1-\\theta-\\theta^{2}\\\\\n\\hline\n\\end{array}\n\\] Considere que a amostra observada é \\(\\stackrel{x}{\\sim}=(0,0,1)\\).\na-) Encontre a estimativa da máxima verossimilhança\nSabemos que \\[\nf_{\\theta}(x)= \\theta^{\\mathbb{1}_{\\{0\\}}(x)} \\cdot (\\theta^{2})^{\\mathbb{1}_{\\{1\\}}(x)} \\cdot\n(1-\\theta-\\theta^{2})^{\\mathbb{1}_{\\{2\\}}(x)} \\forall \\theta \\in \\Theta\n\\] portanto,\n\\[\nL_{\\stackrel{x}{\\sim}}(\\theta)\\stackrel{\\text{iid}}{=}\\prod^{n}_{i=1}f_{\\theta}(x_{i})  =\n\\theta^{\\sum\\limits^{n}_{i=1}\\mathbb{1}_{\\{0\\}}(x_{i})} \\cdot (\\theta^{2})^{\\sum\\limits^{n}_{i=1}\\mathbb{1}_{\\{1\\}}(x_{i})}\n\\cdot (1-\\theta-\\theta^{2})^{\\sum\\limits^{n}_{i=1}\\mathbb{1}_{\\{2\\}}(x_{i})} \\forall \\theta \\in \\Theta\n\\]\nPara \\(\\stackrel{x}{\\sim}=(0,0,1)\\), \\[\nL_{\\stackrel{x}{\\sim}}(\\theta) = \\theta^{2} \\cdot \\theta^{2} \\cdot (1-\\theta -\\theta^{2})^{0} = \\theta^{4}\n\\forall \\theta \\in \\Theta\n\\]\nSubstituindo \\(\\forall \\theta \\in \\Theta\\): \\[\n\\theta = \\frac{1}{2}\\Rightarrow L_{\\stackrel{x}{\\sim}}\\left(\\frac{1}{2}\\right)=\\frac{1}{16} ~~~~ \\theta =\n\\frac{1}{3}\\Rightarrow L_{\\stackrel{x}{\\sim}}\\left(\\frac{1}{3}\\right)=\\frac{1}{81}\n\\]\nPortanto, \\(\\hat\\theta_{n}=\\frac{1}{2}\\) é a estimativa de máxima verossimilhança.",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Estimação via máxima verossimilhança (EMV)</span>"
    ]
  },
  {
    "objectID": "emv.html#exemplo",
    "href": "emv.html#exemplo",
    "title": "10  Estimação via máxima verossimilhança (EMV)",
    "section": "",
    "text": "10.1.1 Invariância dos EMVs\nTeorema. Se \\(\\hat\\theta_{(X_{1},\\dots,X_{n})}\\) for EMV para \\(\\theta\\), então \\(g(\\hat\\theta_{(X_{1},\\dots,X_{n})})\\) é o EMV para \\(g(\\theta)\\), ou seja, \\(g(\\hat\\theta_n)\\) é a estimativa de máxima verossimilhança para \\(g(\\theta)\\)\nMais um exemplo:\nSeja \\((X_{1},\\dots,X_{n})\\) a.a. de \\(X\\sim N(\\mu, \\sigma^2)\\) em que \\(\\theta = (\\mu, \\sigma^{2}) \\in \\Theta=\\mathbb{R}\\times \\mathbb{R}^{+}\\) Assuma que \\(\\stackrel{x}{\\sim} = (x_{1},\\dots,x_{n})\\) é a amostra observada. Lembrando que estaremos chamando \\(\\theta=(\\mu, \\sigma^{2})\\), mas estes são parâmetros genéricos. Poderíamos, por exemplo, chamá-los de \\(\\theta=(\\theta_{1},\\theta_{2})\\), o que pode facilitar a visualizar algumas derivadas.\na-) Encontre as estimativas de máxima verossimilhança para \\(\\theta = (\\mu, \\sigma^{2})\\):\nA Função de Verossimilhança é: \\[\n\\begin{aligned}\nL_{\\underset{\\sim}{x}}(\\theta)&\\stackrel{\\text{iid}}{=}\\prod^{n}_{i=1}f_\\theta(x_{i}) = \\prod^{n}_{i=1}\n\\left\\{ \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}}\\cdot \\exp\\left\\{\\frac{-1}{2}\\cdot\\frac{(x_{i}-\\mu)^{2}}{\\sigma^{2}} \\right\\} \\right\\}\\\\\n&= \\frac{1}{(2\\pi \\sigma^{2})^{\\frac{n}{2}}} \\cdot \\exp\\left\\{ \\frac{-1}{2\\sigma^{2}}\\sum\\limits^{n}_{i=1}(x_{i}-\\mu)^{2}\\right\\}\n\\end{aligned}\n\\]\nPodemos derivar para encontrar o máximo da FMV. Para isso, derivaremos e igualamos a zero primeiro em relação a \\(\\mu\\), então a \\(\\sigma^{2}\\) (podemos aplicar o logaritmo para facilitar as operações.)\n\\[\n\\begin{aligned}\n\\frac{\\partial\\ln(L_{\\underset{\\sim}{x}})}{\\partial \\mu} &= \\frac{1}{\\sigma^{2}}\\sum\\limits^{n}_{i=1}(x_{i}-\\mu) =0\n\\Rightarrow \\hat \\mu =\\frac{1}{n} \\sum\\limits^{n}_{i=1}x_{i} \\\\\n\\frac{\\partial \\ln(L_{\\underset{\\sim}{x}})}{\\partial \\sigma^{2}} &= - \\frac{n}{2\\sigma^{2}} + \\frac{1}{2\\sigma^{4}}\n\\sum\\limits^{n}_{i=1}(x_{i}-\\mu)^{2} =0 \\\\\n\\therefore \\\\\n\\mathrm{Estimativas~ MV} & =\n\\begin{cases}\n\\mu = \\bar{x} \\\\\n-\\frac{n}{2\\sigma^{2}} + \\frac{1}{2 \\sigma^{4}} \\sum\\limits^{n}_{i=1}(x_{i}-\\mu)^{2}=0 \\\\\n\\end{cases} \\\\\n&\\Leftrightarrow \\begin{cases}\n\\hat{\\mu}=\\bar{x} \\\\\n\\hat{\\sigma}^{2}= \\frac{1}{n}\\sum\\limits^{n}_{i=1}(x_{i}-\\bar{x})^{2}\n\\end{cases}\n\\end{aligned}\n\\]\nEstes são os pontos que maximizam a Função de Máxima Verossimilhança. (Provados em cálculo), ou seja, são as estimativas de máxima verossimilhança para \\(\\mu, \\sigma^{2}\\) respectivamente, e \\(\\hat{\\mu}(X_{1},\\dots,X_{n})=\\bar{X}, \\sigma^{2}(X_{1},\\dots,X_{n})=\\frac{1}{n}\\sum\\limits^{n}_{i=1}(X_{i}-\\bar{X})^{2}\\) são os estimadores de máxima verossimilhança.\nPela propriedade de invariância podemos encontrar o EMV para \\(g(\\theta)= \\frac{\\sqrt{\\mathrm{Var}_\\theta(X)}}{E_{\\theta(X)}}\\): \\[\n\\widehat{g(\\theta)} = \\frac{\\sqrt{\\frac{1}{n}\\sum\\limits^{n}_{i=1}(X_{i}-\\bar{X})}}{\\bar{X}}\n\\]\n\n\n\n\n\n\nObservação\n\n\n\nSeja \\((X_{1},\\dots,X_{n})\\) a.a. de \\(X\\sim N(\\mu, \\sigma^{2})\\). Então,\n\n\\(\\bar{X} \\underset{\\text{Exata!}}{\\sim}N\\left(\\mu, \\frac{\\sigma^{2}}{n}\\right)\\forall \\mu, \\sigma \\in \\mathbb{R} : \\sigma^{2}&gt;0 \\text{ e } n\\geq 1\\)\n\\(\\sum\\limits^{n}_{i=1} \\frac{(x_{1}-\\bar{X})^{2}}{\\sigma^{2}}\\underset{\\text{Exata!}}{\\sim}\\chi^{2}_{(n-1)}\\)\n\nem que \\(\\chi^{2}_{k}\\) representa a Distribuição Qui-Quadrado com \\(k\\) grau de liberdade, cuja função densidade de probabilidade é: \\[\nf(x) = \\frac{1}{\\Gamma(\\frac{k}{2})2^{\\frac{k}{2}}} \\cdot x^{\\frac{k}{2}-1} \\cdot \\exp\\left\\{\\frac{-x}{2}\\right\\} \\cdot \\mathbb{1}_{(0, \\infty)}(x).\n\\]\nPara qualquer outra distribuição, existe um resultado aproximado pelo Teorema do Limite Central.",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Estimação via máxima verossimilhança (EMV)</span>"
    ]
  },
  {
    "objectID": "qui-quadrado.html",
    "href": "qui-quadrado.html",
    "title": "11  Distribuição Qui-Quadrado",
    "section": "",
    "text": "Se \\(Z\\sim N(0,1)\\), então \\(Z^{2}\\sim \\chi^2\\), como provado por transformação de variáveis aleatórias\nSe \\(W\\sim \\chi^{2}_{k}\\), então sua função densidade de probabilidade é dada por \\[\n\\begin{cases}\n\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)\\cdot 2 ^{\\frac{k}{2}}}\\cdot w^{\\frac{k}{2} -1} \\cdot e^{-\\frac{1}{2}w}, ~ ~~ w &gt; 0 \\\\\n0, cc\n\\end{cases}\n\\] Sendo assim, \\[\n\\begin{cases}\nE(W) = k \\\\\n\\mathrm{Var}(W) = 2k\n\\end{cases}\n\\] Se \\(Z_{1},Z_{2,}\\dots,Z_{N}\\stackrel{\\text{iid}}{\\sim} N(0,1)\\), então \\[Z_{1}^{2}+\\dots+Z_{n}^{2}\\sim \\chi^{2}_{n}\\] Prova por função característica\nSe \\(X_{1},\\dots,X_{n} \\stackrel{\\text{iid}}{\\sim}N(\\mu,\\sigma^{2}),\\) então \\[\n\\frac{(X_{1}-\\mu)^{2}+\\dots+(X_{n}-\\mu)^{2}}{\\sigma^{2}} \\sim \\chi^{2}_{n}\n\\] Se \\(X_{1},\\dots,X_{n} \\stackrel{\\text{iid}}{\\sim}N(\\mu,\\sigma^{2}),\\) então \\[\n\\frac{(X_{1}-\\bar{X})^{2}+\\dots+(X_{n}-\\bar{X})^{2}}{\\sigma^{2}} \\sim \\chi^{2}_{n-1}\n\\]\nAdemais, se \\(Y\\sim \\chi^2_{\\nu}\\), então \\[\n\\frac{Y-\\nu}{\\sqrt{2\\nu}} \\stackrel{a}{\\approx} N(0,1)\n\\] para \\(\\nu &gt; 30\\)",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Distribuição Qui-Quadrado</span>"
    ]
  },
  {
    "objectID": "tlc.html",
    "href": "tlc.html",
    "title": "12  Teorema do Limite Central",
    "section": "",
    "text": "12.1 Exemplo\nTambém conhecido como Teorema Central do Limite, é fundamental para a teoria da probabilidade e a estatística.\nSeja \\(X_{1},\\dots,X_{n}\\) a.a de \\(X\\sim f_{\\theta},\\theta \\in \\Theta : E_{\\theta}(X^{2}) &lt; \\infty\\), então: \\[\n\\bar{X} \\stackrel{\\text{Aproximadamente}}{\\sim }N\\left(E_\\theta(X), \\frac{Var_{\\theta}{X}}{n}\\right)\n\\]\nFormalmente, temos o enunciado do Teorema do Limite Central: \\[\n\\frac{\\sqrt{n}(\\bar{X}-E_\\theta(x))}{\\sqrt{\\mathrm{Var}_\\theta(X)}} \\underset{n\\rightarrow \\infty}{\\stackrel{\\text{Distribuição}}{\\rightarrow}} N\\sim (0,1) \\forall \\theta \\in \\Theta\n\\]\nSe \\(X\\sim N(\\mu, \\sigma)\\), então a distribuição é exata.\nAdemais, seja \\(g\\) uma função contínua e diferenciável tal que \\(g'(\\theta)\\neq0\\). Então, \\[\ng(\\bar{X}) \\stackrel{\\text{Aproximadamente}}{\\sim}N\\left(g(E_\\theta(X)), \\frac{g'(E_\\theta(X))^{2}\\mathrm{Var}_\\theta(X)}{n}\\right)\n\\]\nSeja \\((X_{1},\\dots,X_{n})\\) a.a de \\(X\\sim Ber(\\theta), \\theta \\in (0,1)\\). Já vimos que EMV \\(p/ \\theta\\) é \\[\n\\bar{\\theta}(X_{1},\\dots,X_{n}) = \\bar{X}\n\\]\ne o EMV p/ \\(g(\\theta) = \\mathrm{Var}_{\\theta(x)}= \\theta(1-\\theta)\\) é: \\[\n\\widehat{g(\\theta)} = \\bar{X}(1-\\bar{X}).\n\\]\nAgora, \\[\n\\begin{aligned}\n\\bar{X} &\\stackrel{\\text{approx.}}{\\sim} N\\left(\\theta, \\frac{\\theta(1-\\theta)}{n}\\right)\\\\\ng(\\bar{X}) = \\bar{X}(1-\\bar{X}) & \\stackrel{\\text{approx}}{\\sim } N\\left(\\theta(1-\\theta),\n\\frac{[g'(\\theta)]^{2} \\theta(1-\\theta)}{n}\\right) \\\\\n&~~~\\Rightarrow~N\\left(\\theta(1-\\theta), \\frac{(1-2\\theta)^{2}\\theta(1-\\theta)}{n}\\right)\n\\end{aligned}\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Teorema do Limite Central</span>"
    ]
  },
  {
    "objectID": "estimador-intervalar.html",
    "href": "estimador-intervalar.html",
    "title": "13  Intervalo de Confiança ou Estimador Intervalar",
    "section": "",
    "text": "13.1 IC sob normalidade para \\(\\mu\\) com \\(\\sigma^2\\) conhecido\nSeja \\((X_1,\\ldots, X_n)\\) a.a. de \\(X\\sim f_\\theta, \\theta \\in \\Theta \\subseteq \\mathbb{R}\\).\nUm intervalo de confiança (IC) com coeficiente de confiança \\(\\gamma\\) é um intervalo aleatório que satisfaz: \\[\nP_{\\theta} (I_{1}(\\pmb{X}_{n}) \\leq \\theta \\leq I_{2} (\\pmb{X}_{n})) = \\gamma \\forall \\theta \\in \\Theta\n\\]\nem que \\(\\pmb{X}_{n} = (X_{1}\\ldots,X_{n})\\)\nEm uma distribuição normal \\((\\theta, \\sigma^2)\\), por exemplo, conseguimos de forma genérica para qualquer \\(\\gamma \\in [0,1]\\) encontrar pela tabela um valor de \\(c_{\\gamma}\\) que satisfaça \\(P(-c_{\\gamma} \\leq Z \\leq c_{\\gamma})\\). Assim, \\[\n-c_{\\gamma} \\leq Z \\leq c_{\\gamma} \\Leftrightarrow -c_{\\gamma} \\leq \\frac{\\sqrt{n}(\\bar{X}-\\theta)}{\\sqrt{\\sigma^{2}}}\n\\leq c_{\\gamma} \\Leftrightarrow \\bar{X} - c_{\\gamma} \\frac{\\sqrt{\\sigma^2}}{\\sqrt{n}} \\leq \\theta \\leq \\bar{X} + c_{\\gamma}\n\\frac{\\sqrt{\\sigma^2}}{\\sqrt{n}}\n\\]\nPortanto, \\[\nP_{\\theta}\\left(\\bar{X} - c_{\\gamma} \\frac{\\sqrt{\\sigma^2}}{\\sqrt{n}} \\leq \\theta \\leq \\bar{X} + c_{\\gamma}\n\\frac{\\sqrt{\\sigma^2}}{\\sqrt{n}}\\right) = \\gamma \\forall \\theta \\in \\Theta\n\\]\nDessa forma, \\(\\bar{X} - c_{\\gamma} \\frac{\\sqrt{\\sigma^2}}{\\sqrt{n}} \\leq \\theta \\leq \\bar{X} + c_{\\gamma}\n\\frac{\\sqrt{\\sigma^2}}{\\sqrt{n}}\\) é um intervalo de confiança cujo coeficiente de confiança é \\(\\gamma\\).",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intervalo de Confiança ou Estimador Intervalar</span>"
    ]
  },
  {
    "objectID": "estimador-intervalar.html#ic-sob-normalidade-para-mu-com-sigma2-conhecido",
    "href": "estimador-intervalar.html#ic-sob-normalidade-para-mu-com-sigma2-conhecido",
    "title": "13  Intervalo de Confiança ou Estimador Intervalar",
    "section": "",
    "text": "13.1.1 Exemplo\nConsidere que a amostra observada foi \\(1, 2.2, 3, 3.5\\) de uma distribuição \\(N(\\theta, 3)\\).\nEncontre o IC observado com coeficiente de confiança de \\(99\\%\\).\nPrimeiro encontramos a média: \\(\\bar{x} = 2.42\\).\nDessa forma, \\(c_{99\\%} = 2.58\\) e nosso intervalo de confiança é \\[\n\\left[2.42 - 2.58 \\frac{\\sqrt{3}}{\\sqrt{4}}, 2.42 + 2.58\\frac{\\sqrt{3}}{\\sqrt{4}}\\right] = [0.18, 4.65]\n\\]\nDessa forma, se repetirmos o experimento \\(N\\) vezes, esperamos que \\(\\gamma = 99\\%\\) dos ICs observados contenham a quantidade de interesse.\n\n13.1.1.1 Notação\n\\[\n\\mathrm{IC}(\\theta, \\gamma) = [I_{1} (\\pmb{X}_{n}), I_{2} (\\pmb{X}_{n})]\n\\] Denotará o intervalo de confiança teórico\n\\[\n\\mathrm{IC}_{\\mathrm{Obs}}(\\theta, \\gamma) = [I_{1} (\\pmb{x}_{n}), I_{2} (\\pmb{x}_{n})]\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intervalo de Confiança ou Estimador Intervalar</span>"
    ]
  },
  {
    "objectID": "estimador-intervalar.html#ic-sob-normalidade-para-mu-quando-sigma2-é-desconhecido",
    "href": "estimador-intervalar.html#ic-sob-normalidade-para-mu-quando-sigma2-é-desconhecido",
    "title": "13  Intervalo de Confiança ou Estimador Intervalar",
    "section": "13.2 IC sob normalidade para \\(\\mu\\) quando \\(\\sigma^2\\) é desconhecido",
    "text": "13.2 IC sob normalidade para \\(\\mu\\) quando \\(\\sigma^2\\) é desconhecido\nSeja uma a.a de \\(X\\sim N(\\mu, \\sigma^2)\\) em que \\(\\mu, \\sigma^2\\) são desconhecidos. Então, o IC para \\(\\mu\\) com coeficiente de confiança \\(\\gamma\\) é dado por\n\\[\n\\mathrm{IC}(\\mu,\\gamma) = \\left[\\bar{X} - t_{\\gamma, n-1} \\sqrt{\\frac{s^2 (\\pmb{X}_{n})}{n}}, \\bar{X} +t_{\\gamma, n-1}\n\\sqrt{\\frac{s^2 (\\pmb{X}_{n})}{n}}\\right]\n\\]\nEm que \\(s^2 (\\pmb{x}_{n}) = \\frac{1}{n-1}\\sum^{k}_{i=1}(X_i - \\bar{x})^2\\) e \\(t_{y,(n-1)}\\) deve ser calculado da tabela \\(P(-t_{\\gamma, n-1} \\leq T_{n-1} \\leq t_{\\gamma, n-1}) = \\gamma\\), \\(T_{n-1} \\sim \\mathrm{t-Student}(n-1)\\)\nSe a variância for desconhecida, substituímos \\(\\sigma^2\\) pelo estimador \\[\ns^2 = \\frac{1}{n-1}\\sum\\limits^{n}_{i=1} (X_{i} - \\bar{X})^2\n\\] e o valor de \\(c_{\\gamma}\\) obtido de uma t-Student com \\(n-1\\) graus de liberdade.\nJustificativa:\nSe uma a.a. de \\(X\\sim N(\\mu,\\sigma^2)\\), \\(\\theta = (\\mu, \\sigma^2) \\in \\mathbb{R} \\times\\mathbb{R_{+}}\\).\n\n\\(\\bar{X} \\sim N (\\mu, \\frac{\\sigma^2}{n})\\)\n\\(\\sqrt{n}\\frac{\\bar{X} - \\mu}{\\sqrt{\\sigma^2}}\\sim N(0,1)\\)\n\\(\\sum\\limits^{n}_{i=1}\\frac{(X_{i} - \\bar{X})^2}{\\sqrt{\\sigma^2}} \\sim \\chi^2_{n-1}\\)\n\\(\\sqrt{n}\\frac{\\bar{X} - \\mu}{\\sqrt{s^2}} = \\sqrt{n}\\frac{\\bar{X} - \\mu}{\\sqrt{s^2}} \\cdot\n\\frac{\\sqrt{\\sigma^2}}{\\sqrt{\\sigma^2}}  = \\frac{\\sqrt{n}\\frac{\\bar{X} - \\mu}{\\sqrt{\\sigma^2}}}\n{\\sqrt{\\frac{\\sum(X_i - \\bar{X})^2}{\\sigma^2} \\cdot \\frac{1}{n-1}}}\\)\nSe \\(Z \\sim N(0,1)\\) e \\(W \\sim \\chi^{2}_{k}\\), então \\(t = \\frac{Z}{\\sqrt{\\frac{W}{k}}} \\sim t_{k}\\)\n\\(\\sqrt{n} \\frac{(\\bar{X} - \\mu)}{\\sqrt{s^2}} \\sim t_{(n-1)}\\)\n\n\n13.2.1 Exercício\nConsidere uma a.a de \\(X\\sim N(\\mu, \\sigma^2)\\), ambos parâmetros desconhecidos, em que \\(X\\) é o retorno de um ativo específico. Considere que a seguinte amostra foi observada: \\[\n(-1.3\\%, 0.4\\%, -1.7\\%, 3.2\\%, 0.7\\%, -1.6\\%, 1.0\\%, 1.5\\%, 1.2\\%, -0.6\\%)\n\\]\nEncontre o IC para \\(\\mu\\), com coeficiente de confiança \\(\\gamma = 99\\%\\)\nTemos que \\(s^2(\\pmb{x}_{n}) = 2.48\\) e \\(\\bar{x} = 0.28\\). Da tabela, \\(t_{99\\%, 9} = 3.25\\)\nSendo assim,\n\\[\n\\mathrm{IC}_{\\mathrm{Obs}}(\\mu,99\\%) = \\left[0.28 - 3.25 \\sqrt{\\frac{2.48}{10}}, 0.28 + 3.25\\sqrt{\\frac{2.48}{10}}\\right] = [-1.34, 1.9]\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intervalo de Confiança ou Estimador Intervalar</span>"
    ]
  },
  {
    "objectID": "estimador-intervalar.html#ic-sob-normalidade-para-sigma2-com-mu-desconhecido",
    "href": "estimador-intervalar.html#ic-sob-normalidade-para-sigma2-com-mu-desconhecido",
    "title": "13  Intervalo de Confiança ou Estimador Intervalar",
    "section": "13.3 IC sob normalidade para \\(\\sigma^2\\) com \\(\\mu\\) desconhecido",
    "text": "13.3 IC sob normalidade para \\(\\sigma^2\\) com \\(\\mu\\) desconhecido\nSeja uma a.a. de \\(X \\sim N(\\mu, \\sigma^2)\\) em que \\(\\mu, \\sigma^2\\) são desconhecidos.\nO intervalo de confiança para \\(\\sigma^2\\) com coeficiente de confiança \\(\\gamma\\) é dado por:\n\\[\n\\begin{aligned}\n\\mathrm{IC} (\\sigma^2, \\gamma) = \\left [\\frac{(n-1)s^2(\\pmb{X}_{n})}{q^{(2)}_{\\gamma, n-1}},\n\\frac{(n-1)s^2(\\pmb{X}_{n})}{q^{(1)}_{\\gamma, n-1}}\\right] \\\\\n\\mathrm{IC}_{\\mathrm{Obs}} (\\sigma^2, \\gamma) = \\left [\\frac{(n-1)s^2(\\pmb{x}_{n})}{q^{(2)}_{\\gamma, n-1}},\n\\frac{(n-1)s^2(\\pmb{x}_{n})}{q^{(1)}_{\\gamma, n-1}}\\right]\n\\end{aligned}\n\\]\nem que \\(s^2(\\pmb{X}_{n}) = \\frac{1}{n-1} \\sum^{n}_{i=1} (X_i - \\bar{X})^2\\) e \\(s^2(\\pmb{x}_{n}) =\n\\frac{1}{n-1} \\sum^{n}_{i=1} (x_i - \\bar{x})^2\\) E \\(q^{(1)}_{\\gamma, n-1}, q^{(2)}_{\\gamma, n-1}\\) são obtidos calculando \\(P(q^{(1)}_{\\gamma, n-1} \\leq W \\leq q^{(2)}_{\\gamma, n-1}) = \\gamma\\) no qual \\(W\\sim \\chi^{2}_{n-1}\\) e \\(P(\\chi^2_{n-1} \\leq q^{(1)}_{\\gamma, n-1}) = \\frac{1-\\gamma}{2} = P( \\chi^2_{n-1} \\geq q^{(2)}_{\\gamma,n-1})\\)\nDemonstração:\nVamos construir um IC para a variância.\nNote que \\[\\sum\\limits^{n}_{i=1}\\frac{(X_{i} - \\bar{X})^2}{\\sqrt{\\sigma^2}} \\sim \\chi^2_{n-1}\\]\nNote ainda que \\(W \\sim \\chi^{2}_{n-1}\\), ou seja, \\(P(q^{(1)}_{\\gamma, n-1} \\leq W \\leq q^{(2)}_{\\gamma, n-1}) = \\gamma\\). Assim, \\(W = \\frac{(n-1)s^2}{\\sigma^2}\\).\nDessa forma,\n\\[\n\\frac{1}{q^{(2)}_{\\gamma, n-1}} \\leq \\frac{\\sigma^2}{(n-1) s^2} \\leq \\frac{1}{q^{(1)}_{\\gamma, n-1}} \\Leftrightarrow\n\\frac{(n-1)s^2}{q^{(2)}_{\\gamma, n-1}} \\leq \\sigma^2 \\leq \\frac{(n-1)s^2}{q^{(1)}_{\\gamma, n-1}}\n\\]\nem que \\(s^2 = \\frac{1}{n-1} \\sum^{n}_{i=1} (X_i - \\bar{X})^2\\)\nPortanto, \\[ P_{\\theta}\\left (\\frac{(n-1)s^2}{q^{(2)}_{\\gamma, n-1}} \\leq \\sigma^2 \\leq \\frac{(n-1)s^2}{q^{(1)}_{\\gamma, n-1}}\\right)\n= \\gamma \\forall \\theta \\in \\Theta\\]\nExercício:\nConsidere uma a.a de \\(X\\sim N(0, \\theta), \\theta &gt; 0\\) em que \\(X\\) é o retorno de um ativo específico. Considere que a seguinte amostra foi observada: \\[\n(-1.3\\%, 0.4\\%, -1.7\\%, 3.2\\%, 0.7\\%, -1.6\\%, 1.0\\%, 1.5\\%, 1.2\\%, -0.6\\%)\n\\]\nConstrua um intervalo de confiança para a variância (populacional) \\(\\theta\\) com coeficiente de confiança \\(\\gamma = 95\\%\\).\nO IC para a variância é \\[\n\\mathrm{IC}(\\theta, \\gamma) = \\left[\\frac{(n-1)s^2 (\\pmb{x}_{n})}{q^2_{\\gamma,n-1}},\\frac{(n-1)s^2 (\\pmb{x}_{n})}{q^1_{\\gamma,n-1}}\\right]\n\\]\nem que \\(s^2 (\\pmb{x}_{n}) = \\frac{1}{n-1}\\sum^{k}_{i=1}(X_i - \\bar{x})^2\\) e \\(q^{1}_{\\gamma, n-1}, q^{2}_{\\gamma, n-1}\\) satisfazem as fronteiras que delimitam uma área de \\(\\gamma\\) em torno da média. Nesse caso, como \\(n = 10\\), \\(q^{1}_{9} = 2.7, q^{2}_{9} = 19.023\\). Dessa forma, \\(\\bar{x} = 0.28, \\sum_{i}x_{i}^2=2.308 \\Rightarrow s^2(\\pmb{x}_{n}) = \\frac{10}{9}(2.308 - 0.28^2) = 2.48\\)\nSendo assim \\[\n\\mathrm{IC}_{\\mathrm{Obs}}(\\theta, 95\\%) = \\left[\\frac{9 \\cdot 2.48}{19.023}, \\frac{9 \\cdot 2.48}{2.7}\\right] = [1.17, 8.27]\n\\]\nPodemos concluir que, com uma confiança de \\(95\\%\\), a variância está nesse intervalo.",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intervalo de Confiança ou Estimador Intervalar</span>"
    ]
  },
  {
    "objectID": "estimador-intervalar.html#intervalo-de-confiança-para-a-proporção",
    "href": "estimador-intervalar.html#intervalo-de-confiança-para-a-proporção",
    "title": "13  Intervalo de Confiança ou Estimador Intervalar",
    "section": "13.4 Intervalo de Confiança para a proporção",
    "text": "13.4 Intervalo de Confiança para a proporção\nSeja \\((\\pmb{x}_{n})\\)uma amostra aleatória de \\(X\\sim Ber(\\theta)\\) em que \\(\\theta \\in [0,1]\\). Sabemos que, pelo Teorema do Limite Central\n\\[\n\\bar{X} \\stackrel{a}{\\approx} N\\left( \\theta, \\frac{\\theta (1-\\theta)}{n} \\right)\n\\]\nDessa forma, \\[\n\\frac{\\sqrt{ n }(\\bar{X}-\\theta)}{\\sqrt{ \\theta(1-\\theta) }} \\stackrel{a}{\\approx} N(0,1)\n\\]\nAlém disso, pelo Teorema de Slutsky \\[\n\\frac{\\sqrt{ n }(\\bar{X}-\\theta)}{\\sqrt{ \\bar{X}(1-\\bar{X}) }} \\stackrel{a}{\\approx} N(0,1)\n\\]\nObserve que, se \\(Z \\sim N(0,1)\\) então \\(-c_{\\gamma}\\leq Z \\leq c_{\\gamma} \\Leftrightarrow -c_{\\gamma }\n\\leq \\frac{\\sqrt{ n }(\\bar{X}-\\theta)}{\\sqrt{ \\bar{X}(1-\\bar{X}) }}\\leq c_{\\gamma}\\) \\[\n\\Leftrightarrow \\bar{X} - c_{\\gamma} \\sqrt{ \\frac{\\bar{X} (1-\\bar{X})}{n} } \\leq \\theta \\leq \\bar{X} + c_{\\gamma}\n\\sqrt{\\frac{\\bar{X} (1-\\bar{X})}{n}}\n\\]\nEntão, seja \\(c_{\\gamma}\\) tal que \\[\n\\begin{aligned}\n&P (-c_{\\gamma}\\leq N(0,1)\\leq c_{\\gamma }) = \\gamma \\\\\n\\Rightarrow &P_{\\theta}\\left(\\bar{X} - c_{\\gamma} \\sqrt{ \\frac{\\bar{X} (1-\\bar{X})}{n} } \\leq \\theta \\leq\n\\bar{X} + c_{\\gamma} \\sqrt{ \\frac{\\bar{X} (1-\\bar{X})}{n} }\\right) \\approx \\gamma~ \\forall \\theta \\in \\Theta = [0,1]\n\\end{aligned}\n\\]\nque melhora conforme \\(n \\rightarrow \\infty\\)\nLogo, um IC aproximado com coeficiente de confiança \\(\\gamma\\) é dado por \\[\nIC(\\theta, \\gamma) = \\left[\\bar{X} - c_{\\gamma} \\sqrt{ \\frac{\\bar{X} (1-\\bar{X})}{n} }, \\bar{X} + c_{\\gamma}\n\\sqrt{ \\frac{\\bar{X} (1-\\bar{X})}{n} }\\right] \\cap \\Theta\n\\]\n\n13.4.1 Exemplo\nSeja \\((\\pmb x_{n })\\) uma amostra aleatória de \\(X\\sim \\mathrm{Ber} (\\theta)\\) em que \\(\\theta \\in [0,1]\\). \\[\nX(w) = \\begin{cases}\n1, \\text{ se w disser que vota no candidato} \\\\ \\\\\n0, c.c\n\\end{cases}\n\\] A amostra observada foi \\((0,0,0,1,0,0,0,1)\\). Encontre o IC para a proporção de intenção de votos no candidato considerando \\(\\gamma = 99\\%\\)\n\\(\\bar{x} = \\frac{1}{4 }, \\bar{x}(1-\\bar{x}) = \\frac{3}{16}, n = 8, c_{\\gamma} = 2.58\\)\n\\[\n\\begin{aligned}\n\\mathrm{IC}_{\\mathrm{Obs}}(\\theta, 99\\%) &= \\left[0.25- 2.58 \\frac{\\sqrt{3}}{4\\sqrt{8}}, 0.25 - 2.58\n\\frac{\\sqrt{3}}{4 \\sqrt{8}}\\right] \\cap [0,1] \\\\\n&= [0.25 - 0.39, 0.25 + 0.39] \\cap [0,1]  \\\\\n&= [0,0.64]\n\\end{aligned}\n\\]\n\n\n13.4.2 Intervalos conservadores e otimistas\nUm intervalo de confiança de proporção é dito ser conservador quando o calculamos tomando \\(\\theta\\) Na variância (\\(\\theta(1-\\theta)\\)) como o valor mais alto possível. No exemplo Bernoulli, o valor máximo para \\(\\theta\\) (derivando \\(\\theta(1-\\theta)\\) e igualando a 0, \\(1-2 \\theta = 0\\)) é \\(\\theta=0.5\\). Dessa forma, o IC conservador é calculado usando \\(\\theta=0.5\\).\nPor sua vez, um IC otimista é calculado usando o valor de \\(\\theta\\) obtido através do EMV para \\(\\theta\\), no caso Bernoulli, usaríamos essa variância como \\(\\bar{X}(1-\\bar{X})\\)",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intervalo de Confiança ou Estimador Intervalar</span>"
    ]
  },
  {
    "objectID": "estimador-intervalar.html#como-interpretar-intervalos-de-confiança",
    "href": "estimador-intervalar.html#como-interpretar-intervalos-de-confiança",
    "title": "13  Intervalo de Confiança ou Estimador Intervalar",
    "section": "13.5 Como interpretar intervalos de confiança",
    "text": "13.5 Como interpretar intervalos de confiança\nImportante:\nNa estatística clássica (frequentista), devemos interpretar um intervalo de confiança \\([a,b]\\) com \\(\\gamma=0.95\\) da seguinte forma:\n“Com \\(95\\%\\) de confiança, o intervalo \\([a,b]\\) conterá o valor da quantidade de interesse”.\nIsso é importante para diferenciar a interpretação frequentista (Theta do espaço paramétrico) da Bayesiana (Theta como variável aleatória). Dessa forma, estaria incorreto na estatística clássica dizer que\n“O intervalo \\([a,b]\\) conterá a quantidade de interesse com probabilidade \\(95\\%\\)” ou “O intervalo \\([a,b]\\) conterá a quantidade de interesse \\(95\\%\\) das vezes”",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intervalo de Confiança ou Estimador Intervalar</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html",
    "href": "teste-hipotese.html",
    "title": "14  Teste de Hipótese simples",
    "section": "",
    "text": "14.1 Etapas de um teste de hipótese\nUm dos principais objetivos da estática é testar hipóteses. Veja algumas dessas hipóteses potenciais:",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#etapas-de-um-teste-de-hipótese",
    "href": "teste-hipotese.html#etapas-de-um-teste-de-hipótese",
    "title": "14  Teste de Hipótese simples",
    "section": "",
    "text": "Formular as hipóteses de interesse;\n\nNa estatística clássica, pela abordagem Fisheriana (uma hipótese) ou Neyman-Pearson (mais de uma hipótese).\n\nObservar dados experimentais do estudo relacionado ao problema;\nElaborar uma conclusão utilizando um procedimento estatístico.",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#exemplo-da-hipótese-5.",
    "href": "teste-hipotese.html#exemplo-da-hipótese-5.",
    "title": "14  Teste de Hipótese simples",
    "section": "14.2 Exemplo da hipótese 5.",
    "text": "14.2 Exemplo da hipótese 5.\nConsidere uma pessoa que está sendo acusada de ter cometido um crime.\nAs duas hipóteses envolvidas aqui são (abordagem de Neyman-Pearson):\n\n“O suspeito não é culpado” -&gt; \\(h_{0}\\) Hipótese Nula ou de não-efeito;\n“O suspeito é culpado” -&gt; \\(h_{1}\\) Hipótese alternativa ou hipótese que contém o efeito.\n\n\nApós coletar as evidências, dizemos que, se houver evidências de que o suspeito cometeu o crime, a pessoa é culpada.\nSe não, concluímos que não é culpado.\n\nContudo, devemos nos atentar aos erros de decisão\n\n14.2.1 Erros de decisão\n\\[\n\\begin{array}{c|cc}\n\\ & H_{0} & H_{1} \\\\\n\\hline\n\\text{Decisão}  & \\text{Não cometeu o crime}  & \\text{Cometeu o crime}\\\\\n\\hline\n\\text{Inocente} & \\text{Acerto} & \\text{Erro Tipo II}\\\\\n\\text{Culpado} & \\text{Erro Tipo I} & \\text{Acerto}\\\\\n\\hline\n\\end{array}\n\\] - Erro Tipo I: Decidir que o acusado é culpado quando na verdade é inocente (Rejeitar \\(H_{0}\\)). - Erro Tipo II: Decidir que o acusado é inocente quando na verdade é culpado (Rejeitar \\(H_{1}\\)).",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#exemplo-da-hipótese-1.",
    "href": "teste-hipotese.html#exemplo-da-hipótese-1.",
    "title": "14  Teste de Hipótese simples",
    "section": "14.3 Exemplo da hipótese 1.",
    "text": "14.3 Exemplo da hipótese 1.\nEstamos interessados em verificar se uma moeda é honesta. Executaremos \\(n\\) experimentos de Bernoulli e verificaremos se a face voltada para cima após o lançamento é cara. Dessa forma, sendo \\(X\\) o resultado dum lançamento, teremos a a.a \\((\\pmb{X}_{n})\\) de \\(X\\sim \\mathrm{Ber}(\\theta), \\theta \\in \\Theta = [0,1]\\). Suspeitamos que a moeda é honesta ou que \\(\\theta = 0.9\\).\nNossas hipóteses são\n\n\\(H_{0}\\) -&gt; \\(\\theta = 0.5\\)\n\\(H_{1}\\) -&gt; \\(\\theta = 0.9\\)\n\n\n14.3.1 Erros Tipo I e II\nNote que \\(\\bar{X}\\) é um estimador para \\(\\theta\\) e \\(\\bar{x}\\) é uma estimativa. - Se \\(\\bar{x}&gt;0.7\\), rejeitaremos a hipótese nula \\(h_{0}\\), a moeda não seria honesta e haveria um viés agindo sobre seus lançamentos. - Se \\(\\bar{x}\\leq 0.7\\), concluiremos que a moeda é honesta. \\[\n\\begin{array}{c|cc}\n& H_{0} & H_{1} \\\\\n\\hline\n\\text{Decisão}  & \\text{Honesta} & \\text{Viesada}\\\\\n\\hline\n\\bar{x} &lt; 0.7 & \\text{Acerto} & \\text{Erro Tipo II}\\\\\n\\bar{x} \\geq 0.7& \\text{Erro Tipo I} & \\text{Acerto}\\\\\n\\hline\n\\end{array}\n\\]\n\nErro Tipo I: Rejeitar que a moeda é honesta (rejeitar \\(h_{0}\\)) quando na verdade é.\nErro Tipo II: Rejeitar que a moeda é enviesada (rejeitar \\(h_{1}\\)) quando na verdade é.\n\nCalculando a probabilidade dos erros \\[\n\\begin{aligned}\nP(\\text{Erro Tipo I}) &= P(\\text{Probabilidade de rejeitar $h_{0}$}|\\text{$h_{0}$ é verdadeiro}) \\\\\n\\text{Incorreto na Estatística Clássica} &= P(\\bar{X} &gt; 0.7 | \\theta = 0.5) \\\\\n\\text{Correto na Estatística Clássica} & = P_{0.5}(\\bar{X} &gt; 0.7)\n\\end{aligned}\n\\]\nNa segunda notação, correta na estatística frequentista, \\(P\\) está sob a hipótese nula \\(h_{0}\\) verdadeira. Dessa forma \\[\nP(\\text{Erro Tipo II}) = P_{0.9}(\\bar{X}\\leq 0.7)\n\\]\nCalcule as probabilidades dos erros considerando \\(n=10\\) e a aproximação pela distribuição normal.\n\n14.3.1.1 Calculando Exato e pela aproximação do Teorema do Limite Central\nSabemos que \\(\\sum ^{n}_{i=1}X_{i}\\sim \\mathrm{Bin}(n, \\theta)\\), logo \\[\n\\begin{aligned}\n\\alpha &= P(\\text{Erro Tipo I}) \\stackrel{\\text{Sob $h_{0}$}}{=} P_{0.5}\\left( \\frac{1}{n} \\sum^n_{i=1}X_{i}&gt; 0.7\\right)\n= P_{0.5}\\left( \\sum^{10}_{i=1}X_{i} &gt; 7 \\right) \\\\\n&= P_{0.5}\\left( \\sum^{10}_{i=1} X_{i} \\geq 8 \\right) \\\\\n&= \\binom{10}{8} 0.5^{8} \\cdot 0.5^{2} + \\binom{10}{9} 0.5^{9} \\cdot 0.5^{1} + \\binom{10}{10} 0.5^{10} \\approx 0.05469 \\\\\n\\alpha & = P_{0.5}\\left( \\sqrt{ \\frac{n}{0.25}}(\\bar{X}-0.5) &gt;\\sqrt{ \\frac{n}{0.25}}(0.7-0.5) \\right) \\\\\n& \\approx P(N(0,1) &gt; 1.26) \\approx 0.103\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\beta &= P(\\text{Erro Tipo II}) \\stackrel{\\text{Sob $h_{1}$}}{=} P_{0.9}\\left( \\frac{1}{n} \\sum^n_{i=1}X_{i}\\leq 0.7\\right)\n= P_{0.9}\\left( \\sum^{10}_{i=1}X_{i} \\leq 7 \\right) \\\\\n&= 1-P_{0.9}\\left( \\sum^{10}_{i=1} X_{i} \\geq 8 \\right) \\\\\n&= 1-\\left(\\binom{10}{8} 0.9^{8} \\cdot 0.9^{2} + \\binom{10}{9} 0.9^{9} \\cdot 0.9^{1} + \\binom{10}{10} 0.9^{10}\\right)\n\\approx 0.0702 \\\\\n\\alpha & = P_{0.9}\\left( \\sqrt{ \\frac{n}{0.09}}(\\bar{X}-0.9) \\leq \\sqrt{ \\frac{n}{0.09}}(0.7-0.9) \\right) \\\\\n& \\approx P(N(0,1) \\leq -2.1) \\approx 0.018\n\\end{aligned}\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#poder-do-teste",
    "href": "teste-hipotese.html#poder-do-teste",
    "title": "14  Teste de Hipótese simples",
    "section": "14.4 Poder do teste",
    "text": "14.4 Poder do teste\nChamamos de poder do teste a probabilidade de rejeitar \\(h_{0}\\) quando este é falso.\nNo exemplo anterior, \\[\n\\pi = P_{0.9}(\\bar{X} &gt; 0.7) = 1-P_{0.9}(\\bar{X}\\leq 0.7) = 1 - \\beta = 92.92\\%\n\\]\nConsidere nesse exemplo uma a.a do lançamento de quatro moedas: \\((\\pmb{x}_{10}) = (1, 0, 1, 0,0,1,1,0,0,0)\\). Como \\(\\bar{x}=0.4 \\leq 0.7\\), não rejeitamos a hipótese nula \\(h_{0}\\).\nEm uma outra amostra, \\((\\pmb{x}_{10}) = (0,0,1,1,1,1,1,1,1,1)\\). Como \\(\\bar{x}=0.8 &gt; 0.7\\), rejeitamos a hipótese nula \\(h_{0}\\).",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#um-exemplo-mais-amplo-diferença",
    "href": "teste-hipotese.html#um-exemplo-mais-amplo-diferença",
    "title": "14  Teste de Hipótese simples",
    "section": "14.5 Um exemplo mais amplo (Diferença)",
    "text": "14.5 Um exemplo mais amplo (Diferença)\nSeja \\((X_{n})\\) uma a.a. de \\(X\\sim\\mathrm{Ber}(\\theta)\\), em que \\(\\theta \\in (0,1)\\). Considere as hipóteses: \\[\n\\begin{cases}\nH_{0}: \\theta =0.5 \\\\\nH_{1}: \\theta \\neq 0.5\n\\end{cases}\n\\]\nDecisões elaboradas:\n\nSe \\(\\bar{x}&lt;0.3\\) ou \\(\\bar{x} &gt; 0.7\\), rejeitamos \\(H_{0}\\)\nCaso contrário, não rejeitamos \\(H_0\\)\n\nRelembrando: \\(\\alpha\\) = Probabilidade do Erro Tipo I (Rejeitar um \\(H_0\\) verdadeiro). \\(\\beta\\) Probabilidade do Erro Tipo II (Rejeitar um \\(H1\\) verdadeiro). \\(\\pi =\\) Poder do Teste. Lembre-se que \\(\\sum^n_{i=1}X_{i}\\sim \\mathrm{Bin}(n,\\theta)\\). Tome \\(n =10\\).\n\\[\n\\begin{aligned}\n\\alpha &= P_{\\theta=0.5}(\\bar{X} &lt; 0.3 \\text{ ou } \\bar{X} &gt; 0.7) = P_{0.5}(\\bar{X}&lt;0.3) + P_{0.5}(\\bar{X}&gt;0.7) \\\\\n&=P(\\mathrm{Bin}(n,0.5) &lt; 3) + P(\\mathrm{Bin}(10,0.5)&gt;7) \\\\\n&=P(\\mathrm{Bin}(n,0.5) \\leq 2) + P(\\mathrm{Bin}(10,0.5)\\geq 8) \\\\\n&= 0.055 +0.055 = 0.11 \\\\\n\\end{aligned}\n\\]\nNote que para o Erro Tipo II, não existe uma única probabilidade para o erro sob \\(H_1\\). Optaremos por tentar calcular seu máximo. \\(\\beta_{\\max}\\) \\[\n\\begin{aligned}\n\\beta &= P_{\\theta} (0.3\\leq \\bar{X} \\leq 0.7) \\theta \\in \\Theta \\setminus \\{ 0.5 \\} \\\\\n\\beta_{\\max} &= \\sup_{\\theta \\in \\Theta \\setminus \\{ 0.5 \\}} \\beta(\\theta)\n\\end{aligned}\n\\]\nPara \\(n=10\\), \\[\n\\beta(\\theta) = P_{\\theta}\\left( 3\\leq \\sum^{n=10}_{i=1} X_{i} \\leq 7 \\right) = P\\left( 3\\leq \\mathrm{Bin}(10,\\theta)\n\\leq 7 \\right), \\theta \\in \\Theta \\setminus \\{ 0.5 \\}\n\\]\nPodemos encontrar o valor que maximiza \\(\\beta(\\theta)\\), \\(\\theta =0.5\\) derivando.",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#hipóteses-como-subconjuntos-do-espaço-paramétrico",
    "href": "teste-hipotese.html#hipóteses-como-subconjuntos-do-espaço-paramétrico",
    "title": "14  Teste de Hipótese simples",
    "section": "14.6 Hipóteses como subconjuntos do espaço paramétrico",
    "text": "14.6 Hipóteses como subconjuntos do espaço paramétrico\nSeja \\((X_{n})\\) uma a.a. de \\(X\\sim\\mathrm{Ber}(\\theta)\\), em que \\(\\theta \\in (0,1)\\). Considere as hipóteses: \\[\n\\begin{cases}\nH_{0}: \\theta \\in \\Theta_{0} \\\\\nH_{1}: \\theta \\in \\Theta_{1}\n\\end{cases}\n\\]\nem que \\(\\Theta_{0} \\cup \\Theta_{1} = \\Theta, \\Theta_{0},\\Theta_{1} \\neq \\emptyset, \\Theta_{0}\\cap\\Theta_{1}= \\emptyset\\).\nExemplos de decisões elaboráveis: \\[\n\\begin{aligned}\n&\\begin{cases}\n\\Theta_{0}=\\{ 0.5 \\} \\\\\n\\Theta_{1} = \\left( 0, \\frac{1}{2} \\right) \\cup \\left( \\frac{1}{2}, 1 \\right)\n\\end{cases} \\Rightarrow\n\\begin{cases}\nH_{0}: \\theta=0.5 \\\\\nH_{1}: \\theta \\neq 0.5 \\\\\n\\end{cases} \\text{ Hipótese alternativa bilateral} \\\\\n&\\begin{cases}\n\\Theta_{0}= \\left(  0, \\frac{1}{2} \\right] \\\\\n\\Theta_{1} = \\left( \\frac{1}{2}, 1 \\right)\n\\end{cases} \\Rightarrow\n\\begin{cases}\nH_{0}: \\theta \\leq 0.5\\\\\nH_{1}: \\theta &gt; 0.5 \\\\\n\\end{cases} \\text{ Hipótese alternativa unilateral} \\\\\n&\\begin{cases}\n\\Theta_{0}= \\left[ \\frac{1}{2}, 1\\right)   \\\\\n\\Theta_{1} = \\left( 0, \\frac{1}{2} \\right)\n\\end{cases} \\Rightarrow\n\\begin{cases}\nH_{0}: \\theta \\geq 0.5 \\\\\nH_{1}: \\theta &lt; 0.5 \\\\\n\\end{cases} \\text{ Hipótese alternativa uniteral}\n\\end{aligned}\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#função-poder",
    "href": "teste-hipotese.html#função-poder",
    "title": "14  Teste de Hipótese simples",
    "section": "14.7 Função poder",
    "text": "14.7 Função poder\nNo caso geral, calculamos a função poder definida por \\[\n\\pi (\\theta) = P_{\\theta}(\\{ \\text{Rejeitar } H_{0} \\}), \\theta \\in \\Theta\n\\]\nem que “Rejeitar \\(H_0\\)” é o procedimento de decisão para rejeitar \\(H_0\\).\nA partir da função poder conseguimos calcular as probabilidades máximas de cometer os erros tipo I e II.\nProbabilidade Máxima do Erro Tipo I: \\[\n\\alpha_{\\max} = \\sup_{\\theta \\in \\Theta_{0}}(\\pi(\\theta))\n\\]\nProbabilidade Máxima do Erro Tipo II: \\[\n\\beta_{\\max} = \\sup_{\\theta \\in \\Theta_{1}}[1-\\pi(\\theta)]\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#um-exemplo-do-cálculo-de-erros-com-hipótese-unilateral",
    "href": "teste-hipotese.html#um-exemplo-do-cálculo-de-erros-com-hipótese-unilateral",
    "title": "14  Teste de Hipótese simples",
    "section": "14.8 Um exemplo do cálculo de erros com hipótese unilateral",
    "text": "14.8 Um exemplo do cálculo de erros com hipótese unilateral\nSeja \\((X_{n})\\) uma a.a. de \\(X\\sim\\mathrm{Ber}(\\theta)\\), em que \\(\\theta \\in (0,1) = \\Theta\\). Considere as hipóteses: \\[\n\\begin{cases}\nH_{0}: \\theta \\geq 0.6\\\\\nH_{1}: \\theta &lt; 0.6\n\\end{cases}\n\\]\nPrecisamos de decisões que fazem sentido. Uma delas seria\n\nSe \\(\\bar{x}&lt;0.4\\), rejeitamos \\(H_{0}\\)\nSe \\(\\bar{x} \\geq 0.4\\), não rejeitamos \\(H_{0}\\)\n\nVamos calcular as probabilidades máximas dos erros I e II.\nPrimeiro, encontramos a função poder \\[\n\\pi(\\theta) = P_{\\theta}(\\bar{X}&lt;0.4)\n\\]\nComo \\(\\sum^n_{i=1}X_{i} \\sim \\mathrm{Bin}(n,\\theta)\\), temos que \\[\n\\pi(\\theta) = P_{\\theta} \\left( \\sum^n_{i=1}X_{i} &lt; 0.4 \\cdot n \\right) = P(\\mathrm{Bin}(n,\\theta) &lt; 0.4 \\cdot n)\n\\]\nRelembrando: \\[\n\\begin{aligned}\n\\alpha_{\\max} &= \\sup_{\\theta \\in [0.6,1)} \\pi(\\theta) \\\\\n&= \\sup_{\\theta \\in [0.6,1)} P(\\mathrm{Bin}(n, \\theta) &lt; 0.4 \\cdot n) \\\\\n\\beta_{\\max} &= \\sup_{\\theta \\in (0,0.6)} (1-\\pi(\\theta)) \\\\\n&= \\sup_{\\theta \\in (0,0.6)}P(\\mathrm{Bin}(n,\\theta)\\geq 0.4 \\cdot n)\n\\end{aligned}\n\\]\nPara \\(n = 2\\), \\[\n\\begin{aligned}\n\\alpha_{\\max} &= \\sup_{\\theta \\in [0.6,1)} \\pi(\\theta) \\\\\n&= \\sup_{\\theta \\in [0.6,1)} P(\\mathrm{Bin}(2, \\theta) &lt; 0.4 \\cdot 2) \\\\\n&= \\sup_{\\theta \\in [0.6,1)} P(\\mathrm{Bin(2,\\theta)}  = 0)\\\\\n\\beta_{\\max} &= \\sup_{\\theta \\in (0,0.6)} (1-\\pi(\\theta)) \\\\\n&= \\sup_{\\theta \\in (0,0.6)}P(\\mathrm{Bin}(2,\\theta)\\geq 0.8 \\cdot n) \\\\\n&= \\sup_{\\theta \\in (0,0.6)}P(\\mathrm{Bin}(2,\\theta) \\geq 1) \\\\\n&= \\sup_{\\theta \\in (0,0.6)}[1-P(\\mathrm{Bin}(2,\\theta) = 0)]\n\\end{aligned} ~~~ \\Rightarrow ~~~~~\n\\begin{aligned}\n\\alpha_{\\max} &= \\sup_{\\theta \\in [0.6,1]}\\left[\\binom{2}{0} \\theta^0 (1-\\theta)^2\\right] \\\\\n&=\\sup_{\\theta \\in [0.6,1]} (1-\\theta)^2 \\\\\n\\beta_{\\max} &= \\sup_{\\theta \\in (0,0.6)} (1-(1-\\theta)^2)\n\\end{aligned}\n\\]\nPodemos analisar os gráficos:\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComo é uma função decrescente, seu supremo está no ponto \\(0.6\\)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComo é uma função crescente, seu supremo está também no \\(0.6\\) Portanto, \\[\n\\begin{aligned}\n\\alpha_{\\max}&=(1-0.6)^2 = 0.16\\\\\n\\beta_{\\max} &= (1-(1-0.6)^2) = 0.84\n\\end{aligned}\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#teste-sob-normalidade",
    "href": "teste-hipotese.html#teste-sob-normalidade",
    "title": "14  Teste de Hipótese simples",
    "section": "14.9 Teste sob Normalidade",
    "text": "14.9 Teste sob Normalidade\nSeja \\((x_{n})\\) amostra aleatória de \\(X\\sim N(\\mu,\\sigma^2)\\) em que \\(\\sigma^2\\) é conhecido. Considere as hipóteses \\[\n\\begin{cases}\nH_{0}: \\mu = \\mu_{0}\\\\\nH_{1}: \\mu \\neq \\mu_{0}\n\\end{cases}\n\\] com \\(\\mu_{0} \\in \\mathbb{R}\\) e fixado.\nCalcule as probabilidades (máximas) dos erros tipo I e II, para as seguintes decisões\n\nSe \\(\\bar{x} &lt; \\mu_{0} - 1.96\\sqrt{\\frac{\\sigma^2}{n}}\\) ou \\(\\bar{x} &gt; \\mu+1.96 \\sqrt{ \\frac{\\sigma^2}{n} }\\), então rejeitamos \\(H_{0}\\)\nCaso contrário, não rejeitamos \\(H_0\\)\n\nTemos a função poder \\[\n\\pi(\\theta) = P_{\\theta}(\\mathrm{Rejeitar} H_{0}) = P_{\\theta}\\left(\\bar{X}&lt;\\mu_{0} - 1.96 \\sqrt{\\frac{\\sigma^2}{n}}\\right)\n+P_{\\theta}\\left( \\bar{X} &gt; \\mu_{0} + 1.96 \\sqrt{  \\frac{\\sigma^2}{n} } \\right)\n\\]\nem que \\(theta = \\mu \\in \\mathbb{R}\\). Portanto, \\[\n\\alpha_{\\max} = \\sup_{\\theta \\in \\Theta_{0}} \\pi(\\theta)\n\\]\nComo \\(H_{0}=\\mu=\\mu_{0} \\Leftrightarrow H_{0}: \\theta \\in \\Theta\\), em que \\(\\Theta_{0}=\\{ \\mu_{0} \\}\\), logo, \\(\\sup_{\\theta \\in \\Theta_{0}} = \\mu_{0}\\) Portanto, temos que \\[\n\\alpha_{\\max} = \\pi(\\mu_{0}) = P_{\\mu_{0}}\\left( \\bar{X}&lt;\\mu_{0}-1.96\\sqrt{ \\frac{\\sigma^2}{n} } \\right) +\nP_{\\mu_{0}}\\left( \\bar{X}&gt;\\mu_{0} + 1.96 \\sqrt{  \\frac{\\sigma^2}{n} } \\right)\n\\]\nSabemos que, pelo enunciado \\(\\bar{X} \\sim N\\left( \\mu, \\frac{\\sigma^2}{n} \\right) \\forall \\mu \\in \\mathbb{R}\\) sob \\(H_{0}\\), ou seja, quando \\(\\mu= \\mu_{0}\\) temos que \\(\\bar{X}\\sim N\\left( \\mu_{0}, \\frac{\\sigma^2}{n} \\right)\\). Note que \\(P_{\\mu_{0}}\\left( \\bar{X}&lt;\\mu_{0}-1.96 \\sqrt{ \\frac{\\sigma^2}{n} } \\right) =\nP_{\\mu_{0}}\\left(\\frac{\\bar{X}-\\mu_{0}}{\\sqrt{ \\frac{\\sigma^2}{n} }} \\right) = 2.5\\%\\) Pela simetria da distribuição normal, \\(P_{\\mu_{0}}\\left( \\bar{X}&gt;\\mu_{0}+1.96 \\sqrt{ \\frac{\\sigma^2}{n} } \\right) = 2.5\\%\\) Portanto a probabilidade máxima do erro tipo 1 é \\[\n\\alpha_{\\max} = 2.5\\% + 2.5\\% = 5.0\\%\n\\]\nComo \\(H_{1}: \\mu \\neq \\mu_{0} \\Leftrightarrow H_{1}: \\theta \\in \\Theta_{1}\\), em que \\(\\Theta_{1} = \\mathbb{R} \\setminus \\{\\mu_{0}\\}\\), temos que\n\\[\n\\begin{aligned}\n\\beta_{\\max} &= \\sup_{\\theta \\in \\Theta_{1}}[1-\\pi(\\theta)] \\\\\n\\pi(\\theta) &= P_{\\theta}\\left( \\bar{X} &lt; \\mu_{0} - 1.96 \\sqrt{\\frac{\\sigma^2}{n}} \\right) +\nP_{\\theta}\\left(\\bar{X}&gt;\\mu_{0}+1.96\\sqrt{\\frac{\\sigma^2}{n}}\\right)\n\\end{aligned}\n\\]\nSabemos que \\(\\bar{X} \\sim \\mathrm{N}\\left( \\mu, \\frac{\\sigma^2}{n} \\right)\\) para todo \\(\\mu \\in \\mathbb{R}\\). Assim, \\[\n\\begin{aligned}\n\\pi(\\theta) &= P_{\\theta}\\left( \\bar{X}&lt;\\mu_{0}-1.96 \\sqrt{  \\frac{\\sigma^2}{n} } \\right) +\nP_{\\theta}\\left( \\bar{X} &gt; \\mu_{0} + 1.96 \\sqrt{  \\frac{\\sigma^2}{n} } \\right) \\\\\n&= P_{\\theta}\\left( \\frac{\\bar{X}-\\theta}{\\sqrt{ \\frac{\\sigma^2}{n} }} &lt;\n\\frac{\\mu_{0}-\\theta-1.96 \\sqrt{  \\frac{\\sigma^2}{n} } }{\\sqrt{  \\frac{\\sigma^2}{n} }}\\right) +\nP_{\\theta}\\left( \\frac{\\bar{X}-\\theta}{\\sqrt{ \\frac{\\sigma^2}{n} }} &gt;\n\\frac{\\mu_{0}-\\theta+1.96 \\sqrt{\\frac{\\sigma^2}{n}}}{\\sqrt{\\frac{\\sigma^2}{n}}}\\right)\n\\end{aligned}\n\\]\nDessa forma, \\[\n\\beta_{max} = \\sup_{\\theta \\in \\Theta_{1}}[1-\\pi(\\theta)] = 1 - \\inf_{\\theta \\in \\Theta_{1}} \\pi(\\theta)\n\\]\nOu seja, o supremo dessa expressão é dado por 1 - o ínfimo da função poder, o que significa que queremos encontrar o valor de \\(\\theta\\) para o qual \\(P_{\\theta}\\left( \\frac{\\bar{X}-\\theta}{\\sqrt{ \\frac{\\sigma^2}{n} }} &lt;\n\\frac{\\mu_{0}-\\theta-1.96 \\sqrt{\\frac{\\sigma^2}{n} } }{\\sqrt{  \\frac{\\sigma^2}{n} }}\\right) +\nP_{\\theta}\\left(\\frac{\\bar{X}-\\theta}{\\sqrt{ \\frac{\\sigma^2}{n} }} &gt;\n\\frac{\\mu_{0}-\\theta+1.96 \\sqrt{\\frac{\\sigma^2}{n}}}{\\sqrt{\\frac{\\sigma^2}{n}}}\\right)\\) é o menor possível.\nVamos usar \\(\\mu_0 = 7\\) e \\(\\sigma^2 = 5\\) para visualizarmos o comportamento de \\(\\alpha\\) e \\(\\beta\\)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n14.9.1 Um outro exemplo\nSeja \\((X_{n})\\) amostra aleatória de \\(X\\sim \\mathrm{N}(\\mu,\\sigma^2)\\) com \\(\\sigma^2 = 5, n = 10\\) Com as hipóteses \\[\n\\begin{cases}\nH_{0} : \\mu = 10 \\\\\nH_{1}: \\mu \\neq 10\n\\end{cases}\n\\] Com as decisões 1. Rejeitamos \\(H_{0}\\) se \\(\\bar{x} &gt; 10 + 1.96 \\sqrt{  \\frac{5}{10} }\\) ou \\(\\bar{x} &lt; 10 - 1.96 \\sqrt{  \\frac{5}{10} }\\) Foram observados os seguintes valores \\[\n\\begin{array}{ccccc}\n7.1 & 8.9 & 12 & 13 & 11.7 \\\\\n6.1 & 2.5 & 3.1 & 5.2 & 7\n\\end{array}\n\\] Temos então que \\(\\bar{x} = 7.66\\) que, como é abaixo de \\(8.6\\), rejeitamos a hipótese nula de que \\(\\mu = 10\\)",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#exemplos",
    "href": "teste-hipotese.html#exemplos",
    "title": "14  Teste de Hipótese simples",
    "section": "15.1 Exemplos",
    "text": "15.1 Exemplos\n\n15.1.1 Eexemplo um\nSeja \\((X_{n})\\) a.a de \\(X\\sim \\mathrm{N}(\\mu,\\sigma^2)\\) em que \\(\\sigma^2\\) é conhecido. Considere (\\(\\mu_{0}\\) fixado)\n\\[\n\\begin{cases}\nH_{0} : \\mu = \\mu_{0} \\\\\nH_{1}: \\mu \\neq \\mu_{0}\n\\end{cases}\n\\]\n\nConstrua uma decisão para rejeitar \\(H_0\\) que produza no máximo \\(\\alpha = 5\\%\\) (que tenha nível de significância de \\(5\\%\\))\nComo a hipótese alternativa é bilateral, \\(H_{1}:\\mu\\neq \\mu_{0}\\) e \\(\\bar{x}\\) é a EMV para o parâmetro \\(\\mu\\) - a esperança da distribuição Normal - definimos a regra: Se \\(\\bar{x}&lt; x_{a}\\) ou \\(\\bar{x}&gt; x_{b}\\), rejeitamos \\(H_{0}\\). Caso contrário, não rejeitamos. \\[\n\\begin{aligned}\n     \\alpha_{\\max} &= \\sup_{\\theta \\in \\Theta_{0}} P_{\\theta}(\\text{Rejeitar }H_{0}), \\Theta_{0} = \\{ \\mu_{0} \\} \\\\\n     &= P_{\\mu_{0}}(\\bar{X}&lt;x_{a}) + P_{\\mu_{0}}(\\bar{X}&gt;x_{b}) \\leq 5\\%\n\\end{aligned}\n\\]\nNote que \\(\\bar{X} \\sim \\mathrm{N}\\left(\\mu_{0}, \\frac{\\sigma^{2}}{n} \\right)\\), sob \\(H_{0}\\)\nLogo, \\[\n\\begin{aligned}\n     \\alpha_{\\max} &= P\\left( \\mathrm{N}(0,1) &lt; \\frac{{x_{a}-\\mu_{0}}}{\\sqrt{ \\frac{\\sigma^2}{n} }} \\right)+\nP\\left( \\mathrm{N}(0,1) &gt; \\frac{{x_{a}-\\mu_{0}}}{\\sqrt{ \\frac{\\sigma^2}{n} }} \\right)\n\\end{aligned}\n\\]\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTomando \\(\\frac{{x_{a}-\\mu_{0}}}{\\sqrt{\\frac{\\sigma^2}{n}}} = -1.96\\) e \\(\\frac{{x_{b}-\\mu_{0}}}{\\sqrt{ \\frac{\\sigma^2}{n}}}\n= 1.96\\) (tabela normal padrão simétrica), temos que \\(\\alpha_{\\max}=5\\%\\). Assim, resolvendo as equações, \\[\n\\begin{cases}\n     x_{a} = \\mu - 1.96 \\sqrt{\\frac{\\sigma^2}{n}} \\\\\n     x_{b} = \\mu + 1.96 \\sqrt{\\frac{\\sigma^2}{n}}\n\\end{cases}\n\\]\n\nConsidere \\(n=100\\), \\(\\mu_{0}=1\\), \\(\\sigma^2=0.1\\) e \\(\\bar{x}=0.99\\). Conclua o teste considerando o mesmo nível de significância \\(\\alpha=5\\%\\).\nO pontos pontos de corte são \\(x_{a} = 0.93\\) e \\(x_{b} = 1.069\\). Como \\(0.93\\leq 0.99 \\leq 1.069\\), concluímos que não há evidências para rejeitarmos \\(H_{0}\\) a \\(5\\%\\) de significância.\nRefaça considerando \\(15\\%\\) de significância estatística.\nUsando os mesmos argumentos do item 1, podemos encontrar novos valores para \\(x_{a}, x_{b}\\) através da tabela da Normal-Padrão: \\[\n\\begin{cases}\n    x_{a} = \\mu - 1.44 \\sqrt{\\frac{\\sigma^2}{n}} \\\\\n    x_{b} = \\mu + 1.44 \\sqrt{\\frac{\\sigma^2}{n}}\n\\end{cases}\n\\]\nSubstituindo esses valores para os fornecidos em 2, temos que \\(0.95 \\leq 0.99 \\leq 1.045\\). Portanto, continuaríamos a dizer que não há evidências para rejeitarmos \\(H_{0}\\) a \\(15\\%\\) de significância.\n\n\n\n15.1.2 2\nSeja \\((X_{n})\\) a.a de \\(X\\sim \\mathrm{N}(\\mu,\\sigma^2)\\) em que \\(\\sigma^2\\) é conhecido.\nConsidere (\\(\\mu_{0}\\) fixado)\n\\[\n\\begin{cases}\nH_{0} : \\mu \\geq \\mu_{0} \\\\\nH_{1}: \\mu &lt; \\mu_{0}\n\\end{cases}\n\\]\n\nConstrua uma decisão para rejeitar \\(H_0\\) que produza no máximo \\(\\alpha = 5\\%\\) (que tenha nível de significância de \\(5\\%\\))\nPelos parâmetros e hipóteses envolvidos, \\((\\mu, \\text{unilateral})\\), podemos considerar a seguinte decisão:\nSe \\(\\bar{x} &lt; x_{c}\\), rejeitamos \\(H_{0}\\). Caso contrário, não rejeitamos.\n\\[\n\\begin{aligned}\n     \\alpha_{\\max} &= \\sup_{\\mu \\geq \\mu_{0}} P_{\\theta}(\\text{Rejeitar }H_{0}) \\\\\n     &= \\sup_{\\mu\\geq \\mu_{0}}P_{\\mu}(\\bar{X}&lt;x_{c}) \\\\\n     \\Rightarrow \\alpha_{\\max} &=\\sup_{\\mu\\geq \\mu_{0}}P_{\\mu}\\left( \\mathrm{N}(0,1)&lt; \\frac{{x_{c}-\\mu}}\n{\\sqrt{\\frac{\\sigma^2}{n} }} \\right)\n  \\end{aligned}\n\\]\nComo essa função (acumulada) é decrescente em \\(\\mu\\), temos que \\[\n\\begin{aligned}\n     \\alpha_{\\max} &= P_{\\mu_{0}}(\\text{Rejeitar }H_{0}) \\\\\n     &= P_{\\mu_{0}}(\\bar{X}&lt;x_{c}) \\\\\n     \\Rightarrow \\alpha_{\\max} &=P_{\\mu_{0}}\\left( \\mathrm{N}(0,1)&lt; \\frac{{x_{c}-\\mu_{0}}}\n{\\sqrt{\\frac{\\sigma^2}{n} }} \\right) \\leq 5\\%\n  \\end{aligned}\n\\]\nLogo, para encontrarmos \\(x_{c}\\) tal que \\(\\frac{{x_{c}-\\mu_{0}}}{\\sqrt{ \\frac{\\sigma^2}{n} }} = -1.64\\) (da tabela da normal padrão) \\(\\Rightarrow x_{c} = \\mu_{0}-1.64 \\sqrt{ \\frac{\\sigma^2}{n} }\\)\nConsidere \\(n=100, \\mu_{0} = 1, \\sigma^2 = 0.1, \\bar{x}=0.99\\). Conclua o teste anterior a \\(\\alpha = 5\\%\\) de significância.\nO ponto de corte é \\(x_{c} = 0.9836\\). Como \\(0.99 \\geq 0.9836\\), concluímos que não há evidências para rejeitar a hipótese nula a \\(5\\%\\) de significância.\n\n\n\n15.1.3 3\nSeja \\((X_{n})\\) amostra aleatória de \\(X \\sim \\mathrm{N}(\\mu,\\sigma^2)\\) em que \\(\\theta = (\\mu, \\sigma^2) = \\mathrm{R} \\times \\mathrm{R}^+\\), ou seja, ambos parâmetros são desconhecidos.\n\\[\n\\begin{cases}\nH_{0} : \\sigma^2 = \\sigma^2_{0} \\\\\nH_{1}: \\sigma^2 \\neq \\sigma^2_{0}\n\\end{cases} \\Rightarrow\n\\begin{aligned}\n& \\text{Decisão com significância } \\alpha \\\\\n&\\text{Rejeita $H_{0}$ se} \\\\\n&\\begin{cases}\ns^2 &lt; c_{1c} \\\\\ns^2 &gt; c_{2c} \\\\\n\\end{cases} \\\\\n& \\text{Em que $c_{1c},c_{2c}$ são tais que} \\\\\n& \\sup_{\\theta\\in\\Theta_{0}}P_{\\theta}(\\text{Erro Tipo I}) = \\alpha_{\\max} = \\alpha \\\\\n&\\text{E } s^2 = \\frac{1}{n-1} \\sum^n_{i=1}(x_{i}-\\bar{x})^2\n\\end{aligned} \\\\ \\\\\n\\]\nSabemos que \\(\\frac{\\sum_{i=1}^n(X_{i}-\\bar{X})^2}{\\sigma^2}\\sim \\chi^2_{n-1},\\forall \\mu \\in \\mathbb{R}, \\sigma^2 &gt; 0\\). Em particular, sob \\(H_{0}\\) \\[\n\\begin{aligned}\n&\\frac{(n-1)s^2(\\underset{\\sim}{X_{n}})}{\\sigma^2_{0}} \\sim \\chi^2_{n-1}\n\\\\ \\Rightarrow& \\alpha_{\\max} = \\sup_{\\theta \\in \\Theta_{0}}\\left\\{ P_{\\theta}(s^2(\\underset{\\sim}{X_{n}}) &lt; c_{1c}) +\nP_{\\theta}(s^2(\\underset{\\sim}{X_{n}})&gt; c_{2c}) \\right\\}\n\\end{aligned}\n\\]\nAlém disso, note que \\(\\Theta_{0}=\\{ (\\mu,\\sigma^2)\\in \\Theta:\\sigma^2=\\sigma^2_{0} \\}\\). Portanto, temos que \\[\n\\begin{aligned}\n& \\alpha_{\\max} = \\sup_{\\theta \\in \\Theta_{0}} \\underbracket{ \\left\\{  P\\left( \\chi^2_{n-1} &lt;\n\\frac{c_{1c}(n-1)}{\\sigma^2_{0}}\\right) + P\\left( \\chi^2_{n-1} &gt;\n\\frac{c_{2c}(n-1)}{\\sigma^2_{0}} \\right) \\right\\}}_{\\text{Não depende de $\\theta$}}\\\\\n\\Rightarrow & \\alpha_{\\max} =  P\\left( \\chi^2_{n-1} &lt; \\frac{c_{1c}(n-1)}{\\sigma^2_{0}} \\right) + P\\left( \\chi^2_{n-1} &gt;\n\\frac{c_{2c}(n-1)}{\\sigma^2_{0}} \\right)\n\\end{aligned}\n\\]\nFixando \\(\\alpha_{\\max}=\\alpha\\) (significância), encontramos pela tabela os valores de \\(q_{\\frac{\\alpha}{2},n-1}^{(1)}, q_{\\frac{\\alpha}{2},n-1}^{(2)}\\) tais que dividam a distribuição \\(\\chi^2_{n-1}\\) criando duas seções de \\(\\frac{\\alpha}{2}\\) de área. Portanto,\n\\[\n\\begin{cases}\nH_{0} : \\sigma^2 = \\sigma^2_{0} \\\\\nH_{1}: \\sigma^2 \\neq \\sigma^2_{0}\n\\end{cases} \\Rightarrow\n\\begin{aligned}\n& \\text{Decisão com significância } \\alpha \\\\\n&\\text{Rejeita $H_{0}$ se} \\\\\n&\\begin{cases}\ns^2 &lt; q_{\\frac{\\alpha}{2},n-1}^{(1)} \\cdot \\frac{\\sigma^2_{0}}{(n-1)} \\\\\ns^2 &gt; q_{\\frac{\\alpha}{2},n-1}^{(2)} \\cdot \\frac{\\sigma^2_{0}}{(n-1)}\\\\\n\\end{cases}\n\\end{aligned}\n\\]\n\n\n15.1.4 Exemplo\nSeja \\((X_{n})\\) amostra aleatória de \\(X\\sim N(\\mu,\\sigma^2)\\) em que \\(X\\) é o peso do pacote de café. Colheu-se uma amostra de \\(n=16\\) pacotes e observou-se uma variância de \\(s^2 =169g^2\\). O processo de fabricação diz que a média dos pacotes é \\(500g\\) e desvio-padrão 10 gramas (\\(\\sigma^2_{0}=100g^2\\)).\nQueremos verificar se há alguma evidência de que o processo não esteja sendo cumprido com \\(\\alpha=5\\%\\) de significância \\[\n\\begin{cases}\nH_{0} : \\sigma^2 = 100 \\\\\nH_{1}: \\sigma^2 \\neq 100\n\\end{cases} \\Rightarrow\n\\begin{aligned}\n& \\text{Decisão com significância } 5\\% \\\\\n&\\text{Rejeita $H_{0}$ se} \\\\\n&\\begin{cases}\ns^2 &lt; q_{2.5\\%,15}^{(1)} \\cdot \\frac{100}{15} \\\\\ns^2 &gt; q_{2.5\\%,15}^{(2)} \\cdot \\frac{100}{15}\\\n\\end{cases}\n\\end{aligned}\n\\] Da tabela Qui-quadrado, temos \\(q_{2.5\\%,15}^{(1)} = 6.26\\) e \\(q_{2.5\\%,15}^{(2)} =27.49\\).\nComo \\(41.73&lt;100&lt;183.26\\), concluímos que não há evidências para rejeitar a hipótese nula a \\(5\\%\\) de significância",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#sob-normalidade-variância-conhecida",
    "href": "teste-hipotese.html#sob-normalidade-variância-conhecida",
    "title": "14  Teste de Hipótese simples",
    "section": "16.1 Sob normalidade, variância conhecida",
    "text": "16.1 Sob normalidade, variância conhecida\nSeja \\((X_{n})\\) amostra aleatória de \\(X\\sim \\mathrm{N}(\\mu,\\sigma^2)\\) em que \\(\\sigma^2\\) é conhecido. \\[\n\\begin{aligned}\n&1.\n\\begin{cases}\nH_{0} : \\mu = \\mu_{0} \\\\\nH_{1}: \\mu \\neq \\mu_{0}\n\\end{cases} \\Rightarrow\n\\begin{aligned}\n& \\text{Decisão com significância } \\alpha \\\\\n&\\text{Rejeita $H_{0}$ se} \\\\\n&\\begin{cases}\n\\bar{x} &lt; \\mu_{0} - z_{\\frac{\\alpha}{2}} \\sqrt{ \\frac{\\sigma^2}{n} } \\\\\n\\bar{x} &gt; \\mu_{0} + z_{\\frac{\\alpha}{2}} \\sqrt{ \\frac{\\sigma^2}{n} }\n\\end{cases} \\\\\n& \\text{Em que $z_{\\frac{\\alpha}{2}}$ é tal que} \\\\\n& P\\left( \\mathrm{N(0,1)} &lt; z_{\\frac{\\alpha}{2}} \\right) = \\frac{\\alpha}{2}\n\\end{aligned} \\\\ \\\\\n& 2.\n\\begin{cases}\nH_{0} : \\mu \\geq \\mu_{0} \\\\\nH_{1}: \\mu &lt; \\mu_{0}\n\\end{cases} \\Rightarrow\n\\begin{aligned}\n&\\text{Rejeita $H_{0}$ se} \\\\\n&\\begin{cases}\n\\bar{x} &lt; \\mu_{0} - z_{\\alpha} \\sqrt{ \\frac{\\sigma^2}{n} } \\\\\n\\end{cases} \\\\\n& \\text{Em que $z_{\\alpha}$ é tal que} \\\\\n& P\\left( \\mathrm{N(0,1)} \\leq z_{\\alpha} \\right) = \\alpha\n\\end{aligned} \\\\ \\\\\n& 3.\n\\begin{cases}\nH_{0} : \\mu \\leq \\mu_{0} \\\\\nH_{1}: \\mu &gt; \\mu_{0}\n\\end{cases} \\Rightarrow\n\\begin{aligned}\n&\\text{Rejeita $H_{0}$ se} \\\\\n&\\begin{cases}\n\\bar{x} &gt; \\mu_{0} + z_{\\alpha} \\sqrt{ \\frac{\\sigma^2}{n} } \\\\\n\\end{cases} \\\\\n& \\text{Em que $z_{\\alpha}$ é tal que} \\\\\n& P\\left( \\mathrm{N(0,1)} \\geq z_{\\alpha} \\right) = \\alpha\n\\end{aligned} \\\\\n\\end{aligned}\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#sec-normalidade-vardesc",
    "href": "teste-hipotese.html#sec-normalidade-vardesc",
    "title": "14  Teste de Hipótese simples",
    "section": "16.2 Sob normalidade, variância desconhecida",
    "text": "16.2 Sob normalidade, variância desconhecida\nSeja \\((X_{n})\\) amostra aleatória de \\(X \\sim \\mathrm{N}(\\mu,\\sigma^2)\\) em que \\(\\theta = (\\mu, \\sigma^2) = \\mathrm{R}\n\\times \\mathrm{R}^+\\), ou seja, ambos parâmetros são desconhecidos. \\[\n\\begin{aligned}\n&1.\n\\begin{cases}\nH_{0} : \\mu = \\mu_{0}  \\\\\nH_{1} : \\mu \\neq \\mu_{0} \\\\\n\\Rightarrow \\Theta_{0}=\\{ (\\mu,\\sigma^2) \\in \\Theta : \\mu = \\mu_{0} \\}\\\\\n\\Rightarrow \\Theta_{1}=\\{ (\\mu,\\sigma^2) \\in \\Theta : \\mu \\neq \\mu_{0} \\}\\\\\n\\end{cases} \\Rightarrow\n\\begin{aligned}\n& \\text{Decisão com significância } \\alpha \\\\\n&\\text{Rejeita $H_{0}$ se} \\\\\n&\\begin{cases}\n\\frac{\\bar{x}-\\mu_{0}}{\\sqrt{ \\frac{s^2}{n} }} &lt; - t_{\\frac{\\alpha}{2},n-1} \\\\\n\\frac{\\bar{x}-\\mu_{0}}{\\sqrt{ \\frac{s^2}{n} }} &gt; t_{\\frac{\\alpha}{2},n-1} \\\\\n\\end{cases} \\\\\n& \\text{Em que $t_{\\frac{\\alpha}{2},n-1}$ é tal que} \\\\\n& P\\left( t_{n-1} &lt; - t_{\\frac{\\alpha}{2},n-1} \\right) = \\frac{\\alpha}{2}\n\\end{aligned} \\\\ \\\\\n& 2.\n\\begin{cases}\nH_{0} : \\mu \\geq \\mu_{0} \\\\\nH_{1}: \\mu &lt; \\mu_{0}\n\\end{cases} \\Rightarrow\n\\begin{aligned}\n&\\text{Rejeita $H_{0}$ se} \\\\\n&\\begin{cases}\n\\frac{\\bar{x}-\\mu_{0}}{\\sqrt{ \\frac{s^2}{n} }} &lt; - t_{\\alpha,n-1} \\\\\n\\end{cases} \\\\\n& \\text{Em que $t_{\\alpha,n-1}$ é tal que} \\\\\n& P\\left( t_{n-1}\\leq -t_{\\alpha,n-1} \\right) = \\alpha\n\\end{aligned} \\\\ \\\\\n& 3.\n\\begin{cases}\nH_{0} : \\mu \\leq \\mu_{0} \\\\\nH_{1}: \\mu &gt; \\mu_{0}\n\\end{cases} \\Rightarrow\n\\begin{aligned}\n&\\text{Rejeita $H_{0}$ se} \\\\\n&\\begin{cases}\n\\frac{\\bar{x}-\\mu_{0}}{\\sqrt{ \\frac{s^2}{n} }} &gt; - t_{\\alpha,n-1} \\\\\n\\end{cases} \\\\\n& \\text{Em que $t_{\\alpha,n-1}$ é tal que} \\\\\n& P\\left( t_{n-1} &gt; t_{\\alpha,n-1} \\right) = \\alpha\n\\end{aligned} \\\\\n\\end{aligned}\n\\]\nEm que \\(s^2 = \\frac{1}{n-1} \\sum^n_{i=1}(x_{i}-\\bar{x})^2\\) é a variância amostral (não enviesada)",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#sob-normalidade-para-a-variância.",
    "href": "teste-hipotese.html#sob-normalidade-para-a-variância.",
    "title": "14  Teste de Hipótese simples",
    "section": "16.3 Sob normalidade, para a variância.",
    "text": "16.3 Sob normalidade, para a variância.\nSeja \\((X_{n})\\) amostra aleatória de \\(X \\sim \\mathrm{N}(\\mu,\\sigma^2)\\) em que \\(\\theta = (\\mu, \\sigma^2) = \\mathrm{R} \\times \\mathrm{R}^+\\), ou seja, ambos parâmetros são desconhecidos. \\[\n\\begin{cases}\nH_{0} : \\sigma^2 = \\sigma^2_{0} \\\\\nH_{1}: \\sigma^2 \\neq \\sigma^2_{0}\n\\end{cases} \\Rightarrow\n\\begin{aligned}\n& \\text{Decisão com significância } \\alpha \\\\\n&\\text{Rejeita $H_{0}$ se} \\\\\n&\\begin{cases}\ns^2 &lt; q_{\\frac{\\alpha}{2},n-1}^{(1)} \\cdot \\frac{\\sigma^2_{0}}{(n-1)} \\\\\ns^2 &gt; q_{\\frac{\\alpha}{2},n-1}^{(2)} \\cdot \\frac{\\sigma^2_{0}}{(n-1)}\\\\\n\\end{cases}\n\\end{aligned}\n\\]\nEm que \\(s^2 = \\frac{1}{n-1} \\sum^n_{i=1}(x_{i}-\\bar{x})^2\\) é a variância amostral (não enviesada) e \\(q_{\\frac{\\alpha}{2},n-1}^{(1)}, q_{\\frac{\\alpha}{2},n-1}^{(2)}\\) tais que dividam a distribuição \\(\\chi^2_{n-1}\\) criando duas seções de \\(\\frac{\\alpha}{2}\\) de área.",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese-duas-pops.html",
    "href": "teste-hipotese-duas-pops.html",
    "title": "15  Teste de Hipótese para duas populações",
    "section": "",
    "text": "15.1 Dados independentes e dependentes\nSejam \\(X, Y\\) duas variáveis de interesse representando duas sub-populações. Estamos interessados em verificar se a média populacional de \\(X\\) é menor, maior ou igual à de \\(Y\\). Sendo assim, precisamos considerar os casos em que \\(X\\) é independente de \\(Y\\) e o caso em que não são independentes (pareados).\nUm pesquisador propôs um novo método de investimento para aumentar o rendimento mensal. Selecionou 20 investidores aleatoriamente de um universo de investidores cadastrados. Em um primeiro momento, o pesquisador deixou os investidores investirem do jeito que sabem e ao final verificou a renda obtida. \\(X\\) é o rendimento dos investidores sem ter o conhecimento do método.\nEntão, o pesquisador ensinou seu método aos investidores, onde \\(Y\\) passou a ser o rendimento dos investidores após a aplicação do método ensinado.\nClaramente, \\(X,Y\\) são dependentes.\nO mesmo pesquisador testará o mesmo método de forma diferente. Para testar o seu método, o pesquisador selecionou 20 indivíduos com características similares do universo de investidores, dos quais\nNesse caso, as variáveis \\(X,Y\\) são independentes.\nEm ambas abordagens, temos as mesmas hipóteses de interesse: \\[\n1.\n\\begin{cases}\nH_{0}:\\mu_{X}\\geq \\mu_{y} \\\\\nH_{1}: \\mu_{X} &lt; \\mu_{Y}\n\\end{cases} ~~~2.\n\\begin{cases}\nH_{0}:\\mu_{X}\\leq \\mu_{Y} \\\\\nH_{1}: \\mu_{X} &gt; \\mu_{Y}\n\\end{cases} ~~~3.\n\\begin{cases}\nH_{0}:\\mu_{X}= \\mu_{Y} \\\\\nH_{1}: \\mu_{X} \\neq \\mu_{Y}\n\\end{cases}\n\\]\nPodemos definir \\(\\mu_{D}=\\mu_{X}-\\mu_{Y}\\) e reescrever as hipóteses \\[\n1.\n\\begin{cases}\nH_{0}:\\mu_{D} \\geq 0 \\\\\nH_{1}: \\mu_{D} &lt; 0\n\\end{cases} ~~~2.\n\\begin{cases}\nH_{0}:\\mu_{D} \\leq 0 \\\\\nH_{1}: \\mu_{D} &gt; 0\n\\end{cases} ~~~3.\n\\begin{cases}\nH_{0}:\\mu_{D} = 0 \\\\\nH_{1}: \\mu_{D} \\neq 0\n\\end{cases}\n\\]\nPara os próximos exemplos, assumiremos normalidade para \\(X,Y\\)",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Teste de Hipótese para duas populações</span>"
    ]
  },
  {
    "objectID": "teste-hipotese-duas-pops.html#dados-independentes-e-dependentes",
    "href": "teste-hipotese-duas-pops.html#dados-independentes-e-dependentes",
    "title": "15  Teste de Hipótese para duas populações",
    "section": "",
    "text": "10 foram designados aleatoriamente a não receber o método \\(X:\\) rendimento de um indivíduo que não recebeu o método\n10 foram designados aleatoriamente a receberem o método \\(Y:\\) rendimento de um indivíduo que recebeu o método\n\n\n\n\n\n\n15.1.1 Caso pareado (variáveis dependentes)\nNo caso em que \\(X,Y\\) são dependentes, as amostras são \\((X_{n})\\) amostras aleatórias de \\(X\\) e \\((Y_{n})\\) a.a de \\(Y\\) tais que \\(X_{i},Y_{i}\\) são dependentes.\nPara este caso, fazemos \\(D_{i}=X_{i}-Y_{i},i=1,2,\\dots, n\\). Temos que \\((D_{n})\\) é uma amostra aleatória de \\(D=X-Y \\sim N(\\mu_{D},\\sigma^2_{D})=N(\\mu_{X}-\\mu_{Y},\\sigma^2_{X}+\\sigma^2_{Y}-2 \\rho\\sigma_{X}\\sigma _{Y})\\)\nObserve ainda que \\[\n\\bar{D}_{\\mathrm{Par}}=\\sum^{n}_{i=1} \\frac{D_{i}}{n} \\sim N\\left( \\mu_{D}, \\frac{\\sigma^2_{D}}{n} \\right)\n\\] Note ainda que as variância e covariância de \\(X,Y\\) estão embutidas em \\(\\sigma^2_{D}\\). Podemos usar \\[\ns^2_{D}(\\underset{\\sim}{D})=\\frac{1}{n-1}\\sum^n_{i=1}(D_{i}-\\bar{D}_{\\mathrm{Par}})^2\n\\] Para estimar \\(\\sigma^2_{D}\\) Podemos construir as decisões como já vimos anteriormente em testes sob normalidade com variância desconhecida.\n\n\n15.1.2 Exemplo\nForam coletados os rendimentos (em mil reais) antes e após a aplicação o método para 12 investidores. Queremos verificar se o método aumentou o rendimento médio. Chamaremos de \\(X\\) o rendimento anterior ao treinamento e \\(Y\\) o rendimento após. Isto é, queremos verificar se \\(\\mu_{D}=\\mu_{X}-\\mu_{Y}\\leq 0\\). Portanto, nossa hipótese nula é de que o treinamento não tem efeito positivo no rendimento:\n\\[\n\\begin{cases}\nH_{0}:\\mu_{D} \\geq 0 \\\\\nH_{1}: \\mu_{D} &lt; 0\n\\end{cases}\n\\]\n\\[\n\\begin{array}{c|c|c|c|c}\n\\mathrm{Indíce}  & \\mathrm{Antes}(X)  &  \\mathrm{Depois}(Y) & \\mathrm{Dif}(D) & \\mathrm{Dif^2}(D^2)\\\\\n\\hline\n1 & 2.4 &  4.3 & -1.9 & 3.61\\\\\n2 & 2.8  & 3.4  & -0.6 & 0.36\\\\\n3 & 4.6  & 3.2  & 1.4 & 1.96\\\\\n4 & 3.1  & 3.3 & -0.2 & 0.04\\\\\n5 & 3.1  & 3.3 & -0.2 & 0,04\\\\\n6 & 4.7 &  5.8 & -1.1 & 1.21\\\\\n7 & 3.5  & 3.8 & -0.3 & 0.09\\\\\n8 & 1.7  & 3.5 & -1.8 & 3.24\\\\\n9 & 2.3  & 3.2 & -0.9 & 0.81\\\\\n10 & 2.6  & 3.9 & -1.3 & 1.69\\\\\n11 & 4.2  & 3.6 & 0.6 & 0.36\\\\\n12 & 3.4  & 4.3  & -0.9  & 0.81\\\\\n\\hline\n\\mathrm{Média}  & 3.2  & 3.8  & -0.6 \\\\\ns^2 & 0.87  & 0.55  &  0.9\n\\end{array}\n\\]\nTemos que \\(s^2(\\underset{\\sim}{D})=0.9\\). (Podemos calcular diretamente ou usando a coluna \\(D^2\\) e substituindo no somatório \\(s^2_{D}=\\left( \\frac{\\sum^n_{i=1}d_{i}^2}{n}-\\bar{d}^2 \\right) \\frac{n}{n-1}\\) ) Rejeitamos \\(H_{0}\\) se \\(\\frac{\\bar{d}_{\\mathrm{Par}}}{\\sqrt{ \\frac{s^2_{D}}{n}}}&lt;-t_{\\alpha,n-1}\\) onde \\(t_{\\alpha,n-1}\\) é tal que \\(P(t_{n-1}&lt;-t_{\\alpha,n-1})=\\alpha\\), onde \\(t_{n-1}\\) é a distribuição T de Student com \\(n-1\\) graus de liberdade.\nTemos que \\(\\frac{\\bar{d}_{\\mathrm{Par}}}{\\sqrt{ \\frac{s^2_{D}}{n}}} =-2.19\\) e, a \\(\\alpha=0.05\\), \\(-t_{0.05,11}=-1.796\\). Como \\(-2.19&lt; -1.796\\), podemos dizer que há evidências de que o método aumenta o rendimento médio dos investidores a \\(5\\%\\) de significância. Por outro lado, com \\(\\alpha=0.01, -t_{0.01,11}=-2.718\\) e, por \\(-2.19\\geq -2.718\\), dizemos que não há evidências para rejeitar a hipótese de que o método não aumenta o rendimento (rejeitar a hipótese nula) a 1% de significância estatística.",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Teste de Hipótese para duas populações</span>"
    ]
  },
  {
    "objectID": "teste-hipotese-duas-pops.html#caso-de-independência",
    "href": "teste-hipotese-duas-pops.html#caso-de-independência",
    "title": "15  Teste de Hipótese para duas populações",
    "section": "15.2 Caso de independência",
    "text": "15.2 Caso de independência\nSejam \\(X, Y\\) variáveis aleatórias independentes tais que \\[ \\begin{cases}\nX\\sim N(\\mu_{X},\\sigma^2_{X}) \\\\\nY\\sim (\\mu_{Y},\\sigma^2_{Y})\n\\end{cases} \\] Considere \\((X_{n})\\) amostra aleatória de \\(X\\) e \\((Y_{m})\\). Estamos interessados em testar as hipóteses: \\[\n1.\n\\begin{cases}\nH_{0}:\\mu_{X}\\geq \\mu_{Y} \\\\\nH_{1}: \\mu_{X} &lt; \\mu_{Y}\n\\end{cases} ~~~2.\n\\begin{cases}\nH_{0}:\\mu_{X}\\leq \\mu_{Y} \\\\\nH_{1}: \\mu_{X} &gt; \\mu_{Y}\n\\end{cases} ~~~3.\n\\begin{cases}\nH_{0}:\\mu_{X}= \\mu_{Y} \\\\\nH_{1}: \\mu_{X} \\neq \\mu_{Y}\n\\end{cases}\n\\] Podemos definir \\(\\mu_{D}=\\mu_{X}-\\mu_{Y}\\) e obter a equivalência dessas hipóteses \\[\n1.\n\\begin{cases}\nH_{0}:\\mu_{D} \\geq 0 \\\\\nH_{1}: \\mu_{D} &lt; 0\n\\end{cases} ~~~2.\n\\begin{cases}\nH_{0}:\\mu_{D} \\leq 0 \\\\\nH_{1}: \\mu_{D} &gt; 0\n\\end{cases} ~~~3.\n\\begin{cases}\nH_{0}:\\mu_{D} = 0 \\\\\nH_{1}: \\mu_{D} \\neq 0\n\\end{cases}\n\\] Considere \\[\n\\bar{D}_{\\mathrm{NPar}}=\\bar{X}-\\bar{Y}\n\\]\nComo \\(\\bar{X}\\sim N\\left( \\mu_{X}, \\frac{\\sigma^2_{X}}{n} \\right), \\bar{Y} \\sim N\\left( \\mu_{Y}, \\frac{\\sigma^2_{Y}}{m} \\right)\\), temos que \\(\\bar{D}_{\\mathrm{NPar}} \\sim N\\left( \\mu_{D}, \\frac{\\sigma^2_{X}}{n} + \\frac{\\sigma^2_{Y}}{m} \\right)\\)\n\n15.2.1 Ambas variâncias conhecidas\nPodemos substituir o valor numérico das variâncias na distribuição de \\(\\bar{D}_{\\mathrm{NPar}}\\), obtendo uma distribuição normal com pontos de corte para as decisões:\n\\[\n\\begin{aligned}\n1. &\\text{ Rejeita $H_{0}$ se } \\bar{d}_{\\mathrm{NPar}} &lt; -z_{\\alpha} \\underbrace{\\sqrt{ \\frac{\\sigma^2}{n} +\n\\frac{\\sigma^2}{m} }}_{\\mathrm{Var}(\\bar{D}_{\\mathrm{NPar}})}, \\sigma_{2}=\\sigma^2_{X}=\\sigma^2_{Y} \\\\\n2. & \\text{ Rejeita $H_{0}$ se } \\bar{d}_{\\mathrm{NPar}} &gt; z_{\\alpha} \\sqrt{ \\frac{\\sigma^2}{n} + \\frac{\\sigma^2}{m} } \\\\\n3. & \\text{ Rejeita $H_{0}$ se } \\bar{d}_{\\mathrm{NPar}} &lt; -z_{\\frac{\\alpha}{2}} \\sqrt{ \\frac{\\sigma^2}{n} +\n\\frac{\\sigma^2}{m} } \\text{ ou } \\bar{d}_{NPar} &gt; z_{\\alpha} \\sqrt{ \\frac{\\sigma^2}{n} + \\frac{\\sigma^2}{m} }\n\\end{aligned}\n\\]\n\n\n15.2.2 Variâncias desconhecidas e iguais\nTemos que \\(\\sigma^2_{X}=\\sigma^2_{Y}=\\sigma^2\\) é conhecido.\n\n15.2.2.1 Estimando via t-Student\nAtravés da distribuição t-Student com \\(n+m-2\\) graus de liberdade, podemos estimar os pontos de corte. Temos nossas decisões: \\[\n\\begin{aligned}\n1. &\\text{ Rejeita $H_{0}$ se } \\bar{d}_{\\mathrm{NPar}} &lt; -t_{n+m-2,\\alpha} \\sqrt{ \\frac{s_{p}^2}{n} + \\frac{s_{p}^2}{m} } \\\\\n2. & \\text{ Rejeita $H_{0}$ se } \\bar{d}_{\\mathrm{NPar}} &gt; t_{n+m-2,\\alpha} \\sqrt{ \\frac{s^2_{p}}{n} + \\frac{s^2_{p}}{m} } \\\\\n3. & \\text{ Rejeita $H_{0}$ se } \\bar{d}_{\\mathrm{NPar}} &lt; -t_{n+m-2,\\frac{\\alpha}{2}} \\sqrt{ \\frac{s^2_{p}}{n} +\n\\frac{s^2_{p}}{m} } \\text{ ou } \\bar{d}_{NPar} &gt; t_{n+m-2, \\frac{\\alpha}{2}} \\sqrt{ \\frac{s^2_{p}}{n} + \\frac{s^2_{p}}{m} }\n\\end{aligned}\n\\] Onde \\(s^2_{p}= \\frac{(n-1)s^2_{X}+(m-1)s^2_{Y}}{n+m-2}\\), com \\(s^2_{X}, s^2_{Y}\\) sendo os estimadores não enviesados para as variâncias de \\(X\\) e \\(Y\\), respectivamente. (Ponderamos os estimadores com base no tamanho de sua amostra, assim favorecendo os estimadores mais precisos)\n\n\n\n15.2.3 Variâncias desconhecidas e diferentes (caso geral)\nTemos que \\(\\sigma^2_{X},\\sigma^2_{Y}\\) são desconhecidos e diferentes.\n\n15.2.3.1 Estimando via t-Student\nusaremos a distribuição t-Student com \\(n'\\) graus de liberdade. Temos nossas decisões:\n\\[\n\\begin{aligned}\n1. &\\text{ Rejeita $H_{0}$ se } \\bar{d}_{\\mathrm{NPar}} &lt; -t_{n',\\alpha} \\sqrt{ \\frac{s_{p}^2}{n} + \\frac{s_{p}^2}{m} } \\\\\n2. & \\text{ Rejeita $H_{0}$ se } \\bar{d}_{\\mathrm{NPar}} &gt; t_{n',\\alpha} \\sqrt{ \\frac{s^2_{p}}{n} + \\frac{s^2_{p}}{m} } \\\\\n3. & \\text{ Rejeita $H_{0}$ se } \\bar{d}_{\\mathrm{NPar}} &lt; -t_{n',\\frac{\\alpha}{2}} \\sqrt{\\frac{s^2_{p}}{n} + \\frac{s^2_{p}}{m}}\n\\text{ ou } \\bar{d}_{NPar} &gt; t_{n', \\frac{\\alpha}{2}} \\sqrt{ \\frac{s^2_{p}}{n} + \\frac{s^2_{p}}{m} }\n\\end{aligned}\n\\]\nOnde \\(s^2_{p}= \\frac{(n-1)s^2_{X}+(m-1)s^2_{Y}}{n+m-2}\\).\n\n15.2.3.1.1 Encontrando \\(n’\\)\nNa fórmula acima, temos os graus de liberdade da t-Student dado por \\[\nn' \\approx \\frac{\\left(\\frac{s^2_{X}}{n}+\\frac{s^2_{Y}}{m}\\right)^2}{\\frac{\\left(\\frac{s^2_{X}}{n}\\right)^2}\n{n-1}+\\frac{\\left(\\frac{s^2_{Y}}{m}\\right)^2}{m-1}}\n\\] Esse valor, caso não inteiro, deverá ser arredondado.\n\n\n\n15.2.3.2 Exemplo (Importante)\nQueremos testar a resistência de dois tipos de viga de aço, \\(A\\) e \\(B\\). Tomando-se \\(n=15\\) vigas do tipo \\(A\\) e \\(m=20\\) vigas do tipo \\(B\\). de um teste \\(f\\), conseguimos com \\(10\\%\\) de significância que as variâncias não são iguais. Obtemos os valores da tabela: \\[\n\\begin{array}{c|c|c}\n\\text{Tipo}  & \\text{Média}  & \\text{Variância} (s^2) \\\\\n\\hline\nA & 70.5 & 81.6 \\\\\nB  & 84.3  & 210.8 \\\\\n\\bar{d}_{\\mathrm{NPar}}  & -13.8 & --\n\\end{array}\n\\] Teste a hipótese \\[\n\\begin{cases}\nH_{0} : \\mu_{X} = \\mu_{Y} \\\\\nH_{1}:\\mu_{X} \\neq \\mu_{Y}\n\\end{cases}\n\\] Com significância \\(\\alpha = 0.05\\) para os casos\n\n15.2.3.2.1 Caso 1. Variâncias conhecidas\nTemos do produtor que \\(\\sigma^2_{X}=81, \\sigma^2_{Y}=209\\) \\(\\bar{d}_{\\mathrm{NPar}}=70.5-84.3 = -13.8\\). Logo, \\(z_{\\frac{\\alpha}{2}}\\sqrt{ \\frac{81}{15} + \\frac{209}{20}}=7.8\\). Como \\(-13.8 &lt; -7.8\\), concluímos que há evidências para rejeitarmos a hipótese nula de que as resistências médias das vigas \\(A,B\\) são iguais a \\(\\alpha = 5\\%\\) de significância estatística\n\n\n15.2.3.2.2 Caso 2. Variâncias desconhecidas e iguais.\nPara \\(\\alpha=0.05\\), \\(t_{33,0.025}=2.03\\). Encontrando \\(s^2_{P} = 155.988\\). Finalmente, \\(t_{33,0.025}\n\\sqrt{  \\frac{s^2_{P}}{n} + \\frac{s^2_{P}}{m} }=8.65\\). Como \\(-13.8 &lt; -8.65\\), concluímos que há evidências para rejeitarmos a hipótese nula a \\(\\alpha=5\\%\\) de significância estatística.\n\n\n15.2.3.2.3 Caso 3. Variâncias desconhecidas e diferentes\nPrimeiro calculamos \\(n'= 32.08 \\stackrel{\\text{Arrendonda}}{=}32\\). Assim, \\(t_{32,0.025} = 2.037\\). Portanto, \\(t_{32,0.025} \\sqrt{  \\frac{s^2_{X}}{n} + \\frac{s^2_{Y}}{m} }=8.14\\). Como \\(-13.8 &lt; -8.15\\), concluímos que há evidências para rejeitarmos a hipótese nula a \\(\\alpha=5\\%\\) de significância estatística.",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Teste de Hipótese para duas populações</span>"
    ]
  },
  {
    "objectID": "teste-nao-para-media.html",
    "href": "teste-nao-para-media.html",
    "title": "16  Testes não Paramétricos para a média populacional",
    "section": "",
    "text": "Alguns Testes não-paramétricos para a média populacional podem ser estudados nas seções 13.3.2 e 13.4.2 do livro “Estatística Básica” (7ª edição), Morettin e Bussab.\nEssa seção será completada em um futuro.",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Testes não Paramétricos para a média populacional</span>"
    ]
  },
  {
    "objectID": "tabela-frequencias.html",
    "href": "tabela-frequencias.html",
    "title": "17  Tabela de frequências",
    "section": "",
    "text": "Sejam \\(X,Y\\) variáveis aleatórias cujos valores observados são \\(B_{1},B_{2},\\dots,B_{l}\\) e \\(A_{1},A_{2}, A_{k}\\), respectivamente. Observam-se os seguintes dados \\[\n\\begin{array}{ccc}\n\\mathrm{ind.}  & X & Y \\\\\n1  & B_{2} & A_{1} \\\\\n2 & B_{7} & A_{3} \\\\\n\\vdots & \\vdots  & \\vdots \\\\\nn  & B_{1} & A_{5}\n\\end{array}\n\\] Colocamos nossos dados numa tabela de frequências absolutas observadas \\[\n\\begin{array}{c|cccc|c}\nX\\setminus Y  & A_{1} & A_{2} & \\dots & A_{k} & \\mathrm{Total}~X \\\\\n\\hline\nB_{1} & O_{11} & O_{12} & \\dots & O_{1k} & O_{1\\cdot} \\\\\nB_{2} & O_{11} & O_{12} & \\dots & O_{1k} & O_{2\\cdot} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\nB_{l} & O_{l1} & O_{l2} & \\dots & O_{lk} & O_{l\\cdot} \\\\\n\\hline\n\\mathrm{Total}~Y  & O_{\\cdot_{1}} & O_{\\cdot_{2}}  & \\dots  & O_{\\cdot k}  & n\n\\end{array}\n\\]\nTemos nossa tabela de frequências esperadas [[Teste de Hipótese|sob]] \\(H_{0}\\) (Independência) \\[\n\\begin{array}{c|cccc|c}\nX\\setminus Y  & A_{1} & A_{2} & \\dots & A_{k} & \\mathrm{Total}~X \\\\\n\\hline\nB_{1} & E_{11} & E_{12} & \\dots & E_{1k} & O_{1\\cdot} \\\\\nB_{2} & E_{11} & E_{12} & \\dots & E_{1k} & O_{2\\cdot} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\nB_{l} & E_{l1} & E_{l2} & \\dots & E_{lk} & O_{l\\cdot} \\\\\n\\hline\n\\mathrm{Total}~Y  & O_{\\cdot_{1}} & O_{\\cdot_{2}}  & \\dots  & O_{\\cdot k}  & n\n\\end{array}\n\\] Em que \\[\nE_{ij} = \\frac{O_{i \\cdot} \\cdot O_{\\cdot j}}{n}\n\\] Note que, sob independência \\[\n\\begin{aligned}\nP(B_{i}\\cap A_{j}) &= P(B_{i}) \\cdot P(A_{j}) \\\\\nE_{ij} &= n \\cdot P(B_{i}\\cap A_{j}) \\stackrel{\\mathrm{ind.}}{=} n P(B_{i}) \\cdot P_{A_{j}}\n\\end{aligned}\n\\] Estimando \\(P(B_{i}), P(A_{j})\\) temos \\[\n\\widehat{P(B_{i})} = \\frac{O_{i\\cdot}}{n}, \\widehat{P(A_{j})}= \\frac{O_{\\cdot j}}{n}\n\\] Logo, o valor esperado estimado é \\[\n\\widehat{E_{ij}}=n \\cdot \\widehat{P(B_{i})}\\cdot\\widehat{P(A_{j})} = \\frac{O_{i \\cdot} \\cdot O_{\\cdot j}}{n}\n\\]\nEm ambos testes, usaremos a seguinte estatística para testar suas hipóteses (independência e homogeneidade) \\[\n\\chi^2 = \\sum^k_{i=1}\\sum^l_{j=1} \\frac{(O_{ij}-E_{ij})^2}{E_{ij}}\n\\] Sob \\(H_{0}\\), ou seja, \\[\n\\chi^2_{obs}\\sim \\chi^2_{(k-1)(l-1)}\n\\] Dessa forma, rejeitamos a hipótese \\(H_{0}\\) a \\(\\alpha\\) graus de liberdade se \\[\n\\chi^2_{obs} &gt; c_{p}\n\\] em que \\(c_{p}\\) satisfaz \\(P(\\chi^2_{(k-1)(l-1)} &gt; c_{p})=\\alpha\\).\n\n\n\n\n\n\nObservação\n\n\n\nEssa aproximação com a \\(\\chi^2\\) só funciona de modo razoável quando cada \\(E_{ij}&gt;5\\)",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tabela de frequências</span>"
    ]
  },
  {
    "objectID": "analise-aderencia.html",
    "href": "analise-aderencia.html",
    "title": "18  Teste Qui-Quadrado e análise de aderência",
    "section": "",
    "text": "18.1 Exemplo\nA análise de aderência testa a distribuição dos dados: \\[\n\\begin{cases}\nH_{0}: P= P_{0} \\\\\nH_{1}: P \\neq P_{0}\n\\end{cases}\n\\] Em que \\(P_{0}\\) é a medida de probabilidade especificada que governaria (sob \\(H_0\\)) os eventos observados.\nNeste teste co,paramos a frequência observada com a frequência esperada em \\(k\\) eventos disjuntos e distintos observáveis. \\[\n\\begin{array}{c|cccc}\n\\text{Eventos}  & 1  & 2 & \\dots & k \\\\\n\\hline\nP_{0} & P_{01} & P_{02} & \\dots & P_{0k} \\\\\nE_{i}  & E_{1} & E_{2} & \\dots & E_{k} \\\\\nO_{i} & O_{1} & O_{2} & \\dots & O_{k}\n\\end{array}\n\\]\nEm que observou-se uma amostra de tamanho \\(n\\). Temos também que \\(E_{i}\\) é o valor esperado do número de eventos \\(i\\) sob \\(H_{0}\\) \\[\n\\mathrm{Freq. Esperada} = E_{i} = P_{0i} \\cdot n\n\\]\ne \\(\\mathrm{Freq. Observada} = O_{i}\\) é o numero real de eventos \\(i\\) observados na amostra. A estatística para testar \\(H_{0}\\) é \\[\n\\chi^2 = \\sum^k_{i=1} \\frac{(E_{i}-O_{i})^2}{E_{i}}\n\\]\nque, sob \\(H_0\\) - ou seja, sob a hipótese de que \\(P_{0}\\) é de fato a medida de probabilidade que governa o comportamento probabilístico do evento - é aproximadamente \\[\n\\underbracket{\\chi^2 \\sim \\chi^2_{(k-1)}}_{\\mathrm{Sob}~H_{0}}\n\\]\n*Esse procedimento é confiável sempre que \\(E_{i}&gt;5 \\forall i \\in \\{ 1,\\dots,k \\}\\)\nConsidere que queremos verificar se os números sorteados nos concursos da Mega Sena são de fato uniformemente distribuídos. Nesse caso, analisaremos 60 eventos, cuja probabilidade de cada um seria, caso uniformemente distribuídos, \\(\\frac{1}{60}\\). \\[\n\\begin{cases}\nH_{0}: P = P_{0} \\\\\nH_{1}: P \\neq P_{0}\n\\end{cases}\n\\]\nEm que \\(P_{0}(\\{ i \\}) = \\frac{1}{60} \\forall i \\in \\{ 1,2,\\dots 60 \\}\\)\nVamos criar a tabela para as frequências. Consideraremos a primeira bola de todos os \\(2800\\) sorteios da Mega. \\[\n\\begin{array}{c|cccc}\n\\mathrm{Eventos}  &  1 & 2 & \\dots & 60\\\\\n\\hline\nP_{0} & \\frac{1}{60} & \\frac{1}{60} & \\dots & \\frac{1}{60} \\\\\nE_{i} & \\frac{2800}{60}  & \\frac{2800}{60}  & \\dots  & \\frac{2800}{60} \\\\\nO_{i} & 42 & 48 & \\dots  & 55\n\\end{array}\n\\] Portanto, \\[\n\\chi^2 = \\sum^{60}_{i} \\frac{(46.7 - O_{i})^2}{46.7} \\stackrel{a}{\\sim} \\chi^2_{59}\n\\] Considerando um nível de significância de \\(\\alpha=5\\%\\), calculamos o ponto crítico \\(c\\) tal que \\[\nP(\\chi^2_{59}&gt;c) = 0.05\n\\]\nPelo computador, encontramos \\(c = 77.93\\) Logo, como \\(\\chi^2=56.68 &lt; 77.93\\), concluímos que, sob \\(H_{0}\\), não há evidências de que o modelo não seja equiprovável a \\(5\\%\\) de significância de estatística.",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Teste Qui-Quadrado e análise de aderência</span>"
    ]
  },
  {
    "objectID": "analise-aderencia.html#k-grupos",
    "href": "analise-aderencia.html#k-grupos",
    "title": "18  Teste Qui-Quadrado e análise de aderência",
    "section": "18.2 K-Grupos",
    "text": "18.2 K-Grupos\n\n(Morettin, Pag.404 E.7) Considere os \\(n=30\\) dados abaixo que supostamente seguem uma distribuição normal \\(N(10,25)\\). (usando os dados do livro já em ordem) \\[\n\\begin{array}{cccccc}\n1.01 & 1.73 & 3.93 & 4.44 & 6.37 & 6.51 \\\\\n\\vdots  & \\vdots  & \\vdots & \\vdots  & \\vdots & \\vdots \\\\\n14.11 & 14.6 & 14.64 & 14.75 & 16.68 & 22.14\n\\end{array}\n\\] Queremos testar se os dados de fato se distribuem de acordo com \\(N(10,25)\\). \\[\n\\begin{cases}\nH_{0}:P=N(10,25) \\\\\nH_{1}:P\\neq N(10,25)\n\\end{cases}\n\\] Sob \\(H_{0}\\), podemos dividir a distribuição normal em \\(k\\) blocos. Escolheremos \\(k=4\\) delimitado pelos quartis teóricos dessa distribuição normal. (Primeiro padronizamos, encontramos os valores pela tabela, então voltamos para nossa normal) \\[\n\\begin{cases}\nq_{1} = 6.63 \\\\\nq_{2} = 10 \\\\\nq_{3} = 13.3\n\\end{cases} \\stackrel{\\mathrm{Intervalos}}{\\Rightarrow}\n\\begin{cases}\n1.(-\\infty, q_{1}) \\\\\n2.[q_{1},q_{2}] \\\\\n3.(q_{2},q_{3}] \\\\\n4.(q_{3},\\infty)\n\\end{cases}\n\\] Podemos produzir uma tabela com as frequências por intervalo \\[\n\\begin{array}{c|cccc}\n\\mathrm{Eventos}  &  1.  & 2. & 3. & 4.\\\\\n\\hline\nE_{i} & 0.25 \\cdot 30=7.5  & 7.5 & 7.5 & 7.5 \\\\\nO_{i} & 6  & 9 & 9 & 6 \\\\\n\\end{array}\n\\] \\[\n\\chi^2 = \\sum^4_{i=1} \\frac{(7.5 - O_{i})^2}{7.5} = 1.2\n\\] Na \\(\\chi^2_{3}\\) (número de nichos), com nível de significância \\(\\alpha=0.10\\), \\(c = 6.25\\). Como \\(\\chi^2=1.2&lt;6.25\\), concluímos que não há evidências de que a distribuição dos dados difere de uma \\(N(10,25)\\) a \\(\\alpha=10\\%\\) de significância estatística",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Teste Qui-Quadrado e análise de aderência</span>"
    ]
  },
  {
    "objectID": "testes-ind-homo.html",
    "href": "testes-ind-homo.html",
    "title": "19  Testes de Independência e Homogeneidade",
    "section": "",
    "text": "19.1 Teste de Homogeneidade\nCom a ajuda da tabela de frequências, conseguimos testar independência entre eventos e homogeneidade em distribuição de eventos. Por mais que utilizem o mesmo mecanismo, os dois testes são interpretados de forma diferentes e, portanto, também apresentados individualmente nesta seção.\nUsamos esse teste para verificar se as medidas de probabilidade de vários grupos diferentes são iguais (seguem uma mesma distribuição).\nOs totais marginais para cada grupo devem ser fixados antes de executarmos o experimento. \\[\n\\begin{cases}\nH_{0}: \\text{Os grupos são independentes} \\\\\nH_{1} : \\text{Pelo menos um dos grupos não é indepndente}\n\\end{cases}\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Testes de Independência e Homogeneidade</span>"
    ]
  },
  {
    "objectID": "testes-ind-homo.html#teste-de-independência",
    "href": "testes-ind-homo.html#teste-de-independência",
    "title": "19  Testes de Independência e Homogeneidade",
    "section": "19.2 Teste de independência",
    "text": "19.2 Teste de independência\nUsamos esse teste para verificar se os eventos são independentes.\nAqui, apenas o tamanho amostral (total dos totais) é fixado.\n\\[\n\\begin{cases}\nH_{0}: \\text{Os grupos se distribuem de forma equivalente} \\\\\nH_{1} : \\text{Pelo menos um dos grupos não se distribui de forma equivalente}\n\\end{cases}\n\\]",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Testes de Independência e Homogeneidade</span>"
    ]
  },
  {
    "objectID": "testes-ind-homo.html#exemplos",
    "href": "testes-ind-homo.html#exemplos",
    "title": "19  Testes de Independência e Homogeneidade",
    "section": "19.3 Exemplos",
    "text": "19.3 Exemplos\n\n19.3.1 Primeiro exemplo (homogeniedade)\n510 segurados foram amostrados, sendo 200 de São Paulo, 100 do Ceará e 210 de Pernambuco. O objetivo é verificar se o número de acidentes se distribui igualmente entre os estados. \\[\n\\begin{array}{ccc}\n\\mathrm{Indivíduos}  & \\mathrm{Estado}  & \\mathrm{Sinistralidade}\\\\\n1 & \\mathrm{SP} & 1 \\\\\n\\vdots  & \\vdots & \\vdots \\\\\n200  &  \\mathrm{SP}  & 0 \\\\\n1 & \\mathrm{CE} & 1 \\\\\n\\vdots  & \\vdots & \\vdots \\\\\n100  &  \\mathrm{CE}  & 0 \\\\\n1 & \\mathrm{PE} & 1 \\\\\n\\vdots  & \\vdots & \\vdots \\\\\n210  &  \\mathrm{PE}  & 0\n\\end{array}\n\\] Tabela Observada \\[\n\\begin{array}{c|cc|c}\n&     \\mathrm{Sinistralidade}   &  \\\\\n\\mathrm{Estado}  & 1 & 0 &  \\mathrm{Total} \\\\\n\\hline\n\\mathrm{SP} & 60 & 140 & 200 \\\\\n\\mathrm{CE} & 10 & 90 & 100 \\\\\n\\mathrm{PE} & 50 & 160 & 210 \\\\\n\\hline\n\\mathrm{Total} & 120 & 390 & 510\n\\end{array}\n\\] Tabela esperada \\[\n\\begin{array}{c|cc|c}\n&     \\mathrm{Sinistralidade}   &  \\\\\n\\mathrm{Estado}  & 1 & 0 &  \\mathrm{Total} \\\\\n\\hline\n\\mathrm{SP} & 47 & 153 & 200 \\\\\n\\mathrm{CE} & 24 & 76 & 100 \\\\\n\\mathrm{PE} & 49 & 161 & 210 \\\\\n\\hline\n\\mathrm{Total} & 120 & 390 & 510\n\\end{array}\n\\] Temos nossa estatística qui-quadrado \\[\n\\begin{aligned}\n\\chi^2_{obs} &= \\frac{(60-47)^2}{47} + \\frac{(140-153)^2}{153} + \\frac{(10-24)^2}{24} + \\frac{(90-76)^2}{76} \\\\\n&+ \\frac{(50-49)^2}{49} + \\frac{(160-161)^2}{161} = 15.47\n\\end{aligned}\n\\] Concluiremos o teste tomando \\(\\alpha = 1\\%\\) de significância estatística Sabemos que, sob \\(H_{0}\\), \\[\n\\chi^2_{obs} \\sim \\chi^2_{(3-1)(2-1)}\n\\] Logo, devemos encontrar \\(c_{p}\\) tal que \\[\nP(\\chi^2_{2}&gt; c_{p}) = 1\\%\n\\] Pela tabela, \\(c_{p}=9.21\\) Como \\(15.47 &gt; 9.21\\), concluímos que, a sinistralidade não se distribui de forma homogênea entre os estados de SP, CE e PE a \\(1\\%\\) de significância.\n\n\n19.3.2 Outro Exemplo (independência)\nTemos nossa tabela de valores observados: \\[\n\\begin{array}{c|ccc|c}\n\\mathrm{Opinião}  & \\mathrm{1ª~Tent}  & \\mathrm{2ª~Tent}  & \\mathrm{3ª~Tent}  & \\mathrm{Total} \\\\\n\\hline\n\\mathrm{Excelente}  &  62  & 36 & 12 & 110\\\\\n\\mathrm{Satisfatório}  & 84 & 42 & 14 & 140\\\\\n\\mathrm{Insatisfatório}  & 24 & 22 & 24 & 70 \\\\\n\\hline\n\\mathrm{Total}  & 170 & 100 & 50 & 320\n\\end{array}\n\\] Nossa tabela de valores esperados (arredondados): \\[\n\\begin{array}{c|ccc|c}\n\\mathrm{Opinião}  & \\mathrm{1ª~Tent}  & \\mathrm{2ª~Tent}  & \\mathrm{3ª~Tent}  & \\mathrm{Total} \\\\\n\\hline\n\\mathrm{Excelente}  &  58 & 34 & 17 & 110 \\\\\n\\mathrm{Satisfatório}  & 74 & 44 & 22 & 140\\\\\n\\mathrm{Insatisfatório}  & 37 & 22 & 11 & 70 \\\\\n\\hline\n\\mathrm{Total}  & 170 & 100 & 50 & 320\n\\end{array}\n\\] \\[\n\\begin{aligned}\n\\chi^2_{obs} &= \\frac{(62-58)^2}{58} + \\frac{(36-34)^2}{34} + \\frac{(12-17)^2}{17} + \\frac{(84-74)^2}{74} +\n\\frac{(42-44)^2}{44} \\\\\n&+ \\frac{(14-22)^2}{22} + \\frac{(24-37)^2}{37} + \\frac{(22-22)^2}{22} + \\frac{(24-11)^2}{11} = 26.14\n\\end{aligned}\n\\] Considerando \\(\\alpha= 5\\%\\), precisamos encontrar \\(c_{p}\\) tal que \\[\nP(\\chi^2_{4}&gt;c_{p})=5\\% \\Rightarrow c_{p} = 9.49\n\\] Como \\(26.14 &gt; 9.49\\), podemos concluir que, a \\(5\\%\\) de significância estatística, existem evidências que o número da tentativa tem influência sobre a opinião do cliente.",
    "crumbs": [
      "Introdução à Inferência Frequentista",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Testes de Independência e Homogeneidade</span>"
    ]
  },
  {
    "objectID": "familia-exponencial.html",
    "href": "familia-exponencial.html",
    "title": "20  Família Exponencial (FE)",
    "section": "",
    "text": "20.1 Família Exponencial unidimensional\nPodemos representar a função \\(f_\\theta\\) genericamente, incluindo funções de probabilidade e funções densidade de probabilidade, por meio da família exponencial.\nDizemos que \\(f_\\theta\\) pertence à família exponencial unidimensional se, e somente se,\n\\[\nf_\\theta(x) = \\left \\{\\begin{array}{lr}\n\\mathrm{e}^{c(\\theta) T(x) + d(\\theta) + S(x)}, & x \\in \\mathfrak{X} \\\\\n0, & x \\not \\in \\mathfrak{X}\n\\end{array} \\right.\n\\]\nem que \\(c(\\cdot)\\) e \\(d(\\cdot)\\) são funções de “\\(\\theta\\)” cujas formas são conhecidas e \\(T(\\cdot)\\) e \\(S(\\cdot)\\) são funções de \\(x\\) com formas conhecidas, em que \\(\\mathfrak{X} = \\{x: f_\\theta(x) &gt; 0\\}\\) não depende de “\\(\\theta\\)”. \\(\\mathfrak{X}\\) é dito ser o suporte de \\(f_\\theta\\).",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Família Exponencial (FE)</span>"
    ]
  },
  {
    "objectID": "familia-exponencial.html#família-exponencial-unidimensional",
    "href": "familia-exponencial.html#família-exponencial-unidimensional",
    "title": "20  Família Exponencial (FE)",
    "section": "",
    "text": "20.1.1 Exemplos\n\n20.1.1.1 Exemplo 1 - Exponencial\nConsidere \\(X \\sim f_\\theta, \\theta \\in (0,\\infty)\\) em que \\[\nf_\\theta(x) = \\left\\{\\begin{array}{lr}\n\\theta \\mathrm{e}^{-\\theta x}, & x &gt; 0 \\\\\n0, & \\mathrm{c.c}\n\\end{array}\\right.\n\\]\nOu seja, \\(X\\sim \\mathrm{Exp}(\\theta), \\theta &gt; 0\\). Mostre que \\(f_\\theta\\) pertence à família exponencial.\n\n20.1.1.1.1 Resposta\nObserve que \\(\\mathfrak{X}=\\{x:f_\\theta(x)&gt;0\\} = (0, \\infty)\\) não depende de “\\(\\theta\\)”. Além disso, como \\(\\theta &gt; 0\\), temos que \\(\\theta = \\mathrm{e}^{\\ln(\\theta)}\\). Portanto, para \\(x&gt;0\\), temos que \\[\nf_\\theta(x) = \\mathrm{e}^{-\\theta x + \\ln \\theta}\n\\] Assim, \\[\n\\begin{array}{cc}\nc(\\theta) = -\\theta, & d(\\theta) = \\ln \\theta \\\\\nT(x) = x, & S(x) = 0.\n\\end{array}\n\\] Portanto, \\(f_\\theta\\) pertence à família exponencial.\n\n\n\n20.1.1.2 Exemplo 2 - Bernoulli\nConsidere \\(X\\sim \\mathrm{Ber}(\\theta), \\theta \\in \\Theta = (0,1)\\). Mostre que a sua função de probabilidade pertence à família exponencial.\n\n20.1.1.2.1 Resposta\nObserve que \\[\nf_\\theta(x) = \\left\\{\\begin{array}{lr}\n\\theta^x \\cdot (1-\\theta)^{1-x}, & x \\in \\{0,1\\} \\\\\n0, & x \\not \\in \\{0,1\\}\n\\end{array}\\right.\n\\] o suporte é \\(\\mathfrak{X} = \\{ x : f_\\theta(x)&gt;0\\} = \\{0,1\\}\\) não depende de “\\(\\theta\\)”.\nAlém disso, como \\(\\theta^x (1-\\theta)^{1-x} &gt; 0\\), temos que \\[\n\\begin{aligned}\n\\theta^x (1-\\theta)^{1-x} &= \\mathrm{e}^{\\ln(\\theta^x(1-\\theta)^{1-x})}\\\\\n&= \\mathrm{e}^{x\\ln\\theta + (1-x) \\ln (1-\\theta)}\\\\\n&=\\mathrm{e}^{x\\ln\\theta + \\ln (1-\\theta) - x \\ln(1-\\theta)} \\\\\n&= \\mathrm{e}^{x\\left(\\ln \\theta - \\ln(1-\\theta)\\right) + \\ln(1-\\theta)}\\\\\n&=\\mathrm{e}^{\\ln\\left(\\frac{\\theta}{1-\\theta}\\right)x+\\ln(1-\\theta)}\n\\end{aligned}\n\\]\nlogo, \\(f_\\theta(x) = \\mathrm{e}^{c(\\theta)T(x)+d(\\theta)+S(x)}, x \\in \\{0,1\\}\\), em que \\[\n\\begin{array}{cc}\nc(\\theta) = \\ln(\\frac{\\theta}{1-\\theta}), & T(x) = x \\\\\nd(\\theta) = \\ln(1-\\theta), & S(x) = 0.\n\\end{array}\n\\] Portanto, \\(f_\\theta\\) pertence à família exponencial.\n\n\n\n20.1.1.3 Exemplo 3 - Normal (Média)\nConsidere que \\(X\\sim N(\\theta,1), \\theta \\in \\mathbb{R}\\). Mostre que a sua função densidade de probabilidade pertence à FE unidimensional.\n\n\n20.1.1.4 Resposta\nNote que \\[\nf_\\theta(x) = \\frac{1}{\\sqrt{2\\pi}} \\mathrm{e} ^{-\\frac{1}{2} (x-\\theta)^2}, x \\in \\mathbb{R}\n\\] o suporte é \\(\\mathfrak{X} = \\{x: f_\\theta(x) &gt; 0\\} = (-\\infty, \\infty)\\) e não depende de “\\(\\theta\\)”. Além disso, temos que \\[\n\\begin{aligned}\nf_\\theta(x) & = \\mathrm{e}^{-\\frac{1}{2}(x-\\theta)^2-\\ln\\sqrt{2\\pi}} \\\\\n&= \\mathrm{e}^{-\\frac{1}{2}(x^2 - 2x\\theta + \\theta^2) - \\ln\\sqrt{2\\pi}} \\\\\n&= \\mathrm{e}^{\\theta x - \\frac{1}{2}\\theta^2 - \\frac{1}{2}x^2 - \\ln\\sqrt{2\\pi}}.\n\\end{aligned}\n\\]\nPortanto, \\(f_\\theta\\) pertence à FE: \\[\n\\begin{array}{cc}\nc(\\theta) = \\theta, & T(x) = x \\\\\nd(\\theta) = -\\frac{1}{2}\\theta^2, & S(x) = -\\frac{1}{2}x^2-\\ln\\sqrt{2\\pi}.\n\\end{array}\n\\]\n\n\n20.1.1.5 Exempo 4 - Normal (Variância)\nConsidere que \\(X\\sim N(0,\\theta), \\theta \\in (0,\\infty)\\). Mostre que a sua função densidade de probabilidade pertence à FE unidimensional.\n\n20.1.1.5.1 Resposta\nObserve que \\[\nf_\\theta(x) = \\frac{1}{\\sqrt{2\\pi\\theta}} \\mathrm{e}^{-\\frac{1}{2\\theta}x^2}, x \\in \\mathbb{R}\n\\]\nO suporte \\(\\mathfrak{X} = \\mathbb{R}\\) não depende de “\\(\\theta\\)”. Além disso, \\[\n\\begin{aligned}\nf_\\theta(x) &= \\mathrm{e}^{-\\frac{1}{2\\theta}x^2 - \\frac{1}{2} \\ln(2\\pi\\theta)} \\\\\n&= \\mathrm{e}^{c(\\theta)T(x)+d(\\theta)+S(x)}\n\\end{aligned}\n\\] em que \\[\n\\begin{array}{cc}\nc(\\theta) = -\\frac{1}{2\\theta}, & T(x) = x^2 \\\\\nd(\\theta) = -\\frac{1}{2}\\ln(2\\pi\\theta), & S(x) = 0.\n\\end{array}\n\\]\nPortanto, \\(f_\\theta\\) pertence à FE.\n\n\n\n20.1.1.6 Exemplo 5 - Normal (Média = Variância)\nSeja \\(X\\sim N(\\theta, \\theta)\\), em que \\(\\theta \\in (0, \\infty)\\). Mostre que a sua função densidade de probabilidade pertence à FE de dimensão 1.\n\n20.1.1.6.1 Resposta\nA função densidade de probabilidade de \\(X\\) é \\[\n\\begin{aligned}\nf_\\theta(x) &= \\frac{1}{\\sqrt{2\\pi\\theta}} \\mathrm{e}^{-\\frac{1}{2\\theta}(x-\\theta)^2} \\\\\n&= \\mathrm{e}^{-\\frac{1}{2\\theta}(x^2-2\\theta x+ \\theta^2) -\\frac{1}{2}\\ln(2\\pi\\theta)} \\\\\n&= \\mathrm{e}^{-\\frac{x^2}{2\\theta}+x-\\frac{1}{2}\\theta-\\frac{1}{2}\\ln(2\\pi\\theta)} \\\\\n&= \\mathrm{e}^{c(\\theta)T(x)+d(\\theta)+S(x)}\n\\end{aligned}\n\\] em que \\[\n\\begin{array}{cc}\nc(\\theta) = -\\frac{1}{2\\theta}, & T(x) = x^2 \\\\\nd(\\theta) = -\\frac{1}{2}\\theta-\\frac{1}{2}\\ln(2\\pi\\theta), & S(x) = x.\n\\end{array}\n\\] Portanto, \\(f_\\theta\\) pertence à FE.\n\n\n\n20.1.1.7 Exemplo 6 - Exemplo negativo (Uniforme)\nSe \\(X\\sim U(0,\\theta), \\theta \\in (0,\\infty)\\), então a sua função densidade de probabilidade não pertence à FE, pois o seu suporte depende de “\\(\\theta\\)”. \\[\nf_\\theta(x) = \\left\\{\\begin{array}{lr}\n\\frac{1}{\\theta}, & x \\in (0, \\theta) \\\\\n0, & x \\not \\in (0, \\theta)\n\\end{array}\\right.\n\\]\nDessa forma, \\(\\mathfrak{X} = \\{ x : f_\\theta(x)&gt; 0\\} = (0,\\theta)\\).\n\n\n20.1.1.8 Exemplo 7 - Exemplo negativo (Normal - Média e Variância)\nConsidere que \\(X \\sim N(\\mu,\\sigma^2)\\), em que \\(\\theta = (\\mu, \\sigma^2) \\in \\mathbb{R}\\times\\mathbb{R}^+\\). Então, pode-se mostrar que a sua função densidade de probabilidade não pertence à FE unidimensional.\nObserve que \\[\n\\begin{aligned}\nf_\\theta(x) &= \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\mathrm{e}^{-\\frac{1}{2\\sigma^2}(x-\\mu)^2} \\\\\n&= \\mathrm{e}^{-\\frac{1}{2\\sigma^2}(x^2-2x\\mu+\\mu^2) - \\frac{1}{2} \\ln (2\\pi\\sigma^2)} \\\\\n&= \\mathrm{e}^{-\\frac{1}{2\\sigma^2}x^2+\\frac{x\\mu}{\\sigma^2}-\\frac{1}{2\\sigma^2}\\mu^2-\\frac{1}{2}\\ln(2\\pi\\sigma^2)}\n\\end{aligned}\n\\]\nportanto, não é possível definir \\(c(\\theta),T(x),d(\\theta)\\) e \\(S(x)\\) tais que \\(c(\\cdot), T(\\cdot)\\) representem \\(-\\frac{1}{2\\sigma^2}x^2 + \\frac{x\\mu}{\\sigma^2}\\).\n\n\n\n20.1.2 Propriedades na integração\nComo \\(f_\\theta\\) é uma função (densidade) de probabilidade, temos que \\[\n\\int_{-\\infty}^{\\infty} f_\\theta(x) dx = 1 \\mathrm{(caso \\ contínuo)}, \\forall \\theta \\in \\Theta\n\\]\nSe \\(f_\\theta\\) pertence à FE unidimensional, então\n\\[\n\\begin{aligned}\n&\\int_{\\mathfrak{X}} \\mathrm{e}^{c(\\theta)T(x) +d(\\theta) + S(x)} dx = 1 \\forall \\theta \\in \\Theta \\\\\n\\Rightarrow& \\int_{\\mathfrak{X}} \\mathrm{e}^{c(\\theta)T(x) + S(x)} dx = \\mathrm{e}^{-d(\\theta)} \\forall \\theta \\in \\Theta.\n\\end{aligned}\n\\]",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Família Exponencial (FE)</span>"
    ]
  },
  {
    "objectID": "familia-exponencial.html#sec-fek",
    "href": "familia-exponencial.html#sec-fek",
    "title": "20  Família Exponencial (FE)",
    "section": "20.2 Família Exponencial k-dimensional",
    "text": "20.2 Família Exponencial k-dimensional\nDizemos que a função (densidade) de probabilidade \\(f_\\theta\\) pertence à FE k-dimensional se, e somente se, \\[\nf_\\theta(x) = \\left\\{\\begin{array}{lr}\n\\mathrm{e}^{\\sum^k_{j=1} c_j(\\theta)T_j(x)+d(\\theta)+S(x)}, &x \\in \\mathfrak{X} \\\\\n0, & x \\not \\in \\mathfrak{X}\n\\end{array}\\right.\n\\] em que\n\n\\(\\mathfrak{X} = \\{ x: f_\\theta (x) &gt; 0 \\}\\) não depende de “\\(\\theta\\)”;\nAs funções \\(c_1(\\cdot),\\dots,c_k(\\cdot)\\) e \\(d(\\cdot)\\) dependem apenas de “\\(\\theta\\)” (formas conhecidas) e\nAs funções \\(T_1(\\cdot),\\dots,T_l(\\cdot)\\) e \\(S(\\cdot)\\) dependem apenas de \\(x\\) (formas conhecidas).\n\nComo \\(f_\\theta\\) é uma função (densidade) de probabilidade, temos que (para o caso contínuo) \\[\n\\int_{-\\infty}^{\\infty} f_\\theta(x)dx=1 \\forall \\theta \\in \\Theta.\n\\]\nSe \\(f_\\theta\\) pertence à FE k-dimensional, então \\[\n\\begin{aligned}\n&\\int_{\\mathfrak{X}} \\mathrm{e}^{\\sum^k_{j=1}c_j(\\theta)T_j(x) +d(\\theta) + S(x)} dx = 1 \\forall \\theta \\in \\Theta \\\\\n\\Rightarrow &\\int_{\\mathfrak{X}} \\mathrm{e}^{\\sum^k_{j=1}c_j(\\theta)T_j(x) + S(x)} dx = \\mathrm{e}^{-d(\\theta)} \\forall \\theta \\in \\Theta.\n\\end{aligned}\n\\]\n\n20.2.1 Exemplos\n\n20.2.1.1 Exemplo 1 (Normal - Média e Variância)\nSeja \\(X\\sim N(\\mu, \\sigma^2)\\), em que \\(\\theta = (\\mu, \\sigma^2) \\in \\mathbb{R}\\times\\mathbb{R}^+\\). Mostre que a sua função densidade de probabilidade pertence à FE de dimensão 2.\n\n20.2.1.1.1 Resposta\nNote que \\[\n\\begin{aligned}\nf_\\theta(x) &= \\mathrm{e}^{-\\frac{1}{2\\sigma^2}x^2+\\frac{x\\mu}{\\sigma^2}-\\frac{1}{2\\sigma^2}\\mu^2-\\frac{1}{2}\\ln(2\\pi\\sigma^2)}\n&= \\mathrm{e}^{c_1(\\theta)T_1(x) + c_2(\\theta)T_2(x) + d(\\theta) + S(x)}\n\\end{aligned}\n\\]\nem que \\[\n\\begin{array}{cc}\nc_1(\\theta) = -\\frac{1}{2\\sigma^2}, & T_1(x) = x^2 \\\\\nc_2(\\theta) = \\frac{\\mu}{\\sigma^2}, & T_2(x) = x \\\\\nd(\\theta) = -\\frac{1}{2}\\frac{\\mu^2}{\\sigma^2}-\\frac{1}{2}\\ln(2\\pi\\sigma^2), & S(x) = 0\n\\end{array}\n\\]\n\n\n\n\n20.2.2 Exercício\nRefaça o exemplo 5 do caso univarido com \\(N(\\theta, \\theta^2)\\) e mostre que pertence à FE de dimensão 2.",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Família Exponencial (FE)</span>"
    ]
  },
  {
    "objectID": "distribuicao-amostral.html",
    "href": "distribuicao-amostral.html",
    "title": "21  Distribuição Amostral - Aprofundamento",
    "section": "",
    "text": "21.1 Relação com a FE\nSeja \\((Y_1,\\dots,Y_n)\\) uma amostra aleatória de \\(Y\\sim f_\\theta, \\theta \\in \\Theta\\). A função (densidade) de probabilidade da amostra aleatória é dada por \\[\nf_\\theta^{(n)}(y_1,\\dots,y_n) \\stackrel{\\mathrm{iid}}{=} \\prod_{i=1}^n f_\\theta (y_i), \\forall \\theta \\in \\Theta\n\\] e \\(y_i \\in \\mathbb{R}, i = 1,\\dots,n\\).",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Distribuição Amostral - Aprofundamento</span>"
    ]
  },
  {
    "objectID": "distribuicao-amostral.html#relação-com-a-fe",
    "href": "distribuicao-amostral.html#relação-com-a-fe",
    "title": "21  Distribuição Amostral - Aprofundamento",
    "section": "",
    "text": "21.1.1 Unidimensional\nSe \\(f_\\theta\\) pertence à Família Exponencial Unidimensional, então \\[\n\\begin{aligned}\n&f_\\theta^{(n)}(y_1,\\dots,y_n) =\n\\left\\{ \\begin{array}{lr}\n\\prod^n_{i=1} \\mathrm{e}^{c(\\theta)T(y_i) +d(\\theta)+S(y_i)}, & \\mathrm{se}\\ y_1,\\dots,y_n \\in \\mathfrak{X} \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right. \\\\\n\\Rightarrow\n&f_\\theta^{(n)}(\\underbracket{y_1,\\dots,y_n}_{\\boldsymbol{y}_n}) =\n\\left\\{ \\begin{array}{lr}\n\\mathrm{e}^{c(\\theta)\\sum^n_{i=1}T(y_i) +nd(\\theta)+\\sum^n_{i=1}S(y_i)}, & \\mathrm{se}\\ \\boldsymbol{y}_n\\in \\mathfrak{X}^n \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right.\n\\end{aligned}\n\\]\nEm que \\(\\mathfrak{X}^n = \\mathfrak{X} \\times \\dots \\times \\mathfrak{X}\\). Portanto, \\(f_\\theta^{(n)}\\) pertence à FE unidimensional.\n\n\n21.1.2 k-dimensional\nSe \\(f_\\theta\\) pertence à FE k-dimensional, então \\[\n\\begin{aligned}\n&f_\\theta^{(n)}(\\boldsymbol{y}_n) =\n\\left\\{ \\begin{array}{lr}\n\\prod^n_{i=1} \\mathrm{e}^{\\sum^k_{j=1}c_j(\\theta)T_j(y_i) +d(\\theta)+S(y_i)}, & \\mathrm{se}\\ \\boldsymbol{y}_n \\in \\mathfrak{X}^n \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right. \\\\\n\\Rightarrow\n&f_\\theta^{(n)}(\\boldsymbol{y}_n) =\n\\left\\{ \\begin{array}{lr}\n\\mathrm{e}^{\\sum^k_{j=1}c_j(\\theta)\\sum^n_{i=1}T_j(y_i) +nd(\\theta)+\\sum^n_{i=1}S(y_i)}, & \\mathrm{se}\\ \\boldsymbol{y}_n \\in \\mathfrak{X}^n \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right.\n\\end{aligned}\n\\]\nTome \\(T_j^*(\\boldsymbol{y}_n) = \\sum^n_{i=1}T_j(y_i)\\), \\(d^*(\\theta)=nd(\\theta)\\), \\(S^*(\\boldsymbol{y}_n)= \\sum^n_{i=1}S(y_i)\\) \\[\n\\Rightarrow f_\\theta^{(n)}(\\boldsymbol{y}_n) =\n\\left\\{ \\begin{array}{lr}\n\\mathrm{e}^{\\sum^k_{j=1}c_j(\\theta)T^*_j(\\boldsymbol{y}_n) +d^*(\\theta)+S^*(\\boldsymbol{y}_n)}, & \\mathrm{se}\\ \\boldsymbol{y}_n \\in \\mathfrak{X}^n \\\\\n0, & \\mathrm{c.c}\n\\end{array}\\right.\n\\]\nPortanto, \\(f_\\theta^{(n)}\\) pertence à FE k-dimensional.\n\n\n21.1.3 Exemplos\n\n21.1.3.1 Bernoulli\nSe \\(X\\sim\\mathrm{Ber}(\\theta), \\theta \\in (0,1)\\), então \\[\n\\begin{aligned}\n&f_\\theta(y) = \\left\\{\\begin{array}{ll}\n\\theta^y \\cdot (1-\\theta)^{1-y}, & y \\in \\{0,1\\} \\\\\n0, & \\mathrm{c.c.}\n\\end{array} \\right. \\\\\n\\Rightarrow\n&f_\\theta(y) = \\left\\{\\begin{array}{ll}\n\\mathrm{e}^{\\ln\\left(\\frac{\\theta}{1-\\theta}\\right)y+\\ln(1-\\theta)}, & y \\in \\{0,1\\} \\\\\n0, & \\mathrm{c.c.}\n\\end{array} \\right.\n\\end{aligned}\n\\]\nA função probabilidade da amostra é \\[\nf_\\theta(\\boldsymbol{y}_n) = \\left\\{\\begin{array}{ll}\n\\mathrm{e}^{\\ln\\left(\\frac{\\theta}{(1-\\theta)}\\right)\\sum_{i=1}^n y_i+n\\ln(1-\\theta)}, & \\boldsymbol{y}_n \\in \\{0,1\\}^n \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right.\n\\]\n\n\n21.1.3.2 Binomial\nSe \\(X\\sim\\mathrm{Bin}(m, \\theta), \\theta \\in (0,1)\\) e \\(m\\) fixado, então \\[\nf_\\theta(y) = \\left\\{\\begin{array}{ll}\n\\binom{m}{y}\\theta^y \\cdot (1-\\theta)^{m-y}, & y \\in \\{0,1,\\dots,m\\} \\\\\n0, & \\mathrm{c.c.}\n\\end{array} \\right.\n\\]\nNote que \\[\n\\begin{aligned}\n\\binom{m}{y} \\theta^y(1-\\theta)^{m-y} &= \\mathrm{e}^{\\ln\\binom{m}{y} + y \\ln\\theta + (m-y)\\ln(1-\\theta)} \\\\\n&= \\mathrm{e}^{\\ln\\binom{m}{y} + y\\ln(\\theta) + m\\ln(1-\\theta) - y\\ln(1-\\theta)}.\n\\end{aligned}\n\\]\nPortanto, \\[\nf_\\theta(y) = \\left\\{\\begin{array}{ll}\n\\mathrm{e}^{\\ln\\left(\\frac{\\theta}{1-\\theta}\\right) y + m \\ln(1-\\theta) + \\ln\\binom{m}{y}}, & y \\in \\{0,1\\dots,m\\} \\\\\n0, & \\mathrm{c.c.}\n\\end{array} \\right.\n\\]\nA função probabilidade da amostra é \\[\nf_\\theta(\\boldsymbol{y}_n) = \\left\\{\\begin{array}{ll}\n\\mathrm{e}^{\\ln\\left(\\frac{\\theta}{1-\\theta}\\right)\\sum^n_{i=1}y_i + nm \\ln(1-\\theta) + \\sum_{i=1}^n\\ln\\binom{m}{y_i}}, & \\boldsymbol{y}_n \\in \\mathfrak{X}^n \\\\\n0, & \\mathrm{c.c.}\n\\end{array} \\right.\n\\] em que \\(\\mathfrak{X} = \\{0,1,\\dots,m\\}\\).\n\n\n21.1.3.3 Dirichlet\nSe \\(X\\sim\\mathrm{Dirichlet}(\\alpha_1,\\dots,\\alpha_k)\\), então \\(X=(Y_1,\\dots,Y_k)^T\\) é um vetor (coluna) aleatório k-dimensional cuja função densidade de probabilidade é dada por \\[\nf_\\theta(y) = \\left\\{\\begin{array}{ll}\n\\frac{\\Gamma(\\sum^k_{j=1} \\alpha_j)}{\\prod^k_{j=1}\\Gamma(\\alpha_j)} \\prod^k_{j=1} y_j^{\\alpha_j-1}, & y_j \\in \\mathfrak{X} \\\\\n0, & \\mathrm{c.c.}\n\\end{array} \\right.\n\\]\nem que \\(\\mathfrak{X} = \\{(y_1,\\dots,y_k) \\in (0,1)^k : \\sum^k_{j=1} y_j = 1\\}\\) e o vetor de parâmetros é \\(\\theta=(\\alpha_1,\\dots,\\alpha_k) \\in \\mathbb{R}^k_+\\).\nA amostra aleatória é \\[\nX_1 = \\left(\n\\begin{array}{c}\nY_{11}\\\\\n\\vdots \\\\\nY_{k1}\n\\end{array}\n\\right),\nX_2 = \\left(\n\\begin{array}{c}\nY_{12} \\\\\n\\vdots \\\\\nY_{k2}\n\\end{array}\n\\right), \\dots,\nX_n = \\left(\n\\begin{array}{c}\nY_{1n} \\\\\n\\vdots \\\\\nY_{kn}\n\\end{array}\n\\right)\n\\]\ne sua função densidade de probabilidade é dada por \\[\nf_\\theta(\\boldsymbol{y}_n) \\stackrel{\\mathrm{iid}}{=}\\left\\{\\begin{array}{ll}\n\\left[\\frac{\\Gamma(\\sum^k_{j=1} \\alpha_j)}{\\prod^k_{j=1}\\Gamma(\\alpha_j)}\\right]^n \\prod_{i=1}^n\\left(\\prod^k_{j=1} y_{ij}^{\\alpha_j-1}\\right), & y_i \\in \\mathfrak{X}, \\forall i=1,\\dots,n \\\\\n0, & \\mathrm{c.c.}\n\\end{array} \\right.\n\\]\nTome \\[\n\\begin{aligned}\ng(\\theta) &= \\left[\n\\frac{\\Gamma(\\sum^k_{j=1}\\alpha_j)}{\\prod^k_{j=1}\\Gamma(\\alpha_j}\n\\right]^n \\\\\n\\Rightarrow\nf_\\theta^{(n)}(\\boldsymbol{y}_n) &= \\left\\{ \\begin{array}{ll}\ng(\\theta)\\prod^n_{i=1}\\prod^k_{j=1} y_{ij}^{\\alpha_j-1}, & y_1,\\dots,y_n \\in \\mathfrak{X} \\\\\n0,  & \\mathrm{c.c.}\n\\end{array}\\right.\n\\end{aligned}\n\\]\nNote que \\[\n\\begin{aligned}\ng(\\theta)\\prod^n_{i=1} \\prod^k_{j=1} y_{ij}^{\\alpha_j-1} &= \\mathrm{e}^\n{\\ln g(\\theta) + \\sum^n_{i=1} \\sum^k_{j=1}(\\alpha_j-1)\\ln y_{ij}} \\\\\n&= \\mathrm{e}^{\\ln g(\\theta) + \\sum^k_{j=1}(\\alpha_j -1) \\sum^n_{i=1} \\ln y_{ij}}\n\\end{aligned}\n\\]\n\\[\n\\Rightarrow\nf_\\theta^{(n)}(\\boldsymbol{y}_n) = \\left\\{ \\begin{array}{ll}\n\\mathrm{e}^{\\sum^k_{j=1} c_j^*(\\theta)T_j^*(\\boldsymbol{y}_n)+d^*(\\theta) + S^*(\\boldsymbol{y}_n)}, & \\boldsymbol{y}_n \\in \\mathfrak{X}^n \\\\\n0,  & \\mathrm{c.c.}\n\\end{array}\\right.\n\\]\nPortanto, pertence à FE k-dimensional.\n\n\n\n\n\n\nObservação\n\n\n\nA partir de \\(f_\\theta^{(n)}\\) conseguimos fazer inferência sobre a quantidade de interesse: podemos encontrar a distribuição de estatísticas e estimadores.",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Distribuição Amostral - Aprofundamento</span>"
    ]
  },
  {
    "objectID": "estatisticas-suficientes.html",
    "href": "estatisticas-suficientes.html",
    "title": "22  Estatísticas Suficientes",
    "section": "",
    "text": "22.1 Caso discreto\nSeja \\(\\boldsymbol{X}_n(X_1,\\dots,X_n)\\) uma amostra aleatória de \\(X\\sim f_\\theta,\n\\theta \\in \\Theta\\). Dizemos que uma estatística \\(T(\\boldsymbol{X}_n)\\) é suficiente para o modelo estatístico se, e somente se, a distribuição da amostra dado que \\(T(\\boldsymbol{X}_n) = t\\) não depende de “\\(\\theta\\)”. Ou seja, \\[\n\\begin{aligned}\nP_\\theta(X_1\\leq y_1,\\dots,X_n\\leq y_n \\lvert T(\\boldsymbol{X}_n)=t)\\\n\\text{Não depende de}\\ \\theta, \\forall y_1,\\dots,y_n \\in \\mathbb{R} \\\\\n\\text{E para todo valor de}\\ t \\ \\text{para o quais a distribuição de}\\ T(\\boldsymbol{X}_n)\\ \\text{exista}\n\\end{aligned}\n\\]\nEm outras palavras, a informação probabilística sobre “\\(\\theta\\)” da amostra aleatória está inteiramente contida no modelo induzido pela estatística.\nNo caso discreto, basta mostrar que \\(P_\\theta(X_1=y_1,\\dots,X_n=y_n\\lvert T(\\boldsymbol{X}_n)=t)\\) não depende de “\\(\\theta\\)” para todo \\(y_1,\\dots,y_n \\in \\mathbb{R}\\) e valores de \\(t\\) para os quais a distribuição de \\(T(\\boldsymbol{X}_n)\\) exista.\nNo caso contínuo, basta mostrar que \\(f_\\theta^{(n)}(y_1,\\dots,y_n\\lvert t)\\) não depende de \\(\\theta\\) para todo \\(y_1,\\dots,y_n \\in \\mathbb{R}\\) e valores de \\(t\\) para os quais a função densidade de probabilidade de \\(T(\\boldsymbol{X}_n)\\) exista.\nComo discutiremos adiante, podemos substituir a restrição “\\(\\in \\mathbb{R}\\)” pelo termo “quase certamente” (\\(\\mathrm{q.c.}\\)), isto é, para todos exceto um conjunto enumerável (de medida de probabilidade nula).\nTambém, \\(f_\\theta^{(n)}\\) será reescrita por \\(f_\\theta^{\\boldsymbol{X}_n}\\) para diferenciar a função (densidade) da amostra, da estatística e das condicionais.\nUsaremos \\(y\\) no lugar de \\(x\\) para distinguir que \\(y\\) representa valores observados em potencial, e não realmente observados de uma amostra real colhida, como poderia estar subtendido com o uso de \\(x\\).",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Estatísticas Suficientes</span>"
    ]
  },
  {
    "objectID": "estatisticas-suficientes.html#caso-discreto",
    "href": "estatisticas-suficientes.html#caso-discreto",
    "title": "22  Estatísticas Suficientes",
    "section": "",
    "text": "22.1.1 Exemplos\n\n22.1.1.1 Exemplo 1\nSeja \\(\\boldsymbol{X}_n = (X_1, \\dots, X_n)\\) uma a.a. de \\(X\\sim\\mathrm{Ber}(\\theta), \\theta \\in \\Theta = (0,1)\\). Verifique se \\(T(\\boldsymbol{X}_n)=\\sum^n_{i=1}X_i\\) é suficiente para o modelo estatístico.\n\n22.1.1.1.1 Resposta\nPor definição, \\[\nP_\\theta(X_1=y_1,\\dots,X_n=y_n\\lvert T(\\boldsymbol{X}_n) = t) = \\frac{P_\\theta(X_1=y_1,\\dots,X_n=y_n,T(\\boldsymbol{X}_n)=t)}\n{P_\\theta(T(\\boldsymbol{X}_n)=t)}\n\\]\nSabemos que \\(T(\\boldsymbol{X}_n)=\\sum^n_{i=1}X_i \\sim \\mathrm{Bin}(n,\\theta)\\) (por função geradora de momentos). Portanto, \\[\nP_\\theta(T(\\boldsymbol{X}_n)=t) = \\left\\{ \\begin{array}{ll}\n\\binom{n}{t} \\cdot \\theta^t\\cdot(1-\\theta)^{n-t},&\\text{se}\\ t \\in \\{0,1,\\dots,n\\} \\\\\n0, & \\mathrm{c.c.}\n\\end{array} \\right.\n\\]\nLogo, \\(P_\\theta(X_1=y_1,\\dots,X_n=y_n\\lvert T(\\boldsymbol{X}_n) = t)\\) só está bem definida se \\(t \\in \\{0,1,\\dots,n\\}\\).\n(Numerador) \\[\n\\begin{aligned}\n&P_\\theta(X_1=y_1,\\dots,X_n=y_n,T(\\boldsymbol{X}_n)=t) \\\\\n&\\stackrel{\\mathrm{TPT}}{=}\n\\overbracket{P_\\theta(X_1=y_1,\\dots,X_n=y_n)}^{A} \\cdot \\overbracket{P_\\theta(T(\\boldsymbol{X}_n)=t\\lvert X_1=y_1,\\dots,X_n=y_n)}^B\n\\end{aligned}\n\\]\nObserve que \\(A\\) é a função probabilidade da amostra e \\[\nB = \\left \\{ \\begin{array}{ll}\n1, & \\text{se}\\ t = \\sum^n_{i=1} y_i \\\\\n0, & \\mathrm{c.c}\n\\end{array}\\right.\n\\]\nPara \\(t \\in \\{0,1,\\dots,n\\}\\)\n\\[\nP_\\theta(X_1=y_1,\\dots,X_n=y_n\\lvert T(\\boldsymbol{X}_n) = t) =\n\\left \\{ \\begin{array}{ll}\n\\frac{\\prod^n_{i=1}\\theta^{y_i}(1-\\theta)^{1-y_i}}{\\binom{n}{t}\\cdot\\theta^t\\cdot(1-\\theta)^{n-t}}, & \\text{se}\\ t = \\sum^n_{i=1}y_i\\\\\n0, & \\mathrm{c.c}\n\\end{array}\\right. \\forall \\theta \\in \\Theta\n\\]\nPortanto, \\[\nP_\\theta(X_1=y_1,\\dots,X_n=y_n\\lvert T(\\boldsymbol{X}_n) = t) =\n\\left \\{ \\begin{array}{ll}\n\\frac{\\theta^{\\sum^n_{i=1}y_i}(1-\\theta)^{\\sum^n_{i=1}1-y_i}}{\\binom{n}{t}\\cdot\\theta^t\\cdot(1-\\theta)^{n-t}}, & \\text{se}\\ t = \\sum^n_{i=1}y_i \\\\\n0, & \\mathrm{c.c}\n\\end{array}\\right. \\forall \\theta \\in \\Theta\n\\]\nConcluímos que \\[\n\\begin{aligned}\n&P_\\theta(X_1=y_1,\\dots,X_n=y_n\\lvert T(\\boldsymbol{X}_n) = t) =\n\\left \\{ \\begin{array}{ll}\n\\frac{\\prod_{i=1}^n \\mathbb{1}_{\\{0,1\\}}(y_i)}{\\binom{n}{t}}, & \\text{se}\\ t = \\sum^n_{i=1}y_i \\\\\n0, & \\mathrm{c.c}\n\\end{array}\\right.\\\\\n&\\forall \\theta \\in \\Theta, t \\in \\{0,1,\\dots,n\\}, \\forall y_1,\\dots,y_n \\in \\mathbb{R}\n\\end{aligned}\n\\]\nA estatística \\(T(\\boldsymbol{X}_n)\\) é suficiente para o modelo estatístico Bernoulli.\n\n\n\n22.1.1.2 Exemplo 2\nSeja \\(\\boldsymbol{X}_n=(X_1,\\dots,X_n)\\) uma amostra aleatória de \\(X\\sim\\mathrm{Pois}(\\theta), \\theta \\in \\Theta = (0,\\infty)\\). Verifique se \\(T(\\boldsymbol{X}_n)= \\frac{1}{n}\\sum^n_{i=1}X_i\\) é uma estatística suficiente para o modelo estatístico.\n\n22.1.1.2.1 Resposta\n\\[\nP_\\theta(X_1=y_1,\\dots,X_n=y_n\\lvert T(\\boldsymbol{X}_n) = t) =\n\\frac{P_\\theta(X_1=y_1,\\dots,X_n=y_n,T(\\boldsymbol{X}_n) = t)}\n{P_\\theta(T(\\boldsymbol{X}_n)=t)}\n\\]\n(Numerador) \\[\n\\begin{aligned}\n&P_\\theta(X_1=y_1,\\dots,X_n=y_n,T(\\boldsymbol{X}_n)=t) \\\\\n&\\stackrel{\\mathrm{TPT}}{=}\n\\overbracket{P_\\theta(X_1=y_1,\\dots,X_n=y_n)}^{A} \\cdot \\overbracket{P_\\theta(T(\\boldsymbol{X}_n)=t\\lvert X_1=y_1,\\dots,X_n=y_n)}^B\n\\end{aligned}\n\\]\nObserve que \\(A\\) é a função probabilidade da amostra e \\[\nB = \\left \\{ \\begin{array}{ll}\n1, & \\text{Se}\\ t =\\frac{1}{n} \\sum^n_{i=1} y_i \\\\\n0, & \\mathrm{c.c}\n\\end{array}\\right.\n\\]\n(Denominador)\nJá sabemos que \\(\\sum^n_{i=1}X_i\\sim \\mathrm{Pois}(n\\theta)\\) (por função geradora de momentos)\n\\[\nP_\\theta\\left(\\frac{1}{n}\\sum^n_{i=1}X_i=\\frac{k}{n}\\right) = \\left\\{ \\begin{array}{ll}\n\\mathrm{e}^{-n\\theta}\\cdot \\frac{(n\\theta)^k}{k!}, & k \\in \\{0,1,2,\\dots\\} \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right.\n\\]\nTome \\(t = \\frac{k}{n}\\), então \\(k=nt\\) \\[\nP_\\theta\\left(\\sum^n_{i=1}X_i=k\\right) = \\left\\{ \\begin{array}{ll}\n\\mathrm{e}^{-n\\theta}\\cdot \\frac{(n\\theta)^{nt}}{(nt)!}, & t \\in \\{0,\\frac{1}{n},\\frac{2}{n},\\dots\\} \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right.\n\\]\nPortanto, para \\(t \\in \\{0, \\frac{1}{n}, \\frac{2}{n},\\dots\\}\\), temos que \\[\n\\begin{aligned}\n&P_\\theta(X_1=y_1,\\dots,X_n=y_n\\lvert T(\\boldsymbol{X}_n)=t) = \\\\ &=\n\\left\\{ \\begin{array}{ll}\n\\frac{\\prod^n_{i=1}\\left\\{\\mathrm{e}^{-\\theta}\\cdot\\frac{\\theta^{y_i}}{y_i!}\\cdot \\mathbb{1}_{\\{0,1,\\dots\\}}(y_i)\\right\\}}\n{\\mathrm{e}^{-n\\theta}\\cdot \\frac{(n\\theta)^{nt}}{(nt)!}}, & t=\\frac{1}{n} \\sum^n_{i=1} y_i \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right. \\\\\n&= \\left\\{\\begin{array}{ll}\n\\frac{\\mathrm{e}^{-n\\theta}\\cdot\\theta^{\\sum^n_{i=1}y_i}\\cdot \\prod^n_{i=1}\\mathbb{1}_{\\{0,1,\\dots\\}}(y_i)(nt!)}\n{\\prod^n_{i=1}(y_i!)\\cdot\\mathrm{e}^{-n\\theta}\\cdot (n\\theta)^{nt}}, & t=\\frac{1}{n} \\sum^n_{i=1} y_i \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right. \\\\\n&= \\left\\{\\begin{array}{ll}\n\\frac{\\prod^n_{i=1}\\mathbb{1}_{\\{0,1,\\dots\\}}(y_i)(nt!)}\n{\\prod^n_{i=1}(y_i!)(n)^{nt}}, & t=\\frac{1}{n} \\sum^n_{i=1} y_i \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right. \\\\\n\\forall \\theta \\in \\Theta\n\\end{aligned}\n\\]\nNão depende de “\\(\\theta\\)” para todo \\(y_1,\\dots,y_n \\in \\mathbb{R}\\) e \\(t \\in \\{0,\\frac{1}{n},\\frac{2}{n},\\dots\\}\\).",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Estatísticas Suficientes</span>"
    ]
  },
  {
    "objectID": "estatisticas-suficientes.html#caso-contínuo",
    "href": "estatisticas-suficientes.html#caso-contínuo",
    "title": "22  Estatísticas Suficientes",
    "section": "22.2 Caso Contínuo",
    "text": "22.2 Caso Contínuo\n\n22.2.1 Exemplo (Normal)\nSeja \\(\\boldsymbol{X}_n = (X_1,\\dots,X_n)\\) a.a. de \\(X\\sim N(\\theta,1), \\theta \\in \\mathbb{R}\\). Verifique se \\(T(\\boldsymbol{X}_n) = \\sum^n_{i=1}X_i\\) é suficiente para o modelo estatístico.\n\n22.2.1.1 Resposta\nPor definição \\[\nf_\\theta^{\\boldsymbol{X}_n\\lvert T(\\boldsymbol{X}_n) = t}(y_1,\\dots,y_n) = \\frac{f_\\theta^{\\boldsymbol{X}_n, T(\\boldsymbol{X}_n)}(y_1,\\dots,y_n,t)}\n{f_\\theta^{T(\\boldsymbol{X}_n)}(t)}\n\\]\n(Denominador) Já sabemos que \\(T(\\boldsymbol{X}_n) = \\sum^n_{i=1} X_i \\sim N(n\\theta, n)\\) \\[\n\\Rightarrow f_\\theta^{T(\\boldsymbol{X}_n)}(t) = \\frac{1}{\\sqrt{2\\pi n}} \\cdot \\mathrm{e}^{-\\frac{1}{2n}\\cdot (t-n\\theta)^2}, t \\in \\mathbb{R}\n\\] (Numerador) \\[\nf_\\theta^{\\boldsymbol{X}_n,T(\\boldsymbol{X}_n)}(y_1,\\dots,y_n,t) =\nf_\\theta^{\\boldsymbol{X}_n}(y_1,\\dots,y_n) \\cdot f_\\theta^{T(\\boldsymbol{X}_n)\\lvert \\boldsymbol{X}_n=(y_1,\\dots,y_n)}(t)\n\\]\nNote que \\[\n\\begin{aligned}\n&f_\\theta^{T(\\boldsymbol{X}_n)\\lvert \\boldsymbol{X}_n = (y_1,\\dots,y_n} = \\left\\{ \\begin{array}{ll}\n1, & t = \\sum^n_{i=1} y_1 \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right. \\\\\n\\Rightarrow&\nf_\\theta^{\\boldsymbol{X}_n\\lvert T(\\boldsymbol{X}_n) = t}(y_1,\\dots,y_n) = \\left\\{ \\begin{array}{ll}\n\\frac{f_\\theta^{\\boldsymbol{X}_n}(y_1,\\dots,y_n)}\n{f_\\theta^{T(\\boldsymbol{X}_n)}(t)},& t = \\sum^n_{i=1}y_i \\\\\n0, & \\mathrm{c.c.}\n\\end{array} \\right.\n\\end{aligned}\n\\]\nLogo \\[\nf_\\theta^{T(\\boldsymbol{X}_n)\\lvert \\boldsymbol{X}_n = (y_1,\\dots,y_n)} = \\left\\{ \\begin{array}{ll}\n\\frac{\\frac{1}{\\sqrt{2\\pi\\cdot1}^n} \\cdot \\mathrm{exp}\\left\\{-\\frac{1}{2} \\sum^n_i=1 (y_i - \\theta)^2\\right\\}}\n{\\frac{1}{\\sqrt{2\\pi\\cdot n}}\\cdot \\mathrm{exp}\\left\\{-\\frac{1}{2n}(t-n\\theta)^2\\right\\}}, & t = \\sum y_i \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right.\n\\]\nNote que \\[\n-\\frac{1}{2}\\sum(y_i-\\theta)^2=-\\frac{1}{2}\\left(\\frac{t^2}{n}-2t\\theta+n\\theta^2\\right)\n\\] logo \\(f_\\theta^{\\boldsymbol{X}_n\\lvert T(\\boldsymbol{X}_n)=t}\\) não depende de \\(\\theta\\) e \\(\\sum X_i\\) é suficiente para o modelo estatístico.\n\n\n\n22.2.2 Problema das funções densidade de probabilidade\nA função densidade de probabilidade não é única. Entretanto, é única para quase todo ponto (quase certamente).\nPor exemplo, \\(X\\sim\\mathrm{Exp}(\\theta), \\theta \\in (0,\\infty)\\) \\[\n\\begin{aligned}\nf_\\theta(x) &= \\left\\{\\begin{array}{ll}\n\\theta \\mathrm{e}^{-\\theta x},& x \\in (0, \\infty) \\\\\n0, & x \\not \\in (0,\\infty)\n\\end{array}\\right.\\ \\ \\theta \\in \\Theta \\\\\nP_\\theta(X&gt;2)&=\\int^\\infty_2 \\theta \\cdot \\mathrm{e}^{-\\theta x} dx \\\\\n&\\text{Se $A$ é enumerável e defina} \\\\\nf_{\\theta}^A(x) &= \\left\\{\\begin{array}{ll}\n\\theta \\mathrm{e}^{-\\theta x},& x \\in (0, \\infty) \\setminus A \\\\\n10, & x \\in \\mathrm{A} \\\\\n0, & x \\not \\in (0,\\infty)\n\\end{array}\\right. \\ \\ \\theta \\in \\Theta \\\\\n\\end{aligned}\n\\]\nTemos que \\[\n\\begin{aligned}\nf_\\theta(x) &= f_\\theta^A(x), \\forall x \\in \\mathbb{R} \\setminus A, \\forall \\theta \\in \\Theta \\\\\n\\text{e} \\\\\nf_\\theta(x) &\\neq f_\\theta^A(x), \\forall x \\in A, \\forall \\theta \\in \\Theta\n\\end{aligned}\n\\]\nNote que \\(f_\\theta\\) e \\(f_\\theta^A\\) são diferentes, mas produzem as mesmas probabilidades. Dizemos portanto que \\(f_\\theta\\) e \\(f_\\theta^A\\) são iguais quase certamente, ou seja, \\[\nP_\\theta\\left(f_\\theta(x)=f_\\theta^A(x)\\right) = 1, \\forall \\theta \\in \\Theta\n\\] Ou, de outra forma, \\[\nP_\\theta\\left(f_\\theta(x)\\neq f_\\theta^A(x)\\right) = 0, \\forall \\theta \\in \\Theta\n\\]\nNotação: \\[\nf_\\theta(x) = f_\\theta^A(x)\\ \\mathrm{q.c.}\\ \\forall \\theta \\in \\Theta\n\\]\nNo caso contínuo, portanto, a estatística \\(T(\\boldsymbol{X}_n)\\) será suficiente mesmo se \\(f_\\theta^{\\boldsymbol{X}_n\\lvert T(\\boldsymbol{X}_n)=t}(y_1,\\dots,y_n)\\) depende de “\\(\\theta\\)” para \\((y_1,\\dots,y_n) \\in A\\), DESDE QUE \\(P_\\theta(X \\in A) = 0, \\forall \\theta \\in \\Theta\\). Ou seja, pode depender de “\\(\\theta\\)” em um conjunto com probabilidade zero.",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Estatísticas Suficientes</span>"
    ]
  },
  {
    "objectID": "estatisticas-suficientes.html#sec-crit-fat",
    "href": "estatisticas-suficientes.html#sec-crit-fat",
    "title": "22  Estatísticas Suficientes",
    "section": "22.3 Critério da Fatoração de Neyman-Fisher (Caso Simples)",
    "text": "22.3 Critério da Fatoração de Neyman-Fisher (Caso Simples)\nSeja \\(\\boldsymbol{X}_n=(X_1,\\dots,X_n)\\) uma amostra aleatória de \\(X \\sim f_\\theta, \\theta \\in \\Theta \\subseteq \\mathbb{R}^p\\) (sendo as \\(f_\\theta, \\theta \\in \\Theta\\) do mesmo “tipo”, formalmente, dominadas pela mesma medida), em que \\(p \\in \\{1,2,\\dots\\}\\). Uma estatística \\(T(\\boldsymbol{X}_n)\\) é suficiente para o modelo estatístico se, e somente se, existirem funções \\(h(\\cdot): \\mathbb{R}^n\\rightarrow \\mathbb{R}, m(\\cdot,\\cdot): \\mathrm{Im}(T)\\times\\theta\\rightarrow\\mathbb{R}\\) (mensuráveis) tais que: \\[\nf_\\theta^{\\boldsymbol{X}_n}(y_1,\\dots,y_n) = h(y_1,\\dots,y_n)\\cdot m\\left(T(y_1,\\dots,y_n),\\theta\\right), \\forall \\theta \\in \\Theta\\; \\mathrm{q.c.}\n\\]\nObs:\n\n\\(h\\) não depende de “\\(\\theta\\)”;\n\\(m\\) depende de valores amostrais por meio da estatística \\(T(\\boldsymbol{X}_n)\\);\n\\(f_\\theta^{\\boldsymbol{X}_n} = f_\\theta^{(n)}\\) é a função (densidade) de probabilidade da amostra aleatória.\nNote que a função de verossimilhança é obtida calculando \\(f_\\theta^{\\boldsymbol{X}_n}\\) na amostra observada, ou seja, \\[\nL_{\\boldsymbol{X}_n}(\\theta) = f_\\theta^{\\boldsymbol{X}_n}\\underbracket{(x_1,\\dots,x_n)}_{\\boldsymbol{X}_n}\n\\]\nAlguns livros usam a função de verossimilhança no critério da fatoração \\[\nL_{\\boldsymbol{X}_n}(\\theta) =  h(\\boldsymbol{X}_n) \\cdot m(T(\\boldsymbol{X}_n),\\theta)\\; \\mathrm{q.c.} \\forall \\theta \\in \\Theta\n\\] em que \\(\\boldsymbol{X}_n = (x_1,\\dots,x_n)\\) da amostra observada\n\n\n22.3.1 Prova (caso discreto)\n\n22.3.1.1 \\(\\Rightarrow\\)\nAssuma que \\(T(\\boldsymbol{X}_n)\\) seja suficiente. Por definição, \\(P_\\theta(X_1=y_1,\\dots,X_n=y_n\\lvert T(\\boldsymbol{X}_n) = t)\\) não depende de “\\(\\theta\\)”. Logo, podemos escrever: \\[\nP_\\theta(X_1=y_1,\\dots,X_n=y_n\\lvert T(\\boldsymbol{X}_n)=t) = h^*(y_1,\\dots,y_n,t) \\forall \\theta \\in \\Theta\n\\tag{22.1}\\] Note também que, \\[\nP_\\theta(X_1=y_1,\\dots,X_n=y_n\\lvert T(\\boldsymbol{X}_n)=t) =\n\\frac{P_\\theta(X_1=y_1,\\dots,X_n=y_n, T(\\boldsymbol{X}_n)=t)}\n{P_\\theta(T(\\boldsymbol{X}_n)=t)}\n\\] para valores de \\(t\\) em que a probabilidade condicional exista.\n\\[\n\\begin{aligned}\n&P_\\theta(X_1=y_1,\\dots,X_n=y_n, T(\\boldsymbol{X}_n)=t) \\\\\n&=\nP_\\theta(X_1=y_1,\\dots,X_n=y_n)\\cdot P_\\theta(T(\\boldsymbol{X}_n)=t\\lvert X_1=y_1,\\dots,X_n=y_n).\n\\end{aligned}\n\\]\nComo, com \\(\\boldsymbol{y}_n = y_1,\\dots,y_n\\), \\[\nP_\\theta(T(\\boldsymbol{X}_n)=t\\lvert X_1=y_1,\\dots,X_n=y_n) = \\left\\{\\begin{array}{ll}\n1, & T(\\boldsymbol{y}_n) = t \\\\\n0, & \\mathrm{cc}\n\\end{array}\\right.\n\\] então \\[\n\\begin{aligned}\n&P_\\theta(X_1=y_1,\\dots,X_n=y_n\\lvert T(\\boldsymbol{X}_n)=t)\n&=\n\\frac{P_\\theta(X_1=y_1,\\dots,X_n=y_n)\\cdot \\mathbb{1}(T(\\boldsymbol{y}_n)=t)}\n{P_\\theta(T(\\boldsymbol{X}_n) =t)}\n\\end{aligned}\n\\tag{22.2}\\]\nPor (22.1) e (22.2), temos que \\[\nh^*(y_1,\\dots,y_n, t) \\cdot P_\\theta(T(\\boldsymbol{X}_n)=t) = P_\\theta(X_1=y_1,\\dots,X_n=y_n\\lvert \\mathbb{1}(T(\\boldsymbol{X}_n)=t)).\n\\]\nPara \\(T(\\boldsymbol{X}_n)=t\\), temos que \\[\nP_\\theta(X_1=y_1,\\dots,X_n=y_n) = h^*(y_1,\\dots,y_n, T(\\boldsymbol{y}_n)) \\cdot P_\\theta(T(\\boldsymbol{X}_n)=T(\\boldsymbol{y}_n))\n\\]\n\n\n22.3.1.2 \\(\\Leftarrow\\)\nAssuma que existam \\(h, m\\) tais que \\[\nP_\\theta(X_1=y_1,\\dots,X_n=y_n) = h(\\boldsymbol{y}_n) m(T(\\boldsymbol{y}_n,\\theta))\n\\] Note que \\[\nP_\\theta(X_1=y_1,\\dots,X_n=y_n\\lvert T(\\boldsymbol{X}_n) = t) = \\left\\{ \\begin{array}{ll}\n\\frac{P_\\theta(X_1=y_1,\\dots,X_n=y_n)}{P_\\theta(T(\\boldsymbol{X}_n)=t)}, & T(\\boldsymbol{y}_n)=t \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right.\n\\]\nObserve que \\[\nP_\\theta(T(\\boldsymbol{X}_n) = t) = \\sum_{(y_1,\\dots,y_n) : T(\\boldsymbol{y})_n=t} P_\\theta(X_1=y_1,\\dots,X_n=y_n)\n\\] Por suposição \\[\n\\begin{aligned}\nP_\\theta(T(\\boldsymbol{X}_n) =t) &= \\sum_{\\boldsymbol{y}_n : T(\\boldsymbol{y}_n)=t} h(\\boldsymbol{y}_n) \\cdot m (T(\\boldsymbol{y}_n), \\theta) \\\\\n&= \\sum_{\\boldsymbol{y}_n : T(\\boldsymbol{y}_n)=t} h(\\boldsymbol{y}_n) \\cdot m(t,\\theta) \\\\\n&= m(t,\\theta) \\cdot \\sum_{\\boldsymbol{y}_n : T(\\boldsymbol{y}_n)=t} h(\\boldsymbol{y}_n)\n\\end{aligned}\n\\] Portanto \\[\nP_\\theta(X_1=y_1,\\dots,X_n=y_n\\lvert T(\\boldsymbol{X}_n)=t) = \\left\\{\\begin{array}{ll}\n\\frac{h(\\boldsymbol{y}_n)\\cdot m(T(\\boldsymbol{y}_n),\\theta)}{m(t,\\theta) \\cdot \\sum_{\\boldsymbol{y}_n: T(\\boldsymbol{y}_n) = t} h(\\boldsymbol{y}_n)}, & T(\\boldsymbol{y}_n) = t \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right.\n\\]\nLogo, \\[\nP_\\theta(X_1=y_1,\\dots,X_n=y_n\\lvert T(\\boldsymbol{X}_n)=t) = \\left\\{\\begin{array}{ll}\n\\frac{h(\\boldsymbol{y}_n)}{\\sum_{\\boldsymbol{y}_n: T(\\boldsymbol{y}_n) = t} h(\\boldsymbol{y}_n)}, & T(\\boldsymbol{y}_n) = t \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right.\n\\]\n\n\n\n22.3.2 Exemplo (1 do caso discreto)\nSeja \\(\\boldsymbol{X}_n=(X_1,\\dots,X_n)\\) amostra aleatória de \\(X\\sim\\mathrm{Ber}(\\theta), \\theta \\in \\Theta = (0,1)\\). Verifique se \\(T(\\boldsymbol{X}_n)=\\sum^n_{i=1}X_i\\) é suficiente para o modelo estatístico.\n\n22.3.2.1 Resposta\nObserve que \\[\n\\begin{aligned}\nf_\\theta^{\\boldsymbol{X}_n}(y_1,\\dots,y_n) &= \\left\\{ \\begin{array}{ll}\n\\prod^n_{i=1}\\left\\{\\theta^{y_i}\\cdot(1-\\theta)^{1-y_i}\\right\\}, & y_i \\in \\{0,1\\}, \\forall i = 1,\\dots,n \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right. \\\\\n\\Rightarrow\nf_\\theta^{\\boldsymbol{X}_n}(y_1,\\dots,y_n) &= \\theta^{\\sum y_i} \\cdot (1-\\theta)^{n-\\sum y_i} \\cdot \\prod^n_{i=1}\\mathbb{1}_{\\{0,1\\}} (y_1)\n\\end{aligned}\n\\]\nTome \\(h(y_1,\\dots,y_n) = \\prod^n_{i=1}\\mathbb{1}_{\\{0,1\\}}(y_1)\\) e \\(m(T(y_1,\\dots,y_n),\\theta) = \\theta^{\\sum y_i} \\cdot (1-\\theta)^{n-\\sum y_i}\\) em que \\(T(y_1,\\dots,y_n) = \\sum^n_{i=1}y_i\\). Temos que \\[\nf_\\theta^{\\boldsymbol{X}_n}(y_1,\\dots,y_n) = h(y_1,\\dots,y_n) m(T(y_1,\\dots,y_n),\\theta),\\forall \\theta \\in \\Theta\n\\]\nPelo critério da fatoração, \\(T(\\boldsymbol{X}_n) = \\sum^n_{i=1}X_i\\) é suficiente para o modelo de Bernoulli.\n\n\n\n22.3.3 Mais Exemplos\n\n22.3.3.1 Exemplo a\nSeja \\(\\boldsymbol{X}_n\\) amostra aleatória de \\(X\\sim\\mathrm{Beta}(a,b), \\theta = (a,b) \\in \\Theta = \\mathbb{R}^2_+\\). Encontre uma estatística suficiente para o modelo.\n\n22.3.3.1.1 Resposta\nA função densidade de probabilidade da amostra aleatória é\n\\[\nf^{\\boldsymbol{X}_n}_\\theta(y_1,\\dots,y_n) \\stackrel{\\mathrm{a.a.}}{=}\n\\prod^n_{i=1} f_\\theta(y_i) \\stackrel{\\mathrm{Beta}}{=}\n\\prod^n_{i=1}\n\\left\\{\n\\frac{1}{\\beta(a,b)}y_{i}^{a-1}\\cdot(1-y_i)^{b-1}\n\\right\\}\n\\]\nEm que \\[\n\\begin{aligned}\n\\beta(a,b) &= \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)} \\\\\n\\Gamma(a) &= \\int^\\infty_0 x^{a-1}\\mathrm{e}^{-x}dx\n\\end{aligned}\n\\]\n\\[\n\\Rightarrow f_\\theta^{\\boldsymbol{X}_n}(\\boldsymbol{y}_n) = \\left\\{\\begin{array}{ll}\n\\frac{1}{\\beta(a,b)^n}\n\\left(\\prod^n_{i=1}y_1\\right)^{a-1} \\cdot \\left( \\prod^n_{i=1} (1-y_1)\\right)^{b-1}, & \\boldsymbol{y}_n \\in (0,1)^n \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right.\n\\]\nTome \\(h(\\boldsymbol{y}_n) = \\prod^n_{i=1}\\mathbb{1}(y_i)_{(0,1)}\\) e \\[\nm(t,\\theta) = \\frac{1}{\\beta(a,b)^n} \\cdot t_1^{a-1} \\cdot t_2^{b-1}\n\\] em que \\(t=(t_1,t_2)\\) e \\(t_1 = \\prod^n_{i=1}y_i, t_2 = \\prod^n_{i=1}(1-y_i)\\). Ou seja, \\[\nT(\\boldsymbol{X}_n) = \\left(\\prod^n_{i=1}X_i, \\prod^n_{i=1}(1-X_i)\\right)\n\\] é uma estatística suficiente para o modelo pois \\[\nf_\\theta^{\\boldsymbol{X}_n}(\\boldsymbol{y}_n) =  m(T(\\boldsymbol{y}_n),\\theta)\\ \\mathrm{q.c.}\\ \\forall \\theta \\in \\Theta.\n\\]\n\n\n\n22.3.3.2 Exemplo b\nSeja \\(\\boldsymbol{X}_n\\) amostra aleatória de \\(X\\sim\\mathrm{N}(\\mu,\\sigma^2), \\theta = (\\mu,\\sigma^2) \\in \\Theta = \\mathbb{R}\\times\\mathbb{R}_+\\). Encontre uma estatística suficiente para o modelo.\n\n22.3.3.2.1 Resposta\nA função densidade de probabilidade da amostra aleatória é \\[\n\\begin{aligned}\nf_\\theta^{\\boldsymbol{X}_n} (y_1,\\dots,y_n) &\\stackrel{\\mathrm{a.a.}}{=} \\prod_{i=1}^n f_\\theta(y_i), \\forall \\boldsymbol{y}_n \\in \\mathbb{R}^n \\\\\n&= \\prod_{i=1}^n \\left\\{\n\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\cdot \\mathrm{exp}\\left\\{-\\frac{1}{2\\sigma^2}(y_i-\\mu)^2\\right\\}\n\\right\\} \\\\\n&= \\frac{1}{(2\\pi\\sigma^2)^{\\frac{n}{2}}} \\cdot \\mathrm{exp} \\left\\{-\\frac{1}{2\\sigma^2}\\sum^n_{i=1}(y_1-\\mu)^2\\right\\}\n\\end{aligned}\n\\]\nNote que \\((y_1-\\mu)^2 =y_i^2 -2y_i\\mu + \\mu^2\\), logo,\n\\[\nf_\\theta^{\\boldsymbol{X}_n} = \\frac{1}{(2\\pi\\sigma^2)^{\\frac{n}{2}}} \\cdot \\mathrm{exp} \\left\\{\n-\\frac{1}{\\sigma^2}\\left(\n\\sum^n_{i=1} y_i^2-2\\mu\\sum^n_{i=1}y_i + n\\mu^2\n\\right)\n\\right\\}\n\\]\nTome \\(h(\\boldsymbol{y}_n) = \\frac{1}{(2\\pi\\sigma^2)}\\) e \\[\nm(t,\\theta) = \\frac{1}{(\\sigma^2)^{\\frac{n}{2}}}\\cdot \\mathrm{exp}\\left\\{\n-\\frac{1}{2\\sigma^2} \\left(t_2-2\\mu t_1-\\mu^2\\right)\n\\right\\}.\n\\]\nEm que \\(t=(t_1,t_2)\\) e \\(t_1 = \\sum^n_{i=1}y_i, t_2 = \\sum^n_{i=1}y^2_i\\)\nOu seja, \\(T(\\boldsymbol{X}_n) = \\left(\\sum^n_{i=1}X_i,\\sum^n_{i=1}X^2\\right)\\) é uma estatística suficiente para o modelo pois \\[\nf_\\theta^{\\boldsymbol{X}_n}(\\boldsymbol{y}_n) \\cdot m(T(\\boldsymbol{X}_n),\\theta)\\ \\mathrm{q.c.}\\ \\forall \\theta \\in \\Theta.\n\\]\n\n\n\n22.3.3.3 Exemplo c\nSeja \\(\\boldsymbol{X}_n\\) amostra aleatória de \\(X\\sim\\mathrm{Unif}(0,\\theta), \\theta = (a,b) \\in \\Theta = \\mathbb{R}_+\\). Encontre uma estatística suficiente para o modelo.\n\n22.3.3.3.1 Respostas\nA função densidade de probabilidade da amostra aleatória\n\\[\nf_\\theta^{\\boldsymbol{X}_n}(y_1,\\dots,y_n) \\stackrel{a.a.}{=}\n\\prod^n_{i=1} f_\\theta(y_1) \\forall \\boldsymbol{y}_n \\in \\mathbb{R}^n\n\\]\nNote que \\[\nf_\\theta (x)= \\left\\{\\begin{array}{ll}\n\\frac{1}{\\theta}, & x \\in (0,\\theta] \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right. = \\frac{1}{\\theta}\\mathbb{1}(x)_{(0,\\theta]}\n\\]\nLogo, \\[\nf_\\theta^{\\boldsymbol{X}_n}(\\boldsymbol{y}_n) \\stackrel{\\mathrm{Unif}}{=} \\prod^n_{i=1}\n\\frac{1}{\\theta} \\mathbb{1}_{(0,\\theta]}(y_i) \\\\\n= \\frac{1}{\\theta^n} \\cdot \\prod^n_{i=1} \\mathbb{1}_{(0,\\theta]}(y_i)\n\\]\nNote que \\[\n\\prod^n_{i=1} \\mathbb{1}_{(0,\\theta]}(y_i)=1 \\Leftrightarrow\n\\left\\{\\begin{array}{ll}\n0 &lt; y_1 \\leq \\theta \\\\\n\\vdots \\\\\n0 &lt; y_n \\leq \\theta\n\\end{array}\\right. \\Leftrightarrow\n\\left\\{\\begin{array}{ll}\n\\min(\\boldsymbol{y}_n) &gt; 0 \\\\\n\\max(\\boldsymbol{y}_n) \\leq \\theta\n\\end{array}\\right.\n\\]\nLogo \\[\n\\prod^n_{i=1}\\mathbb{1}_{(0,\\theta]}(y_i) = 1 \\Leftrightarrow\n\\mathbb{1}_{(0,\\infty)}(\\min(\\boldsymbol{y}_n)) \\cdot\n\\mathbb{1}_{(0,\\theta]}(\\max(\\boldsymbol{y}_n)) = 1\n\\]\ne\n\\[\nf_\\theta^{\\boldsymbol{X}_n}(\\boldsymbol{y}_n) = \\frac{1}{\\theta^n}\n\\mathbb{1}_{(0,\\infty)}(\\min(\\boldsymbol{y}_n)) \\cdot\n\\mathbb{1}_{(0,\\theta]}(\\max(\\boldsymbol{y}_n))\n\\]\nTome \\(h(\\boldsymbol{y}_n) = \\mathbb{1}_{(0,\\infty)}(\\min(\\boldsymbol{y}_n))\\) e \\(m(t,\\theta) = \\frac{1}{\\theta^n} \\mathbb{1}_{(0,\\theta]}(t)\\), em que \\(t=\\max(\\boldsymbol{y}_n)\\)\nPortanto, pelo critério da fatoração, \\(T(\\boldsymbol{X}_n) = \\max(X_1,\\dots,X_n)\\) é uma estatística suficiente para o modelo em questão.",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Estatísticas Suficientes</span>"
    ]
  },
  {
    "objectID": "estatisticas-suficientes.html#teorema-invariância-da-estatística-suficiente",
    "href": "estatisticas-suficientes.html#teorema-invariância-da-estatística-suficiente",
    "title": "22  Estatísticas Suficientes",
    "section": "22.4 Teorema (“Invariância” da estatística suficiente)",
    "text": "22.4 Teorema (“Invariância” da estatística suficiente)\nSeja \\(T(\\boldsymbol{X}_n)\\) uma estatística suficiente para o modelo estatístico. Então \\[\nG(\\boldsymbol{X}_n) = s(T(\\boldsymbol{X}_n))\n\\] é uma estatistica suficiente se \\(s(\\cdot)\\) for bijetora (só precisa ser injetora)\n\n22.4.1 Prova\nComo \\(T(\\boldsymbol{X}_n)\\) é suficiente para o modelo temos, pelo critério da fatoração, que \\[\nf_\\theta^{\\boldsymbol{X}_n}(\\boldsymbol{y}_n) = h(\\boldsymbol{y}_n) \\cdot m(T(\\boldsymbol{y}_n),\\theta)\\ \\mathrm{q.c.}\\ \\forall \\theta \\in \\Theta\n\\]\nComo \\(s(\\cdot)\\) é bijetora, temos que sua inversa existe: \\[\nT(\\boldsymbol{X}_n) = s^{-1}(G(\\boldsymbol{X}_n)) \\Rightarrow\nT(\\boldsymbol{y}_n) = s^{-1}(G(\\boldsymbol{y}_n))\n\\]\nPortanto, \\[\nf_\\theta^{\\boldsymbol{X}_n} (\\boldsymbol{y}_n)= h(\\boldsymbol{y}_n) \\cdot m(s^{-1}(G(\\boldsymbol{X}_n)), \\theta)\\ \\mathrm{q.c.}\\ \\forall \\theta \\in \\Theta\n\\]\nTome \\(m^*(\\cdot,\\theta) = m(s^{-1}(\\cdot),\\theta)\\). Substituindo, temos que\n\\[\nf_\\theta^{\\boldsymbol{X}_n} (\\boldsymbol{y}_n)= h(\\boldsymbol{y}_n) \\cdot m^*((G(\\boldsymbol{X}_n)), \\theta)\\ \\mathrm{q.c.}\\ \\forall \\theta \\in \\Theta\n\\]\nLogo, pelo critério da fatoração, \\(G(\\boldsymbol{X}_n)\\) é suficiente para o modelo estatístico.\n\n\n22.4.2 Exemplos\n\nSe \\(T(\\boldsymbol{X}_n) = \\sum^n_{i=1}X_i\\) é suficiente para o modelo, então:\n\n\n\\(G(\\boldsymbol{X}_n)=\\frac{1}{n}\\sum^n_{i=1}X_i\\) é suficiente para o modelo;\n\\(G(\\boldsymbol{X}_n)=\\mathrm{e}^{\\sum^n_{i=1}X_i}\\) é suficiente para o modelo;\nPara \\(\\sum X_i \\neq 0\\ \\mathrm{q.c.}\\), \\(G(\\boldsymbol{X}_n)=\\frac{1}{\\sum^n_{i=1}X_i}\\) é suficiente para o modelo;\n\\(G(\\boldsymbol{X}_n)=\\left(\\sum^n_{i=1}X_i\\right)^2\\) não é necessariamente suficiente para o modelo uma vez que \\(f(x) = x^2\\) não é injetora.\n\n\nSe \\(T(\\boldsymbol{X}_n)\\) for suficiente para o modelo estatístico, então\n\n\n\\(G(\\boldsymbol{X}_n) = (T(\\boldsymbol{X}_n), T(\\boldsymbol{X}_n))\\) é suficiente para o modelo;\n\\(G(\\boldsymbol{X}_n) = (T(\\boldsymbol{X}_n), X_1)\\) é suficiente para o modelo;\n\\(G(\\boldsymbol{X}_n) = (T(\\boldsymbol{X}_n), \\boldsymbol{X}_n)\\) é suficiente para o modelo pois \\(\\boldsymbol{X}_n\\) é suficiente para o modelo;\n\\(G(\\boldsymbol{X}_n) = (X_1\\dots,X_n)\\) “quase” nunca será suficiente para o modelo;\n\\(G(\\boldsymbol{X}_n) = (\\boldsymbol{X}_n,X_1)\\) é suficiente para o modelo.\nA amostra ordenada \\(X_{(1)} \\leq \\dots \\leq X_{(n)}\\), denotada por \\(T^*(\\boldsymbol{X}_n)=(X_{(1)},\\dots,X_{(n)})\\) é uma estatística suficiente para o modelo pelo critério da fatoração, no caso de variáveis aleatórias independentes e identicamente distribuídas. \\(T^*(\\boldsymbol{X}_n)\\) é dita ser a estatística de ordem;\n\n\n\n22.4.3 Corolário útil (Relação das suficientes com a FE)\nSe \\(f_\\theta\\) pertencer à família exponencial k-dimensional, então \\(T(\\boldsymbol{X}_n) = \\left(\\sum^n_{i=1}T_1(X_i),\\dots,\\sum^n_{i=1}T_k(X_i)\\right)\\) é uma estatística suficiente para o modelo.\n\n22.4.3.1 Prova:\n\\[\n\\begin{aligned}\nf_\\theta^{\\boldsymbol{X}_n}(\\boldsymbol{y}_n) &\\stackrel{\\mathrm{iid}}{=}\n\\prod^n_{i=1} f_\\theta(y_i) = \\prod^n_{i=1}\\left\\{\n\\mathrm{exp}\\left\\{\n\\sum_{j=1}^n c_j(\\theta)T_j(y_i)+d(\\theta)+S(y_i)\\cdot\\mathbb{1}_{\\mathfrak{X}}(y_i)\n\\right\\}\n\\right\\} \\\\\n&= \\mathrm{exp}\\left\\{\n\\sum^n_{j=1} c_j(\\theta) \\cdot \\sum^n_{i}T_j(y_i) + nd(\\theta) + \\sum^n_{i=1}S(y_i) \\cdot \\prod^n_{i=1}\\mathbb{1}_{\\mathfrak{X}}(y_i)\n\\right\\}\n\\end{aligned}\n\\]\nTome \\(h(\\boldsymbol{y}_n) = \\prod \\mathbb{1}_{\\mathfrak{X}}(y_i)\\cdot \\mathrm{e}^{\\sum S(y_i)}\\), \\(m(t,\\theta) = \\mathrm{exp}\\left\\{c_1(\\theta)t_1+\\dots+c_k(\\theta)t_k +nd(\\theta)\\right\\}\\), em que \\(t=(t_1,\\dots,t_k)\\) e \\[\n\\begin{aligned}\nt_1 &= \\sum T_1(y_i) \\\\\n\\vdots \\\\\nt_k & = \\sum T_k(y_i)\n\\end{aligned}\n\\]\nPelo critério da fatoração, \\[\nT(\\boldsymbol{X}_n) = \\left(\\sum^n_{i=1} T_1(X_i),\\dots,\\sum^n_{i=1}T_k(X_i)\\right)\n\\]\nIsso é útil para encontrar estatísticas completas e suficientes pelo Teorema das FEs.",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Estatísticas Suficientes</span>"
    ]
  },
  {
    "objectID": "estatisticas-suficientes.html#sec-sm",
    "href": "estatisticas-suficientes.html#sec-sm",
    "title": "22  Estatísticas Suficientes",
    "section": "22.5 Estatísticas Suficientes Minimais (SM)",
    "text": "22.5 Estatísticas Suficientes Minimais (SM)\nDizemos que \\(T(\\boldsymbol{X}_n)\\) é uma estatística suficiente minimal para o modelo se, e somente se:\n\n\\(T(\\boldsymbol{X}_n)\\) é suficiente para o modelo\nPara qualquer outra estatística suficiente \\(U(\\boldsymbol{X}_n)\\), existe uma função \\(H\\) tal que \\[\nT(\\boldsymbol{X}_n) = H(U(\\boldsymbol{X}_n)),\\ \\mathrm{q.c.}\n\\]\n\nObs: A \\(\\sigma\\)-álgebra associada à estatística suficiente minimal é a menor \\(\\sigma\\)-álgebra dentre aquelas associadas às estatísticas suficientes.\n\n22.5.1 Teorema (1 das estatísticas SM)\nSeja \\(\\boldsymbol{X}_n = (X_1,\\dots,X_n)\\) de \\(X\\sim f_\\theta, \\theta \\in \\Theta_0=\\{\\theta_0,\\theta_1,\\dots,\\theta_p\\}\\) em que \\(\\mathfrak{X} = \\{x:f_\\theta(x)&gt;0\\}\\) não depende de “\\(\\theta\\)”. Então, \\[\nT(\\boldsymbol{x}) = \\left(\n\\frac{f_{\\theta_1}^{\\boldsymbol{X}}(\\boldsymbol{x})}{f_{\\theta_0}^{\\boldsymbol{X}_n}(\\boldsymbol{x})}, \\dots,\n\\frac{f_{\\theta_p}^{\\boldsymbol{X}}(\\boldsymbol{x})}{f_{\\theta_0}^{\\boldsymbol{X}_n}(\\boldsymbol{x})}\n\\right)\n\\] em que \\(T:\\mathfrak{X} \\rightarrow \\mathbb{R}^p\\) é uma estatística suficiente minimal (\\(T(\\boldsymbol{X}_n)\\)) para o modelo estatístico.\nIsso trata de razões entre funções verossimilhança.\n\n22.5.1.1 Prova\nNote que, \\(\\forall \\boldsymbol{y}_n \\in \\mathfrak{X}\\), temos que \\[\nf_{\\theta_j}^{\\boldsymbol{X}_n}(\\boldsymbol{y}_n) =\nf_{\\theta_0}^{\\boldsymbol{X}_n}(\\boldsymbol{y}_n) \\cdot\n\\frac{f_{\\theta_j}^{\\boldsymbol{X}_n}(\\boldsymbol{y}_n)}{f_{\\theta_0}^{\\boldsymbol{X}_n}(\\boldsymbol{y}_n)}\n\\] Tome \\(h(\\boldsymbol{y}_n) = f_{\\theta_0}^{\\boldsymbol{X}_n}(\\boldsymbol{y}_n)\\), não depende dos diferentes valores de “\\(\\theta\\)” e\n\\[\nm(T(\\boldsymbol{X}_n), \\theta) =\n\\left\\{\\begin{array}{ll}\nT_1(\\boldsymbol{y}_n), & \\theta = \\theta_1 \\\\\nT_2(\\boldsymbol{y}_n), & \\theta = \\theta_2 \\\\\n\\vdots \\\\\nT_p(\\boldsymbol{y}_n), & \\theta = \\theta_p \\\\\n\\end{array}\\right.\n\\]\nEm que \\(T(\\boldsymbol{x}) = (T_1(\\boldsymbol{x}), \\dots, T_p(\\boldsymbol{x})\\) e \\(T_j = \\frac{f_{\\theta_j}^{\\boldsymbol{X}_n}(\\boldsymbol{y}_n)}{f_{\\theta_0}^{\\boldsymbol{X}_n}(\\boldsymbol{y}_n)}\\).\nLogo, \\(T(\\boldsymbol{X}_n)\\) é suficiente para o modelo pelo critério da fatoração.\nSeja \\(U(\\boldsymbol{X}_n)\\) uma estatística suficiente para o modelo. Então pelo critério da fatoração,\n\\[\nf_\\theta^{\\boldsymbol{X}_n}(\\boldsymbol{y}_n) = h'(\\boldsymbol{y}_n) \\cdot m'(U(\\boldsymbol{y}_n),\\theta)\\ \\mathrm{q.c.}\\ \\forall \\theta \\in \\Theta_0\n\\]\nObserve que \\(\\forall \\boldsymbol{y}_n \\in \\mathfrak{X}\\), \\[\n\\frac{f_{\\theta_j}^{\\boldsymbol{X}_n}(\\boldsymbol{y}_n)}{f_{\\theta_0}^{\\boldsymbol{X}_n}(\\boldsymbol{y}_n)} =\n\\frac{h'(\\boldsymbol{y}_n)\\cdot m'(U(\\boldsymbol{y}_n),\\theta_j)}{h'(\\boldsymbol{y}_n)\\cdot m'(U(\\boldsymbol{y}_n),\\theta_0)}\\ \\mathrm{q.c.}\\ \\forall \\theta \\in \\Theta_0\n\\]\nLogo,\n\\[\nT_j(\\boldsymbol{y}_n) =\n\\frac{f_{\\theta_j}^{\\boldsymbol{X}_n}(\\boldsymbol{y}_n)}{f_{\\theta_0}^{\\boldsymbol{X}_n}(\\boldsymbol{y}_n)} =\n\\frac{m'(U(\\boldsymbol{y}_n),\\theta_j)}{m'(U(\\boldsymbol{y}_n),\\theta_0)},\\ j=1,\\dots,p\n\\]\nPortanto, existe \\(H\\) tal que \\[\nT_j(\\boldsymbol{X}_n) = H(U(\\boldsymbol{X}_n))\n\\] basta tomar \\[\nH(u) = \\left(\n\\frac{m'(u,\\theta_1)}{m'(u,\\theta_0)}, \\dots,\n\\frac{m'(u\\theta_p)}{m'(u,\\theta_0)}\n\\right)\n\\]\n\n\n22.5.1.2 Exemplo (Bernoulli)\nSeja \\(\\boldsymbol{X}_n = (X_1,\\dots,X_n)\\) amostra aleatória de \\(X\\sim\\mathrm{Ber}(\\theta), \\theta \\in \\Theta = \\{0.1,0,5\\}\\). Encontre uma estatística suficiente minimal.\n\n22.5.1.2.1 Resposta\nPelo teorema anterior, \\[\nT(\\boldsymbol{y}_n) = \\frac{f_{\\theta_1}^{\\boldsymbol{X}_n}(\\boldsymbol{y}_n)}{f_{\\theta_0}^{\\boldsymbol{X}_n}(\\boldsymbol{y}_n)}\n\\] é suficiente minimal, em que \\(\\theta_0=0.1,\\theta_1=0.5\\) e\n\\[\n\\begin{aligned}\nf_{0.1}^{\\boldsymbol{X}_n} (\\boldsymbol{y}_n) &= 0.1^{\\sum y_i} \\cdot 0.9^{n-\\sum y_i} \\\\\nf_{0.5}^{\\boldsymbol{X}_n} (\\boldsymbol{y}_n) &= 0.5^{\\sum y_i} \\cdot 0.5^{n-\\sum y_i} \\\\\n&\\forall \\boldsymbol{y}_n \\in \\mathfrak{X} = \\{0,1\\}^n\n\\end{aligned}\n\\] Logo \\[\n\\begin{aligned}\nT(\\boldsymbol{y}_n) &= \\frac{0.5^n}{0.1^{\\sum y_i} \\cdot 0.9^{n-\\sum y_i}} \\\\\n&= \\frac{0.5^n}{\\left(\\frac{0.1}{0.9}\\right)^{\\sum y_i} \\cdot 0.9^{\\sum y_i}} \\\\\n&= 9^{\\sum y_i} \\cdot \\left(\\frac{5}{9}\\right)^n\n\\end{aligned}\n\\]\nNote que \\(T(\\boldsymbol{y}_n)\\) é função 1:1 de \\(T'(\\boldsymbol{y}_n) = \\sum^n_{i=1} y_i\\). Logo, \\[\nT'(\\boldsymbol{X}_n) = \\sum^n_{i=1} X_i\n\\] é também uma estatística suficiente minimal para o modelo.\n\n\n\n22.5.1.3 Exemplo (Normal)\nSeja \\(\\boldsymbol{X}_n = (X_1,\\dots,X_n)\\) amostra aleatória de \\(X\\sim\\mathrm{N}(\\mu,\\sigma^2), \\theta = (\\mu,\\sigma^2) \\in \\Theta = \\{(0,1),(1,1),(0,2)\\}\\). Encontre uma estatística suficiente minimal.\n\n22.5.1.3.1 Resposta\nPelo teorema, \\[\nT(\\boldsymbol{y}_n) = \\left(\n\\frac{f_{\\theta_1}^{\\boldsymbol{X}_n}(\\boldsymbol{y}_n)}{f_{\\theta_0}^{\\boldsymbol{X}_n}(\\boldsymbol{y}_n)},\n\\frac{f_{\\theta_2}^{\\boldsymbol{X}_n}(\\boldsymbol{y}_n)}{f_{\\theta_0}^{\\boldsymbol{X}_n}(\\boldsymbol{y}_n)}\n\\right)\n\\]\nTomando \\(\\theta_0 = (0,1), (1,1), (0,2)\\), temos\n\\[\n\\begin{aligned}\nf_{\\theta_0}^{\\boldsymbol{X}_n}(\\boldsymbol{y}_n) &=\n\\frac{\\mathrm{exp}\\left\\{-\\frac{1}{2}\\sum y^2_i\\right\\}}{(\\sqrt{2\\pi})^n} \\\\\nf_{\\theta_1}^{\\boldsymbol{X}_n}(\\boldsymbol{y}_n) &=\n\\frac{\\mathrm{exp}\\left\\{-\\frac{1}{2}(\\sum y^2_i - 2\\sum y_i + n)\\right\\}}{(\\sqrt{2\\pi})^n} \\\\\nf_{\\theta_2}^{\\boldsymbol{X}_n}(\\boldsymbol{y}_n) &=\n\\frac{\\mathrm{exp}\\left\\{-\\frac{1}{4}\\sum y^2_i \\right\\}}{(\\sqrt{4\\pi})^n} \\\\\n\\end{aligned}\n\\]\nPortanto, \\[\n\\begin{aligned}\nT(\\boldsymbol{y}_n) &= \\left(\n\\mathrm{exp}\\left\\{-\\frac{1}{2}(n-2\\sum y_i)\\right\\},\n\\frac{\\mathrm{exp}\\left\\{-\\frac{1}{4}\\sum y_i^2 + \\frac{1}{2}\\sum y_i^2)\\right\\}}{2^{\\frac{n}{2}}},\n\\right)\n\\\\\n&= \\left(\n\\mathrm{exp}\\left\\{-\\frac{1}{2}(n-2\\sum y_i)\\right\\},\n\\frac{\\mathrm{exp}\\left\\{\\frac{1}{4}\\sum y_i^2)\\right\\}}{2^{\\frac{n}{2}}},\n\\right)\n\\end{aligned}\n\\] dessa forma, \\(T(\\boldsymbol{X}_n)\\) é SM para o modelo.\nNote que \\(T(\\boldsymbol{y}_n)\\) é função 1:1 de \\((\\sum y_i, \\sum y_i^2)\\). Portanto, \\[\nT'(\\boldsymbol{X}_n) = \\left(\n\\sum^n_{i=1} X_i, \\sum^n_{i=1} X_i^2\n\\right)\n\\] é também SM para o modelo.\n\n\n\n\n22.5.2 Teorema (2 das estatísticas SM)\nSeja \\(\\boldsymbol{X}_n=(X_1,\\dots,X_n)\\) amostra aleatória de \\(X\\sim f_\\theta, \\theta \\in \\Theta\\). Considere \\(\\Theta_0 \\subseteq \\Theta\\) não vazio. Então, se \\(T(\\boldsymbol{X}_n)\\) for SM para o modelo reduzido a \\(\\Theta_0\\) e suficiente para \\(\\Theta\\), então será suficiente minimal para \\(\\Theta\\).\n\n22.5.2.1 Prova\nComo \\(T(\\boldsymbol{X}_n)\\) é SM para o modelo restrito a \\(\\Theta_0 \\subseteq \\Theta\\), para qualquer estatística \\(U(\\boldsymbol{X}_n)\\) suficiente para o modelo restrito a \\(\\Theta_0\\), existe \\(H\\) tal que\n\\[\nT(\\boldsymbol{X}_n) = H(U(\\boldsymbol{X}_n))\\ \\mathrm{q.c.}\n\\]\nObserve que todas as estatísticas suficientes para o modelo completo \\(\\Theta\\) são também suficientes para os modelos restritos. Como \\(T(\\boldsymbol{X}_n)\\) também é, por hipótese, suficiente para o modelo completo, então é função de qualquer estatística suficiente minimal para o modelo completo.\n\n\n22.5.2.2 Exemplo (Bernoulli)\nSeja \\(\\boldsymbol{X}_n = (X_1,\\dots,X_n)\\) amostra aleatória de \\(X\\sim\\mathrm{Ber}(\\theta), \\theta \\in \\Theta = (0,1)\\). Mostre que \\(T(\\boldsymbol{X}_n) = \\sum X_i\\) é uma estatística suficiente minimal para o modelo.\n\n22.5.2.2.1 Resposta\nTome \\(\\Theta_0 = \\{0.1,0.5\\} \\subseteq \\Theta\\). Já mostramos que \\(T'(\\boldsymbol{X}_n) = \\sum^n_{i=1} X_i\\) é suficiente minimal para o modelo reduzido a \\(\\Theta_0\\). Além disso, \\(T'(\\boldsymbol{X}_n) = \\sum X_i\\) é suficiente para o modelo completo. Logo, pelo Teorema 2 para estatísticas suficientes minimais, concluímos que \\(T'(\\boldsymbol{X}_n) = \\sum X_i\\) é também SM para o modelo completo.\n\n\n\n22.5.2.3 Exemplo (Normal)\nSeja \\(\\boldsymbol{X}_n = (X_1,\\dots,X_n)\\) amostra aleatória de \\(X\\sim\\mathrm{N}(\\mu,\\sigma^2), \\theta = (\\mu,\\sigma^2) \\in \\Theta = \\mathbb{R}\\times\\mathbb{R}^+\\). Mostre que \\(T(\\boldsymbol{X}_n) = (\\sum X_i, \\sum X_i^2)\\) é suficiente minimal para o modelo.\n\n22.5.2.3.1 Resposta correta\nTome \\(\\Theta_0 = \\{(0,1),(1,1),(0,2)\\}\\). Já mostramos que essa estatística é suficiente minimal para o modelo reduzido a \\(\\Theta_0\\). Além disso, já mostramos (lista) que é suficiente para o modelo completo. Logo, pelo teorema 2, é SM para o modelo completo.\n\n\n22.5.2.3.2 Tentativa alternativa (frustrada)\nTome \\(\\Theta_0 = \\{(0,1),(1,1)\\}\\). Pelo teorema 1, temos que \\[\nT(\\boldsymbol{X}_n) = \\frac{f_{\\theta_1}^{\\boldsymbol{X}_n}(\\boldsymbol{X}_n)}{f_{\\theta_0}^{\\boldsymbol{X}_n}(\\boldsymbol{X}_n)}\n\\] é SM para o modelo reduzido.\n\\[\nT(\\boldsymbol{X}_n) = \\frac{\\frac{1}{(2\\pi)^{\\frac{n}{2}}}\\mathrm{exp}\\left\\{-\\frac{1}{2} (\\sum X_i^2 -2\\sum X_i + n)\\right\\}}\n{\\frac{1}{(2\\pi)^{\\frac{n}{2}}}\\mathrm{exp}\\left\\{-\\frac{1}{2} \\sum X_i^2\\right\\}} = \\mathrm{e}^{\\sum X_i -\\frac{1}{2} n}\n\\]\nComo \\(\\sum X_i\\) é uma função 1:1 da estatística, temos que \\(\\sum X_i\\) é também SM para o modelo reduzido. Entretanto, \\(\\sum X_i\\) não é suficiente para o modelo completo.\n\n\n22.5.2.3.3 Tentativa 2\nTome \\(\\Theta_0 = \\{(0,1),(1,1), (2,1)\\}\\). Então, pelo teorema 1, temos que\n\\[\nT(\\boldsymbol{X}_n) = \\left(\\frac{f_{\\theta_1}^{\\boldsymbol{X}_n}(\\boldsymbol{X}_n)}{f_{\\theta_0}^{\\boldsymbol{X}_n}(\\boldsymbol{X}_n)},\n\\frac{f_{\\theta_2}^{\\boldsymbol{X}_n}(\\boldsymbol{X}_n)}{f_{\\theta_0}^{\\boldsymbol{X}_n}(\\boldsymbol{X}_n)}\\right) \\stackrel{\\text{Tent. Ant.}}{=}\n\\left(\n\\mathrm{e}^{\\sum X_i - \\frac{1}{2}n},\n\\frac{f_{\\theta_2}^{\\boldsymbol{X}_n}(\\boldsymbol{X}_n)}{f_{\\theta_0}^{\\boldsymbol{X}_n}(\\boldsymbol{X}_n)}\\right)\n\\]\nTemos que\n\\[\n\\frac{f_{\\theta_2}^{\\boldsymbol{X}_n}(\\boldsymbol{X}_n)}{f_{\\theta_0}^{\\boldsymbol{X}_n}(\\boldsymbol{X}_n)} =\n\\frac{\\frac{1}{(2\\pi)^{\\frac{n}{2}}}\\mathrm{exp}\\left\\{-\\frac{1}{2} (\\sum X_i^2 -4\\sum X_i + 4n)\\right\\}}\n{\\frac{1}{(2\\pi)^{\\frac{n}{2}}}\\mathrm{exp}\\left\\{-\\frac{1}{2} \\sum X_i^2\\right\\}} = \\mathrm{e}^{2\\sum X_i -2 n}\n\\] Logo, \\[\nT(\\boldsymbol{X}_n) =\n\\left(\n\\mathrm{e}^{\\sum X_i - \\frac{1}{2}n},\n\\mathrm{e}^{2\\sum X_i -2 n}\\right)\n\\]\nComo \\(\\sum X_i\\) é função 1:1 de \\(T(\\boldsymbol{X}_n)\\), temos que é SM para o modelo reduzido a \\(\\Theta_0\\). Entretanto, não é suficiente para o modelo completo.\nObserve que o espaço paramétrico do modelo completo é \\(\\mathbb{R}\\times\\mathbb{R}^+\\)\n\n\n\n22.5.2.4 Exemplo (Uniforme)\nSeja \\(\\boldsymbol{X}_n=(X_1,\\dots,X_n)\\) amostra aleatória de \\(X\\sim U(0,\\theta), \\theta \\in \\Theta = (0,\\infty)\\). Verifique se \\(\\boldsymbol{X}_{(n)} = \\max\\{X_1,\\dots,X_n\\}\\) é SM para o modelo.\n\n22.5.2.4.1 Resposta\nJá sabemos que é uma estatística suficiente para o modelo. Com o suporte \\(\\mathfrak{X}_\\theta = (0,\\theta]\\) depende de “\\(\\theta\\)”, não podemos usar o teorema anterior. Precisamos mostrar que, para qualquer estatística suficiente para o modelo \\(U(\\boldsymbol{X}_n)\\) existe \\(H(\\cdot)\\) tal que\n\\[\nX_{(n)} = H(U(\\boldsymbol{X}_n))\\ \\mathrm{q.c.}\n\\]\nSeja \\(f_\\theta^{\\boldsymbol{X}_n}\\) a função densidade probabilidade da amostra aleatória \\[\n\\begin{aligned}\nf_\\theta^{\\boldsymbol{X}_n}(\\boldsymbol{y}_n) &= \\frac{1}{\\theta^n} \\prod \\mathbb{1}_{(0,\\theta]}(y_i)  \\\\\n&= \\frac{1}{\\theta^n} \\mathbb{1}_{(0,\\infty)} (\\min\\{y_1,\\dots,y_n\\}) \\cdot \\mathbb{1}_{(0,\\theta]}(\\max\\{y_1,\\dots,y_n\\})\n\\end{aligned}\n\\]\n\nNote que podemos escrever \\[\n\\max\\{y_1,\\dots,y_n\\} = \\inf\\left\\{ \\theta \\in (0,\\infty) : f_\\theta^{\\boldsymbol{X}_n}(\\boldsymbol{y}_n) &gt; 0\\right\\}\n\\]\nLogo, a estatística \\(X_{(n)}\\) pode ser reescrita por \\[\nX_{(n)} = \\inf\\left\\{\\theta \\in (0,\\infty) : f^{\\boldsymbol{X}_n}_\\theta (\\boldsymbol{X}_n) &gt; 0\\right\\}\n\\]\nSeja \\(U(\\boldsymbol{X}_n)\\) uma estatística suficiente qualquer. Então, pelo CF, temos que existem \\(h'\\) e \\(m'\\) tais que \\[\nf_\\theta{\\boldsymbol{y}_n}^{\\boldsymbol{X}_n} = h'(\\boldsymbol{y}_n) \\cdot m'(U(\\boldsymbol{y}_n), \\theta),\\ \\mathrm{q.c.}\\ \\forall \\theta &gt;0\n\\]\nLogo, \\[\nX_{(n)} = \\inf\\left\\{\\theta \\in (0,\\infty) : h'(\\boldsymbol{X}_n) \\cdot m'(U(\\boldsymbol{X}_n),\\theta) &gt; 0\\right\\} \\mathrm{q.c.}\n\\]\nComo \\(h'(\\boldsymbol{y}_n)&gt; 0\\ \\mathrm{q.c.}\\) temos que\n\\[\nX_{(n)} = \\inf\\left\\{\\theta \\in (0,\\infty) : m'(U(\\boldsymbol{X}_n),\\theta)&gt;0 \\right\\} \\mathrm{q.c.}\n\\]\nPortanto, \\(X_{(n)}\\) é suficiente minimal para o modelo, pois existe \\(H(\\cdot)\\) tal que \\[\nX_{(n)} = H(U(\\boldsymbol{X}_n))\\ \\mathrm{q.c.}\n\\]\nBasta tomar \\[\nH(u) = \\inf\\left\\{\n\\theta \\in \\Theta : m'(u,\\theta)&gt; 0\n\\right\\}\n\\]\n\n\n\n\n\n\nObservação\n\n\n\n\\[\nf_\\theta(x) = \\mathrm{e}^{-\\theta} \\cdot \\mathbb{1}_{(0,\\theta)}(x)\n\\]\nDefina \\(g:\\mathbb{R}^+\\rightarrow\\mathbb{R}\\) \\[\n\\begin{aligned}\ng(u) &= \\inf\\left\\{\\theta \\in (0,\\infty) : f_\\theta(u) &gt; 0\\right\\}\\\\\n&= \\inf\\left\\{\\theta \\in (0,\\infty) : \\mathrm{e}^{-\\theta}\\cdot \\mathbb{1}_{(0,\\theta]}(u) &gt; 0\\right\\}\\\\\n\\Rightarrow g(u) &= u\n\\end{aligned}\n\\]\n\n\n\n\n\n22.5.2.5 Exemplo (Normal Curvada)\nSeja \\(\\boldsymbol{X}_n = (X_1,\\dots,X_n)\\) amostra aleatória de \\(X\\sim\\mathrm{N}(\\theta,\\theta^2), \\theta \\in \\Theta = \\mathbb{R}^+\\). Encontre uma estatística SM para o modelo.\n\n22.5.2.5.1 Resposta\nSabemos que \\((\\sum X_i, \\sum X_i^2)\\) é suficiente para o modelo completo\nTome \\(\\Theta_0 = \\{1,2,3\\}\\). Pelo Teorema 1, temos que: \\[\nT'(\\boldsymbol{X}_n) = \\left(\\frac{f_{\\theta_1}^{\\boldsymbol{X}_n}(\\boldsymbol{X}_n)}{f_{\\theta_0}^{\\boldsymbol{X}_n}(\\boldsymbol{X}_n)},\n\\frac{f_{\\theta_2}^{\\boldsymbol{X}_n}(\\boldsymbol{X}_n)}{f_{\\theta_0}^{\\boldsymbol{X}_n}(\\boldsymbol{X}_n)}\\right)\n\\] em que \\(\\theta_0=1,\\theta_1=2,\\theta_2=3\\). Dessa forma, \\[\n\\begin{aligned}\nf_{\\theta_0}^{\\boldsymbol{X}_n} &= \\frac{\\mathrm{exp}\\left\\{-\\frac{1}{2}\\sum(X_i-1)^2\\right\\}}{(\\sqrt{2\\pi})^n}\n= \\frac{\\mathrm{exp}\\left\\{-\\frac{1}{2}(\\sum X_i^2 - 2 \\sum X_i +n)\\right\\}}{(\\sqrt{2\\pi})^n} \\\\\nf_{\\theta_1}^{\\boldsymbol{X}_n} &= \\frac{\\mathrm{exp}\\left\\{-\\frac{1}{2\\cdot 4}(\\sum X_i^2 - 2\\cdot 2 \\cdot \\sum X_i +4n)\\right\\}}{(\\sqrt{2\\pi\\cdot4})^n} \\\\\nf_{\\theta_2}^{\\boldsymbol{X}_n} &= \\frac{\\mathrm{exp}\\left\\{-\\frac{1}{2\\cdot 9}(\\sum X_i^2 - 2\\cdot 3 \\cdot \\sum X_i +9n)\\right\\}}{(\\sqrt{2\\pi\\cdot9})^n}.\n\\end{aligned}\n\\] Portanto, \\[\n\\begin{aligned}\nT'(\\boldsymbol{X}_n) &= \\left(\n\\frac{\\frac{\\mathrm{exp}\\left\\{-\\frac{1}{2}(\\sum X_i^2 - 2 \\sum X_i +n)\\right\\}}{(\\sqrt{2\\pi})^n}}\n{\\frac{\\mathrm{exp}\\left\\{-\\frac{1}{2\\cdot 4}(\\sum X_i^2 - 2\\cdot 2 \\cdot \\sum X_i +4n)\\right\\}}{(\\sqrt{2\\pi\\cdot4})^n}},\n\\frac{\\frac{\\mathrm{exp}\\left\\{-\\frac{1}{2}(\\sum X_i^2 - 2 \\sum X_i +n)\\right\\}}{(\\sqrt{2\\pi})^n}}\n{\\frac{\\mathrm{exp}\\left\\{-\\frac{1}{2\\cdot 9}(\\sum X_i^2 - 2\\cdot 3 \\cdot \\sum X_i +9n)\\right\\}}{(\\sqrt{2\\pi\\cdot9})^n}}\n\\right) \\\\\n&=\\left(\n\\frac{\\mathrm{exp}\\left\\{-\\frac{1}{2}\\left(\n\\frac{1}{4} \\sum X_i^2 - \\sum X_i^2 - 2(\\frac{2}{4} \\sum X_i - \\sum X_i)\n\\right)\n\\right\\}}{4^{\\frac{n}{2}}},\\right. \\\\ % o \\\\ só funciona com o left e right.\n&\\left.\\frac{\\mathrm{exp}\\left\\{-\\frac{1}{2}\\left(\n\\frac{1}{9} \\sum X_i^2 - \\sum X_i^2 - 2(\\frac{3}{9} \\sum X_i - \\sum X_i)\n\\right)\n\\right\\}}{9^{\\frac{n}{2}}}\n\\right) \\\\\n&=\\left(\n\\frac{\\mathrm{exp}\\left\\{-\\frac{1}{2}\\left(\n-\\frac{3}{4} \\sum X_i^2 - 2(-\\frac{1}{2} \\sum X_i)\n\\right)\n\\right\\}}{4^{\\frac{n}{2}}},\\right. \\\\\n&\\left.\\frac{\\mathrm{exp}\\left\\{-\\frac{1}{2}\\left(\n-\\frac{8}{9} \\sum X_i^2 - 2(\\frac{6}{9} \\sum X_i)\n\\right)\n\\right\\}}{9^{\\frac{n}{2}}}\n\\right) \\\\\n&=\\left(\n\\frac{\\mathrm{exp}\\left\\{\n-\\frac{3}{8} \\sum X_i^2 -\\frac{1}{2} \\sum X_i)\n\\right\\}}{4^{\\frac{n}{2}}},\n\\frac{\\mathrm{exp}\\left\\{\n-\\frac{4}{9} \\sum X_i^2 - \\frac{6}{9} \\sum X_i\n\\right\\}}{9^{\\frac{n}{2}}}\n\\right).\n\\end{aligned}\n\\]\nObserve que \\(T'(\\boldsymbol{X}_n)\\) é função 1:1 de \\(T(\\boldsymbol{X}_n)\\). Pois, \\[\n\\left\\{\\begin{array}{l}\nT(\\boldsymbol{X}_n) = (t_1,t_2) \\\\\n\\Rightarrow T'(\\boldsymbol{X}_n) = \\left(\n\\frac{\\mathrm{exp}\\left\\{\n-\\frac{3}{8} t_2 -\\frac{1}{2} t_1)\n\\right\\}}{4^{\\frac{n}{2}}},\n\\frac{\\mathrm{exp}\\left\\{\n-\\frac{4}{9} t_2 - \\frac{6}{9} t_1\n\\right\\}}{9^{\\frac{n}{2}}},\n\\right)\n\\end{array}\\right.\n\\] \\[\n\\begin{aligned}\n&\\left\\{\n\\begin{array}{l}\nT'(\\boldsymbol{X}_n)= (t'_1,t'_2) \\\\\nt'_1 = \\frac{\\mathrm{exp}\\left\\{\n-\\frac{3}{8} t_2 -\\frac{1}{2} t_1)\n\\right\\}}{4^{\\frac{n}{2}}} \\\\\nt'_2 = \\frac{\\mathrm{exp}\\left\\{\n-\\frac{4}{9} t_2 - \\frac{6}{9} t_1\n\\right\\}}{9^{\\frac{n}{2}}}\n\\end{array}\\right. \\\\\n\\Rightarrow&\n\\left\\{\n\\begin{array}{l}\n\\ln(4^{\\frac{n}{2}}t'_1) = \\frac{3}{8}t_2 - \\frac{1}{2}t_1 \\\\\n\\ln(9^{\\frac{n}{2}}t'_2) = \\frac{4}{9}t_2 - \\frac{6}{9} t_1\n\\end{array}\\right.\\\\\n\\Rightarrow&\n\\left\\{\\begin{array}{l}\n\\ln(4^{\\frac{n}{2}}t'_1) = \\frac{3}{8}t_2 - \\frac{1}{2}t_1 \\\\\n\\ln(9^{\\frac{n}{2}}t'_2) = \\frac{4}{9}t_2 - \\frac{6}{9} t_1\n\\end{array}\\right.\\\\\n\\Rightarrow&\n\\left\\{\\begin{array}{l}\nt_1 = -2\\ln(4^{\\frac{n}{2}}t'_1) + \\frac{3}{4} t_2 \\\\\n\\ln(9^{\\frac{n}{2}}t'_2) = \\frac{4}{9} t_2 - \\frac{6}{9}\\left(\\frac{3}{4}t_2 - 2\\ln(4^{\\frac{n}{2}}t'_1)\\right)\n\\end{array}\\right.\\\\\n\\Rightarrow&\n\\left\\{\\begin{array}{l}\nt_1 = \\frac{3}{4}t_2 - 2\\ln(4^{\\frac{n}{2}}t'_1) \\\\\n\\ln(9^{\\frac{n}{2}}t'_2) - \\frac{12}{9} \\ln(4^{\\frac{n}{2}}t'_1) = \\left(\\frac{4}{9} - \\frac{18}{36}\\right) t_2\n\\end{array}\\right.\\\\\n\\Rightarrow&\nt_2 = \\frac{\\ln(9^{\\frac{n}{2}}t'_2) - \\frac{12}{9}(4^{\\frac{n}{2}}t'_1)}{\\left(\\frac{4}{9} - \\frac{18}{36}\\right)} \\\\\n\\Rightarrow&\n\\left\\{\\begin{array}{l}\nt_1 = \\frac{\\frac{3}{4}\\left(\\ln(9^{\\frac{n}{2}}t_2'\\right) - \\frac{12}{9}\\ln(4^{\\frac{n}{2}}t'_1)}{\\left(\\frac{4}{9}-\\frac{18}{36}\\right)} - 2\\ln(4^{\\frac{n}{2}}t'_1)\\\\\nt_2 = \\frac{ln(9^{\\frac{n}{2}}t'_2) - \\frac{12}{9}(4^{\\frac{n}{2}}t'_1)}{\\left(\\frac{4}{9} - \\frac{18}{36}\\right)}\n\\end{array}\\right.\n\\end{aligned}\n\\]\nLogo \\(T(\\boldsymbol{X}_n)=(\\sum X_i,\\sum X_i^2)\\) é SM apara o modelo reduzido. Como é suficiente para o modelo completo, temos pelo Teorema 2 que também é SM para o modelo completo.",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Estatísticas Suficientes</span>"
    ]
  },
  {
    "objectID": "estatisticas-ancilares.html",
    "href": "estatisticas-ancilares.html",
    "title": "23  Estatísticas Ancilares",
    "section": "",
    "text": "23.1 Exemplo 1 (Normal \\(\\rightarrow \\chi^2\\))\nDizemos que \\(U(\\boldsymbol{X}_n)\\) é uma estatística ancilar ao modelo se, e somente se, sua distribuição não depende de “\\(\\theta\\)”.\nSeja \\(\\boldsymbol{X}_n = (X_1,\\dots,X_n)\\) amostra aleatória de \\(X\\sim\\mathrm{N}(\\theta,2),\\theta \\in \\Theta=\\mathbb{R}\\). \\[\nU(\\boldsymbol{X}_n) = \\sum^{n}{i=1} \\frac{(X_i-\\bar{X})^2}{2}\\sim \\chi^2_{(n-1)}\n\\] é, portanto, ancilar ao modelo.",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Estatísticas Ancilares</span>"
    ]
  },
  {
    "objectID": "estatisticas-ancilares.html#exemplo-2-normal-rightarrow-t-student",
    "href": "estatisticas-ancilares.html#exemplo-2-normal-rightarrow-t-student",
    "title": "23  Estatísticas Ancilares",
    "section": "23.2 Exemplo 2 (Normal \\(\\rightarrow\\) t-student)",
    "text": "23.2 Exemplo 2 (Normal \\(\\rightarrow\\) t-student)\nSeja \\(\\boldsymbol{X}_n = (X_1,\\dots,X_n)\\) amostra aleatória de \\(X\\sim\\mathrm{N}(\\mu,\\sigma^2),\\theta = (\\mu,\\sigma^2) \\in \\Theta=\\mathbb{R}\\times\\mathbb{R}^+\\).\nNote que:\n\\[\nS(\\boldsymbol{X}_n) = \\sum \\frac{(X_i-\\bar{X})^2}{\\sigma^2} \\sim \\chi^2_{(n-1)}\n\\]\nnão é uma estatística apesar de sua distribuição não depender de “\\(\\theta\\)”, uma vez que a estatística depende de “\\(\\theta\\)”.\nConstruiremos duas novas estatísticas\nSejam, com \\(k&lt;n\\), \\[\n\\begin{aligned}\nS_1 (\\boldsymbol{X}_n) &= \\sum^k_{i=1}\\frac{(X_i - \\bar{X}_1)^2}{k-1} \\\\\nS_2 (\\boldsymbol{X}_n) &= \\sum^n_{i=k+1}\\frac{(X_i - \\bar{X}_2)^2}{n-k-1}\n\\end{aligned}\n\\]\nem que \\[\n\\begin{cases}\n\\bar{X}_1 &= \\frac{1}{k} \\sum^k_{i=1} X_i \\\\\n\\bar{X}_2 &= \\frac{1}{n-k} \\sum^n_{i=k+1} X_i \\\\\n\\end{cases}\n\\]\nLogo, \\(S_2(\\boldsymbol{X}_n)\\) é independente de \\(S_2(\\boldsymbol{X}_n)\\). Defina \\[\nS(\\boldsymbol{X}_n) = \\frac{S_1(\\boldsymbol{X}_n)}{S_2(\\boldsymbol{X}_n)} =\n\\frac{\\sum^k_{i=1}\\frac{(X_i - \\bar{X}_1)^2}{k-1}}{\\sum^n_{i=k+1}\\frac{(X_i - \\bar{X}_2)^2}{n-k-1}}\n\\sim \\mathrm{F}_{k-1,n-k-1}\n\\]\nPortanto, essa estatística \\(S(\\boldsymbol{X}_n)\\) é ancilar ao modelo.\n\nObservações: (serão transferidas para uma seção própria posteriormente)\nSe \\((X_1,\\dots,X_n)\\) é amostra aleatória de \\(X\\sim \\mathrm{N}(\\mu,\\sigma^2)\\), então\n\n\\(\\bar{X} \\sim \\mathrm{N}(\\mu, \\frac{\\sigma^2}{n})\\) (por função geradora de momentos);\n\\(\\sum \\frac{(X_i - \\mu)^2}{\\sigma^2} \\sim \\chi^2_{n})\\) (por função geradora de momentos e transformação de VAs);\n\\(\\sum \\frac{(X_i - \\bar{X})^2}{\\sigma^2} \\sim \\chi^2_{n-1})\\) (por função geradora de momentos, álgebra linear e transformação de VAs);\n\nSe \\((X_1,\\dots,X_n)\\) é amostra aleatória de \\(X\\sim \\mathrm{N}(\\mu,\\sigma^2)\\) e \\((Y_1,\\dots,Y_n)\\) a.a. de \\(Y\\sim\\mathrm{N}(\\mu,\\sigma^2)\\) são independentes, então\n\\[\n\\begin{aligned}\n1.&\\begin{cases}\n\\sum^{n_1}_{i=1} \\frac{(X_i - \\mu_X)^2}{\\sigma_X^2} \\sim \\chi^2_{n_1}) \\\\\n\\sum^{n_2}_{i=1} \\frac{(Y_i - \\mu_X)^2}{\\sigma_Y^2} \\sim \\chi^2_{n_2}) \\\\\n\\end{cases}\\\\\n2.&\\begin{cases}\n\\sum^{n_1}_{i=1} \\frac{(X_i - \\bar{X})^2}{\\sigma_X^2} \\sim \\chi^2_{n_1-1}) \\\\\n\\sum^{n_2}_{i=1} \\frac{(Y_i - \\bar{Y})^2}{\\sigma_Y^2} \\sim \\chi^2_{n_2-1}) \\\\\n\\end{cases}\\\\\n\\end{aligned}\n\\]\nSe \\(W\\sim\\chi^2_k\\) e \\(M\\sim\\chi^2_m\\) são independentes, então\n\\[\n\\frac{\\frac{W}{k}}{\\frac{Q}{m}} \\sim F_{(k,m)}\n\\]\nSe \\(Z\\sim\\mathrm{N}(0,1)\\) e \\(W\\sim\\chi^2_{k}\\), então\n\\[\n\\frac{Z}{\\sqrt{\\frac{W}{k}}} \\sim t_{k}\n\\]",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Estatísticas Ancilares</span>"
    ]
  },
  {
    "objectID": "estatisticas-ancilares.html#ancilar-de-primeira-ordem",
    "href": "estatisticas-ancilares.html#ancilar-de-primeira-ordem",
    "title": "23  Estatísticas Ancilares",
    "section": "23.3 Ancilar de Primeira Ordem",
    "text": "23.3 Ancilar de Primeira Ordem\nDizemos que \\(U(\\boldsymbol{X}_n)\\) é uma estatística ancilar de primeira ordem se, e somente se, \\(E_{\\theta}\\left(U(\\boldsymbol{X}_n)\\right)\\) (o primeiro momento) não depende de “\\(\\theta\\)”.\nToda estatística ancilar é também ancilar de primeira ordem, mas a recíproca não é verdadeira. Segue disso que uma estatística que não é ancilar de primeira ordem não é ancilar.\n\n23.3.1 Exemplo 1 (Normal)\nSeja \\(\\boldsymbol{X}_n\\) amostra aleatória de \\(X\\sim\\mathrm{N}(\\mu,\\sigma^2)\\) e \\(U(\\boldsymbol{X}_n) = X_1 - \\bar{X}\\)\n\\[\nE_\\theta(U(\\boldsymbol{X}_n)) = E_\\theta(X_1) - E_\\theta(\\bar{X}) = \\mu - \\mu = 0, \\forall \\theta \\in \\Theta\n\\]\n\\[\n\\begin{aligned}\n\\mathrm{Var}_\\theta (U(\\boldsymbol{X}_n)) &= \\mathrm{Var}_\\theta (X_i - \\bar{X}) \\\\\nX_1 - \\bar{X} &= X_1 - \\frac{1}{n}X_1 - \\dots - \\frac{1}{n}X_n \\\\\n&= (1-\\frac{1}{n})X_1 - \\frac{1}{n}X_2 - \\dots - \\frac{1}{n}X_n \\\\\n\\Rightarrow\n\\mathrm{Var}_\\theta (U(\\boldsymbol{X}_n)) &= \\left(1-\\frac{1}{n}\\right)^2 \\sigma^2 + \\underbracket{\\frac{1}{n^2}\\sigma^2 + \\dots + \\frac{1}{n^2}\\sigma^2}_{n-1} \\\\\n\\Rightarrow\n\\mathrm{Var}_\\theta (U(\\boldsymbol{X}_n)) &= \\left(1-\\frac{1}{n}\\right)^2 \\sigma^2 + \\frac{n-1}{n^2}\\sigma^2 \\forall \\theta \\in \\Theta\n\\end{aligned}\n\\]\ndepende de “\\(\\theta\\)”.",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Estatísticas Ancilares</span>"
    ]
  },
  {
    "objectID": "estatisticas-completas.html",
    "href": "estatisticas-completas.html",
    "title": "24  Estatísticas Completas",
    "section": "",
    "text": "24.1 Exemplo 1 (Bernoulli, Soma)\nDizemos que \\(T(\\boldsymbol{X}_n)\\) é uma estatística completa se, e somente se, existe a função real \\(h(\\cdot)\\) tal que \\[\nE_\\theta(h(T(\\boldsymbol{X}_n)) = 0, \\forall \\theta \\in \\Theta\n\\] é a função nula quase certamente, isto é, \\[\nP_\\theta(h(T(\\boldsymbol{X}_n))=0) = 1, \\forall \\theta \\in \\Theta\\; \\text{ou simplesmente}\\ h(T(\\boldsymbol{X}_n)) = 0 \\mathrm{q.c.}\n\\]\nSeja \\(\\boldsymbol{X}_n = (X_1,\\dots,X_n)\\) amostra aleatória de \\(X\\sim\\mathrm{Ber}(\\theta)\\). Mostre que \\(T(\\boldsymbol{X}_n) = \\sum X_i\\) é uma estatistica completa para o modelo.",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Estatísticas Completas</span>"
    ]
  },
  {
    "objectID": "estatisticas-completas.html#sec-ex-comp-ber",
    "href": "estatisticas-completas.html#sec-ex-comp-ber",
    "title": "24  Estatísticas Completas",
    "section": "",
    "text": "24.1.1 Resposta\nNote que \\(T(\\boldsymbol{X}_n) \\sim \\mathrm{Binom}(n,\\theta), \\theta \\in (0,1)\\)\n\\[\nP_\\theta(T(\\boldsymbol{X}_n)= k) \\left\\{\\begin{array}{ll}\n\\binom{n}{k}\\theta^k (1-\\theta)^{n-k}, & k \\in \\{0,1,\\dots,n\\} \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right.\n\\]\nSeja \\(h(\\cdot)\\) uma função real (contradomínio em \\(\\mathbb{R}\\)) tal que \\[\n\\begin{aligned}\n&E_\\theta(h(T(\\boldsymbol{X}_n))) = 0,\\ \\forall \\theta \\in \\Theta = (0,1) \\\\\n\\stackrel{\\mathrm{a.a.}\\ X\\sim\\mathrm{Ber}}{\\iff}&\\sum_{k=0}^n h(k) P_\\theta(T(\\boldsymbol{X}_n) = k) = 0 \\\\\n\\iff&\\sum h(k) \\binom{n}{k}\\theta^k (1-\\theta)^{n-k}= 0\\\\\n\\iff&\\sum h(k) \\binom{n}{k} \\left(\\frac{\\theta}{(1-\\theta)}\\right)^k (1-\\theta)^n = 0\\\\\n\\iff&\\sum h(k) \\binom{n}{k} \\left(\\frac{\\theta}{(1-\\theta)}\\right)^k (1-\\theta)^n = 0\\\\\n\\iff&\\sum h(k) \\binom{n}{k} \\left(\\frac{\\theta}{(1-\\theta)}\\right)^k  = 0\\\\\n\\stackrel{\\rho = \\frac{\\theta}{1-\\theta} \\in (0,\\infty)}{\\iff}&\\sum h(k) \\binom{n}{k} \\rho^k = 0, \\forall \\theta \\in \\Theta = (0,1), \\rho \\in (0,\\infty).\n\\end{aligned}\n\\]\nObserve que, tomando \\(a_k = h(k) \\binom{n}{k}, k = 0,1,\\dots,n\\), temos que \\[\na_0 \\rho^0 + a_1 \\rho^1 + \\dots + a_n \\rho^n = 0,\\ \\forall \\rho \\in (0,\\infty).\n\\] Por igualdade de polinômios, \\[\n\\left\\{\n\\begin{array}{l}\na_0 = 0 \\\\\n\\vdots \\\\\na_n = 0\n\\end{array}\\right. \\Rightarrow\n\\left\\{\n\\begin{array}{l}\nh(0)\\binom{n}{0} = 0 \\\\\n\\vdots \\\\\nh(n)\\binom{n}{n} = 0\n\\end{array}\\right. \\Rightarrow\n\\]\nComo \\(\\binom{n}{k}&gt;0\\ \\forall k \\in \\{0,1,\\dots,n\\}\\), temos que \\(h(k) = 0,\\ \\forall k \\in \\{0,1,\\dots,n\\}\\)\n\\[\n\\Rightarrow P_\\theta(h(T(\\boldsymbol{X}_n)) = 0) = 1,\\ \\forall \\theta \\in \\Theta.\n\\]\nPortanto, \\(T(\\boldsymbol{X}_n)\\) é completa para o modelo.",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Estatísticas Completas</span>"
    ]
  },
  {
    "objectID": "estatisticas-completas.html#exemplo-2-uniforme",
    "href": "estatisticas-completas.html#exemplo-2-uniforme",
    "title": "24  Estatísticas Completas",
    "section": "24.2 Exemplo 2 (Uniforme)",
    "text": "24.2 Exemplo 2 (Uniforme)\nSeja \\(\\boldsymbol{X}_n = (X_1,\\dots,X_n)\\) a.a. de \\(X\\sim\\mathrm{Unif}(0,\\theta),\\theta \\in \\Theta = (0,\\infty)\\). Mostre que \\(T(\\boldsymbol{X}_n) = \\max\\{X_1,\\dots,X_n\\}\\) é uma estatística completa para o modelo.\n\n24.2.1 Resposta\nSeja \\(h(\\cdot)\\) uma função tal que \\[\nE_\\theta(h(\\boldsymbol{X}_n)) = 0,\\ \\forall \\theta \\in \\Theta = (0,\\infty)\n\\]\nPrecisamos encontrar a função densidade de probabilidade de \\(X_(n)\\) (máximo). Sabemos que a f.d.p é a derivada da função de distribuição acumulada. Isto é,\n\\[\n\\begin{aligned}\nP_\\theta(X_{(n)} \\leq t) &= P_\\theta(X_1\\leq t, \\dots, X_n\\leq t)\\\\\n&\\stackrel{\\mathrm{ind.}}{=} \\prod^n_{i=1} P_\\theta(X_i\\leq t) \\stackrel{\\mathrm{id}}{=} \\prod P_\\theta(X\\leq t) \\\\\n\\Rightarrow P_\\theta(X_{(n)} \\leq t) &\\stackrel{\\mathrm{iid}}{=} [P_\\theta(X\\leq t)]^n\n\\end{aligned}\n\\]\nem que \\[\nP_\\theta(X\\leq t) = \\left\\{\\begin{array}{ll}\n0, & t \\leq 0 \\\\\n\\frac{t}{\\theta}, & t \\in (0,\\theta] \\\\\n1,& t &gt; \\theta\n\\end{array}\\right.\n\\]\nPortanto,\n\\[\n\\begin{aligned}\n\\frac{d}{dt} P_\\theta(X_{(n)} \\leq t) &= n [P_\\theta(X_{(n)}\\leq t)]^{n-1} \\frac{d}{dt} P_\\theta (X_{(n)} \\leq t) \\\\\n\\Rightarrow f_\\theta^{\\boldsymbol{X}_{(n)}}(x) &= \\left.\\frac{dP_\\theta(X_{(n)} \\leq t)}{dt} \\right\\rvert_{t=x} \\\\\n&= n [P_\\theta(X_{(n)}\\leq x)]^{n-1} f_\\theta(x) \\\\\n&= \\left\\{\\begin{array}{ll}\nn \\left(\\frac{x}{\\theta}\\right)^{n-1} \\frac{1}{\\theta},& x \\in (0,\\theta] \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right.\n\\end{aligned}\n\\]\nDessa forma, \\[\n\\begin{aligned}\nE_\\theta(h(X_{(n)})) &= \\int^\\infty_{-\\infty} h(x) f^{\\boldsymbol{X}_{(n)}}_\\theta (x) dx,\\\\\n&= \\int_0^\\theta h(x) \\cdot n \\left(\\frac{x}{\\theta}\\right)^{n-1} \\frac{1}{\\theta} dx,\\  \\forall \\theta \\in \\Theta.\n\\end{aligned}\n\\]\nSe \\[\nE_\\theta(h(X_{(n)})) = 0,\\ \\forall \\theta \\in \\Theta\n\\] então \\[\n\\begin{aligned}\n&\\int_0^\\theta h(x) \\cdot n \\left(\\frac{x}{\\theta}\\right)^{n-1} \\frac{1}{\\theta} dx = 0,\\forall \\theta &gt; 0 \\\\\n\\iff & \\frac{n}{\\theta^n} \\int_0^\\theta h(x) x^{n-1} dx = 0,\\ \\forall \\theta &gt; 0 \\\\\n\\iff & \\int_0^\\theta h(x) x^{n-1} dx = 0,\\ \\forall \\theta &gt; 0.\n\\end{aligned}\n\\]\nPelo Teorema Fundamental do Cálculo: \\(\\frac{d}{dt} \\int^t_a f(x)dx = f(t)\\), temos que\n\\[\n\\begin{aligned}\n& \\frac{d}{d\\theta} \\int_0^\\theta h(x) x^{n-1} dx = 0 \\\\\n\\stackrel{\\mathrm{TFC}}{\\Rightarrow}& h(\\theta)\\theta^{n-1} = 0 \\\\\n\\iff& h(\\theta) = 0, \\forall \\theta &gt; 0.\n\\end{aligned}\n\\]\nOu seja, \\(h(x)= 0,\\forall x &gt; 0\\). Portanto, \\(h\\) é a função nula quase certamente. Logo, \\(X_{(n)}\\) é completa para o modelo.",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Estatísticas Completas</span>"
    ]
  },
  {
    "objectID": "estatisticas-completas.html#exemplo-negativo-bernoulli-amostra",
    "href": "estatisticas-completas.html#exemplo-negativo-bernoulli-amostra",
    "title": "24  Estatísticas Completas",
    "section": "24.3 Exemplo negativo (Bernoulli, amostra)",
    "text": "24.3 Exemplo negativo (Bernoulli, amostra)\nSeja \\(\\boldsymbol{X}_n\\) amostra aleatória de \\(X\\sim\\mathrm{Ber}(\\theta),\\theta \\in \\Theta=(0,1)\\). Mostre que \\(T(\\boldsymbol{X}_n) = \\boldsymbol{X}_n\\) é suficiente porém não é completa para o modelo.\n\n24.3.1 Resposta\nPelo critério da fatoração, já sabemos que \\(\\boldsymbol{X}_n\\) é suficiente para o modelo. Tome \\[\nE_\\theta(h(T(\\boldsymbol{X}_n)) = E_\\theta(X_1 - X_2) = \\theta - \\theta = 0, \\forall \\theta \\in \\Theta.\n\\]\nPortanto, \\(T(\\boldsymbol{X}_n)\\) não é completa para o modelo.",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Estatísticas Completas</span>"
    ]
  },
  {
    "objectID": "estatisticas-completas.html#teorema-1-da-relação-de-ida-com-estatísticas-suficientes-minimais",
    "href": "estatisticas-completas.html#teorema-1-da-relação-de-ida-com-estatísticas-suficientes-minimais",
    "title": "24  Estatísticas Completas",
    "section": "24.4 Teorema 1 (da relação de ida com estatísticas suficientes minimais)",
    "text": "24.4 Teorema 1 (da relação de ida com estatísticas suficientes minimais)\nToda estatística suficiente e completa é também suficiente minimal para o modelo estatístico.",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Estatísticas Completas</span>"
    ]
  },
  {
    "objectID": "estatisticas-completas.html#sec-teo-dois-completa",
    "href": "estatisticas-completas.html#sec-teo-dois-completa",
    "title": "24  Estatísticas Completas",
    "section": "24.5 Teorema 2 (da relação de volta com estatísticas suficientes minimais)",
    "text": "24.5 Teorema 2 (da relação de volta com estatísticas suficientes minimais)\nSe existir uma estatística suficiente e completa, então toda estatística suficiente minimal será também completa.\n\n24.5.1 Exemplo negativo (Normal curvada)\nSeja \\(\\boldsymbol{X}_n\\) a.a. de \\(X\\sim\\mathrm{N}(\\theta,\\theta^2), \\theta \\in \\Theta = (0,\\infty)\\). Mostre que \\(T(\\boldsymbol{X}_n) = (\\sum X_i, \\sum X_i^2)\\) não é completa para o modelo.\n\n24.5.1.1 Resposta\nNote que \\[\n\\begin{aligned}\nE_\\theta(\\sum X_i) &= n\\theta, \\forall \\theta \\in \\Theta \\\\\nE_\\theta(\\sum X_i^2) &\\stackrel{\\mathrm{iid}}{=} n E_\\theta(X^2) = n(\\theta^2+ \\theta^2) = 2n\\theta^2 \\\\\n\\Rightarrow E_\\theta(T(\\boldsymbol{X})n) = (n\\theta, 2n\\theta^2)\n\\end{aligned}\n\\]\nTentativas para encontrar \\(h(\\cdot)\\).\n\\[\n\\begin{aligned}\nE_\\theta\\left(\\left(\\sum X_i\\right)^2\\right) &= E_\\theta\\left(\\sum_{i=1}^n \\sum_{j=1}^n X_i X_j\\right) \\\\\n&= \\sum_{i=1}^n \\sum_{j=1}^n E_\\theta(X_i\\cdot X_j) \\\\\n&= \\sum_{i=1}^n E(X_i^2) + \\underset{i\\neq j}{\\sum \\sum} E_\\theta(X_i X_j) \\\\\n&\\stackrel{\\mathrm{ind}}{=} 2n\\theta^2 + \\underset{i\\neq j}{\\sum \\sum} E_\\theta(X_i)E_\\theta(X_j) \\\\\n&\\stackrel{\\mathrm{id}}{=} 2n\\theta^2 + \\underset{i\\neq j}{\\sum \\sum} [E_\\theta(X)]^2 \\\\\n&= 2n\\theta^2 + (n^2-n)\\theta^2 \\\\\n&= (2n + n^2 -n)\\theta^2 \\\\\n&= (n^2 +n)\\theta^2.\n\\end{aligned}\n\\]\nOu seja,\n\\[\nE_\\theta\\left(\\frac{(\\sum X_i)^2}{n^2 + n}\\right) = \\theta^2, \\forall \\theta \\in \\Theta.\n\\]\nNote ainda que \\[\nE_\\theta\\left(\\frac{\\sum X_i^2}{2n}\\right) = \\theta^2, \\forall \\theta \\in \\Theta.\n\\]\nLogo, tome \\[\nh(T(\\boldsymbol{X}_n)) = \\frac{T_1(\\boldsymbol{X}_n)^2}{n^2+n} - \\frac{T_2(\\boldsymbol{X}_n)}{2n}\n\\] em que \\[\n\\begin{aligned}\nT_1(\\boldsymbol{X}_n) = \\sum X_i \\\\\nT_2(\\boldsymbol{X}_n) = \\sum X_i^2.\n\\end{aligned}\n\\]\nComo \\(E_\\theta(h(T(\\boldsymbol{X}_n)) = 0, \\forall \\theta &gt; 0\\), concluímos que \\(T(\\boldsymbol{X}_n)\\) não é conpleta.\n\n\n\n\n\n\nNem toda SM é completa!\n\n\n\nJá demonstramos que \\(T(\\boldsymbol{X}_n)\\) é SM. Portanto, nem toda estatística SM é também completa. Dessa forma, pelo Teorema 2, concluímos que não existe uma estatística completa para o modelo.",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Estatísticas Completas</span>"
    ]
  },
  {
    "objectID": "estatisticas-completas.html#sec-teo-tres-completa",
    "href": "estatisticas-completas.html#sec-teo-tres-completa",
    "title": "24  Estatísticas Completas",
    "section": "24.6 Teorema 3 (da relação com a família exponencial)",
    "text": "24.6 Teorema 3 (da relação com a família exponencial)\nSeja \\(\\boldsymbol{X}_n\\) a.a. de \\(X\\sim f_\\theta, \\theta \\in \\Theta\\), em que \\(f_\\theta\\) pertence à família exponencial k-dimensional. Se “couber” um “retângulo” k-dimensional no conjunto \\[\n\\mathcal{C} = \\{(c_1(\\theta),\\dots,c_k(\\theta) : \\theta \\in \\Theta \\},\n\\] em que \\(c_i\\) representa a \\(i\\)-ésima componente \\(c\\) obtida do pertencimento à família exponencial. Então a estatística \\[\nT(\\boldsymbol{X}_n) = \\left(\\sum T_1(\\boldsymbol{X}_n),\\dots,\\sum T_k(\\boldsymbol{X}_n)\\right),\n\\] em que \\(T_i\\) representa a \\(i\\)-ésima componente \\(T\\) obtida do pertencimento à FE, é suficiente e completa para o modelo.\nSe \\(k=1\\), então \\(\\mathcal{C}\\) deve conter um intervalo (aberto). Se \\(k=2\\), \\(\\mathcal{C}\\) deve conter o interior de um retângulo. Se \\(k=3\\), então deve conter o interior de um cubo, e assim por diante.\n\n24.6.1 Exemplo (Normal livre)\nSeja \\(\\boldsymbol{X}_n\\) a.a. de \\(X\\sim\\mathrm{N}(\\mu,\\sigma^2), \\theta=(\\mu,\\sigma^2)\\in\\Theta = \\mathbb{R}\\times\\mathbb{R}^+\\). Mostre que \\(T(\\boldsymbol{X}_n) = (\\sum X_i, \\sum X_i^2)\\) é suficiente e completa para o modelo.\n\n24.6.1.1 Resposta\nNote que \\[\nf_\\theta(x) = \\mathrm{e}^{c_1(\\theta)T_1(x)+c_2(\\theta)T_2(x) + d(\\theta) + S(x)}\n\\] em que \\[\n\\begin{array}{cc}\nc_1(\\theta) = -\\frac{1}{2\\sigma^2} & T_1(x) = X^2 \\\\\nc_2(\\theta) = \\frac{\\mu}{\\sigma^2} & T_2(x) = x \\\\\nd(\\theta) = -\\frac{\\mu^2}{2\\sigma^2} - \\frac{1}{2}\\ln(2\\pi\\sigma^2) & S(x) = 0.\n\\end{array}\n\\]\nObserve também que \\[\n\\mathcal{C} =\\left \\{(c_1(\\theta),c_2(\\theta)) : \\theta \\in \\Theta\\right\\} = \\left\\{\\left(-\\frac{1}{2\\sigma^2}, \\frac{\\mu}{\\sigma^2}\\right)\\right\\}\n\\]\nOu seja \\[\n\\mathcal{C} = \\mathbb{R}^-\\times\\mathbb{R}\n\\]\n\nTome, por exemplo, \\[\n(-2,-1)\\times(1,2) \\subseteq \\mathcal{C}\n\\]\nLogo, concluímos que \\[\nT'(\\boldsymbol{X}_n) = (\\sum X_i^2, \\sum X_i)\n\\] é suficiente e completa para o modelo. Como \\(T(\\boldsymbol{X}_n)=(\\sum X_i, \\sum X_i^2)\\) é função injetora (1:1) de \\(T'(\\boldsymbol{X}_n)\\), concluímos que \\(T(\\boldsymbol{X}_n)\\) também é suficiente e completa.\n\n\n\n24.6.2 Exemplo (Exponencial)\nSeja \\(\\boldsymbol{X}_n\\) a.a. de \\(X\\sim\\mathrm{Exp}(\\theta), \\theta\\in\\Theta = (0,\\infty)\\). Encontre uma estatística suficiente e completa.\n\n24.6.2.1 Resposta\nNote que \\[\nf_\\theta(x) = \\begin{array}{ll}\n\\theta \\mathrm{e}^{-\\theta x}, & x &gt; 0\\\\\n0, & \\mathrm{c.c.}\n\\end{array}\n\\]\nJá mostramos que pertence à FE com \\(c_1(\\theta) = -\\theta\\), e \\(T_1(x) = x\\). Note que \\[\n\\mathcal{C} = \\{c_1(\\theta) : \\theta \\in \\Theta\\} = (-\\infty,0)\n\\]\nComo \\((-1,-0.5) \\subseteq \\mathcal{C}\\), concluímos que \\(\\sum T_1(X_i)\\) é, pelo Teorema 3, suficiente e completa.\n\n\n\n24.6.3 Exemplo negativo (Normal curvada)\nSeja \\(\\boldsymbol{X}_n\\) a.a. de \\(X\\sim\\mathrm{N}(\\theta,\\theta^2), \\theta\\in\\Theta = \\mathbb{R}^+\\). Verifique se o conjunto \\(\\mathcal{C} = \\{(c_1(\\theta),c_2(\\theta)) : \\theta \\in \\mathbb{R}^+\\).\n\n24.6.3.1 Resposta\nNote que \\[\nf_\\theta(x) = \\frac{1}{\\sqrt{2\\pi\\theta^2}} \\mathrm{exp}\\left\\{-\\frac{1}{2\\theta^2} (x^2 - 2x\\theta + \\theta^2)\\right\\}\n\\]\nTomando \\[\n\\begin{array}{cc}\nc_1(\\theta) = -\\frac{1}{2\\theta^2} & T_1(x) = x^2 \\\\\nc_2(\\theta) = \\frac{1}{\\theta} & T_2(x) = x \\\\\nd(\\theta) = -\\ln(\\sqrt{2\\pi\\theta^2}) & S(x) = -\\frac{1}{2}\n\\end{array}\n\\]\nLogo \\[\n\\mathcal{C} = \\{(c_1(\\theta),c_2(\\theta)):\\theta&gt;0\\} = \\left\\{\\left(-\\frac{1}{2\\theta^2},\\frac{1}{\\theta}\\right)\\right\\}\n\\]\nComo \\(c_1(\\theta)\\) e \\(c_2(\\theta)\\) estão relacionados funcionalmente (podemos escrever \\(c_1(\\theta)\\) como função de \\(c_2(\\theta)\\), temos uma curva ao invés de uma área e, portanto, não é possível encontrar um retângulo cujo interior esteja contido em \\(\\mathcal{C}\\).\n\nNote que esse teorema não diz se não há uma estatística completa. Para isso, é necessário testar as estatísticas pela definição",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Estatísticas Completas</span>"
    ]
  },
  {
    "objectID": "envvum.html",
    "href": "envvum.html",
    "title": "25  Estimadores não-viciados com variância uniformemente mínima (ENVVUM)",
    "section": "",
    "text": "25.1 Teorema de Lehmamn-Scheffé\nEstimadores são estatísticas cujo objetivo é estimar uma quantidade de interesse \\(g(\\theta)\\).\nDizemos que \\(T(\\boldsymbol{X}_n)\\) é um ENVVUM se, e somente se,\n\\[\nE_\\theta(T(\\boldsymbol{X}_n)) = g(\\theta), \\forall \\theta \\in \\Theta;\n\\]\n\\[\n\\mathrm{Var}_\\theta(T(\\boldsymbol{X}_n)) \\leq \\mathrm{Var}_\\theta(U(\\boldsymbol{X}_n)), \\forall \\theta \\in \\Theta;\n\\]\nSeja \\(\\boldsymbol{X}_n = (X_1,\\dots,X_j)\\) amostra aleatória de \\(X\\sim f_\\theta, \\theta \\in \\Theta\\), \\(f_\\theta\\) a distribuição amostral. Considere \\(T(\\boldsymbol{X}_n)\\) uma estatística suficiente e completa para o modelo e \\(S(\\boldsymbol{X}_n)\\) um estimador não-viciado para \\(g(\\theta)\\). Então,\n\\[\n\\stackrel{\\sim}{T}(\\boldsymbol{X}_n) = E_\\theta(S(\\boldsymbol{X}_n)\\lvert T(\\boldsymbol{X}_n))\n\\]\né o ENVVUM para \\(g(\\theta)\\), quase certamente, baseado em \\(T(\\boldsymbol{X}_n)\\).",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Estimadores não-viciados com variância uniformemente mínima (ENVVUM)</span>"
    ]
  },
  {
    "objectID": "envvum.html#sec-teo-ls",
    "href": "envvum.html#sec-teo-ls",
    "title": "25  Estimadores não-viciados com variância uniformemente mínima (ENVVUM)",
    "section": "",
    "text": "Observação\n\n\n\n\n\\(\\stackrel{\\sim}{T}(\\boldsymbol{X}_n)\\) não depende de “\\(\\theta\\)” pois \\(T(\\boldsymbol{X}_n)\\) é suficiente para o modelo.\n\\(E_\\theta(\\stackrel{\\sim}{T}(\\boldsymbol{X}_n)) = E_\\theta(E_\\theta(S(\\boldsymbol{X}_n)\\lvert T(\\boldsymbol{X}_n)) = E_\\theta(S(\\boldsymbol{X}_n))\\)\n\n\n\n\n25.1.1 Prova\nObserve que\n\\[\nE_\\theta(\\stackrel{\\sim}{T}(\\boldsymbol{X}_n)) = E_\\theta(E_\\theta(S(\\boldsymbol{X}_n)\\lvert T(\\boldsymbol{X}_n)) = E_\\theta(S(\\boldsymbol{X}_n)).\n\\]\nComo \\(S(\\boldsymbol{X}_n)\\) é, por hipótese, não-viciado para \\(g(\\theta)\\), temos que \\[\nE_\\theta(\\stackrel{\\sim}{T}(\\boldsymbol{X}_n)) = g(\\theta),\\forall\\theta\\in\\Theta.\n\\]\nPortanto, \\(\\stackrel{\\sim}{T}(\\boldsymbol{X}_n)\\) é não-viciado para \\(g(\\theta)\\).\nSeja \\(U(\\boldsymbol{X}_n)\\) um estimador não-viciado para \\(g(\\theta)\\) que dependa apenas de \\(T(\\boldsymbol{X}_n)\\). Então, como \\(T(\\boldsymbol{X}_n)\\) é, por hipótese, completa para o modelo, tome, como \\(\\stackrel{\\sim}{T}(\\boldsymbol{X}_n)\\) é uma função de \\(T(\\boldsymbol{X}_n)\\),\n\\[\n\\begin{aligned}\nh(T(\\boldsymbol{X}_n)) &= \\stackrel{\\sim}{T}(\\boldsymbol{X}_n) - U(\\boldsymbol{X}_n) \\\\\n\\Rightarrow E_\\theta(h(T(\\boldsymbol{X}_n))) &= 0, \\forall \\theta \\in \\Theta \\\\\n\\Rightarrow h(T(\\boldsymbol{X}_n)) &= 0,\\mathrm{q.c.}\n\\end{aligned}\n\\]\nPortanto, \\(\\stackrel{\\sim}{T}(\\boldsymbol{X}_n) = U(\\boldsymbol{X}_n), \\mathrm{q.c.}\\)\nSeja \\(\\stackrel{\\sim}{T_1}(\\boldsymbol{X}_n)\\) um estimador não-viciado para \\(g(\\theta)\\). Então,\n\\[\n\\begin{aligned}\n\\mathrm{Var}_\\theta(\\stackrel{\\sim}{T_1}(\\boldsymbol{X}_n)) &=\nE_\\theta(\\mathrm{Var}_\\theta(\\stackrel{\\sim}{T_1}(\\boldsymbol{X}_n) \\lvert T(\\boldsymbol{X}_n)) +\n\\mathrm{Var}_\\theta(\\underbracket{E_\\theta(\\stackrel{\\sim}{T_1}(\\boldsymbol{X}_n)|T(\\boldsymbol{X}_n)}_{=\\stackrel{\\sim}{T}(\\boldsymbol{X}_n), \\mathrm{q.c.}}) \\\\\n&= \\underbracket{E_\\theta(\\mathrm{Var}_\\theta(\\stackrel{\\sim}{T_1}(\\boldsymbol{X}_n) \\lvert T(\\boldsymbol{X}_n))}_{\\geq 0} +\n\\mathrm{Var}_\\theta(\\stackrel{\\sim}{T}(\\boldsymbol{X}_n)), \\forall \\theta \\in \\Theta \\\\\n\\Rightarrow \\mathrm{Var}_\\theta(\\stackrel{\\sim}{T_1}(\\boldsymbol{X}_n)) &\\geq \\mathrm{Var}_\\theta(\\stackrel{\\sim}{T}(\\boldsymbol{X}_n)),\\mathrm{q.c.}\\ \\forall \\theta \\in \\Theta\n\\end{aligned}\n\\]\nOu seja, \\(\\stackrel{\\sim}{T}(\\boldsymbol{X}_n)\\) é quase certamente o ENVVUM para \\(g(\\theta)\\) baseado em \\(T(\\boldsymbol{X}_n)\\)",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Estimadores não-viciados com variância uniformemente mínima (ENVVUM)</span>"
    ]
  },
  {
    "objectID": "envvum.html#exemplo-bernoulli",
    "href": "envvum.html#exemplo-bernoulli",
    "title": "25  Estimadores não-viciados com variância uniformemente mínima (ENVVUM)",
    "section": "25.2 Exemplo Bernoulli",
    "text": "25.2 Exemplo Bernoulli\nSeja \\(\\boldsymbol{X}_n\\) amostra aleatória de \\(X\\sim \\mathrm{Ber}(\\theta), \\theta \\in \\Theta = (0,1)\\).\na-) Encontre o ENVVUM para \\(g(\\theta) = P_\\theta(X=1)\\).\nJá sabemos que \\(\\sum X_i\\) é suficiente e completa para o modelo de Bernoulli. Além disso, \\[\nE_\\theta\\left(\\frac{1}{n} \\sum X_i\\right) = \\theta, \\forall \\theta \\in \\Theta.\n\\]\nOu seja, \\(T(\\boldsymbol{X}_n) =\\frac{1}{n} \\sum X_i\\) é suficiente e completa, pois é função 1:1 de \\(\\sum X_i\\) e, além disso, é não viciada para \\(g(\\theta) = P_\\theta(X=1) = \\theta\\). Portanto,\n\\[\nE_\\theta(T(\\boldsymbol{X}_n)\\lvert T(\\boldsymbol{X}_n)) = T(\\boldsymbol{X}_n).\n\\]\nLogo, o ENVVUM é, pelo Teorema de Lehmamn-Scheffé, \\[\n\\stackrel{\\sim}{T}(\\boldsymbol{X}_n) =  E_\\theta(T(\\boldsymbol{X}_n)\\lvert T(\\boldsymbol{X}_n)) = T(\\boldsymbol{X}_n)\n= \\frac{1}{n} \\sum^n_{i=1} X_i\n\\]\nObs: Poderíamos iniciar com \\(S(\\boldsymbol{X}_n) = X_1\\) pois \\(E_\\theta(X_1) = \\theta, \\forall \\theta \\in \\Theta\\). \\[\n\\stackrel{\\sim}{T}(\\boldsymbol{X}_n) = E_\\theta\\left(S(\\boldsymbol{X}_n)\\lvert \\sum X_i\\right)\n\\] também é ENVVUM. Para demonstrar, precisamos calcular\n\\[\nE_\\theta\\left(X_1\\lvert \\sum X_i\\right).\n\\]\nUma estratégia é encontrar\n\\[\nE_\\theta(S(\\boldsymbol{X}_n)\\lvert \\sum X_i = t) = m(t), \\forall \\theta \\in \\Theta\n\\] e substituir \\(t\\) pela estatística suficiente e completa, neste caso \\(\\sum X_i\\). Note que, para \\(t=0\\)\n\\[\nE_\\theta(X_1\\lvert \\sum X_i = 0) = 0, \\forall \\theta \\in \\Theta.\n\\]\nSe \\(t\\neq 0\\), então\n\\[\n\\begin{aligned}\n\\frac{t}{t} = 1 &\\iff E_\\theta\\left(\\frac{t}{t}\\lvert \\sum X_i = t\\right) = 1, \\forall \\theta \\in \\Theta \\\\\n&\\iff E_\\theta(\\frac{t}{t} \\lvert \\sum X_i = t) = E_\\theta\\left(\\frac{\\sum X_i}{\\sum X_i}\\lvert \\sum X_i = t\\right) = 1, \\forall \\theta \\in \\Theta \\\\\n&\\iff\\sum E_\\theta\\left(\\frac{X_i}{\\sum X_i}\\lvert \\sum X_i = t\\right) = 1, \\forall \\theta \\in \\Theta \\\\\n&\\stackrel{\\mathrm{iid}}{\\iff} n E_\\theta\\left(\\frac{X_1}{\\sum X_i}\\lvert \\sum X_i = t\\right) = 1, \\forall \\theta \\in \\Theta \\\\\n&\\iff \\frac{n}{t} E_\\theta\\left(X_1 \\lvert \\sum X_i = t\\right) = 1, \\forall \\theta \\in \\Theta \\\\\n&\\Rightarrow E_\\theta\\left(X_1\\lvert \\sum X_i = t\\right) = \\frac{t}{n}, \\forall\\theta \\in \\Theta.\n\\end{aligned}\n\\]\nLogo, substituindo \\(t\\) por \\(\\sum X_i\\), temos que \\[\nE_\\theta\\left(S(\\boldsymbol{X}_n)\\lvert \\sum X_i\\right) = \\frac{1}{n} \\sum X_i\n\\]\nb-) Encontre o ENVVUM para \\(g(\\theta) = P_\\theta(X=0)\\).\nNote que \\(g(\\theta) = P_\\theta(X=0) = 1-\\theta\\). Como \\(\\frac{1}{n} \\sum X_i\\) é suficiente e completa para o modelo e \\(1 - \\frac{1}{n} \\sum X_i\\) é não viciada para \\(g(\\theta)\\), temos que\n\\[\n\\stackrel{\\sim}{T}(\\boldsymbol{X}_n) =  E_\\theta\\left(1-\\frac{1}{n}\\sum X_i \\left\\lvert\\right. \\sum X_i\\right) = 1- \\frac{1}{n}\\sum X_i\n\\]\nLogo, pelo Teorema de Lehmamn-Scheffé, é o ENVVUM para \\(g(\\theta) = 1-\\theta\\).",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Estimadores não-viciados com variância uniformemente mínima (ENVVUM)</span>"
    ]
  },
  {
    "objectID": "envvum.html#exemplo-normal",
    "href": "envvum.html#exemplo-normal",
    "title": "25  Estimadores não-viciados com variância uniformemente mínima (ENVVUM)",
    "section": "25.3 Exemplo (Normal)",
    "text": "25.3 Exemplo (Normal)\nSeja \\(\\boldsymbol{X}_n\\) a.a. de \\(X\\sim\\mathrm{N}(\\mu,\\sigma^2), \\theta = (\\mu, \\sigma^2) \\in \\Theta = \\mathbb{R}\\times\\mathbb{R}_+\\).\n\n25.3.1 Item a (parâmetros)\nEncontre o (quase certamente) ENVVUM para \\(g(\\theta) = (\\mu, \\sigma^2)\\).\nJá sabemos que, pelo Teorema das FEs, \\[\nT(\\boldsymbol{X}_n) = \\left(\\sum X_i,\\sum X_i^2\\right)\n\\] é suficiente e completa para o modelo. Observe ainda que \\[\nT'(\\boldsymbol{X}_n) = \\left(\\frac{1}{n} \\sum X_i, \\frac{1}{n} \\sum X_i^2 - \\left(\\frac{1}{n} \\sum X_i\\right)^2\\right)\n\\] é função 1:1 de \\(T(\\boldsymbol{X}_n)\\). Portanto, \\(T'(\\boldsymbol{X}_n)\\) é uma estatística suficiente e completa para o modelo.\nNote que \\[\nE_\\theta\\left(\\frac{1}{n}\\sum X_i\\right) = \\mu, \\forall \\theta \\in \\Theta\n\\] e \\[\n\\begin{aligned}\nE_\\theta\\left(\\frac{1}{\\sigma^2} \\sum (X_i-\\bar{X})^2\\right) &\\stackrel{\\chi^2_{n-1}}{=} n-1 \\\\\n\\Rightarrow E_\\theta\\left(\\frac{1}{n} \\sum (X_i - \\bar{X})^2 \\right) &= \\frac{n-1}{n} \\sigma^2, \\forall \\theta \\in \\Theta.\n\\end{aligned}\n\\]\nPortanto, \\[\nT''(\\boldsymbol{X}_n) = \\left(\\bar{X}, S_{n-1}^2\\right),\n\\] em que \\(\\bar{X} = \\frac{1}{n} \\sum X_i\\) e \\(S^2_{n-1}(\\boldsymbol{X}_n) = \\frac{1}{n-1} \\sum (X_i - \\bar{X})^2\\), é suficiente completa para o modelo não viciado para \\(g(\\theta)\\) pois \\[\nE_\\theta(T''(\\boldsymbol{X}_n)) = (\\mu,\\sigma^2), \\forall \\theta \\in \\Theta.\n\\]\nLogo, \\(T''(\\boldsymbol{X}_n)\\) é o ENVVUM quase certamente para \\(g(\\theta) = \\theta\\).\n\n\n25.3.2 Item b (DP)\nEncontre o ENVVUM para \\(g(\\theta) = \\sigma\\)\nJá temos, do item a, uma estatística suficiente completa para o modelo, como \\(T''(\\boldsymbol{X}_n)\\).\nTentativa 1.\nIntuitivamente, podemos esperar que \\(E_\\theta(\\sqrt{S^2_{n-1}(\\boldsymbol{X}_n)})= c \\sigma + b\\). Se for, então bastaria tomar \\[\nS'(\\boldsymbol{X}_n) = \\frac{\\sqrt{S^2_{n-1}(\\boldsymbol{X}_n)}-b}{c}\n\\] para obtermos um estimador não viciado. Sob normalidade, \\(\\bar{X}\\) e \\(S^2_{n-1}(\\boldsymbol{X}_n)\\) são independentes. Logo, \\(\\bar{X}\\) e \\(S'_{n-1}\\) também são independentes. Portanto, \\[\nE_\\theta(S'(\\boldsymbol{X}_n)\\lvert T''(\\boldsymbol{X}_n)) \\stackrel{\\mathrm{Ind.}}{=} E_\\theta(S'(\\boldsymbol{X}_n) \\lvert S^2_{n-1}(\\boldsymbol{X}_n))\n\\stackrel{\\mathrm{Cond.}}{=}S'(\\boldsymbol{X}_n).\n\\]\nVerificando:\nObserve que \\[\nW(\\sigma^2) = \\frac{(n-1)S^2_{n-1}(\\boldsymbol{X}_n)}{\\sigma^2} \\sim \\chi^2_{n-1}.\n\\] Logo, para \\(n&gt;1\\) \\[\n\\begin{aligned}\nE_\\theta(\\sqrt{S^2_{n-1}(\\boldsymbol{X}_n)}) &= E_\\theta\\left(\\frac{\\sqrt{n-1}}{\\sqrt{n-1}}\\frac{\\sqrt{\\sigma^2}}{\\sqrt{\\sigma^2}} \\sqrt{S^2_{n-1}(\\boldsymbol{X}_n)}\\right) \\\\\n&= E_\\theta\\left(\\frac{\\sqrt{\\sigma^2}}{\\sqrt{n-1}}\\sqrt{\\frac{(n-1)S^2_{n-1}(\\boldsymbol{X}_n)}{\\sigma^2}}\\right) \\\\\n&= \\frac{\\sigma}{\\sqrt{n-1}}E_\\theta\\left(\\sqrt{W(\\sigma^2)}\\right).\n\\end{aligned}\n\\]\nVamos calcular \\[\nE_\\theta\\left(\\sqrt{W(\\sigma^2})\\right) = \\int_0^\\infty \\sqrt{x} f_\\theta^{W(\\sigma^2)}(x) dx\n\\] em que \\[\nf_\\theta^{W(\\sigma^2)}(x) = \\frac{1}{2^{\\frac{n-1}{2}}\\Gamma(\\frac{n-1}{2})} x^{\\frac{n-1}{2}-1}\\mathrm{e}^{-\\frac{1}{2} x}\n\\]\nPortanto, \\[\n\\begin{aligned}\nE_\\theta\\left(\\sqrt{W(\\sigma^2)}\\right) &= \\int^\\infty_0 \\frac{x^{\\frac{1}{2}}x^{\\frac{n-1}{2}}\\mathrm{e}^{-\\frac{1}{2}x}}\n{2^{\\frac{n-1}{2}}\\Gamma\\left(\\frac{n-1}{2}\\right)} dx \\\\\n&= \\frac{1}{2^{\\frac{n-1}{2}}\\Gamma\\left(\\frac{n-1}{2}\\right)} \\int^\\infty_0 x^{\\frac{n}{2}-1} \\mathrm{e}^{-\\frac{x}{2}} dx \\\\\n&\\stackrel{x=2u}{=} \\frac{1}{2^{\\frac{n-1}{2}}\\Gamma\\left(\\frac{n-1}{2}\\right)}  \\int^\\infty_0 (2u)^{\\frac{n}{2}-1}\\mathrm{e}^{-u} 2du \\\\\n& = \\frac{2^{\\frac{n}{2}}}{2^{\\frac{n}{2} - \\frac{1}{2}}\\Gamma\\left(\\frac{n-1}{2}\\right)} \\int^\\infty_0 u^{\\frac{n}{2}-1} \\mathrm{e}^{-u} du \\\\\n&= \\frac{\\sqrt{2}}{\\Gamma\\left(\\frac{n-1}{2}\\right)}\n\\end{aligned}\n\\] Dessa forma, \\[\nE_\\theta\\left(\\sqrt{S^2_{n-1}(\\boldsymbol{X}_n)}\\right) = \\frac{\\sigma}{\\sqrt{n-1}} \\frac{\\sqrt{2}}{\\Gamma\\left(\\frac{n-1}{2}\\right)} \\Gamma\\left(\\frac{n}{2}\\right),\n\\] tomando \\[\nS'(\\boldsymbol{X}_n) = \\frac{\\sqrt{(n-1) S^2_{n-1}(\\boldsymbol{X}_n)}}{\\sqrt{2}} \\frac{\\Gamma\\left(\\frac{n-1}{2}\\right)}{\\Gamma\\left(\\frac{n}{2}\\right)},\n\\] é, como \\(S'(\\boldsymbol{X}_n) = c\\sqrt{S^2_{n-1}(\\boldsymbol{X}_n)}\\) é função 1:1 de \\(S^2_{n-1}(\\boldsymbol{X}_n)\\), o ENVVUM para \\(g(\\theta) = \\sigma\\).",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Estimadores não-viciados com variância uniformemente mínima (ENVVUM)</span>"
    ]
  },
  {
    "objectID": "envvum.html#sec-ex-pois",
    "href": "envvum.html#sec-ex-pois",
    "title": "25  Estimadores não-viciados com variância uniformemente mínima (ENVVUM)",
    "section": "25.4 Exemplo (Poisson)",
    "text": "25.4 Exemplo (Poisson)\nSeja \\(\\boldsymbol{X}_n\\) a.a. de \\(X\\sim\\mathrm{Poiss}(\\theta), \\theta \\in (0,\\infty)\\).\nEncontre o ENVVUM para \\(g(\\theta) = P_\\theta(X = 0)\\).\nPelo Teorema da FE k-dimensional, temos que \\[\nT(\\boldsymbol{X}_n) = \\sum X_i\n\\] é suficiente e completa para o modelo.\nNote que \\[\ng(\\theta) = P_\\theta(X=0) = \\mathrm{e^-\\theta}\n\\]\nUm estimador não-viesado para essa quantidade de interesse é \\(\\mathbb{1}_{\\{0\\}}(X_1)\\), uma vez que \\[\nP_\\theta(\\mathbb{1}_{\\{0\\}}(X_1) = P_\\theta(X_1 = 0) \\stackrel{\\mathrm{a.a.}}{=} P_\\theta(X = 0) = \\mathrm{e}^{-\\theta}\n\\]\nPortanto, pelo Teorema de Lehmamn-Scheffé, \\[\n\\stackrel{\\sim}{T} = E_\\theta(S(\\boldsymbol{X}_n)\\lvert T(\\boldsymbol{X}_n))\n\\] é o ENVVUM para \\(g(\\theta) = \\mathrm{e}^{-\\theta}\\)\n\numa estratégia é calcular \\(m(t) = E_\\theta(S(\\boldsymbol{X}_n)\\lvert T(\\boldsymbol{X}_n)=t)\\) e depois substituir \\(t\\) por \\(T(\\boldsymbol{X}_n)\\) em \\(m(t)\\). Note que, por definição,\n\\[\n\\begin{aligned}\nE_\\theta(S(\\boldsymbol{X}_n)\\lvert T(\\boldsymbol{X}_n)=t) &= \\sum_{s \\in \\{0,1\\}} s \\cdot P_\\theta(S(\\boldsymbol{X}_n) = s \\lvert T(\\boldsymbol{X}_n) = t) \\\\\n&= P_\\theta(S(\\boldsymbol{X}_n) = 1 \\lvert T(\\boldsymbol{X}_n) = t) \\\\\n&= \\frac{P_\\theta(S(\\boldsymbol{X}_n) = 1, T(\\boldsymbol{X}_n) = t)}{P_\\theta(T(\\boldsymbol{X}_n) = t)}\n\\end{aligned}\n\\]\nNote que, por FGM, \\[\nT(\\boldsymbol{X}_n) = \\sum X_i \\sim \\mathrm{Poiss}(n\\theta), \\theta \\in (0,\\infty)\n\\]\ne para \\(t \\in \\{0,1,\\dots\\}\\) \\[\n\\text{\"}S(\\boldsymbol{X}_n) = 1\\text{\"} \\iff \\text{\"}X_1 = 0\\text{\"} \\Rightarrow\nE_\\theta(S(\\boldsymbol{X}_n)\\lvert T(\\boldsymbol{X}_n) = t) = \\frac{P_\\theta(X_1=0\\lvert \\sum X_i =t)}{e^{-n\\theta}\\frac{(n\\theta)^t}{t!}}\n\\]\nNote que \\[\n\\begin{aligned}\nP_\\theta(X_1=0\\lvert \\sum X_i =t) &= P_\\theta(\\sum X_i =t \\lvert X_1 = 0) \\cdot P_\\theta(X_1 = 0) \\\\\n&=P_\\theta(X_1 + X_2 + \\dots + X_n = t \\lvert X_1 = 0)\\cdot \\mathrm{e}^{-\\theta} \\\\\n&=P_\\theta(0 + X_2 + \\dots + X_n = t \\lvert X_1 = 0)\\cdot \\mathrm{e}^{-\\theta} \\\\\n&\\stackrel{\\mathrm{iid}}{=}P_\\theta\\left(\\sum^n_{i=2} X_i = t - 0\\right)\\cdot \\mathrm{e}^{-\\theta} \\\\\n\\end{aligned}\n\\]\nComo, por FGM, \\[\n\\sum^n_{i=2} X_i \\sim \\mathrm{Poiss}((n-1)\\theta), \\theta \\in (0,\\infty)\n\\] temos que \\[\nP_\\theta(X_1 = 0, \\sum^n_{i=1} X_i = t) = \\frac{\\mathrm{e}^{-(n-1)\\theta}((n-1)\\theta)^t}{t!} \\mathrm{e}^{-\\theta}\n\\]\nLogo, \\[\nE_\\theta(S(\\boldsymbol{X}_n)\\lvert T(\\boldsymbol{X}_n) = t) = \\frac{(n-1)^t}{n^t} = \\left(1 - \\frac{1}{n}\\right)^t\n\\]\nPortanto, substituindo \\(t\\) por \\(T(\\boldsymbol{X}_n)\\), temos que \\[\nE_\\theta(S(\\boldsymbol{X}_n)\\lvert T(\\boldsymbol{X}_n)) = \\left(1 - \\frac{1}{n}\\right)^{T(\\boldsymbol{X}_n)}\n\\]",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Estimadores não-viciados com variância uniformemente mínima (ENVVUM)</span>"
    ]
  },
  {
    "objectID": "envvum.html#exemplo-patológico",
    "href": "envvum.html#exemplo-patológico",
    "title": "25  Estimadores não-viciados com variância uniformemente mínima (ENVVUM)",
    "section": "25.5 Exemplo “patológico”",
    "text": "25.5 Exemplo “patológico”\nSejam \\(\\boldsymbol{X}_n\\) a.a. de \\(X\\sim \\mathrm{Pois}(\\theta), \\theta \\in \\Theta = \\mathbb{R}_+\\) e \\(g(\\theta) = \\mathrm{e}^{-2\\theta}\\).\nPode-se mostrar que \\[\nT(\\boldsymbol{X}_n) = \\left(1 - \\frac{2}{n}\\right)^{\\sum X_i}\n\\] é o ENVVUM para \\(g(\\theta) = \\mathrm{e}^{-2\\theta}\\). Para \\(n=1\\), temos que \\[\nT(\\boldsymbol{X}_n) = (-1)^{X_1}\n\\] A quantidade de interesse está estritamente entre \\((0,1)\\), mas o estimador pode apenas assumir valores \\(-1\\) e \\(1\\).\nPara \\(n=2\\), temos que \\[\nT(\\boldsymbol{X}_n) = 0\n\\]\nAinda assim, em média, (como podemos verificar por simulação de Monte Carlo), ele estimará \\(g(\\theta)\\) sem viés com variância uniformemente mínima. Disso, temos que só deveríamos usar o ENVVUM para \\(n\\geq 3\\).\n\n\nusing Random, Distributions, Plots, LaTeXStrings\nRandom.seed!(15)\nfunction simulacaoENVVUM(M=10000, n=50)\n  # Seja X_n a.a. de X~Pois(theta), theta em (0, infinito). Vamos avaliar o\n  # ENVVUM para g(theta) = e^{-2theta}, T(X_n) = (1-2/n)^{soma de X_i}\n  theta = log(rand())^2 # fixo: número real (não uniformemente) aleatório\n  d = Poisson(theta)\n  estatisticas = []\n  for i in 1:M\n    amostra = rand(d, n)\n    envvum = (1-2/n)^(sum(amostra))\n    append!(estatisticas, envvum)\n  end\n  media = mean(estatisticas)\n  e2theta = MathConstants.e^(-2*theta)\n  h = histogram(estatisticas,\n                title = LaTeXString(\"Histograma de \\$T(X_{$n})\\$\"),\n                normalize=:pdf,\n                label=\"\",\n                xlabel=LaTeXString(\"\\$T(X_{$n})\\$\"),\n                ylabel=\"Frequência\")\n  vline!([e2theta],label=L\"e^{-2\\theta}\")\n  vline!([media],label=\"Média\")\n  display(h)\n  display(\"Média: $media\")\n  display(\"e^{-2 * theta} = $e2theta\")\nend\nsimulacaoENVVUM()\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\"Média: 0.07683263869408652\"\n\n\n\"e^{-2 * theta} = 0.0765628110644349\"\n\n\nPodemos ainda avaliar os nossos casos patológicos.\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\"Média: 0.999\"\n\n\n\"e^{-2 * theta} = 0.9993081423710376\"\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\"Média: 0.8187\"\n\n\n\"e^{-2 * theta} = 0.81973744188805\"",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Estimadores não-viciados com variância uniformemente mínima (ENVVUM)</span>"
    ]
  },
  {
    "objectID": "escore.html",
    "href": "escore.html",
    "title": "26  Função Escore",
    "section": "",
    "text": "26.1 Condições de regularidade (simples)\nSeja \\(\\boldsymbol{X}_n\\) amostra aleatória de \\(X\\sim f_\\theta, \\theta \\in \\Theta\\). Considere \\(\\boldsymbol{x}=(x_1,\\dots,x_n)\\) uma amostra observada. A função de verossimilhança \\(L_{\\boldsymbol{x}_n} : \\Theta \\rightarrow \\mathbb{R}\\) é definida por \\[\nL_{\\boldsymbol{x}_n}(\\theta) = f_\\theta^{\\boldsymbol{X}_n}(\\boldsymbol{x}_n)\n\\] em que \\(f_\\theta^{\\boldsymbol{X}_n}\\) é a função (densidade) de probabilidade da amostra. Assuma que \\(f_\\theta\\) é diferenciável com respeito a “\\(\\theta\\)”, \\(\\forall \\theta \\in \\Theta\\). A função escore é definida por \\[\nU : \\mathfrak{X} \\times \\Theta \\rightarrow \\mathbb{R}^P, \\Theta \\subseteq \\mathbb{R}^P\n\\] e \\(\\mathfrak{X} = \\{x : f_\\theta(x) &gt; 0\\}\\) é o suporte de \\(X\\), tal que \\[\nU(x,\\theta) = \\frac{\\partial \\ln f_\\theta(x)}{\\partial \\theta} = \\left(\n\\begin{array}{c}\n\\frac{\\partial \\ln f_\\theta(x)}{\\partial \\theta_1} \\\\\n\\vdots \\\\\n\\frac{\\partial \\ln f_\\theta(x)}{\\partial \\theta_P} \\\\\n\\end{array}\n\\right)\n\\] e a função escore da amostra é definida por \\[\nU_n(\\boldsymbol{x}_n,\\theta) = \\frac{\\partial \\ln L_{\\boldsymbol{x}_n}(\\theta)}{\\partial \\theta} = \\left(\n\\begin{array}{c}\n\\frac{\\partial \\ln L_{\\boldsymbol{x}_n}(\\theta)}{\\partial \\theta_1} \\\\\n\\vdots \\\\\n\\frac{\\partial \\ln L_{\\boldsymbol{x}_n}(\\theta)}{\\partial \\theta_P} \\\\\n\\end{array}\n\\right)\n\\]\nNote que, para amostras aleatórias, \\[\nU_n(\\boldsymbol{x}_n,\\theta) = \\sum^n_{i=1} U(x_i, \\theta)\n\\] pois, por definição, \\[\n\\begin{aligned}\nU_n(\\boldsymbol{x}_n, \\theta) = \\frac{\\partial \\ln L_{\\boldsymbol{x}_n}(\\theta)}{\\partial \\theta}\n&= \\frac{\\partial \\ln }{\\partial \\theta} \\prod f_\\theta(x_i) \\\\\n&= \\frac{\\partial }{\\partial \\theta} \\sum \\ln f_\\theta(x_i) \\\\\n&= \\sum \\frac{\\partial \\ln f_\\theta(x_i)}{\\partial \\theta} \\\\\n&= \\sum U(x_i, \\theta).\n\\end{aligned}\n\\]\n\\(C_1: \\mathfrak{X} = \\{x : f_\\theta (x) &gt; 0 \\}\\) não depende de “\\(\\theta\\)”.\n\\(C_2: f_\\theta\\) é duas vezes diferenciável com respeito a “\\(\\theta\\)” e suas derivadas são contínuas.\n\\(C_3:\\) é possível trocar as derivadas pela integrais da seguinte forma: \\[\n\\left\\{\\begin{aligned}\n&a)\\ \\frac{\\partial}{\\partial\\theta} \\int f_\\theta(x) dx &= \\int \\frac{\\partial}{\\partial \\theta} f_\\theta(x) dx \\\\\n&b)\\ \\frac{\\partial^2}{\\partial \\theta \\partial \\theta^T} \\int f_\\theta(x) dx &= \\int \\frac{\\partial^2}{\\partial \\theta \\partial \\theta^T} f_\\theta(x) dx\n\\end{aligned}\\right.\n\\]\n\\(C_4:\\) \\[\n\\left\\{\\begin{aligned}\n&\\mathrm{a})\\ E_\\theta\\left(\\lVert\\frac{\\partial}{\\partial\\theta} \\ln f_\\theta(x)\\rVert\\right) &lt; \\infty,\\ \\forall \\theta \\in \\Theta \\\\\n&\\mathrm{b})\\ E_\\theta\\left(\\lVert\\frac{\\partial^2}{\\partial\\theta\\partial\\theta^T} \\ln f_\\theta(x)\\rVert^2\\right) &lt; \\infty,\\ \\forall \\theta \\in \\Theta.\n\\end{aligned}\\right.\n\\]\nPara as duas últimas condições, substituímos no caso discreto as integrais por somatórios.\nNas provas e listas, se não afirmado o contrário (explicitamente ou com suporte dependendo de \\(\\theta\\)), os exemplos satisfazem as condições de regularidade.",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Função Escore</span>"
    ]
  },
  {
    "objectID": "escore.html#sec-condicoes",
    "href": "escore.html#sec-condicoes",
    "title": "26  Função Escore",
    "section": "",
    "text": "26.1.1 Teorema (das condições do escore)\nSeja \\(\\boldsymbol{X}_n\\) a.a. de \\(X\\sim f_\\theta, \\theta \\in \\Theta \\subseteq \\mathbb{R}^P\\). Se as condições \\(C_1 : C_4\\) estiverem satisfeitas, então \\[\nE_\\theta(U_n(\\boldsymbol{X}_n,\\theta)) = 0,\\ \\forall \\theta \\in \\Theta.\n\\]\n\n26.1.1.1 Prova\nNote que \\(U_n(\\boldsymbol{X}_n,\\theta) = \\sum U(X_i,\\theta)\\). Por \\(C_4(a)\\), podemos calcular a esperança: \\[\n\\begin{aligned}\nE_\\theta(U_n(\\boldsymbol{X}_n,\\theta)) &= \\sum E_\\theta(U(X_i,\\theta)) \\\\\n&\\stackrel{\\mathrm{id.}}{=} nE_\\theta(U(X,\\theta)) \\\\\n\\Rightarrow E_\\theta(U(X,\\theta)) &= \\int_{\\mathfrak{X}} U(x,\\theta)f_\\theta(x)dx \\\\\n&= \\int_{\\mathfrak{X}} \\frac{\\partial}{\\partial\\theta}\\ln f_\\theta(x) \\cdot f_\\theta(x) dx \\\\\n&= \\int_{\\mathfrak{X}} \\frac{\\partial}{\\partial\\theta} f_\\theta(x) dx, \\forall \\theta \\in \\Theta\n\\end{aligned}\n\\]\nNote que \\[\n\\int_{\\mathfrak{X}} f_\\theta(x)dx = 1, \\forall \\theta \\in \\Theta\n\\]\nLogo, por \\(C_3\\) \\[\n\\frac{\\partial}{\\partial \\theta} \\int_{\\mathfrak{X}}  f_\\theta(x) dx = 0\n= \\int_{\\mathfrak{X}} \\frac{\\partial}{\\partial \\theta} f_\\theta(x) dx,\\ \\forall \\theta \\in \\Theta\n\\]",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Função Escore</span>"
    ]
  },
  {
    "objectID": "infofisher.html",
    "href": "infofisher.html",
    "title": "27  Informação de Fisher",
    "section": "",
    "text": "27.1 Teorema (da informação de Fisher sob as condições)\nSeja \\(U(X,\\theta)\\) função escore. A informação de Fisher é a variância da função escore: \\[\nI_1(\\theta) = \\mathrm{Var}_\\theta(U(X,\\theta))\n\\]\nA informação de Fisher total é a variância da função escore da amostra: \\[\nI_n(\\theta) = \\mathrm{Var}_\\theta(U_n(\\boldsymbol{X}_n,\\theta))\n\\]\nNote que, sob as condições \\(C_1:C_4\\), temos que \\[\n\\begin{aligned}\n&E_\\theta(U(X,\\theta)) = 0,\\ \\forall \\theta \\in \\Theta \\\\\n&E_\\theta(U(\\boldsymbol{X}_n, \\theta)) = 0,\\ \\forall \\theta \\in \\Theta \\\\\n\\Rightarrow &\\left\\{\\begin{array}{ll}\nI_1(\\theta) = E_\\theta(U(X,\\theta)\\cdot U(X,\\theta)^T) \\\\\nI_n(\\theta) = E_\\theta(U_n(\\boldsymbol{X}_n,\\theta)\\cdot U_n(\\boldsymbol{X}_n,\\theta)^T)\\\\\n\\end{array}\\right.\\ \\forall \\theta \\in \\Theta \\\\\n&\\text{Se } \\Theta \\in \\mathbb{R}, \\\\\n\\Rightarrow &\\left\\{\\begin{array}{ll}\nI_1(\\theta) = E_\\theta(U(X,\\theta)^2) \\\\\nI_n(\\theta) = E_\\theta(U_n(\\boldsymbol{X}_n,\\theta)^2)\\\\\n\\end{array}\\right.\\ \\forall \\theta \\in \\Theta \\\\\n\\end{aligned}\n\\]\nSob as condições \\(C_1:C_4\\), temos que\n\\[\n\\left\\{\\begin{array}{ll}\nI_1(\\theta) = - E_\\theta\\left(\\frac{\\partial}{\\partial \\theta^T} U(X,\\theta)\\right) \\\\\nI_n(\\theta) = - E_\\theta\\left(\\frac{\\partial}{\\partial \\theta^T} U_n(\\boldsymbol{X}_n,\\theta)\\right)\n\\end{array}\\right.\\ \\forall \\theta \\in \\Theta\n\\]",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Informação de Fisher</span>"
    ]
  },
  {
    "objectID": "infofisher.html#sec-teo-infofisher",
    "href": "infofisher.html#sec-teo-infofisher",
    "title": "27  Informação de Fisher",
    "section": "",
    "text": "27.1.1 Prova\nSem perda de generalidade, considere apenas o caso contínuo e \\(\\Theta \\subseteq \\mathbb{R}\\). Para \\(\\Theta \\subseteq \\mathbb{R}^P\\), basta utilizar da transposição de \\(\\theta\\) nas derivadas e \\(U(X,\\theta)\\).\nSabemos que \\(U(X,\\theta) = \\frac{\\partial}{\\partial\\theta}f_\\theta(x)\\). Além disso, \\[\n\\int_{\\mathfrak{X}} f_\\theta(x) dx = 1,\\ \\forall \\theta \\in \\Theta\n\\] em que \\(\\mathfrak{X} = \\{x : f_\\theta(x) &gt; 0\\}\\) não depende de “\\(\\theta\\)”.\nPor \\(C_3(b)\\), \\[\n\\frac{\\partial^2}{\\partial \\theta \\partial \\theta^T} \\int_{\\mathfrak{X}} f_\\theta(x) dx\n= \\int_{\\mathfrak{X}} \\frac{\\partial^2}{\\partial \\theta \\partial \\theta^T} f_\\theta(x) dx = 0, \\forall \\theta \\in \\Theta\n\\tag{27.1}\\]\nNote que \\[\n\\begin{aligned}\nI_1(\\theta) &= E_\\theta(U(X,\\theta)^2) \\\\\n&= \\int_{\\mathfrak{X}} U(x,\\theta)^2 f_\\theta(x)dx \\\\\n&= \\int_{\\mathfrak{X}} \\left[\\frac{\\partial \\ln}{\\partial \\theta} f_\\theta(x)\\right]^2 f_\\theta(x)dx,\\ \\forall \\theta \\in \\Theta\n\\end{aligned}\n\\]\nObserve que, por (27.1), \\[\n\\begin{aligned}\n&\\int_{\\mathfrak{X}}\\frac{\\partial}{\\partial\\theta} \\left[\\frac{\\partial f_\\theta(x)}{\\partial \\theta} \\frac{f_\\theta(x)}{f_\\theta(x)}\\right]dx = 0\\\\\n\\Rightarrow &\\int_{\\mathfrak{X}}\\frac{\\partial}{\\partial\\theta} \\left[\\frac{\\partial \\ln f_\\theta(x)}{\\partial \\theta} f_\\theta(x)\\right]dx = 0 \\\\\n\\Rightarrow &\\int_{\\mathfrak{X}}\\frac{\\partial}{\\partial\\theta}\n\\left[\\frac{\\partial^2 \\ln f_\\theta(x)}{\\partial \\theta^2} f_\\theta(x) +\n\\frac{\\partial\\ln f_\\theta(x)}{\\partial\\theta}\\frac{\\partial f_\\theta(x)}{\\partial\\theta}\\right]dx = 0\\\\\n\\Rightarrow &\\int_{\\mathfrak{X}}\n\\frac{\\partial^2 \\ln f_\\theta(x)}{\\partial \\theta^2} f_\\theta(x) +\n\\int_{\\mathfrak{X}} \\frac{\\partial\\ln f_\\theta(x)}{\\partial\\theta}\\frac{\\partial f_\\theta(x)}{\\partial\\theta}dx = 0\\\\\n\\Rightarrow &E_\\theta\\left(\n\\frac{\\partial^2 \\ln }{\\partial \\theta^2} f_\\theta(X)\\right) +\n\\int_{\\mathfrak{X}} \\frac{\\partial\\ln f_\\theta(x)}{\\partial\\theta}\\frac{\\partial\\ln f_\\theta(x)}{\\partial\\theta}f_\\theta(x)dx = 0\\\\\n\\Rightarrow &-E_\\theta\\left(\n\\frac{\\partial^2 \\ln }{\\partial \\theta^2} f_\\theta(X)\\right) =\nE_\\theta\\left(\\left[\\frac{\\partial\\ln}{\\partial\\theta}f_\\theta(X)\\right]^2\\right) \\\\\n\\Rightarrow &E_\\theta(U(X,\\theta)^2)= -E_\\theta\\left(\\frac{\\partial U(X,\\theta)}{\\partial \\theta}\\right) \\\\\n\\Rightarrow &I_1(\\theta) = -E_\\theta\\left(\\frac{\\partial U(X,\\theta)}{\\partial \\theta}\\right),\\ \\forall \\theta \\in \\Theta.\n\\end{aligned}\n\\]\nAdemais, temos que \\[\nI_n(\\theta) = \\mathrm{Var}_\\theta(U_n(\\boldsymbol{X}_n, \\theta)) \\stackrel{C_1:C_4}{=} E_\\theta(U_n(\\boldsymbol{X}_n,\\theta)^2)\n\\] Sabemos que \\(U_n(\\boldsymbol{X}_n,\\theta) = \\sum\\limits^n_{i=1} U(X_i,\\theta)\\). Portanto, \\[\n\\begin{aligned}\nI_n(\\theta) &= E_\\theta\\left[\\left( \\sum U(X_i,\\theta)\\right)^2\\right]\\\\\n&= E_\\theta(\\sum_i\\sum_j U(X_i,\\theta)U(X_j,\\theta)) \\\\\n&= E_\\theta\\left(\\sum_{i=1}^n U(X_i,\\theta)^2 + \\sum_{i\\neq j} U(X_i,\\theta) U(X_j,\\theta)\\right) \\\\\n&= \\sum E_\\theta (U(X_i,\\theta)^2) + \\sum_{i\\neq j }E_\\theta(U(X_i,\\theta)U(X_j,\\theta)), \\forall \\theta \\in \\Theta.\n\\end{aligned}\n\\]\nComo \\(X_1,\\dots,X_n\\) são i.i.d, temos que \\(U(X_1,\\theta),\\dots,U(X_n,\\theta)\\) também são i.i.d. Logo, \\[\nI_n(\\theta) = \\sum E_\\theta(U(X_i,\\theta)^2) + \\sum_{i\\neq j} E_\\theta(\\cancelto{0}{U(X_i,\\theta)}) E_\\theta(\\cancelto{0}{U(X_j,\\theta)})\n\\]\nPor ser i.i.d., temos portanto que \\[\nI_n(\\theta) = nI_1(\\theta)\n\\]\nNote também que \\[\n\\begin{aligned}\nE_\\theta\\left(\\frac{\\partial}{\\partial\\theta}U(\\boldsymbol{X}_n, \\theta)\\right) &= E_\\theta\\left(\\frac{\\partial}{\\partial \\theta} \\sum U(X_i,\\theta)\\right) \\\\\n&= E_\\theta\\left(\\sum \\frac{\\partial}{\\partial \\theta} U(X_i,\\theta)\\right) \\\\\n&= n E_\\theta\\left(\\frac{\\partial}{\\partial \\theta} U(X_i,\\theta)\\right),\\ \\forall \\theta \\in \\Theta.\n\\end{aligned}\n\\]\nComo \\(I_1(\\theta) = - E_\\theta\\left(\\frac{\\partial}{\\partial} U(X,\\theta)\\right)\\). Temos que \\[\n\\begin{aligned}\n&-E_\\theta\\left(\\frac{\\partial}{\\partial \\theta} U(\\boldsymbol{X}_n,\\theta)\\right) = n I_1(\\theta) \\\\\n\\Rightarrow &I_n(\\theta) = nI_1(\\theta) = -E_\\theta\\left(\\frac{\\partial}{\\partial \\theta} U(\\boldsymbol{X}_n,\\theta)\\right), \\forall \\theta \\in \\Theta._\\blacksquare\n\\end{aligned}\n\\]",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Informação de Fisher</span>"
    ]
  },
  {
    "objectID": "infofisher.html#exemplo",
    "href": "infofisher.html#exemplo",
    "title": "27  Informação de Fisher",
    "section": "27.2 Exemplo",
    "text": "27.2 Exemplo\nSeja \\(\\boldsymbol{X}_n\\) a.a de \\(X\\sim f_\\theta, \\theta\\in \\mathbb{R}_+\\), tal que \\[\nf_\\theta(x) = \\left\\{\\begin{array}{ll}\n\\theta x^{\\theta-1},& x \\in (0,1) \\\\\n0,& \\mathrm{c.c.}\n\\end{array}\\right.\n\\]\nEncntre a função escore da amostra e a informação de Fisher total.\n\n27.2.1 Resposta\nNote que \\[\nL_{\\boldsymbol{X}_n}(\\theta) = \\prod^n \\left\\{ \\theta x_i^{\\theta-1}\\right\\} = \\theta^n (\\prod x_i)^{\\theta-1},\\ \\forall \\theta \\in \\Theta.\n\\]\nA função escore da amostra é \\[\n\\begin{aligned}\nU_n(\\boldsymbol{X}_n,\\theta) &= \\frac{\\partial}{\\partial \\theta} \\ln L_{\\boldsymbol{X}_n} (\\theta) \\\\\n&= \\frac{\\partial}{\\partial \\theta} \\left[n \\ln \\theta + (\\theta-1) \\sum \\ln x_i\\right] \\\\\n&= \\frac{n}{\\theta} + \\sum \\ln x_i, \\forall \\theta \\in \\Theta.\n\\end{aligned}\n\\]\nA informação de Fisher total é, pelo teorema, \\[\nI_n(\\theta) = - E_\\theta\\left(\\frac{\\partial}{\\partial \\theta} U_n(\\boldsymbol{X}_n,\\theta)\\right), \\forall \\theta \\in \\Theta.\n\\]\nNote que \\[\n\\frac{\\partial}{\\partial \\theta} U_n(\\boldsymbol{X}_n,\\theta) = - \\frac{n}{\\theta^2}, \\forall \\theta \\in \\Theta.\n\\]\nPortanto, \\[\nI_n(\\theta) = -\\frac{n}{\\theta^2}, \\forall \\theta \\in \\Theta.\n\\]",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Informação de Fisher</span>"
    ]
  },
  {
    "objectID": "infofisher.html#sec-ex-bin",
    "href": "infofisher.html#sec-ex-bin",
    "title": "27  Informação de Fisher",
    "section": "27.3 Exemplo (Binomial)",
    "text": "27.3 Exemplo (Binomial)\nSeja \\(\\boldsymbol{X}_n\\) a.a. de \\(X \\sim \\mathrm{Bin}(3,\\theta), \\theta \\in (0,1)\\). Encontre a função escore e a informação de Fisher total.\n\n27.3.1 Resposta\nNote que \\[\n\\begin{aligned}\nL_{\\boldsymbol{X}_n}(\\theta) &= \\prod f_\\theta(x) = \\prod \\left\\{\\binom{3}{x_i} \\theta^{x_i} (1-\\theta)^{3-x}\\right\\} \\\\\n\\Rightarrow \\ln L_{\\boldsymbol{X}_n}(\\theta) &= \\sum \\ln \\binom{3}{x_i} + \\sum x_i \\ln \\theta + \\sum (3-x_i)\\ln(1-\\theta) \\\\\n\\Rightarrow U_n(\\boldsymbol{x}_n,\\theta) &= \\sum \\frac{x_i}{\\theta} + \\sum \\frac{(3-x_i)}{1-\\theta} (-1) \\\\\n&= \\frac{x_i}{\\theta} - \\sum \\frac{(3-x_i)}{1-\\theta},\\ \\forall \\theta \\in \\Theta.\n\\end{aligned}\n\\]\nA informação de Fisher total é, pelo teorema, \\[\n\\begin{aligned}\nI_n(\\theta) &= -E_\\theta\\left(\\frac{\\partial}{\\partial\\theta} U_n(\\boldsymbol{X}_n,\\theta)\\right) \\\\\n&= -E_\\theta\\left( - \\sum \\frac{X_i}{\\theta^2} + \\sum \\frac{(3-X)}{(1-\\theta)^2}(-1)\\right) \\\\\n&= E_\\theta\\left(\\sum \\frac{X_i}{\\theta^2} + \\sum \\frac{(3-X)}{(1-\\theta)^2}\\right) \\\\\n&= \\sum \\frac{E_\\theta(X_i)}{\\theta^2} + \\sum \\frac{3 - E_\\theta(X)}{(1-\\theta)^2} \\\\\n&= \\frac{n 3\\theta}{\\theta^2} + \\frac{n(3-3\\theta)}{(1-\\theta)^2} \\\\\n&= \\frac{ 3n}{\\theta} + \\frac{3n(1-\\theta)}{(1-\\theta)^2} \\\\\n&= \\frac{3n}{\\theta} + \\frac{3n}{(1-\\theta)} \\\\\n&= \\frac{3n}{\\theta(1-\\theta)},\\ \\forall \\theta \\in \\Theta.\n\\end{aligned}\n\\]\nPodemos também fazer pela definição.\nObserve que \\[\n\\begin{aligned}\nE_\\theta(U_n(\\boldsymbol{X}_n,\\theta) &= E_\\theta\\left(\\sum \\frac{X_i}{\\theta} - \\sum \\frac{3-X_i}{1-\\theta}\\right) \\\\\n&= \\sum \\frac{E_\\theta(X_i)}{\\theta} - \\frac{(3-E_\\theta(X_i)}{1-\\theta} \\\\\n&= \\frac{n 3 \\theta}{\\theta} - \\frac{n (3- 3\\theta)}{1-\\theta} \\\\\n&= n3 - n3 \\frac{1-\\theta}{1-\\theta} = 0,\\ \\forall \\theta \\in \\Theta.\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nI_n(\\theta) &= \\mathrm{Var}_\\theta(U_n(\\boldsymbol{X}_n,\\theta)) \\\\\n&= E_\\theta(U_n(\\boldsymbol{X},\\theta)^2) \\\\\n&= E_\\theta\\left(\\left[\\sum \\left(\\frac{X_i}{\\theta} - \\frac{(3-X_i)}{1-\\theta}\\right)\\right]^2\\right) \\\\\n&= E_\\theta\\left(\\sum_i \\sum_j \\left(\\frac{X_i}{\\theta} - \\frac{(3-X_i)}{1-\\theta}\\right)\\left(\\frac{X_j}{\\theta} - \\frac{(3-X_j)}{1-\\theta}\\right)\\right) \\\\\n&= \\sum_i E_\\theta\\left(\\left(\\frac{X_i}{\\theta} - \\frac{(3-X_i)}{1-\\theta}\\right)^2\\right) \\\\\n&\\cdot\\underset{i \\neq j}{\\sum\\sum}E_\\theta\\left(\\cancelto{0}{\\frac{X_i}{\\theta} - \\frac{(3-X_i)}{1-\\theta}}\\right)\nE_\\theta\\left(\\cancelto{0}{\\frac{X_j}{\\theta} - \\frac{(3-X_j)}{1-\\theta}}\\right)\\\\\n&= n E_\\theta\\left[\\left( \\frac{X}{\\theta} - \\frac{3-X}{1-\\theta}\\right)^2\\right] \\\\\n&= n E_\\theta\\left[\\left( \\frac{X(1-\\theta)-(3-x)\\theta}{\\theta(1-\\theta)}\\right)^2\\right] \\\\\n&= \\frac{n E_\\theta\\left[(X - \\theta X - 3\\theta + \\theta X)^2\\right]}{\\theta(1-\\theta)} \\\\\n&= \\frac{n E_\\theta\\left[(X - 3\\theta)^2\\right]}{\\theta^2(1-\\theta)^2} \\\\\n&= \\frac{n \\mathrm{Var}_\\theta}{\\theta^2(1-\\theta)^2} = \\frac{n3\\theta(1-\\theta)}{\\theta^2(1-\\theta)^2}\\\\\n&= \\frac{3n}{\\theta(1-\\theta)},\\ \\forall \\theta \\in \\Theta.\n\\end{aligned}\n\\]",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Informação de Fisher</span>"
    ]
  },
  {
    "objectID": "infofisher.html#sec-licr",
    "href": "infofisher.html#sec-licr",
    "title": "27  Informação de Fisher",
    "section": "27.4 Teorema (Desigualdade da informação, LICR)",
    "text": "27.4 Teorema (Desigualdade da informação, LICR)\nSeja \\(\\boldsymbol{X}_n\\) a.a. de \\(X\\sim f_\\theta, \\theta \\in \\Theta \\subseteq \\mathbb{R}\\), em que \\(f_\\theta\\) satisfaz as condições \\(C_1:C_4\\). Considere \\(T(\\boldsymbol{X}_n)\\) um estimador não-viciado para \\(g(\\theta) = \\theta\\). Então, \\[\n\\mathrm{Var}_\\theta(T(\\boldsymbol{X}_n)) \\geq \\frac{1}{n I_1(\\theta)} = \\frac{1}{I_n(\\theta)}, \\forall \\theta \\in \\Theta,\n\\] sempre que \\(E_\\theta(T(\\boldsymbol{X}_n)^2)&lt;\\infty, \\forall \\theta \\in \\Theta\\), em que \\(I_1(\\theta)\\) é a informação de Fisher e \\(I_n(\\theta)\\) é a informação de Fisher total.\n\n\n\n\n\n\nObservação\n\n\n\nA quantidade \\(\\frac{1}{I_n(\\theta)}\\) é chamada de Limite Inferior de Crammer Rao (LICR)\n\n\n\n27.4.1 Prova\nComo \\(T(\\boldsymbol{X}_n)\\) é não-viciado para \\(g(\\theta) = \\theta\\), temos que \\[\nE_\\theta(T(\\boldsymbol{X}_n)) = \\theta, \\forall \\theta \\in \\Theta.\n\\]\nPor definição de esperança (para o caso contínuo), \\[\nE_\\theta(T(\\boldsymbol{X}_n)) = \\int_{-\\infty}^{\\infty} \\dots \\int_{-\\infty}^{\\infty} T(x_1,\\dots,x_n) f_\\theta^{\\boldsymbol{X}_n}(x_1,\\dots,x_n) dx_1,\\dots,dx_n\n\\tag{27.2}\\]\n\n\n\n\n\n\nObservação\n\n\n\n\\[\nE((X_1+X_2)^2) = \\int^\\infty_{-\\infty} \\int^\\infty_{-\\infty} (x_1 + x_2)^2 f^{X_1, X_2}_\\theta(x_1,x_2) dx_1dx_2\n\\]\n\n\nDiferenciando (27.2) com relação a “\\(\\theta\\)” \\[\n\\begin{aligned}\n&\\frac{\\partial}{\\partial \\theta}\n\\int_{-\\infty}^{\\infty} \\dots \\int_{-\\infty}^{\\infty} T(\\boldsymbol{x}_n) f_\\theta^{\\boldsymbol{X}_n} d\\boldsymbol{x}_n = 1 \\\\\n&\\stackrel{C_3}{\\Rightarrow}  \\int_{-\\infty}^{\\infty} \\dots \\int_{-\\infty}^{\\infty}\nT(\\boldsymbol{x}_n) \\frac{\\partial}{\\partial \\theta} f^{\\boldsymbol{X}_n}_\\theta(\\boldsymbol{x}_n) = 1 \\\\\n&\\Rightarrow  \\int_{-\\infty}^{\\infty} \\dots \\int_{-\\infty}^{\\infty}\nT(\\boldsymbol{x}_n) \\frac{\\partial\\ln}{\\partial \\theta} f_\\theta^{\\boldsymbol{X}_n}(\\boldsymbol{x}_n)\nf_\\theta^{\\boldsymbol{X}_n}(\\boldsymbol{x}_n) d\\boldsymbol{x}_n= 1,\\forall \\theta \\in \\Theta. \\\\\n&\\Rightarrow E_\\theta(T(\\boldsymbol{X}_n) \\cdot U_n(\\boldsymbol{X}_n,\\theta)) = 1, \\forall \\theta \\in \\Theta.\n\\end{aligned}\n\\]\nNote que \\[\n\\begin{aligned}\n\\mathrm{Cor}_\\theta(T(\\boldsymbol{X}_n), U_n(\\boldsymbol{X}_n,\\theta))^2 \\leq 1, \\forall \\theta \\in \\Theta \\\\\n\\iff \\frac{\\mathrm{Cov}_\\theta(T(\\boldsymbol{X}_n), U_n(\\boldsymbol{X}_n,\\theta))^2}{\\mathrm{Var}_\\theta (T(\\boldsymbol{X}_n))\\mathrm{Var}_\\theta(U_n(\\boldsymbol{X}_n,\\theta))} \\leq 1, \\forall \\theta \\in \\Theta.\n\\end{aligned}\n\\tag{27.3}\\]\nAdemais, \\[\n\\mathrm{Cov}_\\theta(T(\\boldsymbol{X}_n), U_n(\\boldsymbol{X}_n,\\theta)) = E_\\theta(T(\\boldsymbol{X}_n \\cdot U_n(\\boldsymbol{X}_n,\\theta)) -\n\\cancelto{\\theta}{E_\\theta(T(\\boldsymbol{X}_n)} \\cdot \\cancelto{0}{E_\\theta(U_n(\\boldsymbol{X}_n,\\theta))} = 1, \\forall \\theta \\in \\Theta\n\\]\nSubstituindo em (27.3), temos que\n\\[\n\\frac{1^2}{\\mathrm{Var}_\\theta (T(\\boldsymbol{X}_n))\\mathrm{Var}_\\theta(U_n(\\boldsymbol{X}_n,\\theta))} \\leq 1, \\forall \\theta \\in \\Theta.\n\\]\nPor definição de informação de Fisher total, \\[\n\\frac{1}{I_n(\\theta)} \\leq \\mathrm{Var}_\\theta(T(\\boldsymbol{X}_n)), \\forall \\theta \\in \\Theta.\n\\]",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Informação de Fisher</span>"
    ]
  },
  {
    "objectID": "infofisher.html#teorema-generalização",
    "href": "infofisher.html#teorema-generalização",
    "title": "27  Informação de Fisher",
    "section": "27.5 Teorema (generalização)",
    "text": "27.5 Teorema (generalização)\nSeja \\(\\boldsymbol{X}_n\\) a.a. de \\(X \\sim f_\\theta, \\theta \\in \\Theta\\), em que \\(f_\\theta\\) satisfaz as condições \\(C_1,C_4\\). Considere \\(T(\\boldsymbol{X}_n)\\) um estimador não-viciado para \\(g(\\theta) \\in \\mathbb{R}\\) cuja derivada com respeito a “\\(\\theta\\)” existe \\(\\forall \\theta \\in \\Theta\\). Então, \\[\n\\mathrm{Var}_\\theta(T(\\boldsymbol{X}_n)) \\geq \\frac{g'(\\theta)^2}{I_n(\\theta)}, \\forall \\theta \\in \\Theta.\n\\] sempre que \\(E_\\theta(T(\\boldsymbol{X}_n)^2) &lt; \\infty, \\forall \\theta \\in \\Theta\\).\n\n27.5.1 Prova\nExercício!!!",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Informação de Fisher</span>"
    ]
  },
  {
    "objectID": "estimadores-eficientes.html",
    "href": "estimadores-eficientes.html",
    "title": "28  Estimadores Eficientes",
    "section": "",
    "text": "28.1 Exemplo (Binomial)\nDizemos que \\(T(\\boldsymbol{X}_n)\\) é um estimador eificiente para \\(g(\\theta)\\) se, e somente se,\nSeja \\(\\boldsymbol{X}_n\\) amostra aleatória de \\(X\\sim \\mathrm{Bin}(3,\\theta), \\theta \\in \\Theta = (0,1)\\).\na-) Verifique se \\(\\frac{\\bar{X}}{3} = \\frac{1}{n} \\sum X_i \\frac{1}{3}\\) é suficiente para \\(g(\\theta) = \\theta\\).\nb-) Verifique se \\(\\bar{X} = \\frac{1}{n} \\sum X_i\\) é suficiente para \\(g(\\theta) = 3\\theta\\).",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Estimadores Eficientes</span>"
    ]
  },
  {
    "objectID": "estimadores-eficientes.html#exemplo-binomial",
    "href": "estimadores-eficientes.html#exemplo-binomial",
    "title": "28  Estimadores Eficientes",
    "section": "",
    "text": "28.1.1 Resposta a-)\nJá sabemos que \\[\n\\begin{aligned}\nE_\\theta\\left(\\frac{\\bar{X}}{3}\\right) &= \\theta, \\forall \\theta \\in \\Theta. \\\\ \\\\\n\\mathrm{Var}_\\theta\\left(\\frac{\\bar{X}}{3}\\right) &= \\frac{1}{9n^2} \\mathrm{Var}\\left(\\sum X_i\\right) \\\\\n&\\stackrel{\\mathrm{iid}}{=} \\frac{1}{9n^2} \\cdot n \\mathrm{Var}_\\theta(X) \\\\\n&= \\frac{1}{9n} 3\\theta(1-\\theta),\\ \\forall \\theta \\in \\Theta.\n\\end{aligned}\n\\]\nJá calculamos a informação de Fisher total. \\[\nI_n(\\theta) = \\frac{3n}{\\theta(1-\\theta)}\n\\]\nComo \\[\n\\mathrm{Var}_\\theta\\left(\\frac{\\bar{X}}{3}\\right) = \\frac{1}{I_n(\\theta)}, \\forall \\theta \\in \\Theta,\n\\] \\(\\frac{\\bar{X}}{3}\\) é um estimador eficiente para \\(g(\\theta) = \\theta\\)\n\n\n28.1.2 Resposta b-)\nSabemos que \\[\n\\begin{aligned}\nE_\\theta(\\bar{X}) &= 3\\theta,\\ \\forall \\theta \\in \\Theta \\\\ \\\\\n\\mathrm{Var}_\\theta(\\bar{X}) &= \\frac{3}{n} \\theta(1-\\theta),\\ \\forall \\theta \\in \\Theta.\n\\end{aligned}\n\\]\nNote que \\[\n\\frac{g'(\\theta)^2}{I_n(\\theta)} = \\frac{\\theta(1-\\theta)}{3n} [3]^2 = \\frac{3\\theta(1-\\theta)}{n}\n\\]\nLogo, \\(\\bar{X}\\) é eficiente para \\(g(\\theta) = 3\\theta\\).",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Estimadores Eficientes</span>"
    ]
  },
  {
    "objectID": "estimadores-eficientes.html#exemplo-poisson",
    "href": "estimadores-eficientes.html#exemplo-poisson",
    "title": "28  Estimadores Eficientes",
    "section": "28.2 Exemplo (Poisson)",
    "text": "28.2 Exemplo (Poisson)\nSeja \\(\\boldsymbol{X}_n\\) a.a. de \\(X\\sim \\mathrm{Poiss}(\\theta), \\theta \\in \\Theta = \\mathbb{R}_+\\)\na-) Verifique se \\(\\bar{X}\\) é eficiente para \\(g(\\theta) = \\theta\\).\nb-) Verifique se \\(T(\\boldsymbol{X}_n) = \\left(1 - \\frac{1}{n}\\right)^{\\sum X_i}\\) é eficiente para \\(g(\\theta) = P_\\theta(X=0)\\).\n\n28.2.1 Resposta a-)\nJá sabemos que\n\\[\n\\begin{aligned}\nE_\\theta(\\bar{X}) &= \\theta,\\ \\forall \\theta \\in \\Theta \\\\\n\\mathrm{Var}_\\theta(\\bar{X}) &= \\frac{\\theta}{n},\\ \\forall \\theta \\in \\Theta.\n\\end{aligned}\n\\]\nComo \\(T(\\boldsymbol{X}_n) = \\bar{X}\\) é não-viciado, precisamos apenas verificar se \\[\n\\mathrm{Var}_\\theta(\\bar{X}) = \\frac{1}{I_n(\\theta)}\n\\]\nNote que \\[\n\\begin{aligned}\nL_{\\boldsymbol{x}_n}(\\theta) &= f_\\theta^{\\boldsymbol{X}_n}(\\theta) = \\prod \\left\\{e^{-\\theta} \\frac{\\theta^{x_i}}{x_i!}\\right\\} \\\\\n&\\Rightarrow \\ln L_{\\boldsymbol{x}_n}(\\theta) = -n\\theta + \\left(\\sum x_i\\right) \\ln \\theta - \\sum \\ln (x_i) \\\\\n&\\Rightarrow \\frac{\\partial}{\\partial\\theta}\\ln L_{\\boldsymbol{x}_n}(\\theta) = -n + \\frac{\\sum x_i}{\\theta} \\\\\n&\\Rightarrow \\frac{\\partial^2}{\\partial\\theta^2}\\ln L_{\\boldsymbol{x}_n}(\\theta) =  -\\frac{\\sum x_i}{\\theta^2}\n\\end{aligned}\n\\]\nPelo Teorema, como valem as condições \\(C_1:C_4\\), \\[\n\\begin{aligned}\nI_n(\\theta) &= - E_\\theta\\left(\\frac{\\partial^2 \\ln L_{\\boldsymbol{X}}(\\theta)}{\\partial\\theta^2}\\right) \\\\\n&= - E_\\theta\\left(- \\sum \\frac{X_i}{\\theta^2}\\right) \\\\\n&\\stackrel{\\mathrm{iid}}{=} \\frac{1}{\\theta^2} n E_\\theta(X) = \\frac{n}{\\theta}, \\forall \\theta \\in \\Theta.\n\\end{aligned}\n\\]\nComo \\(\\mathrm{Var}_\\theta(\\bar{X}) = \\frac{1}{I_n(\\theta)}\\), temos que \\(\\bar{X}\\) é eficiente para \\(g(\\theta) = \\theta\\).\n\n\n28.2.2 Resposta b-)\nNote que \\(\\sum X_i \\sim \\mathrm{Poiss}(n\\theta), \\forall \\theta \\in \\Theta\\) (por F.G.M.).\n\n\n\n\n\n\nNota\n\n\n\n\\[\nM_{\\sum X_i}(t) = E_\\theta\\left(\\mathrm{e}^{t\\sum X_i}\\right) = \\mathrm{e}^{n\\theta (e^t - 1)}\n\\]\n\n\nObserve que \\[\nE_\\theta\\left(\\left(1-\\frac{1}{n}\\right)^{\\sum X_i}\\right) = E_\\theta\\left( \\mathrm{e}^{\\left(\\sum X_i\\right) \\ln \\left(1-\\frac{1}{n}\\right)}\\right)\n\\] pois \\(n &gt; 1\\).\nTome \\(t = \\ln\\left(1-\\frac{1}{n}\\right)\\). \\[\n\\begin{aligned}\nE_\\theta\\left(\\left(1-\\frac{1}{n}\\right)^{\\sum X_i}\\right) = M_{\\sum X_i}\\left(\\ln\\left(1-\\frac{1}{n}\\right)\\right)\n&= \\mathrm{e}^{n\\theta\\left(\\mathrm{e}^{\\ln\\left(1-\\frac{1}{n}\\right)}-1\\right)} \\\\\n&= \\mathrm{e}^{n\\theta\\left(1-\\frac{1}{n}-1\\right)} \\\\\n&= \\mathrm{e}^{-\\theta}, \\forall \\theta \\in \\Theta. \\\\\n\\end{aligned}\n\\] Portanto, \\(E_\\theta(T(\\boldsymbol{X}_n)) = \\mathrm{e}^{-\\theta} = P_\\theta(X = 0) = g(\\theta)\\), ou seja, é não-viciada.\nNote que \\[\n\\begin{aligned}\n\\mathrm{Var}_\\theta(T(\\boldsymbol{X}_n)) &= E_\\theta(T(\\boldsymbol{X}_n)^2) -  E_\\theta(T(\\boldsymbol{X}_n))^2 \\\\\n&= E_\\theta(T(\\boldsymbol{X}_n)^2) -  \\mathrm{e}^{-2\\theta} \\\\ \\\\\nE_\\theta(T(\\boldsymbol{X}_n)^2) &= E_\\theta\\left(\\left(1-\\frac{1}{n}\\right)^{2 \\sum X_i}\\right) \\\\\n&= E_\\theta\\left(\\mathrm{e}^{2\\left(\\sum X_i\\right)\\ln\\left(1-\\frac{1}{n}\\right)}\\right) \\\\\n&= M_{\\sum X_i} \\left(2\\ln\\left(1- \\frac{1}{n}\\right)\\right) \\\\\n&= \\mathrm{e}^{n\\theta \\left(\\mathrm{e}^{2\\ln\\left(1- \\frac{1}{n}\\right)}-1\\right)} \\\\\n&= \\mathrm{e}^{n\\theta \\left(\\mathrm{e}^{\\ln\\left(1- \\frac{1}{n}\\right)^2}-1\\right)} \\\\\n&= \\mathrm{e}^{n\\theta \\left(\\left(1- \\frac{1}{n}\\right)^2-1\\right)} \\\\\n&= \\mathrm{e}^{n\\theta\\left(1 - \\frac{2}{n} + \\frac{1}{n^2} - 1\\right)} \\\\\n&= \\mathrm{e}^{\\theta\\left(\\frac{1}{n} - 2\\right)} = \\mathrm{e}^{-2\\theta} \\cdot \\mathrm{e}^{\\frac{\\theta}{n}}. \\\\ \\\\\n\\Rightarrow \\mathrm{Var}_\\theta(T(\\boldsymbol{X}_n)) &= \\mathrm{e}^{-2\\theta} \\cdot \\mathrm{e}^{\\frac{\\theta}{n}} - \\mathrm{e}^{-2\\theta} \\\\\n&= \\mathrm{e}^{-2\\theta}\\left(\\mathrm{e}^{\\frac{\\theta}{n}} - 1\\right)\n\\end{aligned}\n\\]\nTemos nosso LICR \\[\n\\frac{g'(\\theta)^2}{I_n(\\theta)} = \\frac{\\theta \\mathrm{e}^{-2\\theta}}{n} = \\mathrm{LICR}\n\\]\nComo \\(\\mathrm{Var}_\\theta \\neq \\frac{g'(\\theta)}{I_n(\\theta)}\\) para algum \\(\\theta \\in \\Theta\\), como \\(\\theta = 1\\), temos que, apesar de ser ENVVUM, \\(T(\\boldsymbol{X}_n)\\) não é eficiente para \\(g(\\theta) = \\mathrm{e}^{-\\theta}\\).",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Estimadores Eficientes</span>"
    ]
  },
  {
    "objectID": "estimadores-eficientes.html#sec-efassin",
    "href": "estimadores-eficientes.html#sec-efassin",
    "title": "28  Estimadores Eficientes",
    "section": "28.3 Eficiência assintótica para \\(g(\\theta)\\)",
    "text": "28.3 Eficiência assintótica para \\(g(\\theta)\\)\nDizemos que \\(T(\\boldsymbol{X}_n)\\) é um estimador assintoticamente eficiente se, e somente se,\n\n\\(\\lim\\limits_{n\\rightarrow\\infty} E_\\theta(T(\\boldsymbol{X}_n)) = g(\\theta), \\forall \\theta \\in \\Theta\\).\n\\(\\lim\\limits_{n\\rightarrow \\infty} n \\mathrm{Var}_\\theta(T(\\boldsymbol{X}_n)) = \\frac{g'(\\theta)^2}{I_1(\\theta)}, \\forall \\theta \\in \\Theta\\).\n\n\n28.3.1 Exemplo (Poisson)\nSeja \\(\\boldsymbol{X}_n\\) a.a. de \\(X\\sim\\mathrm{Poiss}(\\theta),\\theta \\in \\mathbb{R}_+\\).\na-) Verifique se \\(T(\\boldsymbol{X}_n) = \\left(1- \\frac{1}{n}\\right)^{\\sum X_i}\\)\nb-) Verifique se \\(T^*(\\boldsymbol{X}_n) = \\frac{1}{n} \\mathbb{1}(X_i=0)\\)\nc-) Verifique se \\(T_{MV}(\\boldsymbol{X}_n) = \\mathrm{e}^{-\\bar{X}}\\) é eficiente para \\(g(\\theta) = P_\\theta(X=0)\\).\n\n28.3.1.1 Resposta a-)\nJá sabemos que\n\\[\n\\begin{aligned}\n\\mathrm{Var}_\\theta(T(\\boldsymbol{X}_n)) &= \\mathrm{e}^{-2\\theta}(\\mathrm{e}^{\\frac{\\theta}{n}} - 1),\\ \\forall \\theta \\in \\Theta \\\\ \\\\\nE_\\theta(T(\\boldsymbol{X}_n)) = \\mathrm{e}^{-\\theta} = g(\\theta),\\ \\forall \\theta \\in \\Theta.\n\\end{aligned}\n\\]\nOu seja, é não-viciado, mas não eficiente, pois \\[\n\\mathrm{Var}_\\theta(T(\\boldsymbol{X}_n)) \\neq \\frac{\\theta \\mathrm{e}^{-2\\theta}}{n}\n\\]\nPrecisamos calcular\n\\[\n\\lim_{n\\rightarrow\\infty} n \\mathrm{Var}_\\theta(T(\\boldsymbol{X}_n))\n= \\lim_{n\\rightarrow\\infty} n \\mathrm{e}^{-2\\theta}(\\mathrm{e}^{\\frac{\\theta}{n}} - 1)\n\\]\nNote que \\[\n\\mathrm{e}^x = \\sum \\frac{x^i}{i!} = 1 + x + \\frac{x^2}{2!} + \\cdots\n\\]\nPortanto, \\[\n\\begin{aligned}\n\\mathrm{e}^{\\frac{\\theta}{n}} &= 1 + \\frac{\\theta}{n} + (\\frac{\\theta}{n})^2\\frac{1}{x!} + \\dots \\\\\n\\iff \\mathrm{e}^{\\frac{\\theta}{n}} - 1 &= \\frac{\\theta}{n} + \\frac{\\theta^2}{n^2 2!} + \\dots \\\\\n\\iff n (\\mathrm{e}^{\\frac{\\theta}{n}} - 1) &= \\theta + \\frac{\\theta^2}{n 2!} + \\dots \\\\\n\\Rightarrow \\lim_{n\\rightarrow \\infty} n (\\mathrm{e}^{\\frac{\\theta}{n}} - 1) &= \\theta,\\ \\forall \\theta \\in \\Theta.\n\\end{aligned}\n\\]\nDessa forma, \\[\n\\lim_{n\\rightarrow \\infty} n \\mathrm{Var}_\\theta(T(\\boldsymbol{X}_n)) = \\mathrm{e}^{-2\\theta}\\theta = \\frac{g'(\\theta)^2}{I_1(\\theta)},\\ \\forall \\theta \\in \\Theta.\n\\]\nLogo, \\(T(\\boldsymbol{X}_n)\\) é assintoticamente eficiente para \\(g(\\theta)\\)\n\n\n28.3.1.2 Resposta b-)\nNote que \\[\n\\begin{aligned}\nE_\\theta(T^*(\\boldsymbol{X}_n)) &= \\frac{1}{n} \\sum E_\\theta(\\mathbb{1}(X_i=0)) \\\\\n&\\stackrel{\\mathrm{iid}}{=} \\frac{1}{n} \\cdot n \\cdot P_\\theta(X=0) =\\mathrm{e}^{-\\theta},\\ \\forall \\theta \\in \\Theta. \\\\ \\\\\n\\mathrm{Var}_\\theta(T^*(\\boldsymbol{X}_n)) &\\stackrel{\\mathrm{iid}}{=} \\frac{1}{n^2} \\cdot n \\cdot \\mathrm{Var}_\\theta(\\mathbb{1}(X=0)),\\\\\n&= \\frac{1}{n} = \\mathrm{e}^{-\\theta}(1-\\mathrm{e}^{-\\theta}),\\ \\forall \\theta \\in \\Theta.\n\\end{aligned}\n\\]\nObserve que o LICR é \\[\n\\frac{g'(\\theta)^2}{nI_1(\\theta)} = \\frac{\\theta\\mathrm{e}^{-2\\theta}}{n}.\n\\]\nComo \\[\n\\mathrm{Var}_\\theta(T^*(\\boldsymbol{X}_n)) \\neq \\frac{g'(\\theta)^2}{nI_1(\\theta)},\n\\] concluímos que não é eficiente. Além disso, \\[\n\\lim_{n\\rightarrow \\infty} n \\mathrm{Var}_\\theta(T^*(\\boldsymbol{X}_n)) = \\mathrm{e}^{-\\theta}(1-\\mathrm{e}^{-\\theta}) \\neq \\theta \\mathrm{e}^{-2\\theta}.\n\\] Logo, não é assintoticamente eficiente.\n\n\n28.3.1.3 Resposta c-)\nNote que \\[\nE_\\theta(T_{MV}(\\boldsymbol{X}_n)) = E_\\theta(\\mathrm{e}^{-\\bar{X}}) = E_\\theta(\\mathrm{e}^{-\\frac{1}{n} \\sum X_i})\n\\] em que \\(\\sum X_i \\sim \\mathrm{Poiss}(n\\theta), \\theta \\in \\Theta.\\)\nObserve que \\[\nM_{\\sum X_i}(t) = E_\\theta\\left(\\mathrm{e}^{t\\sum X_i}\\right) = \\mathrm{e}^{n\\theta(\\mathrm{e}^t - 1)}\n\\]\nTomando \\(t = -\\frac{1}{n}\\), temos que \\[\nE_\\theta(T_{MV}(\\boldsymbol{X}_n)) = M_{\\sum X_i}(-\\frac{1}{n}) = \\mathrm{e}^{n\\theta (\\mathrm{e}^{-\\frac{1}{n}} - 1)}\n\\]\n\nAdemais, \\[\n\\begin{aligned}\n\\mathrm{e}^{-\\frac{1}{n}} &= 1 - \\frac{1}{n} + \\left(-\\frac{1}{n}\\right)^2 \\cdot \\frac{1}{2} + \\dots \\\\\n\\iff \\mathrm{e}^{-\\frac{1}{n}} - 1 &= - \\frac{1}{n} + \\frac{1}{n^2 2!}+ \\dots \\\\\n\\iff n\\theta(\\mathrm{e}^{-\\frac{1}{n}} - 1) &= -\\theta + \\underbracket{\\left(\\frac{\\theta}{2n}+ \\dots\\right)}_{h(\\theta,n)} \\\\\n\\Rightarrow E_\\theta(T_{MV}(\\boldsymbol{X}_n)) &= \\mathrm{e}^{-\\theta + h(\\theta,n)}\n\\end{aligned}\n\\]\nComo \\(h(\\theta,n) \\neq 0\\), \\[\nE_\\theta(T_{MV}(\\boldsymbol{X}_n)) \\neq \\mathrm{e}^{-\\theta},\n\\] e \\[\n\\lim_{n\\rightarrow\\infty} E_\\theta(T_{MV}(\\boldsymbol{X}_n)) = \\mathrm{e}^{-\\theta},\\ \\forall \\theta \\in \\Theta.\n\\]\nApesar de \\(T_{MV}(\\boldsymbol{X}_n)\\) ser viciado para \\(g(\\theta) = \\mathrm{e}^{\\theta}\\), ela é não-viciada assintoticamente.\nNote ainda que \\[\n\\begin{aligned}\nE_\\theta(T_{MV}(\\boldsymbol{X}_n)^2) &= E_\\theta\\left(\\mathrm{e}^{-\\frac{2}{n} \\sum X_i}\\right) \\\\\n&= M_{\\sum X_i} (-\\frac{2}{n}) \\\\\n&= \\mathrm{e}^{n\\theta \\left(\\mathrm{e}^{-\\frac{2}{n}} -1\\right)}\n\\end{aligned}\n\\]\nPortanto, \\[\n\\mathrm{Var}_\\theta(T_{MV}(\\boldsymbol{X}_n)) = \\mathrm{e}^{n\\theta \\left(\\mathrm{e}^{-\\frac{2}{n}} -1\\right)} - \\mathrm{e}^{-2\\theta + 2h(\\theta,n)}\n\\]\nSeguindo, \\[\n\\begin{aligned}\n\\mathrm{e}^{-\\frac{2}{n}} &= 1 + \\left(-\\frac{2}{n}\\right) + \\left(-\\frac{2}{n}\\right)^2 \\cdot \\frac{1}{2!} + \\dots \\\\\n\\iff n\\theta(\\mathrm{e}^{-\\frac{2}{n}} - 1) &= -2\\theta + \\underbracket{n\\theta\\left(\\frac{2}{n}\\right)^2 + n\\theta(\\dots)}_{w(\\theta,n)} \\\\\n\\Rightarrow \\mathrm{Var}_\\theta(T_{MV}(\\boldsymbol{X}_n)) &=  \\mathrm{e}^{-2\\theta + w(\\theta,n)} - \\mathrm{e}^{-2\\theta + 2h(\\theta,n)} \\\\\n&= \\mathrm{e}^{-2\\theta} \\left(\\mathrm{e}^{w(\\theta,n)} -\\mathrm{e}^{2h(\\theta,n)}\\right)  \\\\\n\\Rightarrow \\lim_{n\\rightarrow \\infty} n \\mathrm{Var}_\\theta(T_{MV}(\\boldsymbol{X}_n)\n&= \\lim_{n\\rightarrow \\infty} n \\mathrm{e}^{-2\\theta} \\left(\\mathrm{e}^{w(\\theta,n)} -\\mathrm{e}^{2h(\\theta,n)}\\right)\\\\\n&= \\mathrm{e}^{-2\\theta} \\lim_{n\\rightarrow \\infty} n \\left(\\mathrm{e}^{w(\\theta,n)} -\\mathrm{e}^{2h(\\theta,n)}\\right) \\\\\n\\end{aligned}\n\\]\nNote que \\[\n\\begin{aligned}\n\\mathrm{e}^{w(\\theta,n)} &= 1 + w(\\theta,n) + \\frac{w(\\theta,n)^2}{2!} + \\dots \\\\\n\\mathrm{e}^{2h(\\theta,n)} &= 1 + 2h(\\theta,n) + \\frac{2h(\\theta,n)^2}{2!} + \\dots \\\\ \\\\\n\\Rightarrow \\mathrm{e}^{w(\\theta,n)} -\\mathrm{e}^{2h(\\theta,n)} &=\n[w(\\theta,n) - 2h(\\theta,n)] + \\left[\\frac{w(\\theta,n)^2}{2!} - \\frac{2h(\\theta,n)^2}{2!}\\right] + \\dots \\\\\n\\Rightarrow n\\left(\\mathrm{e}^{w(\\theta,n)} -\\mathrm{e}^{2h(\\theta,n)}\\right) &=\nn[w(\\theta,n) - 2h(\\theta,n)] +\n\\underbracket{n\\left[\\frac{w(\\theta,n)^2}{2!} - \\frac{2h(\\theta,n)^2}{2!}\\right] + \\dots}_{\\stackrel{n\\uparrow\\infty}{\\rightarrow 0}} \\\\\n\\end{aligned}\n\\]\nEm que \\[\n\\begin{aligned}\n2h(\\theta,n) &= \\frac{2\\theta}{2n} - \\frac{2\\theta}{3!n^2} + \\frac{2\\theta}{4!n^3} + \\dots \\\\\nw(\\theta,n) &= \\frac{4\\theta}{2n} - \\frac{8\\theta}{3!n^2} + \\frac{\\theta}{4!n^3} + \\dots\n\\end{aligned}\n\\]\nAgora, \\[\n\\begin{aligned}\n&n\\left[\\left(\\frac{4\\theta}{2!n} - \\frac{8\\theta}{3!n^2} + \\dots\\right) - \\left(\\frac{\\theta}{n}-\\frac{2\\theta}{3!n^2} + \\dots\\right)\\right] \\\\\n&=\\left[\\left(2\\theta - \\frac{8\\theta}{3!n} + \\dots\\right) - \\left(\\theta - \\frac{2\\theta}{3!n^2} + \\dots\\right)\\right] \\\\\n\\stackrel{n\\uparrow\\infty}{\\rightarrow} 2\\theta - \\theta &= \\theta\n\\end{aligned}\n\\]\nPortanto, \\[\n\\lim_{n\\rightarrow \\infty} n \\mathrm{Var}_\\theta(T(\\boldsymbol{X}_n)) = \\mathrm{e}^{-2\\theta} \\cdot \\theta,\\ \\forall \\theta \\in \\Theta.\n\\] e concluímos que \\(T_{MV}(\\boldsymbol{X}_n) = \\mathrm{e}^{-\\bar{X}}\\) é assintoticamente eficiente.",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Estimadores Eficientes</span>"
    ]
  },
  {
    "objectID": "estimadores-consistentes.html",
    "href": "estimadores-consistentes.html",
    "title": "29  Consistência",
    "section": "",
    "text": "29.1 Teorema (da relação com o EQM)\nDizemos que \\(T(\\boldsymbol{X}_n)\\) é um estimador (fracamente) consistente para \\(g(\\theta)\\) se, e somente se, para cada \\(\\epsilon &gt; 0\\) fixo,\n\\[\n\\lim_{n\\rightarrow \\infty} P_\\theta\\left(\\lvert T(\\boldsymbol{X}_n) - g(\\theta) \\rvert &gt; \\epsilon\\right) = 0, \\forall \\theta \\in \\Theta.\n\\] ou \\[\nT(\\boldsymbol{X}_n) \\stackrel{P_\\theta}{\\rightarrow} g(\\theta),\\ \\forall \\theta \\in \\Theta.\n\\]\nDizemos \\(T(\\boldsymbol{X}_n)\\) é fortemente consistente se, e somente se, \\[\nP_\\theta\\left(\\lim_{n\\rightarrow \\infty} T(\\boldsymbol{X}_n) = g(\\theta)\\right) = 1,\\ \\forall \\theta \\in \\Theta.\n\\] ou \\[\nT(\\boldsymbol{X}_n) \\stackrel{\\mathrm{q.c.}[P_\\theta]}{\\rightarrow} g(\\theta),\\ \\forall \\theta \\in \\Theta.\n\\]\nSeja \\(T(\\boldsymbol{X}_n)\\) um estimador para \\(g(\\theta) \\in \\mathbb{R}\\) tal que o Erro Quadrático Médio \\[\n\\mathrm{EQM}_\\theta(T(\\boldsymbol{X}_n), g(\\theta)) = E_\\theta[(T(\\boldsymbol{X}_n) - g(\\theta))^2]\n\\] converge para \\(0\\) para cada \\(\\theta \\in \\Theta\\). Então, \\(T(\\boldsymbol{X}_n)\\) é fracamente consistente para \\(g(\\theta)\\)",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Consistência</span>"
    ]
  },
  {
    "objectID": "estimadores-consistentes.html#teorema-da-relação-com-o-eqm",
    "href": "estimadores-consistentes.html#teorema-da-relação-com-o-eqm",
    "title": "29  Consistência",
    "section": "",
    "text": "29.1.1 Prova\nSeja \\(\\epsilon &gt; 0\\) fixado. Então, \\[\n\\begin{aligned}\nP_\\theta(\\lvert T(\\boldsymbol{X}_n) - g(\\theta) \\rvert &gt; \\epsilon) &= P_\\theta((T(\\boldsymbol{X}_n) - g(\\theta))^2 &gt; \\epsilon^2) \\\\\n&\\stackrel{\\mathrm{Chebyshev}}{\\leq} \\frac{E_\\theta((T(\\boldsymbol{X}_n - g(\\theta))^2)}{\\epsilon^2}\n=  \\frac{\\mathrm{EQM}_\\theta(T(\\boldsymbol{X}_n),g(\\theta))}{\\epsilon^2} \\\\\n\\Rightarrow \\lim_{n\\rightarrow \\infty} P_\\theta(\\lvert T(\\boldsymbol{X}_n) - g(\\theta) \\rvert &gt; \\epsilon) &\\leq  0,\\ \\forall \\theta \\in \\Theta.\n\\end{aligned}\n\\] Logo, \\(T(\\boldsymbol{X}_n)\\stackrel{P_\\theta}{\\rightarrow}g(\\theta),\\ \\forall\\theta \\in\\Theta.\\)\n\n\n\n\n\n\nNota\n\n\n\nSe \\(\\mathrm{EQM}_\\theta(T(\\boldsymbol{X}_n),g(\\theta)) \\stackrel{n\\uparrow \\infty}{\\rightarrow} 0, \\forall \\theta \\in \\Theta\\), então \\[\nT(\\boldsymbol{X}_n) \\stackrel{P_\\theta}{\\rightarrow}g(\\theta),\\ \\forall \\theta \\in \\Theta.\n\\]\nUsando desse recurso nos exercícios, basta mostrar que o EQM vai para zero quando \\(n\\) cresce.\n\n\n\n\n\n\n\n\nDica\n\n\n\nDisso, temos que, se \\(T(\\pmb{X}_n)\\) for assintóticamente eficiente, então será fracamente consistente.",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Consistência</span>"
    ]
  },
  {
    "objectID": "estimadores-consistentes.html#exemplos",
    "href": "estimadores-consistentes.html#exemplos",
    "title": "29  Consistência",
    "section": "29.2 Exemplos",
    "text": "29.2 Exemplos\n\n29.2.1 Exemplo Bernoulli\nSeja \\(\\pmb{X}_n\\) amostra aleatória de \\(X\\sim \\mathrm{Ber}(\\theta), \\theta \\in \\Theta = (0,1)\\). Mostre que \\(T(\\pmb{X}_n)\\) é consistente para \\(g(\\theta) = \\theta\\).\n\n29.2.1.1 Resposta\nComo \\(E_\\theta(\\bar{X}) = \\theta\\) e \\(\\mathrm{Var}_\\theta(\\bar{X}) \\frac{\\theta(1-\\theta)}{n}, \\forall \\theta \\in \\Theta.\\), temos que \\[\n\\begin{aligned}\n\\mathrm{Viés}_\\theta(\\bar{X},\\theta) &= 0, \\forall \\theta \\in \\Theta, \\\\\n\\lim_{n\\rightarrow \\infty} \\mathrm{Var}_\\theta(\\bar{X}) = 0, \\forall \\theta \\in \\Theta, \\\\\n\\Rightarrow \\mathrm{EQM}_\\theta(\\bar{X},\\theta) &\\rightarrow 0, \\forall \\theta \\in \\Theta\n\\Rightarrow \\bar{X} \\stackrel{P_\\theta}{\\rightarrow} \\theta, \\forall \\theta \\in \\Theta.\n\\end{aligned}\n\\]\n\n\n\n\n\n\nDica\n\n\n\nPoderíamos também, como \\(\\pmb{X}_n\\) é uma amostra aleatória, utilizar a lei forte dos grandes números.\n\n\n\n\n\n29.2.2 Exemplo Normal (média)\nSeja \\(\\pmb{X}_n\\) a.a. de \\(X\\sim \\mathrm{N}(\\mu,\\sigma^2), \\theta = (\\mu,\\sigma^2) \\in \\Theta = \\mathbb{R}\\times\\mathbb{R}_+\\). Mostre que \\(\\bar{X}\\) é consistente para \\(g(\\theta) = \\mu.\\)\n\n29.2.2.1 Resposta\nComo \\(E_\\theta(\\bar{X}) = \\mu, \\mathrm{Var}_\\theta(\\bar{X}) = \\frac{\\sigma^2}{n}, \\forall \\theta \\in \\Theta\\), temos que \\[\n\\begin{aligned}\n\\mathrm{Viés}_\\theta(\\bar{X},\\mu) &= 0, \\forall \\theta \\in \\Theta, \\\\\n\\lim_{n\\rightarrow \\infty} \\mathrm{Var}_\\theta(\\bar{X}) &= 0, \\forall \\theta \\in \\Theta, \\\\\n\\Rightarrow \\lim_{n\\rightarrow \\infty} \\mathrm{EQM}_\\theta(\\bar{X},\\theta) &= 0, \\forall \\theta \\in \\Theta \\\\\n\\Rightarrow T(\\pmb{X}_n) &= \\bar{X} \\text{ é fracamente consistente para } g(\\theta) = \\mu.\n\\end{aligned}\n\\]\n\n\n\n29.2.3 Exemplo Normal (variância)\nSeja \\(\\pmb{X}_n\\) a.a. de \\(X\\sim \\mathrm{N}(\\mu,\\sigma^2), \\theta = (\\mu,\\sigma^2) \\in \\Theta = \\mathbb{R}\\times\\mathbb{R}_+\\). Mostre que \\[\nT_k(\\pmb{X}_n) = \\frac{1}{n-k} \\sum_{i=1}^n (X_i - \\bar{X})^2\n\\] é consistente para \\(g(\\theta) = \\sigma^2.\\)\n\n29.2.3.1 Resposta\nSabemos que \\[\n\\sum_{i=1}^n \\frac{(X_i - \\bar{X})^2}{\\sigma^2} \\sim \\chi^2_{(n-1)}, \\forall \\theta \\in \\Theta.\n\\]\nLogo, \\[\n\\begin{aligned}\nE_\\theta(T_k(\\pmb{X}_n)) &= E_\\theta\\left(\\frac{1}{n-k} \\sum (X_i - \\bar{X})^2\\right) \\\\\n&= \\frac{\\sigma^2}{n-k}E_\\theta\\left(\\sum \\frac{(X_i - \\bar{X})^2}{\\sigma^2}\\right) \\\\\n&= \\frac{\\sigma^2}{n-k}(n-1), \\forall \\theta \\in \\Theta.\n\\end{aligned}\n\\]\nAdemais, \\[\n\\begin{aligned}\n\\mathrm{Var}_\\theta(T_k(\\pmb{X}_n)) &= \\mathrm{Var}_\\theta\\left(\\frac{1}{n-k} \\sum (X_i - \\bar{X})^2\\right) \\\\\n&= \\frac{\\sigma^4}{(n-k)^2}\\mathrm{Var}_\\theta\\left(\\sum \\frac{(X_i - \\bar{X})^2}{\\sigma^2}\\right) \\\\\n&= \\frac{\\sigma^4}{(n-k)^2}2(n-1), \\forall \\theta \\in \\Theta.\n\\end{aligned}\n\\]\nPortanto, \\[\n\\mathrm{EQM}_\\theta = \\frac{2\\sigma^2}{(n-k)^2}(n-1) + \\left(\\frac{\\sigma^2}{n-k}(n-1) - \\sigma^2\\right)^2\n\\]\nComo \\[\n\\lim_{n\\rightarrow \\infty} \\frac{n-1}{n-k} = 1, \\forall k \\neq n, k \\ \\text{fixado}\n\\] e \\[\n\\lim_{n\\rightarrow \\infty} \\frac{n-1}{(n-k)^2} = 0, \\forall k \\neq n, k\\ \\text{fixado}\n\\] temos que \\[\n\\lim_{n\\rightarrow \\infty} \\mathrm{EQM}_\\theta(T_k(\\pmb{X}_n),\\sigma^2) = 0, \\forall \\theta \\in \\Theta.\n\\]\nLogo, \\(T(\\pmb{X}_n)\\) é consistente para \\(\\sigma^2, \\forall k \\neq n, \\ k \\text{fixado}\\)",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Consistência</span>"
    ]
  },
  {
    "objectID": "escolha-estimador.html",
    "href": "escolha-estimador.html",
    "title": "30  Escolha de Estimadores",
    "section": "",
    "text": "O Erro Quadrático Médio (EQM) é o critério para escolher o estimador a ser utilizado.\nSejam \\(T_1(\\pmb{X}_n), T_2(\\pmb{X}_n)\\) estimadores para a quantidade de interesse. Se \\(\\mathrm{EQM}_\\theta(T_1(\\pmb{X}_n),g(\\theta)) \\leq \\mathrm{EQM}_\\theta(T_2(\\pmb{X}_n),g(\\theta))\\) para cada \\(\\theta \\in \\Theta\\), dizemos que \\(T_1(\\pmb{X}_n)\\) é melhor \\(T_2(\\pmb{X}_n)\\).\n\n\n\n\n\n\nEstimadores inadmissíveis\n\n\n\nSe existir \\(\\theta^* \\in \\Theta\\) tal que \\[\n\\mathrm{EQM}_{\\theta^*}(T_1(\\pmb{X}_n),g(\\theta)) &lt; \\mathrm{EQM}_{\\theta^*}(T_2(\\pmb{X}_n),g(\\theta))\n\\] dizemos que \\(T_2(\\pmb{X}_n)\\) é inadmissível para \\(g(\\theta)\\).\n\n\n\n\n\n\n\n\nObservação\n\n\n\nNote que o ENVVUM é aquele que tem menor EQM dentre os estimadores não-viciados.",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Escolha de Estimadores</span>"
    ]
  },
  {
    "objectID": "emv2.html",
    "href": "emv2.html",
    "title": "31  Estimador de Máxima Verossimilhança (EMV) - Aprofundamento",
    "section": "",
    "text": "31.1 Exemplos\nSeja \\(\\mathcal{L}_{\\boldsymbol{x}_n}(\\theta)\\) a função de verossimilhança de um modelo estatístico. Dizemos que \\(\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n)\\) é um estimador de máxima verossimilhança se, e somente se, \\[\n\\mathcal{L}_{\\boldsymbol{x}_n}(\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{x}_n)) \\geq \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta),\\ \\mathrm{q.c.}\\ \\forall \\theta \\in \\Theta.\n\\] em que \\(\\boldsymbol{x}_n\\) representa uma possível amostra observada.",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Estimador de Máxima Verossimilhança (EMV) - Aprofundamento</span>"
    ]
  },
  {
    "objectID": "emv2.html#exemplos",
    "href": "emv2.html#exemplos",
    "title": "31  Estimador de Máxima Verossimilhança (EMV) - Aprofundamento",
    "section": "",
    "text": "31.1.1 Exemplo Uniforme\nSeja \\(\\boldsymbol{X}_n\\) a.a. de \\(X\\sim \\mathrm{U}(0,\\theta), \\theta &gt; 0\\). Encontre o EMV.\n\n31.1.1.1 Resposta\nA função de verossimilhança é dada por \\[\n\\mathcal{L}_{\\boldsymbol{x}_n}(\\theta) \\stackrel{\\mathrm{iid}}{=} \\prod^n_{i=1} f_\\theta(x_i) = \\prod\\left\\{\\frac{1}{\\theta} \\mathbb{1}_{(0,\\theta]}(x_i)\\right\\}\n= \\frac{1}{\\theta^n} \\prod \\mathbb{1}_{(0,\\theta]}(x_i)\n\\]\nNote que \\[\n\\prod \\mathbb{1}_{(0,\\theta]}(x_i) = 1 \\iff \\mathbb{1}_{[0,\\infty)}(\\min\\boldsymbol{x}_n) \\mathbb{1}_{(-\\infty,\\theta]}(\\max\\boldsymbol{x}_n) = 1\n\\] Como \\(0 &lt; \\max \\boldsymbol{x}_n \\leq \\theta &lt; \\infty\\), \\[\n\\mathcal{L}_{\\boldsymbol{x}_n}(\\theta) \\frac{1}{\\theta^n} \\mathbb{1}_{[0,\\infty)}(\\min \\boldsymbol{x}_n) \\mathbb{1}_{[\\max \\boldsymbol{x}_n, \\infty)}\n\\]\nNote que a função é estritamente decrescente para qualquer valor de \\(\\theta \\geq \\max \\boldsymbol{x}_n\\) e vale \\(0\\) para \\(\\theta \\leq \\max \\boldsymbol{x}_n\\), temos que \\(\\theta = \\max\\boldsymbol{x}_n\\) é o ponto que maximiza a função de verossilhança, logo, \\[\n\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n) = \\max \\boldsymbol{X}_n\n\\] é o estimador de máxima verossimilhança.\n\n\n\n31.1.2 Exemplo Beta\nSeja \\(\\boldsymbol{X}_n\\) a.a. de \\(X\\sim f_\\theta, \\theta \\in \\Theta = (0,\\infty)\\) tal que \\[\nf_\\theta(x) = \\left\\{\\begin{array}{ll}\n\\theta x^{\\theta-1}, x \\in (0,1) \\\\\n0, \\mathrm{c.c}\n\\end{array}\\right.\n\\]\nEncontre o EMV para “\\(\\theta\\)”.\n\n31.1.2.1 Resposta\nA função de verossimilhança é \\[\n\\begin{aligned}\n\\mathcal{L}_{\\boldsymbol{x}_n}(\\theta) &\\stackrel{\\mathrm{i.i.d.}}{=} \\prod^n_{i=1} f_\\theta(x_i) \\\\\n&= \\prod \\left\\{\\theta x_i^{\\theta-1}\\right\\} \\\\\n&= \\theta^n\\left(\\prod x_i\\right)^{\\theta-1}\n\\end{aligned}\n\\]\nComo \\(\\mathcal{L}_{\\boldsymbol{x}_n}\\) é positiva \\(\\forall \\theta \\in \\Theta\\), temos que, \\[\n\\ln\\mathcal{L}_{\\boldsymbol{x}_n}(\\theta) = n \\ln \\theta + (\\theta-1) \\sum \\ln x_i\n\\]\nAdemais, \\(\\ln(\\cdot)\\) é uma função estritamente crescente. Portanto, o valor que maximiza \\(\\mathcal{L}_{\\boldsymbol{x}_n}(\\cdot)\\) é o mesmo que maximiza \\(\\ln \\mathcal{L}_{\\boldsymbol{x}_n}(\\cdot)\\).\nEncontramos os pontos críticos de \\(\\ln \\mathcal{L}_{\\boldsymbol{X}_n}(\\theta)\\) \\[\n\\begin{aligned}\n&\\frac{d\\ln \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta)}{d\\theta} = 0 \\\\\n&\\Rightarrow \\frac{n}{\\theta} + \\sum \\ln x_i = 0 \\\\\n&\\iff \\hat{\\theta} = - \\frac{n}{\\sum \\ln x_i}\\ \\text{é um ponto crítico.}\n\\end{aligned}\n\\]\nPara a segunda derivada, \\[\n\\frac{d^2\\ln \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta)}{d\\theta^2} = -\\frac{n}{\\theta^2} &lt; 0, \\forall \\theta \\in \\Theta.\n\\]\nLogo, \\[\n\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n) = - \\frac{n}{\\sum \\ln x_i}\n\\] é o EMV.",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Estimador de Máxima Verossimilhança (EMV) - Aprofundamento</span>"
    ]
  },
  {
    "objectID": "emv2.html#caso-geral",
    "href": "emv2.html#caso-geral",
    "title": "31  Estimador de Máxima Verossimilhança (EMV) - Aprofundamento",
    "section": "31.2 Caso Geral",
    "text": "31.2 Caso Geral\nSeja \\(\\boldsymbol{X}_n\\) a.a. de \\(X \\sim f_\\theta, \\theta \\in \\Theta \\subseteq \\mathbb{R}^P.\\) Considere \\(\\boldsymbol{x}_n\\) uma possível amostra observada e considere as Condições \\(C_1:C_4\\) válidas. Então, o EMV pode ser obtido igualando a função escore a zero. Ou seja,\n\\[\nU_n(\\boldsymbol{X}_n,\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n))\n\\] em que \\[\nU_n(\\boldsymbol{x}_n,\\theta) = \\frac{\\partial \\ln \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta)}{\\partial \\theta}\n\\] desde que \\[\n\\left.\\frac{\\partial U_n(\\boldsymbol{x}_n,\\theta)}{\\partial \\theta^T}\\right\\rvert_{\\theta = \\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{x}_n)}\n\\] é negativa definida para quase todo \\(\\boldsymbol{x}_n\\). Ou seja, \\[\n\\left.\\frac{\\partial U_n(\\boldsymbol{x}_n,\\theta)}{\\partial \\theta^T}\\right\\rvert_{\\theta = \\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{x}_n)}\n=  \\left.\\frac{\\partial^2 \\ln \\mathcal{L}_{\\boldsymbol{x}_n}}{\\partial \\theta \\partial \\theta^T}\\right\\rvert_{\\theta = \\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{x}_n)}\n\\] é negativa definida.\n\n\n\n\n\n\nMatrizes positivas definidas\n\n\n\nDizemos que \\(A\\) é positiva definida se, e somente se, \\[\nx^TAx &gt; 0, \\forall x \\in \\mathbb{R}^P\\setminus \\{\\boldsymbol{0}\\}\n\\]\nTeorema\n\\(A\\) simétrica (\\(A\\) = \\(A^T\\)) é positiva definida se, e somente se, todos seus autovalores são positivos.\n\n\n\n\n\n\n\n\nMatrizes negativas definidas\n\n\n\nDizemos que \\(A\\) é positiva definida se, e somente se, \\[\nx^TAx &lt; 0, \\forall x \\in \\mathbb{R}^P \\setminus \\{\\boldsymbol{0}\\}\n\\]\nTeorema\n\\(A\\) simétrica (\\(A\\) = \\(A^T\\)) é negativa definida se, e somente se, todos seus autovalores são negativos.\n\n\n\n\n\n\n\n\nDica\n\n\n\nUma matriz \\(A\\) de dimensões \\(2\\times 2\\) é dita positiva(negativa) definida se, e somente se, os elementos da diagonal são positivos(negativos) e o determinante de \\(A\\) é positivo.\n\n\n\n31.2.1 Exemplos\n\n31.2.1.1 Exemplo Normal\nSeja \\(\\boldsymbol{X}_n\\) a.a. de \\(X \\sim \\mathrm{N}(\\mu,\\sigma^2), \\theta = (\\mu, \\sigma^2) \\in \\Theta = \\mathbb{R}\\times\\mathbb{R}_+\\)\n\n31.2.1.1.1 Resposta\nA função de verossimilhança é \\[\n\\begin{aligned}\n\\mathcal{L}_{\\boldsymbol{x}_n}(\\theta) &= \\frac{1}{(\\sqrt{2 \\pi \\sigma^2})^n} \\mathrm{e}^{-\\frac{1}{2\\sigma^2} \\sum (x_i - \\mu)^2} \\\\\n\\Rightarrow \\ln \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta) &= -\\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum (x_i - \\mu)^2.\n\\end{aligned}\n\\]\nLogo, \\[\nU_n(\\boldsymbol{x}_n,\\theta) = \\frac{\\partial \\ln }{\\partial \\theta} \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta) = \\left(\\begin{array}{cc}\n\\frac{\\partial \\ln }{\\partial \\mu} \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta) \\\\\n\\frac{\\partial \\ln }{\\partial \\sigma^2} \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta)\n\\end{array}\\right)\n\\] em que \\[\n\\frac{\\partial \\ln }{\\partial \\mu} \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta) = -\\frac{1}{2\\sigma^2} \\sum 2(x_i-\\mu)(-1) = \\frac{1}{\\sigma^2} \\sum (x_i - \\mu)\n\\] e \\[\n\\frac{\\partial \\ln }{\\partial \\sigma^2} \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta) = -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\sum (x_i-\\mu)^2\n\\]\nLogo, \\[\n\\begin{aligned}\nU_n(\\boldsymbol{x}_n,\\theta) &= 0 \\\\\n\\iff &\\left\\{\\begin{array}{ll}\n\\frac{\\partial \\ln }{\\partial \\mu} \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta) = \\frac{1}{\\sigma^2} \\sum (x_i - \\mu) &= 0 \\\\\n\\frac{\\partial \\ln }{\\partial \\sigma^2} \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta) = -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\sum (x_i-\\mu)^2 &= 0\n\\end{array}\\right. \\\\\n\\iff &\\left\\{\\begin{array}{ll}\n\\sum x_i -n \\mu= 0 \\\\\n-n + \\frac{1}{\\sigma^2} \\sum (x_i - \\mu)^2 = 0\n\\end{array}\\right. \\\\\n\\iff &\\left\\{\\begin{array}{ll}\n\\mu = \\sum \\frac{x_i}{n} = 0 \\\\\n\\sigma^2 = \\sum \\frac{(x_i-\\mu)^2}{n}\n\\end{array}\\right. \\\\\n\\Rightarrow &\\left\\{\\begin{array}{ll}\n\\hat{\\mu} = \\bar{x} \\\\\n\\hat{\\sigma^2} = \\frac{1}{n} \\sum (x_i - \\bar{x})^2\n\\end{array}\\right.\n\\end{aligned}\n\\]\nOu seja, \\(\\hat{\\theta} = (\\hat{\\mu}, \\hat{\\sigma^2})\\) é o ponto crítico de \\(\\ln \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta)\\).\nContinuando, \\[\n\\begin{aligned}\n\\frac{\\partial^2 \\ln}{\\partial \\theta \\partial \\theta^T} \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta) &=\n\\frac{\\partial U_n(\\boldsymbol{x}_n,\\theta)}{\\partial \\theta^T} \\\\\n&= \\frac{\\partial}{\\partial \\theta^T}\n\\left[\n\\begin{array}{c}\n\\frac{\\partial \\ln}{\\partial \\mu} \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta) \\\\\n\\frac{\\partial \\ln}{\\partial \\sigma^2} \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta)\n\\end{array}\\right] \\\\\n&= \\left[\n\\begin{array}{cc}\n\\frac{\\partial^2 \\ln}{\\partial \\mu^2} \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta) &\n\\frac{\\partial^2 \\ln}{\\partial \\mu \\partial \\sigma^2} \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta) \\\\\n\\frac{\\partial^2 \\ln}{\\partial \\mu \\partial \\sigma^2} \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta) &\n\\frac{\\partial^2 \\ln}{\\partial (\\sigma^2)^2} \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta) \\\\\n\\end{array}\\right] \\\\\n\\Rightarrow  \\frac{\\partial^2 \\ln}{\\partial \\mu^2} \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta) &= -\\frac{n}{\\sigma^2} \\\\\n\\frac{\\partial^2 \\ln}{\\partial \\mu \\partial \\sigma^2} \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta) &= -\\frac{1}{\\sigma^4} \\sum (x_i - \\mu) \\\\\n\\frac{\\partial^2 \\ln}{\\partial (\\sigma^2)^2} \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta) &= \\frac{n}{2\\sigma^4} + \\frac{1}{2} \\sum (x_i - \\mu)^2 \\cdot \\frac{\\partial (\\sigma^2)^2}{\\partial \\sigma^2} \\\\\n&= \\frac{n}{2\\sigma^4} - \\sum \\frac{(x_i - \\mu)^2}{\\sigma^6} \\\\\n\\Rightarrow\n\\left.\\frac{\\partial^2 \\ln}{\\partial \\theta \\partial \\theta^T} \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta)\\right\\rvert_{\\theta = \\hat{\\theta}} &=\n\\left[\n\\begin{array}{cc}\n-\\frac{n}{\\hat{\\sigma}^2} & -\\frac{1}{(\\hat{\\sigma}^2)^2} \\sum (x_i - \\bar{x}) \\\\\n-\\frac{1}{(\\hat{\\sigma}^2)^2} \\sum (x_i - \\bar{x}) & \\frac{n}{2(\\hat{\\sigma}^2)^2} - \\sum \\frac{(x_i - \\bar{x})^2}{(\\hat{\\sigma}^2)^3}\n\\end{array}\\right] \\\\ \\\\\n\\text{Note que}\\ \\sum(x_i - \\bar{x}) &= \\sum x_i - n \\bar{x} = \\sum x_i - \\sum x_i = 0 \\\\\n\\text{e que}\\ \\frac{n}{2(\\hat{\\sigma}^2)^2} - \\frac{n\\hat{\\sigma}^2}{(\\hat{\\sigma}^2)^3} &= - \\frac{n}{2(\\hat{\\sigma}^2)^2}\\\\ \\\\\n\\Rightarrow\n\\left.\\frac{\\partial^2 \\ln}{\\partial \\theta \\partial \\theta^T} \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta)\\right\\rvert_{\\theta = \\hat{\\theta}} &=\n\\left[\n\\begin{array}{cc}\n-\\frac{n}{\\hat{\\sigma}^2} & 0 \\\\\n0 & \\frac{-n}{(\\hat{\\sigma}^2)^2}\n\\end{array}\\right]\n\\end{aligned}\n\\] desde que \\(\\sum (x_i - \\bar{x})^2 &gt; 0\\).\nComo \\(\\left.\\frac{\\partial^2 \\ln}{\\partial \\theta \\partial \\theta^T} \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta)\\right\\rvert_{\\theta = \\hat{\\theta}}\\) é negativa definida, concluímos que \\(\\hat{\\theta}\\) é pelo menos máximo local. Dizemos que \\(\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n) = (\\bar{X}, S^2_n)\\) é o EMV para \\(\\theta = (\\mu, \\sigma^2)\\), em que \\[\nS^2_n = \\frac{1}{n} \\sum (X_i - \\bar{X})^2\n\\]",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Estimador de Máxima Verossimilhança (EMV) - Aprofundamento</span>"
    ]
  },
  {
    "objectID": "emv2.html#relação-com-a-informação-de-fisher",
    "href": "emv2.html#relação-com-a-informação-de-fisher",
    "title": "31  Estimador de Máxima Verossimilhança (EMV) - Aprofundamento",
    "section": "31.3 Relação com a Informação de Fisher",
    "text": "31.3 Relação com a Informação de Fisher\nNote que a informação de Fisher total sob o modelo normal anterior é \\[\n\\begin{aligned}\nI_n(\\theta) &= - E_\\theta\\left(\\frac{\\partial^2 \\ln}{\\partial \\theta \\partial \\theta^T} \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta)\\right) \\\\\n&= -E_\\theta\\left(\n\\begin{array}{cc}\n-\\frac{n}{\\sigma^2} & -\\frac{1}{\\sigma^4} \\sum (X_i - \\mu) \\\\\n-\\frac{1}{\\sigma^4} \\sum (X_i - \\mu) & \\frac{n}{2\\sigma^4} - \\sum \\frac{(X_i - \\mu)^2}{(\\sigma^2)^3}\n\\end{array}\\right) \\\\\n&= E_\\theta\\left(\n\\begin{array}{cc}\n\\frac{n}{\\sigma^2} & \\frac{1}{\\sigma^4} \\sum (X_i - \\mu) \\\\\n\\frac{1}{\\sigma^4} \\sum (X_i - \\mu) & -\\frac{n}{2\\sigma^4} + \\sum \\frac{(X_i - \\mu)^2}{(\\sigma^2)^3}\n\\end{array}\\right) \\\\ \\\\\n\\text{Note que}\\ E_\\theta[(X_i-\\mu)^2] &= \\mathrm{Var}_\\theta(X) = \\sigma^2 \\\\ \\\\\n\\Rightarrow I_n (\\theta) &= E_\\theta\\left(\n\\begin{array}{cc}\n\\frac{n}{\\sigma^2} & 0 \\\\\n0 & \\frac{n}{2\\sigma^4}\n\\end{array}\\right).\n\\end{aligned}\n\\]",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Estimador de Máxima Verossimilhança (EMV) - Aprofundamento</span>"
    ]
  },
  {
    "objectID": "emv2.html#sec-expnr",
    "href": "emv2.html#sec-expnr",
    "title": "31  Estimador de Máxima Verossimilhança (EMV) - Aprofundamento",
    "section": "31.4 Alguns exemplos",
    "text": "31.4 Alguns exemplos\nSeja \\(\\pmb{X}_n\\) a.a. de \\(X \\sim f_\\theta\\),\n\n\\(X \\sim \\mathrm{Ber}(\\theta), \\theta \\in (0,1)\\), o EMV para “\\(\\theta\\)” é \\(\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n) = \\bar{X}\\).\n\\(X \\sim \\mathrm{Bin}(4, \\theta), \\theta \\in (0,1)\\), o EMV para “\\(\\theta\\)” é \\(\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n) = \\frac{\\bar{X}}{4}\\).\n\\(X \\sim \\mathrm{Poiss}(\\theta), \\theta &gt; 0\\), o EMV para “\\(\\theta\\)” é \\(\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n) = \\bar{X}\\).\n\\(X \\sim \\mathrm{Exp}(\\theta), \\theta &gt; 0\\), o EMV para “\\(\\theta\\)” é \\(\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n) = \\frac{1}{\\bar{X}}\\).\n\\(X \\sim f_\\theta, \\theta &gt; 0,\\) tal que \\[\nf_\\theta(x) = \\left\\{\\begin{array}{ll}\n\\frac{1}{\\Gamma(\\theta)} x^{\\theta-1} \\mathrm{e}^{-x}, & x &gt; 0 \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right.\n\\] com \\(\\Gamma(\\theta) = \\int x^{\\theta-1} \\mathrm{e}^{-x} dx\\), temos \\[\n\\begin{aligned}\n\\mathcal{L}_{\\pmb{x}_n}(\\theta) &\\stackrel{\\mathrm{i.i.d.}}{=} \\prod f_\\theta(x_i)\\\\\n&= \\prod \\left\\{\n\\frac{1}{\\Gamma(\\theta)} x_i^{\\theta-1} \\mathrm{e}^{-x_i}\\right\\} \\\\\n&= \\frac{1}{\\Gamma(\\theta)^n} \\left(\\prod x_i\\right)^{\\theta-1} \\mathrm{e}^{-\\sum x_i} \\\\\n\\Rightarrow \\ln \\mathcal{L}_{\\pmb{x}_n}(\\theta) &= -n \\ln \\Gamma(\\theta) + (\\theta-1) \\sum \\ln x_i - \\sum x_i \\\\\n\\\\\n\\Rightarrow \\frac{d\\ln}{d\\theta}\\mathcal{L}_{\\pmb{x}_n}(\\theta) = 0 &\\Rightarrow -\\frac{n}{\\Gamma(\\theta)}\\Gamma'(\\theta) +\n\\sum \\ln x_i  = 0.\n\\end{aligned}\n\\]\n\nComo resolver: \\[\n\\sum \\ln x_i = n \\frac{\\Gamma'(\\theta)}{\\Gamma(\\theta)}.\n\\]\nPosteriormente, encontraremos um método para encontrar essa solução.\n\n\\(X \\sim f_\\theta, \\theta \\in \\Theta\\), em que \\(f_\\theta\\) pertence à Família Exponencial \\(k\\) dimensional, isto é, \\[\nf_\\theta(x) = \\left\\{\\begin{array}{lr}\n\\mathrm{e}^{\\sum^k_{j=1} c_j(\\theta)T_j(x)+d(\\theta)+S(x)}, &x \\in \\mathfrak{X} \\\\\n0, & x \\not \\in \\mathfrak{X}\n\\end{array}\\right.\n\\]\n\nA função verossimilhança para os valores observados \\(x_i \\in \\mathfrak{X}\\) é dada por \\[\n\\begin{aligned}\n\\mathcal{L}_{\\pmb{x}_n}(\\theta) &\\stackrel{\\mathrm{i.i.d.}}{=} \\prod \\left\\{\n\\mathrm{e}^{\\sum^k_{j=1} c_j(\\theta)T_j(x_i)+d(\\theta)+S(x_i)}\n\\right\\} \\\\\n&= \\mathrm{Exp}\\left\\{\\sum_{j=1}^k c_j(\\theta) \\sum^n_{i=1} T_j(x_i) + nd(\\theta) + \\sum^n_{i=1} S(x_i)\\right\\} \\\\\n\\Rightarrow \\ln \\mathcal{L}_{\\pmb{x}_n}(\\theta) &= \\sum_{j=1}^k c_j(\\theta) \\sum^n_{i=1} T_j(x_i) + nd(\\theta) + \\sum^n_{i=1} S(x_i)\n\\end{aligned}\n\\]\nSe \\(c_1,\\dots,c_k\\) forem diferenciáveis com respeito a “\\(\\theta\\)”, temos que \\[\n\\frac{\\partial \\ln }{\\partial \\theta} \\mathcal{L}_{\\pmb{x}_n}(\\theta) =  \\sum^k_{j=1} \\frac{\\partial c_j(\\theta}{\\partial \\theta}\n\\sum^n_{i=1} T_j(x_i) + n \\frac{\\partial d(\\theta)}{\\partial \\theta}\n\\] igualando a zero, temos que \\[\n\\sum^k_{j=1} \\frac{\\partial c_j(\\theta)}{\\partial \\theta} \\sum^n_{i=1} T(x_i) = - n \\frac{\\partial d(\\theta)}{\\partial \\theta}\n\\tag{31.1}\\]\n\nEm alguns casos, (31.1) não tem solução fechada e utilizaremos um método numério para encontrar as soluções.",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Estimador de Máxima Verossimilhança (EMV) - Aprofundamento</span>"
    ]
  },
  {
    "objectID": "metodo-momentos.html",
    "href": "metodo-momentos.html",
    "title": "32  Estimação pelo método de momentos (EMM)",
    "section": "",
    "text": "32.1 Exemplos\nSeja \\(\\boldsymbol{X}_n\\) uma a.a. de \\(X\\) tal que \\[\nE_\\theta (\\lvert X \\rvert^k) &lt; \\infty, k \\in \\{1,\\dots,p\\}, \\Theta \\subseteq \\mathbb{R}^P\n\\]\nO estimador obtido pelo método de momentos é aquele que satisfaz \\[\nE_\\theta(X^k) = \\frac{1}{n} \\sum^n_{i=1} X_i^k, k = 1, \\dots, p.\n\\]\nNote que a estimação pelo método de momentos não utiliza toda a informação contida na função de verossimilhança. Precisamos conhecer a forma dos primeiros \\(p\\) momentos.",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Estimação pelo método de momentos (EMM)</span>"
    ]
  },
  {
    "objectID": "metodo-momentos.html#exemplos",
    "href": "metodo-momentos.html#exemplos",
    "title": "32  Estimação pelo método de momentos (EMM)",
    "section": "",
    "text": "32.1.1 Exemplo 1\nSeja \\(\\boldsymbol{X}_n\\) a.a. de \\(X\\sim U(0,\\theta), \\theta &gt; 0\\). Encontre o estimador pelo método de momentos.\nCalcule o EQM do EMV e do estimador obtido pelo método de momentos.\n\n32.1.1.1 Resposta\nNote que \\(\\Theta = (0,\\infty) \\subseteq \\mathbb{R}\\). Portanto, \\(p=1\\). Precisamos encontrar o valor de “\\(\\theta\\)” que satisfaz \\[\nE_\\theta(X) = \\frac{1}{n} \\sum X_i\n\\]\nSabemos que \\[\nE_\\theta(X) =  \\int^\\theta_0 x \\frac{1}{\\theta} dx = \\frac{\\theta}{2} = \\bar{X}\n\\]\nLogo, \\(\\hat{\\theta}_{\\mathrm{MM}}(\\boldsymbol{X}_n) = 2\\bar{X}\\) é o estimador obtido pelo método de momentos.\nSabemos que \\(\\hat{\\theta}_{\\mathrm{MV}}(\\pmb{X}_n) = \\max\\{X_1,\\dots,X_n\\}\\), logo, \\[\n\\begin{aligned}\n\\mathrm{EQM}_\\theta(\\hat{\\theta}_{\\mathrm{MM}}(\\pmb{X}_n), \\theta) &= E_\\theta((\\hat{\\theta}_{\\mathrm{MM}}(\\pmb{X}_n) - \\theta)^2) \\\\\n&=\\mathrm{Var}_\\theta(\\hat{\\theta}_{\\mathrm{MM}}(\\pmb{X}_n)) + \\mathrm{Viés}(\\hat{\\theta}_{\\mathrm{MM}}(\\pmb{X}_n), \\theta)^2 \\\\ \\\\\nE_\\theta(\\hat{\\theta}_{\\mathrm{MM}}(\\pmb{X}_n)) &= 2 E_\\theta(X) = 2 \\frac{\\theta}{2} = \\theta \\\\\n\\Rightarrow \\mathrm{Viés}_\\theta(\\hat{\\theta}_{\\mathrm{MV}}(\\pmb{X}_n),\\theta) &= 0, \\forall \\theta \\in \\Theta. \\\\ \\\\\n\\mathrm{Var}_\\theta(\\hat{\\theta}_{\\mathrm{MM}}(\\pmb{X}_n)) = \\mathrm{Var}_\\theta(2\\bar{X}) &\\stackrel{\\mathrm{i.i.d.}}{=} 4 \\frac{\\mathrm{Var}_\\theta(X)}{n}\n\\stackrel{\\mathrm{Unif}}{=}  \\frac{\\theta^2}{3n} \\\\\n\\Rightarrow \\mathrm{EQM}_\\theta(\\hat{\\theta}_{\\mathrm{MM}}(\\pmb{X}_n),\\theta) = \\frac{\\theta^2}{3n}\n\\end{aligned}\n\\]\nPara o EMV, \\[\n\\mathrm{EQM}_\\theta(\\hat{\\theta}_{\\mathrm{MV}}(\\pmb{X}_n), \\theta) = \\mathrm{Var}_\\theta(\\hat{\\theta}_{\\mathrm{MV}}(\\pmb{X}_n))\n+\\mathrm{Viés}(\\hat{\\theta}_{\\mathrm{MV}}(\\pmb{X}_n), \\theta)^2 \\\\ \\\\\n\\]\nPrecisaremos encontrar a distribuição do estimador, \\[\nP_\\theta(\\hat{\\theta}_{\\mathrm{MV}}(\\pmb{X}_n) \\leq t) = P_\\theta(\\max\\pmb{X}_n \\leq t) \\stackrel{\\mathrm{iid}}{=} \\prod P_\\theta(X \\leq t) = P_\\theta(X \\leq t)^n\n\\] note que \\[\nP_\\theta(X \\leq t) = \\left\\{\\begin{array}{ll}\n0, & t \\leq 0 \\\\\n\\frac{t}{\\theta}, & 0 &lt; t \\leq \\theta \\\\\n1, & t \\geq \\theta \\\\\n\\end{array}\\right. \\Rightarrow P_\\theta(\\hat{\\theta}_{\\mathrm{MV}}(\\pmb{X}_n)\\leq t)^n = \\left\\{\\begin{array}{ll}\n0, & t \\leq 0 \\\\\n\\left(\\frac{t}{\\theta}\\right)^n, & 0 &lt; t \\leq \\theta \\\\\n1, & t \\geq \\theta \\\\\n\\end{array}\\right.\n\\]\nComo \\(P_\\theta(\\hat{\\theta}_{\\mathrm{MV}}(\\pmb{X}_n))\\) é absoulatemente contínua para todo \\(\\theta &gt; 0\\), temos que \\(\\hat{\\theta}_{\\mathrm{MV}}(\\pmb{X}_n)\\) é uma variável aleatória contínua cuja f.d.p. é dada por \\[\nf_\\theta^{\\hat{\\theta}_{\\mathrm{MV}}(\\pmb{X}_n)}(x) =\\left. \\frac{d}{dt} P_\\theta(\\hat{\\theta}_{\\mathrm{MV}}(\\pmb{X}_n) \\leq t) \\right\\rvert_{t=x}\n= \\left\\{\\begin{array}{ll}\n\\frac{n x^{n-1}}{\\theta^n}, & 0 &lt; x \\leq \\theta \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right. .\n\\]\nLogo, \\[\n\\begin{aligned}\nE_\\theta(\\hat{\\theta}_{\\mathrm{MV}}(\\pmb{X}_n)) &= \\int^{\\infty}_{-\\infty} w f_\\theta^{\\hat{\\theta}_{\\mathrm{MV}}(\\pmb{X}_n)}(w) dw \\\\\n&= \\int^\\theta_0 w \\frac{n w^{n-1}}{\\theta^n} dw \\\\\n&= \\frac{n}{\\theta^n} \\int^\\theta_0 w^n dw = \\left.\\frac{n}{\\theta^n}\\frac{w^{n+1}}{n+1} \\right\\rvert_{0}^\\theta = \\frac{n}{n+1}\\theta, \\forall \\theta \\in \\Theta, \\\\\n\\Rightarrow E_\\theta(\\hat{\\theta}_{\\mathrm{MV}}(\\pmb{X}_n)^2) &= \\frac{n\\theta^2}{n+2}, \\forall \\theta \\in \\Theta, \\\\\n\\Rightarrow \\mathrm{Var}_\\theta(\\hat{\\theta}_{\\mathrm{MV}}(\\pmb{X}_n)) &= \\frac{n\\theta^2}{n+2} - \\left(\\frac{n}{n+1}\\right)^2\\theta^2 \\\\\n\\Rightarrow \\mathrm{EQM}_\\theta(\\hat{\\theta}_{\\mathrm{MV}}(\\pmb{X}_n),\\theta) &= \\frac{n}{n+2}\\theta^2 -\n\\frac{n^2}{(n+1)^2}\\theta^2 + \\left(\\frac{n}{n+1}\\theta - \\theta\\right)^2 \\\\\n&= \\theta^2\\left(\\frac{n}{n+2} - \\frac{2}{n+1} + 1\\right)\n\\end{aligned}\n\\]\n\n\n32.1.1.2 Simulação\n\nusing Random, Distributions, StatsBase, Plots, LaTeXStrings\n\nRandom.seed!(76)\n\nM = 10_000\nn = 10\ntheta0 = rand(Poisson(4)) + 1\n\nd = Uniform(0, theta0)\n\nthetaMM = zeros(M)\nthetaMV = zeros(M)\nfor i in 1:M\n  x = rand(d, n)\n  thetaMM[i] = 2*mean(x)\n  thetaMV[i] = maximum(x)\nend\n\n# Comparar\nprintln(\"Média simulada do EMM = $(mean(thetaMM)),\n        calculada = $theta0\")\nprintln(\"Média simulada do EMV = $(mean(thetaMV)),\n        calculada = $(10/11 * theta0)\")\nprintln(\"Variância simulada do EMM = $(var(thetaMM)),\n        calculada = $(theta0^2 /(3*n))\")\nprintln(\"Variância simuladado EMV = $(var(thetaMV)),\n        calculada = $(n * theta0^2 / (n+2) - (n/(n+1))^2 * theta0^2)\")\n\ndensidade(x) = 0 &lt; x &lt; theta0 ? n * x^(n-1) / theta0^n : 0\n\npmm = histogram(thetaMM, label = \"\",\n                title = \"Histograma do EMM\", normalize=:pdf)\npmv = histogram(thetaMV, label = \"\",\n                title = \"Histograma do EMV\", normalize=:pdf)\nplot!(minimum(thetaMV):0.001:maximum(thetaMV)-0.01, densidade,\n      label=\"Densidade Teórica\")\ndisplay(plot(pmm, pmv))\n\nMédia simulada do EMM = 4.012626175744703,\n        calculada = 4\nMédia simulada do EMV = 3.6375235806062722,\n        calculada = 3.6363636363636362\nVariância simulada do EMM = 0.5384935090654293,\n        calculada = 0.5333333333333333\nVariância simuladado EMV = 0.11001021153100962,\n        calculada = 0.11019283746556674\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n32.1.2 Exemplos adicionais\nSeja \\(\\boldsymbol{X}_n\\) a.a. de \\(X \\sim f_\\theta, \\theta \\in \\Theta\\). Encontre o estimador pelo método de momentos para os casos abaixo:\n\n\\(X\\sim \\mathrm{Ber}(\\theta), \\theta \\in (0,1)\\)\n\\(X\\sim \\mathrm{Bin}(m, \\theta), \\theta \\in (0,1)\\)\n\\(X\\sim \\mathrm{Poiss}(\\theta), \\theta \\in (0,\\infty)\\)\n\\(X\\sim \\mathrm{N}(\\mu, \\sigma^2), \\theta = (\\mu, \\sigma^2) \\in \\mathbb{R}\\times\\mathbb{R}_+\\)\n\\(X\\sim \\mathrm{Exp}(\\theta), \\theta &gt; 0\\)\n\\(X\\sim \\mathrm{Gamma}(\\alpha, \\beta), \\theta = (\\alpha, \\beta) \\in \\mathbb{R}_+\\times\\mathbb{R}_+\\)\n\n\n32.1.2.1 Respostas\n\n\\(E_\\theta(X) = \\theta\\). Portanto, \\(\\hat{\\theta}_{\\mathrm{MM}}(\\boldsymbol{X}_n) = \\bar{X}\\).\n\\(E_\\theta(X) = m \\theta\\). Portanto, \\(\\hat{\\theta}_{\\mathrm{MM}}(\\boldsymbol{X}_n) = \\frac{\\bar{X}}{m}\\).\n\\(E_\\theta(X) = \\theta\\). Portanto, \\(\\hat{\\theta}_{\\mathrm{MM}}(\\boldsymbol{X}_n) = \\bar{X}\\).\n\\(E_\\theta(X) = \\theta, E_\\theta(X^2) = \\sigma^2 + \\mu^2\\). Note que \\[\n\\begin{aligned}\n&\\left\\{\\begin{array}{ll}\n\\mu &= \\bar{X} \\\\\n\\sigma^2 + \\mu^2 &= \\frac{1}{n} \\sum X_i^2\n\\end{array}\\right. \\\\\n\\Rightarrow&\\left\\{\\begin{array}{ll}\n\\hat{\\mu}_{\\mathrm{MM}}(\\boldsymbol{X}_n) &= \\bar{X} \\\\\n\\hat{\\sigma^2}_{\\mathrm{MM}}(\\boldsymbol{X}_n) + \\hat{\\mu}_{\\mathrm{MM}}(\\boldsymbol{X}_n)^2 &= \\frac{1}{n} \\sum X_i^2\n\\end{array}\\right. \\\\\n\\Rightarrow&\\left\\{\\begin{array}{ll}\n\\hat{\\mu}_{\\mathrm{MM}}(\\boldsymbol{X}_n) &= \\bar{X} \\\\\n\\hat{\\sigma^2}_{\\mathrm{MM}}(\\boldsymbol{X}_n) = \\frac{1}{n} \\sum X_i^2 - \\bar{X}^2\n\\end{array}\\right. \\\\\n\\Rightarrow& \\hat{\\theta}_{\\mathrm{MM}}(\\boldsymbol{X}_n) = \\left(\\bar{X}, \\frac{1}{n} \\sum X_i^2 - \\bar{X}^2\\right)\n\\end{aligned}\n\\]\n\\(E_\\theta(X) = \\frac{1}{\\theta}\\). Portanto, \\(\\hat{\\theta}_{\\mathrm{MM}}(\\boldsymbol{X}_n) = \\frac{1}{\\bar{X}}\\)\n\n\n\n\n\n\nDistribuição Gama\n\n\n\nSe \\(X \\sim \\mathrm{Gama}(\\alpha,\\beta)\\), então \\[\n\\left\\{\\begin{array}{ll}\nf_\\theta^{X}(x) = \\frac{\\beta^\\alpha x^{\\alpha-1}\\mathrm{e}^{-\\beta x}}{\\Gamma(\\alpha)}, & x &gt; 0\\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right.\n\\] Logo, \\[\n\\begin{aligned}\nE_\\theta(X^k) &= \\int^\\infty_0 x^k\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha-1} \\mathrm{e}^{-\\beta x} dx \\\\\n&= \\int^\\infty_0 \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha + k - 1} \\mathrm{e}^{-\\beta x} dx \\\\ \\\\\ny = \\beta x, dy &= \\beta dx, x = \\frac{y}{\\beta} \\\\ \\\\\n\\Rightarrow E_\\theta(X^k) &= \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\int^\\infty_0 \\left(\\frac{y}{\\beta}\\right)^{\\alpha + k - 1}\n\\mathrm{e}^{-y} \\frac{dy}{\\beta} \\\\\n&= \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\frac{1}{\\beta^{\\alpha+k}} \\int^\\infty_0 y^{\\alpha + k - 1} \\mathrm{e}^{-y} dy \\\\\n&= \\frac{1}{\\Gamma(\\alpha) \\beta^k} \\Gamma(\\alpha+k) \\\\\n\\Rightarrow E_\\theta(X) &= \\frac{1}{\\Gamma(\\alpha) \\beta} \\Gamma(\\alpha +1 ) = \\frac{\\alpha \\Gamma(\\alpha)}{\\Gamma(\\alpha) \\beta}\n= \\frac{\\alpha}{\\beta} \\\\\n\\Rightarrow E_\\theta(X^2) &= \\frac{1}{\\Gamma(\\alpha) \\beta^2} \\Gamma(\\alpha +2)\\\\\n&= \\frac{(\\alpha+1) \\Gamma(\\alpha+1)}{\\Gamma(\\alpha) \\beta}\n= \\frac{(\\alpha+1)\\alpha\\Gamma(\\alpha)}{\\Gamma(\\alpha)\\beta^2} = \\frac{\\alpha(\\alpha+1)}{\\beta^2}\n\\end{aligned}\n\\]\n\n\n\nSeguindo para as equações de momentos, \\[\n\\begin{aligned}\n&\\left\\{\\begin{array}{ll}\n\\frac{\\alpha}{\\beta} = \\bar{X} \\\\\n\\frac{\\alpha (\\alpha+1)}{\\beta^2} = \\frac{1}{n} \\sum X_i^2\n\\end{array}\\right. \\\\\n\\Rightarrow &\\left\\{\\begin{array}{ll}\n\\alpha = \\bar{X}\\beta \\\\\n\\frac{\\bar{X}\\beta(\\bar{X}\\beta+1)}{\\beta^2} = \\frac{1}{n} \\sum X_i^2\n\\end{array}\\right. \\\\\n\\Rightarrow &\\left\\{\\begin{array}{ll}\n\\alpha = \\bar{X}\\beta \\\\\n(\\bar{X}\\beta)^2 + \\bar{X}\\beta = \\beta^2 \\frac{1}{n} \\sum X_i^2\n\\end{array}\\right. \\\\\n\\Rightarrow &\\left\\{\\begin{array}{ll}\n\\alpha = \\bar{X}\\beta \\\\\n\\beta^2\\left(\\frac{1}{n}\\sum X_i^2\\right) - (\\beta\\bar{X})^2 - \\beta \\bar{X} = 0\n\\end{array}\\right. \\\\\n\\Rightarrow &\\left\\{\\begin{array}{ll}\n\\alpha = \\bar{X}\\beta \\\\\n\\beta^2\\underbracket{\\left(\\frac{1}{n}\\sum X_i^2 - \\bar{X}^2\\right)}_{S^2} -\\beta\\bar{X} = 0\n\\end{array}\\right. \\\\ \\\\\n\\frac{\\bar{X} \\pm \\sqrt{\\bar{X}^2 - 4\\cdot S^2 \\cdot 0}}{2S^2} &= \\frac{\\bar{X} \\pm \\sqrt{\\bar{X}^2}}{2S^2}\\\\\n&=\n\\left\\{\\begin{array}{l}\n0 \\\\\n\\frac{\\bar{X}}{S^2}\n\\end{array}\\right. \\\\ \\\\\n\\Rightarrow &\\left\\{\\begin{array}{ll}\n\\alpha = \\bar{X}\\beta \\\\\n\\beta^2 S^2 = \\beta \\bar{X}\n\\end{array}\\right. \\\\\n\\Rightarrow &\\left\\{\\begin{array}{ll}\n\\hat{\\alpha}_{\\mathrm{MM}}(\\boldsymbol{X}_n) = \\frac{\\bar{X}^2}{S^2} \\\\\n\\hat{\\beta}_{\\mathrm{MM}}(\\boldsymbol{X}_n) = \\frac{\\bar{X}}{S^2}\n\\end{array}\\right. \\\\\n\\Rightarrow& \\hat{\\theta}_{\\mathrm{MM}}(\\boldsymbol{X}_n) = \\left(\\frac{\\bar{X}^2}{S^2}, \\frac{\\bar{X}}{S^2}\\right).\n\\end{aligned}\n\\]\n\nusing Random, Distributions, StatsBase, Plots, LaTeXStrings\n\nRandom.seed!(31)\n\nM = 10_000\nn = 10 # Tamanho da amostra\nalpha0 = 4\nbeta0 = 6\n\nd = Gamma(alpha0, 1/beta0)\n\nalphaMM = zeros(M)\nbetaMM = zeros(M)\nfor i in 1:M\n  x = rand(d, n)\n  s = var(x, corrected = false)\n  alphaMM[i] = mean(x)^2/s\n  betaMM[i] = mean(x)/s\nend\n\n# Comparar\nprintln(\"Média simulada do EMM para alpha = $(mean(alphaMM)),\n        real = $alpha0\")\nprintln(\"Média simulada do EMM para beta = $(mean(betaMM)),\n        real = $beta0\")\n\n# Plotar\npmmalpha = histogram(alphaMM, label = \"\",\n                title = \"Histograma do EMM para \" * L\"\\alpha\", normalize=:pdf,\n                     bins = 40)\nvline!([alpha0], label=\"Valor real\")\npmmbeta = histogram(betaMM, label = \"\",\n                title = \"Histograma do EMM para \" * L\"\\beta\", normalize=:pdf,\n                    bins = 40)\nvline!([beta0], label=\"Valor real\")\ndisplay(plot(pmmalpha, pmmbeta))\n\nMédia simulada do EMM para alpha = 5.838811157394312,\n        real = 4\nMédia simulada do EMM para beta = 8.994281492594121,\n        real = 6",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Estimação pelo método de momentos (EMM)</span>"
    ]
  },
  {
    "objectID": "metodo-nr.html",
    "href": "metodo-nr.html",
    "title": "33  Método de Newton-Raphson",
    "section": "",
    "text": "33.1 Exemplo\nNem sempre existem soluções fechadas para estimadores de máxima verossimilhança ou obtidos pelo método de momentos. Por exemplo, \\(U_n(\\boldsymbol{X}_n, \\theta) = 0\\) não tem solução fechada para o EMV e \\(E_\\theta(X^k) \\frac{1}{n} \\sum X_i^k\\) para o EMM.\nNos dois casos, estamos interessados em encontrar os valores de “\\(\\theta\\)” que “zeram” uma função \\(G(\\theta)\\),\n\\[\n\\begin{aligned}\n2.& \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  G(\\theta) = \\left(\n\\begin{array}{c}\nE_\\theta(X) - \\frac{1}{n} \\sum X_i \\\\\n\\vdots \\\\\nE_\\theta(X^p) - \\frac{1}{n} \\sum X_i^p\n\\end{array}\\right)\n\\end{aligned}\n\\]\nConsideraremos a seguir apenas o caso \\(p=1\\)\nSeja \\(G : \\Theta \\rightarrow \\mathbb{R}\\) uma função contínua e diferenciável, com derivadas contínuas, tal que \\(G'(\\theta) \\neq 0,\\forall \\theta \\in \\Theta\\). Temos como objetivo encontrar \\(\\hat{\\theta} \\in \\Theta\\) tal que \\(G(\\hat{\\theta}) = 0\\).\nInicia-se o processo com um valor qualquer \\(\\theta_0 \\in \\Theta\\) e calculamos o valor seguinte por meio de: \\[\n\\theta^{(j)} = \\theta^{(j-1)} - \\frac{G(\\theta^{(j-1)})}{G'(\\theta^{(j-1)})}, j = 1, 2, \\dots\n\\]\nContinua-se até que \\(\\lvert \\theta^{(j)} - \\theta^{(j-1)}\\rvert \\leq \\epsilon\\) em que \\(\\epsilon\\) é um valor de erro pequeno fixado.\nQuando tentamos encontrar o EMV para \\(\\theta\\) de \\(X \\sim f_\\theta, \\theta \\in \\Theta\\) com \\[\nf_\\theta(x) = \\left\\{\\begin{array}{ll}\n\\frac{1}{\\Gamma(\\theta)} x^{\\theta-1} \\mathrm{e}^{-x}, & x &gt; 0 \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right.\n\\] não conseguimos finalizar a expressão. Vamos relembrar o escore da amostra igualado a zero:\n\\[\nU_n(\\boldsymbol{x}_n,\\theta) = \\sum \\ln x_i -\\frac{n}{\\Gamma(\\theta)}\\Gamma'(\\theta) + \\sum \\ln x_i  = 0\n\\]\nConsidere os \\(n=7\\) valores observados, \\(\\boldsymbol{x}_7 = (3.1, 4.2, 5.7, 2.3, 7.7, 5.1, 3.5)\\) \\[\n\\Rightarrow U_n(\\boldsymbol{x}_n,\\theta) = \\underbracket{\\frac{7}{\\Gamma(\\theta)}\\Gamma'(\\theta)}_{\\frac{\\partial \\ln}{\\partial \\theta} \\Gamma(\\theta)}\n+\\sum \\ln x_i = 0\n\\]\nUtilizando \\(\\theta^{(0)} = 1\\), \\[\n\\begin{aligned}\n\\theta^{(j)} &= \\theta^{(j-1)} - \\frac{U_n(\\boldsymbol{x}_n,\\theta^{(j-1)})}{U'_n(\\boldsymbol{x}_n, \\theta^{(j-1)})} \\\\\n&= \\theta^{(j-1)} - \\frac{-7 \\psi_1(\\theta^{(j-1)})+ 7 \\sum \\ln x_i}{-7\\psi_2(\\theta^{(j-1)})} \\\\\n&= \\theta^{(j-1)} - \\frac{-\\psi_1(\\theta^{(j-1)})+ \\sum \\ln x_i}{-\\psi_2(\\theta^{(j-1)})}\n\\end{aligned}\n\\]\nem que \\(\\psi_j(\\theta) = \\frac{\\partial^j \\ln \\Gamma(\\theta)}{\\partial \\theta^j}\\)",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Método de Newton-Raphson</span>"
    ]
  },
  {
    "objectID": "metodo-nr.html#algoritmos-e-outros-métodos",
    "href": "metodo-nr.html#algoritmos-e-outros-métodos",
    "title": "33  Método de Newton-Raphson",
    "section": "33.2 Algoritmos e outros métodos",
    "text": "33.2 Algoritmos e outros métodos\nSeja \\(\\boldsymbol{X}_n\\) a.a. de \\(X \\sim \\Gamma(\\theta, 1)\\) (exemplo anterior). Ainda considerando os mesmos valores amostrados, \\(\\boldsymbol{x}_7 = (3.1, 4.2, 5.7, 2.3, 7.7, 5.1, 3.5)\\), utilize os métodos numéricos para encontrar a estimativa de máxima verossimilhança.\n\n33.2.1 Algoritmo por Newton-Raphson\nConsidere um erro \\(\\epsilon = 10^{-5}\\) e \\(\\hat{\\theta}^{(0)} = 1.\\) \\[\n\\theta^{(j+1)} = \\theta^{(j)} - \\frac{U_n(\\boldsymbol{x}_n,\\theta^{(j)})}{U'_n(\\boldsymbol{x}_n, \\theta^{(j)})}\n\\] em que \\(U_n(\\boldsymbol{x}_n,\\theta) = -n\\psi_1(\\theta^{(j-1)})+ \\sum \\ln x_i\\) e \\(U_n'(\\boldsymbol{x}_n,\\theta) = -n\\psi_2(\\theta^{(j-1)})\\) com \\(\\psi_j(\\theta) = \\frac{\\partial^j \\ln \\Gamma(\\theta)}{\\partial \\theta^j}\\) e \\(n=7\\).\nLogo, \\[\n\\theta^{(j+1)} = \\theta^{(j)} - \\frac{U_n(\\boldsymbol{x}_n,\\theta^{(j)})}{U'_n(\\boldsymbol{x}_n, \\theta^{(j)})}\n\\]\n\nJuliaPythonR\n\n\n\nusing SpecialFunctions # Funções di e trigamma (derivadas do log da gama)\n\n\nfunction main()\n\n  x = [3.1, 4.2, 5.7, 2.3, 7.7, 5.1, 3.5]\n  n = length(x)\n\n  theta::Vector{Float64} = []\n  theta0 = 1\n  append!(theta, theta0)\n\n\n  erromax = 10^(-5)\n  erro = Inf\n  i = 1\n  # iteracoesMax = 6 # Podemos também definir apenas um erro máximo\n  while erro &gt; erromax # && i &lt; iteracoesMax\n    append!(theta, theta[i] - (sum(log.(x)) - n * digamma(theta[i]))/\n                                             (-n*trigamma(theta[i])))\n    erro = abs(theta[i+1] - theta[i])\n    println(\"Erro na iteração $i: $erro\")\n    i += 1\n  end\n  println(\"Theta final: $(theta[length(theta)])\")\n  println(\"Total de iterações: $i\")\n\nend\n\nmain()\n\nErro na iteração 1: 1.2248511275743903\nErro na iteração 2: 1.5558203731733413\nErro na iteração 3: 0.8122385697386227\nErro na iteração 4: 0.10638335531193821\nErro na iteração 5: 0.0013809916201203976\nErro na iteração 6: 2.2502724394968254e-7\nTheta final: 4.700674642445657\nTotal de iterações: 7\n\n\n\n\nprint(\"Teste Python\")\nprint(1+1)\n\n\nTeste Python\n2\n\n\n\n\ncat(\"Teste R\")\ndados&lt;-c(1,2,3,4,5)\nplot(dados)\n\n\nTeste R\n\n\n\n\n\n\n\n\n\n\n\n\nNote que, se \\(U'_n(\\pmb{x}_n, \\hat{\\theta}^{(j+1)}) &lt; 0\\), a estimativa \\(\\hat{\\theta}^{(j+1)}\\) será a de MV.\nSeja \\(\\boldsymbol{X}_n\\) a.a. de \\(X\\sim \\mathrm{Exp}(\\theta), \\theta \\in \\Theta = (0,\\infty)\\). Encontre a estimativa de MV para \\(\\theta\\) pelo método de Newton-Raphson considerando \\(\\boldsymbol{x}_n = (2, 2.5, 3, 3.5, 2, 1.5)\\) e \\(\\hat{\\theta}^{(0)} = 0.5\\) Com erro máximo \\(\\epsilon = 10^{-5}\\).\nNote que \\[\n\\begin{aligned}\nf_\\theta(x) &= \\theta \\mathrm{e}^{-\\theta x} \\\\\n\\mathcal{L}_{\\boldsymbol{x}_n}(\\theta) &= \\theta^n \\mathrm{e}^{-\\theta\\sum x_i} \\\\\n\\ln \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta) &= n \\ln\\theta -\\theta\\sum x_i \\\\\nU_n(\\boldsymbol{x}_n,\\theta) &= \\frac{n}{\\theta} - \\sum x_i \\\\\nU_n'(\\boldsymbol{x}_n,\\theta) &= -\\frac{n}{\\theta^2}\n\\end{aligned}\n\\]\nLogo \\[\n\\begin{aligned}\n\\hat{\\theta}^{(j+1)} &= \\hat{\\theta}^{(j)} - \\frac{\\frac{n}{\\hat{\\theta}^{(j)}}- \\sum x_i}{-\\frac{n}{(\\hat{\\theta}^{(j)})^2}} \\\\\n&= \\hat{\\theta}^{(j)} - \\frac{[n - \\hat{\\theta}^{(j)}\\sum x_i]\\hat{\\theta}^{(j)}}{-n}\n\\end{aligned}\n\\]\n\nfunction main()\n\n  x = [2, 2.5, 3, 3.5, 2, 1.5]\n  n = length(x)\n\n  theta::Vector{Float64} = []\n  theta0 = 0.5\n  append!(theta, theta0)\n\n\n  erromax = 10^(-5)\n  erro = Inf\n  i = 1\n  while erro &gt; erromax\n    append!(theta, theta[i] - ((n - sum(x) *theta[i])*theta[i])/(-n))\n    erro = abs(theta[i+1] - theta[i])\n    println(\"Erro na iteração $i: $erro\")\n    i += 1\n  end\n  println(\"Theta final: $(theta[length(theta)])\")\n  println(\"Total de iterações: $i\")\n\nend\n\nmain()\n\nErro na iteração 1: 0.10416666666666669\nErro na iteração 2: 0.01718026620370372\nErro na iteração 3: 0.0007780354808986645\nErro na iteração 4: 1.468425129103057e-6\nTheta final: 0.4137931034430648\nTotal de iterações: 5\n\n\nNote que se \\(\\Theta \\subseteq \\mathbb{R}^P\\), o método é dado por \\[\n\\theta^{(j+1)} = \\theta^{(j)} - \\left[U_n'(\\boldsymbol{x}_n,\\theta^{(j)})\\right]^{-1} \\cdot U_n(\\boldsymbol{x}_n, \\hat{\\theta}^{(j)}), j = 0, 1, \\dots\n\\]\n\n\n33.2.2 Método de Scoring de Fisher\nPara \\(\\Theta \\subseteq \\mathbb{R}\\) \\[\n\\theta^{(j+1)} = \\theta^{(j)} + \\frac{U_n(\\boldsymbol{x}_n,\\theta^{(j)})}{I_n(\\theta^{(j)})}\n\\] Para \\(\\Theta \\subseteq \\mathbb{R}^P\\) \\[\n\\theta^{(j+1)} = \\theta^{(j)} + \\left[I_n(\\boldsymbol{x}_n,\\theta^{(j)})\\right]^{-1} \\cdot U_n(\\boldsymbol{x}_n, \\hat{\\theta}^{(j)}), j = 0, 1, \\dots\n\\]\n\n\n33.2.3 Método de descida do gradiente\n\\[\n\\theta^{(j+1)} = \\theta^{(j)} + \\delta \\cdot U_n(\\boldsymbol{x}_n, \\hat{\\theta}^{(j)}), j = 0, 1, \\dots\n\\] em que \\(\\delta \\in (0,1)\\) é a taxa de “aprendizado”.\nPara o exemplo da Gama,\n\nusing SpecialFunctions # Funções di e trigamma (derivadas do log da gama)\n\n\nfunction main()\n\n  x = [3.1, 4.2, 5.7, 2.3, 7.7, 5.1, 3.5]\n  n = length(x)\n\n  theta::Vector{Float64} = []\n  theta0 = 1\n  append!(theta, theta0)\n  delta = 0.01\n\n\n  erromax = 10^(-5)\n  erro = Inf\n  i = 1\n  while erro &gt; erromax\n    append!(theta, theta[i] + delta *(sum(log.(x)) - n * digamma(theta[i])))\n    erro = abs(theta[i+1] - theta[i])\n    if i % 100 == 0\n      println(\"Erro na iteração $i: $erro\")\n    end\n    i += 1\n  end\n  println(\"Theta final: $(theta[end])\")\n  println(\"Total de iterações: $i\")\n\nend\n\nmain()\n\nErro na iteração 100: 0.007983394465004956\nErro na iteração 200: 0.0013675177845966502\nErro na iteração 300: 0.0002527165874308679\nErro na iteração 400: 4.731072948604975e-5\nTheta final: 4.700082915774669\nTotal de iterações: 494\n\n\n\n\n33.2.4 Descida estocástica do gradiente\n\\[\n\\theta^{(j+1, i)} = \\theta^{(j, i)} + \\delta \\cdot U(x_i, \\hat{\\theta}^{(j, i)}), j = 0, 1, \\dots, i \\in \\{1,\\dots,n\\}\n\\]\nEm que \\(U(x, \\hat{\\theta}^{(j)})\\) é a função escore para uma observação. Usaremos o mesmo modelo gama.\n\nusing SpecialFunctions, Distributions, Plots\n\nfunction descida_estocastica()\n\n  x = [3.1, 4.2, 5.7, 2.3, 7.7, 5.1, 3.5]\n  n = length(x)\n\n  theta::Vector{Float64} = []\n  append!(theta, 1)\n  delta = 0.0001\n\n\n  erro = Inf\n  erromax = 10^(-5)\n  j = 1\n  epoca = 1\n  while erro &gt; erromax\n    for i in 1:n\n      append!(theta, theta[j] + delta * (log(x[i]) - digamma(theta[j])))\n      erro = abs(theta[j+1] - theta[j])\n      j += 1\n    end\n    epoca += 1\n    if epoca % 500 == 0\n      println(\"Erro na época $epoca: $erro\")\n    end\n  end\n  epoca -= 1\n  println(\"Theta final: $(theta[end])\")\n  println(\"Total de epocas: $epoca \\n \\n\")\n\n  println(\"Verificando se é máximo local\")\n  println(\"-n * trigamma(theta calculado) = $(-n * trigamma(theta[end]))\")\n  \n  U(y) = sum(log.(x)) - n*digamma(y)\n  p = plot(U, xlims=(0.1, 100), label=\"U\")\n  vline!(theta[1:4000:end],label=\"(Alguns dos) thetas calculados\")\n  hline!([0],label=\"\", color=:black)\n  display(p)\nend\n\ndescida_estocastica()\n\nErro na época 500: 0.00011562567419964864\nErro na época 1000: 8.455622767256088e-5\nErro na época 1500: 6.523785153733641e-5\nErro na época 2000: 5.164846886707153e-5\nErro na época 2500: 4.1416753598255696e-5\nErro na época 3000: 3.337361869748534e-5\nErro na época 3500: 2.685988163664277e-5\nErro na época 4000: 2.146877836084471e-5\nErro na época 4500: 1.6932425193516565e-5\nErro na época 5000: 1.306548867541224e-5\nTheta final: 3.6540367477421754\nTotal de epocas: 5457 \n \n\nVerificando se é máximo local\n-n * trigamma(theta calculado) = -2.201394773690926\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote que esse método é falível. Caímos em um valor que não é máximo global e diferente dos obtidos pelos outros métodos. Aumentaremos o valor de \\(\\delta\\) e adicionaremos um máximo de 5000 épocas. Também escolheremos um valor mais alto para \\(\\theta_0\\), como \\(4\\).\n\nusing SpecialFunctions, Distributions, Plots\n\nfunction descida_estocastica_limitada()\n\n  x = [3.1, 4.2, 5.7, 2.3, 7.7, 5.1, 3.5]\n  n = length(x)\n\n  theta::Vector{Float64} = []\n  append!(theta, 4)\n  delta = 0.01\n\n\n  erro = Inf\n  erromax = 10^(-5)\n  j = 1\n  epoca = 1\n  while erro &gt; erromax && epoca &lt;= 5000\n    for i in 1:n\n      append!(theta, theta[j] + delta * (log(x[i]) - digamma(theta[j])))\n      erro = abs(theta[j+1] - theta[j])\n      j += 1\n    end\n    epoca += 1\n    if epoca % 1000 == 0\n      println(\"Erro na época $epoca: $erro\")\n    end\n  end\n  epoca -= 1\n  println(\"Theta final: $(theta[end])\")\n  println(\"Total de epocas: $epoca \\n \\n\")\n\n  println(\"Verificando se é máximo local\")\n  println(\"-n * trigamma(theta calculado) = $(-n * trigamma(theta[end]))\")\n  \n  U(y) = sum(log.(x)) - n*digamma(y)\n  p = plot(U, xlims=(0.1, 100), label=\"U\")\n  vline!(theta[1:500:end],label=\"(Alguns dos) thetas calculados\")\n  hline!([0],label=\"\", color=:black)\n  display(p)\nend\n\ndescida_estocastica_limitada()\n\nErro na época 1000: 0.0018561619919994499\nErro na época 2000: 0.001856162087572777\nErro na época 3000: 0.001856162087572777\nErro na época 4000: 0.001856162087572777\nErro na época 5000: 0.001856162087572777\nTheta final: 4.70217698128991\nTotal de epocas: 5000 \n \n\nVerificando se é máximo local\n-n * trigamma(theta calculado) = -1.6580912861780042",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Método de Newton-Raphson</span>"
    ]
  },
  {
    "objectID": "cond-regular.html",
    "href": "cond-regular.html",
    "title": "34  Condições de Regularidade",
    "section": "",
    "text": "\\(C_1: \\mathfrak{X} = \\{x : f_\\theta (x) &gt; 0 \\}\\) não depende de “\\(\\theta\\)”.\n\\(C_2: f_\\theta\\) é duas vezes diferenciável com respeito a “\\(\\theta\\)” e suas derivadas são contínuas.\n\\(C_3:\\) é possível trocar as derivadas pela integrais da seguinte forma: \\[\n\\left\\{\\begin{aligned}\n&a)\\ \\frac{\\partial}{\\partial\\theta} \\int f_\\theta(x) dx &= \\int \\frac{\\partial}{\\partial \\theta} f_\\theta(x) dx \\\\\n&b)\\ \\frac{\\partial^2}{\\partial \\theta \\partial \\theta^T} \\int f_\\theta(x) dx &= \\int \\frac{\\partial^2}{\\partial \\theta \\partial \\theta^T} f_\\theta(x) dx\n\\end{aligned}\\right.\n\\]\n\\(C_4:\\) \\[\n\\left\\{\\begin{aligned}\n&\\mathrm{a})\\ E_\\theta\\left(\\lVert\\frac{\\partial}{\\partial\\theta} \\ln f_\\theta(x)\\rVert\\right) &lt; \\infty,\\ \\forall \\theta \\in \\Theta \\\\\n&\\mathrm{b})\\ E_\\theta\\left(\\lVert\\frac{\\partial^2}{\\partial\\theta\\partial\\theta^T} \\ln f_\\theta(x)\\rVert^2\\right) &lt; \\infty,\\ \\forall \\theta \\in \\Theta.\n\\end{aligned}\\right.\n\\]\nPara as duas últimas condições, substituímos no caso discreto as integrais por somatórios.\n\\(C_5\\): \\(\\Theta\\) é aberto e convexo, ou seja, se \\(\\theta_1, \\theta_2 \\in \\Theta\\), então \\[\n\\lambda\\theta_1 + (1-\\lambda)\\theta_2 \\in \\Theta, \\forall \\lambda \\in (0,1)\n\\]\nA condição (fraca) de identificabilidade \\(C_6\\): \\[\nE_{\\theta_1}(U_n(\\boldsymbol{X}_n,\\theta_2)) = 0 \\iff \\theta_1 = \\theta_2,\n\\]\n\\(C_7\\): A informação de Fisher existe, é positiva (definida) e finita.\n\\(C_8\\): \\[\n\\begin{aligned}\n& \\sup_{\\theta_2 \\in \\Theta} \\left\\lvert M(\\theta,\\theta_2) - \\frac{1}{n}\\lvert U_n(\\boldsymbol{X}_n,\\theta_2) \\rvert \\right\\rvert\n\\stackrel{P_\\theta}{\\rightarrow} 0\\\\\n&\\forall \\theta \\in \\Theta, M(\\theta, \\theta_2) = \\frac{1}{n}\\left\\lvert E_\\theta(U_n(\\boldsymbol{X}_n,\\theta_2))\\right\\rvert\n\\end{aligned}\n\\]\n\n\n\n\n\n\nConsistência e \\(C_8\\)\n\n\n\n\\(C_8\\) não é necessária se \\(\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n)\\) for consistente, ou seja, \\[\n\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n) \\stackrel{P_\\theta}{\\rightarrow} \\theta, \\forall \\theta \\in \\Theta.\n\\]\n\n\n\\(C_9\\): \\[\n\\frac{1}{n} U_n'(\\boldsymbol{X}_n,\\theta^*_n) - \\frac{1}{n} U_n'(\\boldsymbol{X}_n,\\theta) \\stackrel{P_\\theta}{\\rightarrow} 0, \\forall \\theta \\in \\Theta.\n\\] em que \\(\\theta^*_n\\) é um estimador consistente qualquer para “\\(\\theta\\)”.\n\n\n\n\n\n\nSuficientes mas não necessárias\n\n\n\nNem toda condição de regularidade é necessária para provar algum teorema. Em muitos casos, por exemplo, podemos utilizar apenas as condições \\(C_1:C_4\\), enquanto resultados mais fortes podem precisar de mais condições.",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Condições de Regularidade</span>"
    ]
  },
  {
    "objectID": "slutsky.html",
    "href": "slutsky.html",
    "title": "35  Resultados utilizando o Teorema de Slutsky",
    "section": "",
    "text": "Nota 35.1: Método Delta\n\n\n\nSeja \\(T(\\boldsymbol{X}_n)\\) um estimador para “\\(\\theta\\)”, \\(\\theta \\in \\Theta \\subseteq \\mathbb{R}\\), assintoticamente normal, ou seja, \\[\n\\sqrt{n} (T(\\boldsymbol{X}_n) - \\theta) \\stackrel{\\mathcal{D}}{\\rightarrow} N(0, \\mathrm{V}_\\theta), \\forall \\theta \\in \\Theta.\n\\]\nConsidere \\(g : \\Theta \\rightarrow \\mathbb{R}\\) uma função diferenciável com derivada contínua tal que \\(g'(\\theta) \\neq 0, \\forall \\theta \\in \\Theta\\). Então, \\[\n\\sqrt{n} (g(T(\\boldsymbol{X}_n))- g(\\theta)) \\stackrel{\\mathcal{D}}{\\rightarrow} N(0, g'(\\theta)^2V_\\theta), \\forall \\theta \\in \\Theta.\n\\]\n\n\nSeja \\[\n\\frac{\\sqrt{n} (T(\\boldsymbol{X}_n) - \\theta)}{\\sqrt{\\mathrm{V}_\\theta}} \\stackrel{\\mathcal{D}}{\\rightarrow} N(0,1)\n\\]\nPelo teorema de Slutsky, \\[\n\\frac{\\sqrt{n} (T(\\boldsymbol{X}_n) - \\theta)}{\\sqrt{\\mathrm{V}_{T(\\boldsymbol{X}_n)}}} \\stackrel{\\mathcal{D}}{\\rightarrow} N(0,1)\n\\]\nAdemais, seja \\[\n\\frac{\\sqrt{n} (g(T(\\boldsymbol{X}_n)) - g(\\theta))}{\\sqrt{g'(\\theta)^2\\mathrm{V}_\\theta}} \\stackrel{\\mathcal{D}}{\\rightarrow} N(0,1)\n\\]\nPelo teorema de Slutsky, \\[\n\\frac{\\sqrt{n} (g(T(\\boldsymbol{X}_n)) - g(\\theta))}{\\sqrt{g'(T(\\boldsymbol{X}_n))^2\\mathrm{V}_{T(\\boldsymbol{X}_n)}}} \\stackrel{\\mathcal{D}}{\\rightarrow} N(0,1)\n\\]",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Resultados utilizando o Teorema de Slutsky</span>"
    ]
  },
  {
    "objectID": "prop-est.html",
    "href": "prop-est.html",
    "title": "36  Propriedades dos estimadores de máxima verossimilhança e de momentos",
    "section": "",
    "text": "36.1 Teorema (da invariância do EMV)\nSeja \\(\\boldsymbol{X}_n\\) a.a. de \\(X\\sim f_\\theta, \\theta \\in \\Theta.\\) Considere \\(g:\\Theta \\rightarrow \\mathbb{R}^k\\) uma função. Se existe o EMV para “\\(\\theta\\)”, então \\(g(\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n))\\) é o EMV para \\(g(\\theta)\\).",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Propriedades dos estimadores de máxima verossimilhança e de momentos</span>"
    ]
  },
  {
    "objectID": "prop-est.html#sec-invaremv",
    "href": "prop-est.html#sec-invaremv",
    "title": "36  Propriedades dos estimadores de máxima verossimilhança e de momentos",
    "section": "",
    "text": "36.1.1 Exemplo Bernoulli\n\\(X \\sim \\mathrm{Ber}(\\theta),\\theta \\in (0,1)\\). \\(\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n) = \\bar{X}\\).\nSe \\(g(\\theta) = \\theta^2\\), então \\(g(\\bar{X}) = \\bar{X}^2\\) é o EMV para \\(\\theta^2\\).\nSe \\(g(\\theta) = 1-\\theta\\), então \\(g(\\bar{X}) = 1 - \\bar{X}\\) é o EMV para \\(1-\\theta\\).\n\n\n36.1.2 Exemplo Poisson\n\\(X \\sim \\mathrm{Pois}(\\theta), \\theta &gt; 0\\). \\(\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n) = \\bar{X}\\).\nSe \\(g(\\theta) = P_\\theta(X\\geq 3)\\), então \\[\n\\begin{aligned}\ng(\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n)) &= P_{\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n)}(X\\geq 3) \\\\\n&= 1 -\\left(+\\mathrm{e}^{-\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n)} + \\mathrm{e}^{- \\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n)}\n\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n) + \\mathrm{e}^{-\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n)}\n\\frac{\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n)^2}{2!}\\right)\n\\end{aligned}\n\\]\nSe \\(g(\\theta) = E_\\theta(X^2)\\), então \\(g(\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n)) = E_{\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n)}(X^2) = \\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n) + (\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n))^2\\)\nSe \\(g(\\theta) = \\frac{\\sqrt{\\mathrm{Var}_\\theta(X)}}{E_\\theta(X)} = \\frac{\\sqrt{\\theta}}{\\theta} = \\frac{1}{\\sqrt{\\theta}}\\), então \\(g(\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n)) = \\frac{1}{\\sqrt{\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n)}}\\) é o EMV para \\(g(\\theta)\\).",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Propriedades dos estimadores de máxima verossimilhança e de momentos</span>"
    ]
  },
  {
    "objectID": "prop-est.html#sec-assinorm",
    "href": "prop-est.html#sec-assinorm",
    "title": "36  Propriedades dos estimadores de máxima verossimilhança e de momentos",
    "section": "36.2 Teorema (dos estimadores assintoticamente normais)",
    "text": "36.2 Teorema (dos estimadores assintoticamente normais)\nDizemos que \\(T(\\boldsymbol{X}_n)\\) é um estimador para “\\(\\theta\\)” assintoticamente normal se, e somente se, existir \\(\\mathrm{V}_\\theta\\) não negativo tal que \\[\n\\sqrt{n} (T(\\boldsymbol{X}_n) - \\theta) \\stackrel{\\mathcal{D}}{\\rightarrow} N(0,\\mathrm{V}_\\theta), \\forall \\theta \\in \\Theta.\n\\] ou seja, converge em distribuição para uma distribuição Normal de média \\(0\\) e variância \\(\\mathrm{V}_\\theta\\) para todo \\(\\theta\\) no espaço paramétrico.",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Propriedades dos estimadores de máxima verossimilhança e de momentos</span>"
    ]
  },
  {
    "objectID": "prop-est.html#sec-tlcemv",
    "href": "prop-est.html#sec-tlcemv",
    "title": "36  Propriedades dos estimadores de máxima verossimilhança e de momentos",
    "section": "36.3 Teorema (do limite central para o EMV)",
    "text": "36.3 Teorema (do limite central para o EMV)\nSeja \\(\\boldsymbol{X}_n\\) a.a. de \\(X\\sim f_\\theta\\) tal que as condições de regularidade \\(C_1:C_9\\) estejam satisfeitas.\nPortanto, com \\(g\\) diferenciável com derivada contínua tal que \\(g'(\\theta) \\neq 0\\), \\[\n\\sqrt{n}(g(\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n)) - g(\\theta)) \\stackrel{\\mathcal{D}}{\\rightarrow}\nN(0, g'(\\theta)^2 I_1(\\theta)^{-1}), \\forall \\theta \\in \\Theta\n\\]\nLogo, pelo teorema de Slutsky \\[\n\\sqrt{n}I_1(\\hat{\\theta}_{\\mathrm{MV}})\\frac{(g(\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n)) - g(\\theta))}{\\sqrt{g'(\\hat{\\theta}_{\\mathrm{MV}})^2}}\n\\stackrel{\\mathcal{D}}{\\rightarrow} N(0, 1), \\forall \\theta \\in \\Theta\n\\]\n\n\n\n\n\n\nEMVs são assintoticamente eficientes\n\n\n\nNote que \\[\n\\begin{aligned}\nE_\\theta(\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n)) &\\stackrel{n\\uparrow \\infty}{\\rightarrow} \\theta \\\\\nn \\mathrm{Var}_\\theta(\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n)) &\\stackrel{n\\uparrow \\infty}{\\rightarrow} I_1(\\theta)^{-1}\n\\end{aligned}\n\\] Ou seja, estimadores de máxima verossimilhança são assintoticamente eficientes sob as condições de regularidade.\nA variância assintótica do estimador de máxima verossimilhança é denotada por \\[\n\\mathrm{Var}_{\\theta}^{(a)} (\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n)) = \\frac{I_1(\\theta)^{-1}}{n}\n\\]\n\n\n\n\n\n\n\n\nNotação\n\n\n\n\\[\n\\begin{cases}\n\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n) &\\stackrel{a}{\\approx} N(\\theta, I_n(\\theta)) \\\\\ng(\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n)) &\\stackrel{a}{\\approx} N(g(\\theta), g'(\\theta)^2I_n(\\theta)^{-1}) \\\\\n\\end{cases}\n\\]\n\n\nPodemos calcular probabilidades aproximadas mesmo sem saber a distribuição exata, para \\(\\hat{\\theta}_{\\mathrm{MV}}\\)\n\n36.3.1 Exemplo\nSe \\(\\theta = 3\\), então calcule:\n\\[\n\\begin{aligned}\nP_{\\theta}(\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n)\\leq t) &= P_{\\theta}(\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n) - \\theta \\leq t - \\theta) \\\\\n&= P_{\\theta}\\left(\\frac{\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n) - \\theta}{\\sqrt{I_n(\\theta)^{-1}}} \\leq \\frac{t - \\theta}{\\sqrt{I_n(\\theta)^{-1}}}\\right) \\\\\n&\\stackrel{n\\ \\text{grande}}{\\approx} P_{\\theta}(N(0,1) \\leq I_n(\\theta)^{\\frac{1}{2}}(t-\\theta)) \\\\\n&= P_{\\theta}(N(0,1) \\leq I_n(\\theta)^{\\frac{1}{2}}(t-3))\n\\end{aligned}\n\\]",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Propriedades dos estimadores de máxima verossimilhança e de momentos</span>"
    ]
  },
  {
    "objectID": "prop-est.html#sec-tlcemm",
    "href": "prop-est.html#sec-tlcemm",
    "title": "36  Propriedades dos estimadores de máxima verossimilhança e de momentos",
    "section": "36.4 Teorema (do limite central para o estimador do método de momentos)",
    "text": "36.4 Teorema (do limite central para o estimador do método de momentos)\nSeja \\(\\boldsymbol{X}_n\\) a.a. de \\(X\\) tal que \\(\\Theta \\subseteq \\mathbb{R}\\) e \\(E_\\theta(|X|^k) &lt; \\infty\\) para algum \\(k \\in \\mathbb{N} \\setminus \\{0\\}\\). Então, o estimador \\(\\hat{\\theta}_{\\mathrm{MM}}(\\boldsymbol{X}_n)\\) tal que\n\\[\nE_{\\hat{\\theta}_{\\mathrm{MM}}(\\boldsymbol{X}_n)}(X^k) = \\frac{1}{n} \\sum X_i^k\n\\]\né assintoticamente normal se as seguintes condições estiverem satisfeitas:\n\n\\(E_\\theta[(X^k - E_\\theta(X^k))^2] &lt; \\infty\\)\n\\(h(\\theta) = \\frac{\\partial E_\\theta(X^k)}{\\partial \\theta} \\neq 0, \\forall \\theta \\in \\Theta.\\)\n\nCom isso, \\[\n\\sqrt{n}(\\hat{\\theta}_{\\mathrm{MM}}(\\boldsymbol{X}_n) - \\theta) \\stackrel{\\mathcal{D}}{\\rightarrow} N(0, V_\\theta)\n\\] em que \\[\nV_\\theta = \\frac{E_\\theta[(X^k - E_\\theta(X^k))^2]}{\\left(\\frac{\\partial E_\\theta(X^k)}{\\partial \\theta}\\right)^2} = \\frac{\\mathrm{Var}_\\theta (X^k)}{h(\\theta)^2}\n\\]\nAlém disso, se \\(g : \\Theta \\rightarrow \\mathbb{R}\\) for diferenciável com derivada contínua tal que \\(g'(\\theta) \\neq 0\\), então \\[\n\\sqrt{n}(g(\\hat{\\theta}_{\\mathrm{MM}}(\\boldsymbol{X}_n) - \\theta) - g(\\theta)\n\\stackrel{\\mathcal{D}}{\\rightarrow} N\\left(0,g'(\\theta)^2 \\frac{\\mathrm{Var}_\\theta (X^k)}{h(\\theta)^2}\\right)\n\\]\n\n\n\n\n\n\nNotação\n\n\n\n\\[\n\\begin{cases}\n\\hat{\\theta}_{\\mathrm{MM}}(\\boldsymbol{X}_n) &\\stackrel{a}{\\approx} N\\left(\\theta,\\frac{\\mathrm{Var}_\\theta (X^k)}{n h(\\theta)^2}\\right) \\\\\ng(\\hat{\\theta}_{\\mathrm{MM}}(\\boldsymbol{X}_n)) &\\stackrel{a}{\\approx} N\\left(g(\\theta), g'(\\theta)^2 \\frac{\\mathrm{Var}_\\theta (X^k)}{n h(\\theta)^2}\\right) \\\\\n\\end{cases}\n\\]",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Propriedades dos estimadores de máxima verossimilhança e de momentos</span>"
    ]
  },
  {
    "objectID": "prop-est.html#algoritmos",
    "href": "prop-est.html#algoritmos",
    "title": "36  Propriedades dos estimadores de máxima verossimilhança e de momentos",
    "section": "36.5 Algoritmos",
    "text": "36.5 Algoritmos\nSeja \\(X\\sim \\mathrm{Beta}(\\theta, 1)\\).\n\n\n\nusing Distributions, Random, Plots, StatsPlots, StatsBase, LaTeXStrings\n\n\nRandom.seed!(10)\nn = 50\nM = 10_000\n\nMVs = zeros(M)\nMMs = zeros(M)\nMV(obs) = -n/sum(log.(obs))\nMM(obs) = mean(obs)/(1-mean(obs))\ntheta0 = rand(Uniform(0,100))\ndsim = Beta(theta0, 1)\nfor i in 1:M\n  xsim = rand(dsim, n)\n  MVs[i] = MV(xsim)\n  MMs[i] = MM(xsim)\nend\n\n\nVMV(theta) = theta^2/n\nnormMV = Normal(theta0, sqrt(VMV(theta0)))\nVMM(theta) = theta*(theta+1)^2/((theta+2)*n)\nnormMM = Normal(theta0, sqrt(VMM(theta0)))\n\nhistmv = histogram(MVs, normalize=:pdf, label=\"\", color=:blue,\n                   title=\"Histograma EMVs\")\nplot!(normMV, label=\"Aprox. Norm.\", color=:green)\nvline!([theta0], label=L\"\\theta\", color=:orange)\nhistmm = histogram(MMs, normalize=:pdf, label=\"\", color=:tomato,\n                   title=\"Histograma EMMs\")\nplot!(normMM, label=\"Aprox. Norm.\", color=:blue)\nvline!([theta0], label=L\"\\theta\", color=:lightgrey)\np = plot(histmv, histmm)\n\ndisplay(p)\nqMM = quantile(normMM, 0.25)\nqMV = quantile(normMV, 0.25)\nprintln(\"theta0: $theta0\")\nprintln(\"Viés simulado MM: $(mean(MMs)-theta0)\")\nprintln(\"EQM simulado MM: $(n *mean((MMs .- theta0).^2))\")\nprintln(\"Viés simulado MV: $(mean(MVs) - theta0)\")\nprintln(\"EQM simulado MV: $(n * mean((MVs .- theta0).^2))\")\nprintln(\"P(NormalMM &gt; $(qMM)) = $(round(ccdf(normMM, qMM), digits=2))\")\nprintln(\"P(NormalMV &gt; $(qMV)) = $(round(ccdf(normMM, qMV),digits=2))\")\nprintln(\"Real MM &gt; 2.5 = $(mean(MMs .&gt; qMM))\")\nprintln(\"Real MV &gt; 2.5 = $(mean(MVs .&gt; qMV))\")\n\ntheta0: 30.104498710537307\nViés simulado MM: 0.5943591309955814\nEQM simulado MM: 968.056873503618\nViés simulado MV: 0.612391481120099\nEQM simulado MV: 968.182881983385\nP(NormalMM &gt; 27.2314280176146) = 0.75\nP(NormalMV &gt; 27.23291320813612) = 0.75\nReal MM &gt; 2.5 = 0.783\nReal MV &gt; 2.5 = 0.7843",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Propriedades dos estimadores de máxima verossimilhança e de momentos</span>"
    ]
  },
  {
    "objectID": "normal-multivariada.html",
    "href": "normal-multivariada.html",
    "title": "37  Normal Multivariada",
    "section": "",
    "text": "37.1 Normal bivariada \\(d=2\\)\nDizemos que \\(\\boldsymbol{X} = \\left(\\begin{array}{c}X_1 \\\\ \\vdots \\\\ X_n\\end{array}\\right)\\) tem distribuição multivariada \\(N_d(\\boldsymbol{\\mu}, \\Sigma)\\) se, e somente se, a sua função densidade de probabilidade é dada por:\n\\[\nf^{\\boldsymbol{X}}(\\boldsymbol{x}) = \\frac{1}{(2\\pi)^{\\frac{d}{2}} \\lvert \\Sigma \\rvert^{\\frac{1}{2}}}\n\\mathrm{Exp}\\left\\{-\\frac{1}{2} (\\boldsymbol{x} - \\boldsymbol{\\mu})^T \\Sigma^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu})\\right\\}\n\\]\nem que \\(\\mu=\\left(\\begin{array}{c} \\mu_1, \\vdots, \\mu_n\\end{array}\\right)\\) e \\(\\Sigma = \\left[\\begin{array}{cccc}\n\\sigma_{11} & \\sigma_{12} & \\dots & \\sigma_{1d} \\\\\n\\sigma_{21} & \\sigma_{22} & \\dots & \\sigma_{2d} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_{d1} & \\dots & \\sigma_{dd-1} & \\sigma_{dd}\n\\end{array}\\right]\\) é uma matriz simétrica positiva definida, ou seja, \\(\\boldsymbol{y}^T \\Sigma \\boldsymbol{y} &gt; 0, \\forall y \\in \\mathbb{R}^d \\setminus \\{\\boldsymbol{0}\\}\\)\n\\[\n\\boldsymbol{X} = \\left(\\begin{array}{c} X_1 \\\\ X_2 \\end{array}\\right) \\sim N_2 (\\boldsymbol{\\mu}, \\Sigma)\n\\] em que \\(\\boldsymbol{\\mu} =\\left(\\begin{array}{c} \\mu_1 \\\\ \\mu_2 \\end{array}\\right)\\) e \\(\\Sigma = \\left(\\begin{array}{cc} \\sigma_{11} & \\sigma_{12} \\\\ \\sigma_{12} & \\sigma_{22} \\end{array}\\right)\\) é simétrica positiva definida.\n\\[\n\\begin{aligned}\n\\lvert \\Sigma \\rvert &= \\sigma_{11} \\sigma_{22} - \\sigma_{12}^2 \\\\\n\\Sigma^{-1} =& \\frac{1}{\\sigma_{11} \\sigma_{22} - \\sigma_{12}^2}\n\\left(\\begin{array}{cc} \\sigma_{11} & -\\sigma_{12} \\\\ -\\sigma_{12} & \\sigma_{22} \\end{array}\\right) \\\\\n\\Rightarrow f^{\\boldsymbol{X}}(x_1, x_2)\\\\ = \\frac{1}{2\\pi \\sqrt{\\sigma_{11}\\sigma_{22} - \\sigma_{12}^2}}\n&\\mathrm{Exp}\\underbracket{\\left\\{\n-\\frac{1}{2} \\left(\\begin{array}{c} X_1 - \\mu_1 \\\\ X_2 - \\mu_2 \\end{array}\\right)^T\n\\left(\\begin{array}{cc} \\sigma_{11} & -\\sigma_{12} \\\\ -\\sigma_{12} & \\sigma_{22} \\end{array}\\right)\n\\left(\\begin{array}{c} X_1 - \\mu_1 \\\\ X_2 - \\mu_2 \\end{array}\\right) \\frac{1}{\\sigma_{11}\\sigma_{22} - \\sigma_{12}^2}\n\\right\\}}_{*} \\\\ \\\\\n*&= [(x_1 - \\mu_1)\\sigma_{22} - (x_2 - \\mu_2)\\sigma_{12} - (x_1-\\mu_1)\\sigma_{12}+(x_2 - \\mu_2)\\sigma_{11}]\n\\left(\\begin{array}{c} X_1 - \\mu_1 \\\\ X_2 - \\mu_2 \\end{array}\\right)^T \\\\\n&= (x_1 - \\mu_1)^2 \\sigma_{22} - 2(x_1 - \\mu_1)(x_2 - \\mu_2)\\sigma_{12} + (x_2 - \\mu_2)^2 \\sigma_{11} \\\\\n\\Rightarrow \\frac{*}{\\sigma_{11}\\sigma_{22} - \\sigma_{12}^2} &=\n\\frac{(x_1 - \\mu_1)^2}{\\frac{\\sigma_{11}\\sigma_{22} - \\sigma_{12}^2}{\\sigma_{22}}} -\n\\frac{2(x_1 - \\mu_1)(x_2 - \\mu_2)}{\\frac{\\sigma_{11}\\sigma_{22} - \\sigma{12}^2}{\\sigma_{22}}} +\n\\frac{(x_2 - \\mu_2)^2}{\\frac{\\sigma_{11}\\sigma_{22} - \\sigma_{12}^2}{\\sigma_{22}}} \\\\\n\\mathrm{Tome}\\ \\rho = \\frac{\\sigma_{12}}{\\sqrt{\\sigma_{11} \\sigma_{22}}} \\\\ \\\\\nA.\\ \\frac{\\sigma_{11}\\sigma_{22} - \\sigma_{12}^2}{\\sigma_{22}} &= \\sigma_{11} - \\frac{\\sigma_{12}^2}{\\sigma_{22}} \\\\\n&= \\sigma_{11} - \\sigma_{11}\\frac{\\sigma_{12}^2}{\\sigma_{11}\\sigma_{22}} \\\\\n&= \\sigma_{11} - \\sigma_{11} \\rho^2  \\\\\n&= \\sigma_{11}(1-\\rho^2)\\\\ \\\\\nB.\\ \\frac{\\sigma_{11} \\sigma_{22} - \\sigma_{12}^2}{\\sigma_{11}} &= \\sigma_{22}(1-\\rho^2) \\\\ \\\\\nC.\\ \\frac{\\sigma_{11}\\sigma_{22}}{\\sigma_{12}} - \\sigma_{12} &= \\sigma_{12} \\frac{\\sigma_{11}\\sigma_{22}}{\\sigma_{12}^2} - \\sigma_{12} \\\\\n&= \\sigma_{12}(\\rho^{-2} - 1) = \\sigma_{12} \\left(\\frac{1 - \\rho^2}{\\rho^2}\\right) \\\\\n\\Rightarrow \\frac{*}{\\sigma_{11}\\sigma_{22}-\\sigma_{12}^2} &= \\frac{(x_1-\\mu_1)^2}{\\sigma_{11}(1-\\rho^2)}\n-2\\rho^2\\frac{(x_1 - \\mu_1)(x_2 - \\mu_2)}{\\sigma_{12}(1-\\rho^2)} + \\frac{(x_2 - \\mu_2)^2}{\\sigma_{22}(1-\\rho^2)}\n\\end{aligned}\n\\]\nAlém disso, \\[\n\\sqrt{\\sigma_{11}\\sigma_{22} - \\sigma_{12}^2} = \\sqrt{\\sigma_{11}\\sigma_{22}-\\rho^2\\sigma_{11}\\sigma_{22}} = \\sqrt{\\sigma_{11} \\sigma_{22}} \\sqrt{1-\\rho^2}\n\\]\nAssim temos que \\[\n\\begin{aligned}\n&f^{\\boldsymbol{X}}(x_1, x_2) \\\\&= \\frac{1}{2\\pi \\sqrt{\\sigma_{11}, \\sigma_{22}} \\sqrt{1-\\rho^2}}\n\\mathrm{Exp} \\left\\{\n-\\frac{1}{2(1-\\rho^2)} \\left(\\frac{(x_1-\\mu_1)^2}{\\sigma_{11}}\n-2\\rho\\frac{(x_1 - \\mu_1)(x_2 - \\mu_2)}{\\sqrt{\\sigma_{11}\\sigma_{22}}} + \\frac{(x_2 - \\mu_2)^2}{\\sigma_{22}} \\right)\n\\right\\}\n\\end{aligned}\n\\]",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Normal Multivariada</span>"
    ]
  },
  {
    "objectID": "normal-multivariada.html#modelo-estatístico",
    "href": "normal-multivariada.html#modelo-estatístico",
    "title": "37  Normal Multivariada",
    "section": "37.2 Modelo estatístico",
    "text": "37.2 Modelo estatístico\nDizemos que \\(\\boldsymbol{X} = \\left(\\begin{array}{c} X_1 \\\\ \\vdots \\\\ X_d \\end{array}\\right)\\) é um vetor aleatório populacional normal multivariado se \\(\\boldsymbol{X}_n \\sim N_d(\\boldsymbol{\\mu}, \\Sigma)\\), em que o vetor de parâmetros é \\[\n\\theta = \\left[\\begin{array}{c}\n\\mu_1 \\\\\n\\vdots \\\\\n\\mu_d \\\\\n\\sigma_{11} \\\\\n\\vdots \\\\\n\\sigma_{d1} \\\\\n\\vdots \\\\\n\\sigma_{dd}\n\\end{array}\\right] =\n\\left[\\begin{array}{c}\n\\boldsymbol{\\mu} \\\\\n\\mathrm{vech}\\Sigma\n\\end{array}\\right] = (\\boldsymbol{\\mu}^T, (\\mathrm{vech}\\Sigma)^T)^T\n\\]\nem que \\(\\mathrm{vech}\\Sigma\\) é o vetor coluna com os elementos da triangular superior da matriz removidos: \\[\n\\mathrm{vech} \\left(\\begin{array}{cc}\\sigma_{11} & \\sigma_{12} \\\\ \\sigma_{12} & \\sigma_{22} \\end{array}\\right) =\n\\left(\\begin{array}{c} \\sigma_{11} \\\\ \\sigma_{12} \\\\ \\sigma_{22} \\end{array}\\right)\n\\]",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Normal Multivariada</span>"
    ]
  },
  {
    "objectID": "normal-multivariada.html#amostra-aleatória-de-uma-normal-multivariada",
    "href": "normal-multivariada.html#amostra-aleatória-de-uma-normal-multivariada",
    "title": "37  Normal Multivariada",
    "section": "37.3 Amostra aleatória de uma normal multivariada",
    "text": "37.3 Amostra aleatória de uma normal multivariada\nDizemos que \\(\\boldsymbol{X}_1,\\dots, \\boldsymbol{X}_n\\) é uma amostra aleatória de \\(\\boldsymbol{X} \\sim N_d(\\boldsymbol{\\mu}, \\Sigma)\\) se, e somente se, \\(\\boldsymbol{X}_1,\\dots,\\boldsymbol{X}_n\\) são independentes e \\(\\boldsymbol{X}_i \\sim N_d(\\boldsymbol{\\mu}, \\Sigma), i = 1, \\dots, n\\).\n\n\n\n\n\n\nNotação\n\n\n\n\\[\n\\boldsymbol{X}_n^* = (\\boldsymbol{X}_1, \\dots, \\boldsymbol{X}_n)\\ \\mathrm{a.a.}\\ \\text{de}\\ \\boldsymbol{X} \\sim N_d(\\boldsymbol{\\mu}, \\Sigma)\n\\]",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Normal Multivariada</span>"
    ]
  },
  {
    "objectID": "normal-multivariada.html#função-de-verossimilhança",
    "href": "normal-multivariada.html#função-de-verossimilhança",
    "title": "37  Normal Multivariada",
    "section": "37.4 Função de verossimilhança",
    "text": "37.4 Função de verossimilhança\nSeja \\(\\boldsymbol{X}^*\\) a.a. de \\(\\boldsymbol{X} \\sim N_d(\\boldsymbol{\\mu}, \\Sigma)\\), em que \\[\n\\begin{aligned}\n\\theta &= (\\boldsymbol{\\mu}^T, \\mathrm{vech}(\\Sigma)^T)^T \\\\\n\\in \\Theta &= \\left\\{(\\boldsymbol{\\mu}^T,\\mathrm{vech}(\\Sigma)^T) \\in \\mathbb{R}^d\\times \\mathbb{R}^{\\frac{d(d+1)}{2}}\n: \\Sigma\\ \\text{é positiva definida (p.d.)}\\right\\}\\subseteq \\mathbb{R}^{d + \\frac{d(d+1)}{2}}\n\\end{aligned}\n\\]\nA função de verossimilhança é: \\[\n\\begin{aligned}\n\\mathcal{L}_{\\boldsymbol{x}^*}(\\theta) &=  \\prod^n_{i=1} f_\\theta^{\\boldsymbol{X}}(\\boldsymbol{x}_i) =\n\\prod \\left\\{\n\\frac{1}{(2\\pi)^{\\frac{d}{2}}\\lvert\\Sigma\\rvert^{\\frac{1}{2}}} \\mathrm{Exp}\\left\\{\n-\\frac{1}{2}(\\boldsymbol{x}_i - \\boldsymbol{\\mu})^T \\Sigma^{-1}(\\boldsymbol{x}_i - \\boldsymbol{\\mu})\n\\right\\}\n\\right\\} \\\\\n&=  \\frac{1}{(2\\pi)^{\\frac{nd}{2}}\\lvert\\Sigma\\rvert^{\\frac{1}{2}}} \\mathrm{Exp}\\left\\{\n-\\frac{1}{2}\\underbracket{\\sum^n(\\boldsymbol{x}_i - \\boldsymbol{\\mu})^T \\Sigma^{-1}(\\boldsymbol{x}_i - \\boldsymbol{\\mu})}_{\\dagger}\n\\right\\}\n\\end{aligned}\n\\]\nDefina \\(\\bar{\\boldsymbol{x}} = \\frac{1}{n} \\sum^n \\boldsymbol{x}_i\\), então \\[\n\\begin{aligned}\n\\dagger &= \\sum^n (\\boldsymbol{x}_i - \\bar{\\boldsymbol{x}} + \\bar{\\boldsymbol{x}} - \\boldsymbol{\\mu})^T\n\\Sigma^{-1} (\\boldsymbol{x}_i - \\bar{\\boldsymbol{x}} + \\bar{\\boldsymbol{x}} - \\boldsymbol{\\mu}) \\\\\n&= \\sum^n ((\\boldsymbol{x}_i - \\bar{\\boldsymbol{x}}) - (\\boldsymbol{\\mu} - \\bar{\\boldsymbol{x}}))^T\n\\Sigma^{-1} ((\\boldsymbol{x}_i - \\bar{\\boldsymbol{x}}) - (\\boldsymbol{\\mu} - \\bar{\\boldsymbol{x}})) \\\\\n&= \\sum^n \\left\\{\n(\\boldsymbol{x} - \\bar{\\boldsymbol{x}})^T \\Sigma^{-1} (\\boldsymbol{x}_i - \\bar{\\boldsymbol{x}})\n-\\underbracket{(\\boldsymbol{\\mu} - \\bar{\\boldsymbol{x}})^T \\Sigma^{-1} (\\boldsymbol{x}_i - \\bar{\\boldsymbol{x}})}_{A} \\right .\\\\\n&\\left.-\\underbracket{(\\boldsymbol{x}_i - \\bar{\\boldsymbol{x}})^T \\Sigma^{-1} (\\boldsymbol{\\mu} - \\bar{\\boldsymbol{x}})}_{B}\n+(\\boldsymbol{\\mu} - \\bar{\\boldsymbol{x}})^T \\Sigma^{-1} (\\boldsymbol{\\mu} - \\bar{\\boldsymbol{x}})\n\\right\\}\n\\end{aligned}\n\\]\nComo \\(n\\bar{\\boldsymbol{x}} = \\sum^n \\boldsymbol{x}_i\\), temos que\n\\[\n\\begin{cases}\nA = \\sum^n (\\boldsymbol{\\mu} - \\bar{\\boldsymbol{x}})^T \\Sigma^{-1} (\\boldsymbol{x}_i - \\bar{\\boldsymbol{x}}) =\n(\\boldsymbol{\\mu} - \\bar{\\boldsymbol{x}})^T \\Sigma^{-1} (\\sum^n \\boldsymbol{x}_i - \\bar{\\boldsymbol{x}}) = 0 \\\\\nB = \\sum^n (\\boldsymbol{x}_i - \\bar{\\boldsymbol{x}})^T \\Sigma^{-1} (\\boldsymbol{\\mu} - \\bar{\\boldsymbol{x}}) =\n(\\sum^n \\boldsymbol{x}_i - \\bar{\\boldsymbol{x}})^T \\Sigma^{-1} (\\boldsymbol{\\mu} - \\bar{\\boldsymbol{x}}) = 0\n\\end{cases}\n\\]\nPortanto, \\[\n\\begin{aligned}\n\\dagger &=\n\\sum^n (\\boldsymbol{x} - \\bar{\\boldsymbol{x}})^T \\Sigma^{-1} (\\boldsymbol{x}_i - \\bar{\\boldsymbol{x}})\n+n(\\boldsymbol{\\mu} - \\bar{\\boldsymbol{x}})^T \\Sigma^{-1} (\\boldsymbol{\\mu} - \\bar{\\boldsymbol{x}}) \\\\\n&=  \\sum^n \\mathrm{tr}\\{(\\boldsymbol{x} - \\bar{\\boldsymbol{x}})^T \\Sigma^{-1} (\\boldsymbol{x}_i - \\bar{\\boldsymbol{x}})\\}\n+n(\\boldsymbol{\\mu} - \\bar{\\boldsymbol{x}})^T \\Sigma^{-1} (\\boldsymbol{\\mu} - \\bar{\\boldsymbol{x}}) \\\\\n&\\stackrel{\\text{Prop. tr}}{=}  \\sum^n \\mathrm{tr}\\{\\Sigma^{-1}(\\boldsymbol{x} - \\bar{\\boldsymbol{x}})(\\boldsymbol{x}_i - \\bar{\\boldsymbol{x}})^T\\}\n+n(\\boldsymbol{\\mu} - \\bar{\\boldsymbol{x}})^T \\Sigma^{-1} (\\boldsymbol{\\mu} - \\bar{\\boldsymbol{x}}) \\\\\n\\end{aligned}\n\\]\nNote que \\(\\sum^n \\mathrm{tr}\\{A \\boldsymbol{y}_i\\} = \\mathrm{tr}\\{A \\sum \\boldsymbol{y}_i\\}\\), usando essa propriedade, \\[\n\\begin{aligned}\n\\dagger &= n \\mathrm{tr} \\{\\Sigma^{-1}S^2(\\boldsymbol{x}_n^*)\\}\n+n(\\boldsymbol{\\mu} - \\bar{\\boldsymbol{x}})^T \\Sigma^{-1} (\\boldsymbol{\\mu} - \\bar{\\boldsymbol{x}}) \\\\\n\\end{aligned}\n\\] em que \\(S^2(\\boldsymbol{x}_n^*) = \\frac{1}{n} \\sum^n (\\boldsymbol{x}_i - \\bar{\\boldsymbol{x}})(\\boldsymbol{x}_i - \\bar{\\boldsymbol{x}})^T\\).\nDessa forma, podemos escrever a função de verossimilhança da seguinte forma:\n\\[\n\\mathcal{L}_{\\boldsymbol{x}^*}(\\theta) =\n\\frac{1}{(2\\pi)^{\\frac{nd}{2}}\\lvert\\Sigma\\rvert^{\\frac{1}{2}}} \\mathrm{Exp}\\left\\{\n-\\frac{n}{2}\\left( \\mathrm{tr}\\{\\Sigma^{-1}S^2(\\boldsymbol{x}_n^*)\\}\n+(\\boldsymbol{\\mu} - \\bar{\\boldsymbol{x}})^T \\Sigma^{-1}(\\boldsymbol{\\mu} - \\bar{\\boldsymbol{x}})\\right)\n\\right\\}\n\\]\nPelo critério da fatoração de Neyman-Fisher, temos que \\((\\bar{\\boldsymbol{X}}, S^2(\\boldsymbol{X}_n^*))\\) é uma estatística suficiente para o modelo normal multivariado. Como \\(S^2(\\boldsymbol{X}^*_n)\\) é uma matriz simétrica, podemos considerar apenas \\(\\mathrm{vech}(S^2(\\boldsymbol{X}_n^*))\\)\nPara maximizar a função de verossimilhança em relação a \\(\\boldsymbol{\\mu}\\), note que basta minimizar \\[\n(\\boldsymbol{\\mu} - \\bar{\\boldsymbol{x}})^T \\Sigma^{-1}(\\boldsymbol{\\mu} - \\bar{\\boldsymbol{x}})\n\\] note que, como \\(\\Sigma\\) é positiva definida (p.d.), \\(\\Sigma^{-1}\\) também é p.d. Portanto, pela definição, \\(\\boldsymbol{y}^T \\Sigma^{-1} \\boldsymbol{y} &gt; 0, \\forall \\boldsymbol{y} \\in \\mathbb{R}^d \\setminus \\{0\\}\\) e \\((\\boldsymbol{\\mu} - \\bar{\\boldsymbol{x}})^T \\Sigma^{-1}(\\boldsymbol{\\mu} - \\bar{\\boldsymbol{x}})\\) é mínimo quando \\(\\boldsymbol{\\mu} = \\bar{\\boldsymbol{x}}\\).\nPara encontrar \\(\\Sigma\\) que maximiza a função de verossimilhança, substituímos \\(\\boldsymbol{\\mu}\\) por \\(\\bar{\\boldsymbol{x}}\\) e tentamos encontrar seu valor. Portanto, devemos maximizar \\[\n\\frac{1}{(2\\pi)^{\\frac{nd}{2}}\\lvert\\Sigma\\rvert^{\\frac{1}{2}}} \\mathrm{Exp}\\left\\{\n-\\frac{n}{2} \\mathrm{tr}\\{\\Sigma^{-1}S^2(\\boldsymbol{x}_n^*)\\}\n\\right\\}\n\\] em relação a \\(\\Sigma\\). Aplicando \\(\\ln\\), temos \\[\n-\\frac{nd}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln\\lvert\\Sigma\\rvert - \\frac{n}{2} \\mathrm{tr}\\{\\Sigma^{-1}S^2(\\boldsymbol{x}_n^*)\\} =\nc - \\frac{n}{2}\\left( \\ln\\lvert\\Sigma\\rvert +  \\mathrm{tr}\\{\\Sigma^{-1}S^2(\\boldsymbol{x}_n^*)\\}\\right)\n\\]\nNote que, para qualquer \\(\\lambda &gt; 0\\), temos \\[\n\\lambda - \\ln\\lambda \\geq 1.\n\\]\nSejam \\(\\lambda_1, \\dots, \\lambda_d\\) autovalores de uma matriz positiva definida \\(M\\). \\[\n\\begin{aligned}\n\\lambda_i &- \\ln \\lambda_i \\geq 1 \\forall i = 1, \\dots, d \\\\\n\\Rightarrow &\\sum^d \\lambda_i - \\sum^d \\ln_i \\geq d \\\\\n\\iff& \\sum^d \\lambda_i - \\ln(\\prod^d \\lambda_i) \\geq d \\\\\n\\Rightarrow &\\mathrm{tr}\\{M\\} - \\ln\\lvert M\\rvert \\geq d\n\\end{aligned}\n\\] Tome \\(M = \\Sigma^{-1} S^(\\boldsymbol{x}_n^*)\\). Então, assumindo \\(S^2(\\boldsymbol{x}_n^*)\\) positiva definida, \\(M\\) é positiva definida e \\[\n\\begin{aligned}\n&\\mathrm{tr}\\{\\Sigma^{-1}S^2(\\boldsymbol{x}_n^*)\\} - \\ln\\lvert\\Sigma^{-1}S^2(\\boldsymbol{x}_n^*)\\rvert \\geq d \\\\\n\\iff&\\mathrm{tr}\\{\\Sigma^{-1}S^2(\\boldsymbol{x}_n^*)\\} - \\ln(\\lvert\\Sigma^{-1}\\rvert \\lvert S^2(\\boldsymbol{x}_n^*)\\rvert) \\geq d \\\\\n\\iff&\\mathrm{tr}\\{\\Sigma^{-1}S^2(\\boldsymbol{x}_n^*)\\} + \\ln\\lvert\\Sigma\\rvert - \\ln \\lvert S^2(\\boldsymbol{x}_n^*)\\rvert \\geq d \\\\\n\\iff&\\mathrm{tr}\\{\\Sigma^{-1}S^2(\\boldsymbol{x}_n^*)\\} + \\ln\\lvert\\Sigma\\rvert \\geq d + \\ln \\lvert S^2(\\boldsymbol{x}_n^*)\\rvert\n\\end{aligned}\n\\]\nNote que a igualdade (o menor valor possível) é atingida quando \\(\\Sigma = S^2(\\boldsymbol{x}_n^*)\\) pois \\[\n\\mathrm{tr}\\{S^2(\\boldsymbol{x}_n^*)^{-1}S^2(\\boldsymbol{x}_n^*)\\} = \\mathrm{tr}\\{I_d\\} = d\n\\] Logo, \\[\n\\begin{aligned}\n\\hat{\\mu}_{\\mathrm{MV}}(\\boldsymbol{X}^*_n) &= \\bar{\\boldsymbol{X}} = \\frac{1}{n} \\sum^n_{i=1} \\boldsymbol{X}_i \\\\\n\\hat{\\Sigma}_{\\mathrm{MV}}(\\boldsymbol{X}^*_n) &= S^2(\\boldsymbol{X}_n^*) = \\frac{1}{n} \\sum^n_{i=1} (\\boldsymbol{X}_i - \\bar{\\boldsymbol{X}})(\\boldsymbol{X}_i - \\bar{\\boldsymbol{X}})^T\n\\end{aligned}\n\\] são os estimadores de máxima verossimilhança para \\(\\mu\\) e \\(\\Sigma\\), respectivamente. Dessa forma, podemos estimar usando a propriedade de invariância qualquer quantidade de interesse \\(g(\\theta)\\)\n\n37.4.1 Exemplo\nSeja \\(\\boldsymbol{X}_n^*\\) a.a. de \\(\\boldsymbol{X} \\sim N_2(\\boldsymbol{\\mu}, \\Sigma)\\). Encontre o EMV para \\(g(\\theta)\\), em que\n\n\\(g(\\theta) = E_\\theta(\\boldsymbol{X})\\)\n\\(g(\\theta) = \\mathrm{Var}_\\theta(\\boldsymbol{X})\\)\n\\(g(\\theta) = E_\\theta(\\boldsymbol{X})^T \\mathrm{Var}_\\theta(\\boldsymbol{X})^{-1}E_\\theta(\\boldsymbol{X})\\)\n\\(g(\\theta) = P_\\theta(X_1 \\geq 4 X_2), \\boldsymbol{X} = (X_1, X_2)^T\\)\nConsiderando os dados observados abaixo, apresente as estimativas para \\(g(\\theta)\\) dos itens anteriores:\n\n\n37.4.1.1 Resposta\nJá sabemos que os estimadores são dados por \\[\n\\begin{aligned}\n\\hat{\\mu}_{\\mathrm{MV}}(\\boldsymbol{X}^*_n) &= \\bar{\\boldsymbol{X}} = \\frac{1}{n} \\sum^n_{i=1} \\boldsymbol{X}_i \\\\\n\\hat{\\Sigma}_{\\mathrm{MV}}(\\boldsymbol{X}^*_n) &= S^2(\\boldsymbol{X}_n^*) = \\frac{1}{n} \\sum^n_{i=1}\n(\\boldsymbol{X}_i - \\bar{\\boldsymbol{X}})(\\boldsymbol{X}_i - \\bar{\\boldsymbol{X}})^T \\\\\n&= \\frac{1}{n} \\sum \\boldsymbol{X}_i\\boldsymbol{X}^T - \\bar{\\boldsymbol{X}}\\bar{\\boldsymbol{X}}^T\n\\end{aligned}\n\\] e as estimativas por \\[\n\\begin{aligned}\n\\hat{\\mu}_{\\mathrm{MV}}(\\boldsymbol{x}^*_n) &= \\bar{\\boldsymbol{x}} = \\frac{1}{n} \\sum^n_{i=1} \\boldsymbol{x}_i\n= \\frac{1}{n}\\begin{pmatrix}\n\\sum x_{1i} \\\\\n\\sum x_{2i}\n\\end{pmatrix}\\\\\n\\hat{\\Sigma}_{\\mathrm{MV}}(\\boldsymbol{x}^*_n) &= S^2(\\boldsymbol{x}_n^*) = \\frac{1}{n} \\sum \\boldsymbol{x}_i\\boldsymbol{x}^T - \\bar{\\boldsymbol{x}}\\bar{\\boldsymbol{x}}^T\\\\\n&=\\frac{1}{n} \\sum^n_{i=1}\\begin{bmatrix}\n(x_{1i} - \\bar{x}_1)^2 & (x_{1i} - \\bar{x}_1)(x_{2i} - \\bar{x}_2) \\\\\n(x_{2i} - \\bar{x}_2)(x_{1i} - \\bar{x}_1) & (x_{2i} - \\bar{x}_2)^2\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\n\n\\(P_\\theta(X2 &gt; 4X_1) = P_\\theta(X_2 - 4X_1 &gt; 0)\\). Note que \\[\nX_2 - 4X_1 = B \\boldsymbol{X}\n\\] em que \\(B = (-4, 1)\\). Então, \\[\nX_2 - 4X_1 \\sim N(B\\boldsymbol{\\mu}, B\\Sigma B^T)\n\\] Desenvolvendo, \\[\n\\begin{aligned}\nB\\boldsymbol{\\mu} &= -4\\mu_1 + \\mu_2 = \\mu_2 - 4\\mu_1 \\\\\nB\\Sigma B^T &= (-4,1) \\begin{bmatrix}\\sigma_{11} & \\sigma_{12} \\\\ \\sigma_{21} & \\sigma_{22} \\end{bmatrix} \\begin{pmatrix}-4 \\\\ 1 \\end{pmatrix} \\\\\n&\\stackrel{\\sigma_{12} = \\sigma_{21}}{=} 16\\sigma_{11} + \\sigma_{22} - 8 \\sigma_{12}\n\\end{aligned}\n\\]\n\nAssim, \\[\n\\begin{aligned}\ng(\\theta) &= P_\\theta(X_2 - 4 X_1 &gt; 0) \\\\\n&= P_\\theta\\left(\\frac{(X_2 - 4X_1) - (\\mu_2 -4\\mu_1)}{\\sqrt{16\\sigma_{11} + \\sigma_{22} - 8 \\sigma_{12}}} &gt;\n-\\frac{\\mu_2 -4\\mu_1}{\\sqrt{16\\sigma_{11} + \\sigma_{22} - 8 \\sigma_{12}}}\\right) \\\\\n&= P\\left( Z &gt; -\\frac{\\mu_2 -4\\mu_1}{\\sqrt{16\\sigma_{11} + \\sigma_{22} - 8 \\sigma_{12}}}\\right) \\\\\n&= 1 - \\Phi\\left(-\\frac{\\mu_2 -4\\mu_1}{\\sqrt{16\\sigma_{11} + \\sigma_{22} - 8 \\sigma_{12}}}\\right)\n\\end{aligned} \\\\\n\\]",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Normal Multivariada</span>"
    ]
  },
  {
    "objectID": "normal-multivariada.html#aplicações-da-normal-multivariada",
    "href": "normal-multivariada.html#aplicações-da-normal-multivariada",
    "title": "37  Normal Multivariada",
    "section": "37.5 Aplicações da normal multivariada",
    "text": "37.5 Aplicações da normal multivariada\n\n37.5.1 Teorema do limite central para o EMV com variáveis reais\nSeja \\(\\boldsymbol{X}_n\\) a.a. de \\(X\\sim f_\\theta, \\theta \\in \\Theta \\subseteq \\mathbb{R}^P, p \\in \\mathbb{N}\\). Se as condições de regularidade estiverem satisfeitas, então, \\[\n\\sqrt{n} (\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n) - \\theta) \\stackrel{\\mathcal{D}}{\\rightarrow} N_p(\\boldsymbol{0}, I_1(\\theta)^{-1}),\n\\] ou, de outra forma, \\[\nI_n(\\theta)^{\\frac{1}{2}}(\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n) - \\theta) \\stackrel{\\mathcal{D}}{\\rightarrow} N_p(\\boldsymbol{0}, \\boldsymbol{I}).\n\\] Além disso, pelo Teorema de Slutsky, \\[\nI_n(\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n))^{\\frac{1}{2}}(\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n) - \\theta)\n\\stackrel{\\mathcal{D}}{\\rightarrow} N_p(\\boldsymbol{0}, \\boldsymbol{I})\n\\] em que \\(\\boldsymbol{I}\\) é a matriz identidade.\n\n\n\n\n\n\nNotação\n\n\n\n\\[\n\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n) \\stackrel{a}{\\approx} N_p(\\theta, I_n(\\theta)^{-1})\n\\]\n\n\nNote que, se \\(\\theta = (\\theta_1, \\theta_2)^T\\), então \\[\n\\hat{\\theta}_{\\mathrm{MV}} = \\begin{pmatrix}\\hat{\\theta}_{\\mathrm{MV}}^{(1)} \\\\ \\hat{\\theta}_{\\mathrm{MV}}^{(2)} \\end{pmatrix}\n\\stackrel{a}{\\approx} N_2\\left[\\begin{pmatrix}\\theta_1 \\\\ \\theta_2 \\end{pmatrix}; I_n(\\theta)^{-1}\\right],\n\\] logo, \\[\n\\begin{aligned}\n\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n)^{(i)} \\stackrel{a}{\\approx} N_p(\\theta_i, V_i(\\theta)), i = 1,2\n\\end{aligned}\n\\] em que \\(V_i(\\theta)\\) é o elemento \\((i,i)\\) da matriz \\(I_n(\\theta)^{-1}, i = 1, 2\\).\n\n\n\n\n\n\nObservação\n\n\n\nObserve que \\(V_i(\\theta)\\) pode depender do vetor “\\(\\theta\\)” inteiro. Veja o caso normal: \\[\n\\bar{X} \\sim N_1\\left(\\mu,\\frac{\\sigma^2}{n}\\right)\n\\]\n\n\n\n\n37.5.2 Teorema do limite central para o EMM com variáveis reais\nSeja \\(\\boldsymbol{X}_n\\) a.a. de \\(X\\sim f_\\theta, \\theta \\in \\Theta \\subseteq \\mathbb{R}^P, p \\in \\mathbb{N}\\) tal que \\[\nE_\\theta(|X|^k) &lt; \\infty, k = 1, \\dots, 2p.\n\\] Seja \\(h(\\theta) = \\begin{bmatrix}\\frac{\\partial E_\\theta(X)}{\\partial \\theta^T} \\\\ \\vdots \\\\ \\frac{\\partial E_\\theta(X^p)}{\\partial \\theta^T} \\end{bmatrix}\\) e considere que \\(\\det h(\\theta) \\neq 0\\) e cada componente de \\(h(\\theta)\\) é uma função contínua. Então, \\[\n\\sqrt{n} (\\hat{\\theta}_{\\mathrm{MM}}(\\boldsymbol{X}_n) - \\theta) \\stackrel{\\mathcal{D}}{\\rightarrow} N_p(\\boldsymbol{0}, V_\\theta)\n\\] em que \\[\n\\begin{aligned}\nV_\\theta &= h(\\theta)^{-1} \\mathrm{Var}_\\theta(\\boldsymbol{Y})[h(\\theta)^{-1}]^T \\\\\n\\boldsymbol{Y} &= \\begin{bmatrix} X \\\\ X^2 \\\\ \\vdots \\\\ X^p \\end{bmatrix}\n\\end{aligned}\n\\]\n\n\n\n\n\n\nNotação\n\n\n\n\\[\n\\hat{\\theta}_{\\mathrm{MM}}(\\boldsymbol{X}_n) \\stackrel{a}{\\approx} N_p\\left(\\theta, \\frac{h(\\theta)^{-1} \\mathrm{Var}_\\theta(\\boldsymbol{Y})[h(\\theta)^{-1}]^T}{n}\\right)\n\\]\n\n\n\n\n\n\n\n\nGeneralização do Método de Momentos\n\n\n\nO método de momentos pode ser generalizado da seguinte forma:\nSe \\(E_\\theta(|X|^k) &lt; \\infty\\) para \\(k \\in \\{k_1, k_2, \\dots, k_p\\} \\subseteq N\\) ou, se \\(X\\) for não negativa, \\(k \\in \\{k_1, \\dots, k_p\\} \\subseteq \\mathrm{Q}\\). O EMM é obtido igualando \\[\nE_\\theta(X^k) = \\frac{1}{n} \\sum X_i^k, k \\in \\{k_1, \\dots, k_n\\}\n\\] Seja \\[\nh(\\theta) = \\begin{bmatrix}\\frac{\\partial E_\\theta(X^{k_1})}{\\partial \\theta^T} \\\\ \\vdots \\\\ \\frac{\\partial E_\\theta(X^{k_p})}{\\partial \\theta^T} \\end{bmatrix}\n\\]\nConsidere que\n\n\\(E_\\theta(|X|^{2k}) &lt; \\infty, \\forall k \\in \\{k_1, \\dots, k_n\\}\\)\n\\(\\det h(\\theta) \\neq 0\\)\nCada componente de \\(h(\\theta)\\) é uma função contínua.\n\nEntão, \\[\\sqrt{n} (\\hat{\\theta}_{\\mathrm{MM}}(\\boldsymbol{X}_n) - \\theta) \\stackrel{\\mathcal{D}}{\\rightarrow} N_p(\\boldsymbol{0}, V_\\theta)\\] em que \\[\n\\begin{aligned}\nV_\\theta &= h(\\theta)^{-1} \\mathrm{Var}_\\theta(\\boldsymbol{Y})[h(\\theta)^{-1}]^T \\\\\n\\boldsymbol{Y} &= \\begin{bmatrix} X^{k_1} \\\\ X^{k_2} \\\\ \\vdots \\\\ X^{k_p} \\end{bmatrix}\n\\end{aligned}\n\\]\n\n\n\n\n37.5.3 Exemplos\n\n37.5.3.1 Exemplo Beta\nSeja \\(\\boldsymbol{X}_n\\) a.a. de \\(X \\sim f_\\theta, \\theta = (\\alpha, \\beta) \\in \\Theta = \\mathbb{R}^2_+\\) tal que \\[\nf_\\theta(x) = \\begin{cases}\n\\frac{1}{B(\\alpha, \\beta)} x^{\\alpha-1}(1-x)^{\\beta -1}, x \\in (0,1) \\\\\n0, \\mathrm{c.c.}\n\\end{cases}\n\\]\nEncontre o EMV, EMM e suas distribuições assinstóticas para \\(\\theta\\).\n\n37.5.3.1.1 Resposta\nNote que \\(B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha) \\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}\\) \\[\n\\begin{aligned}\n\\mathcal{L}_{\\boldsymbol{x}_n}(\\theta) &= \\frac{1}{B(\\alpha, \\beta)^n} (\\prod x_i)^{\\alpha-1} (\\prod(1-x_i))^{\\beta - 1} \\\\\n\\mathcal{l}_{\\boldsymbol{x}_n}(\\theta) &=\n-n \\ln \\Gamma(\\alpha) - n\\ln\\Gamma(\\beta) + n \\ln \\Gamma(\\alpha+\\beta) \\\\\n&+ (\\alpha-1)\\sum \\ln x_i + (\\beta -1)\\sum \\ln (1 - x_i), \\\\\n\\Rightarrow \\frac{\\partial \\mathcal{l}_{\\boldsymbol{x}_n}}{\\partial \\alpha}(\\theta)& = -n \\psi_1(\\alpha) + n \\psi_1(\\alpha + \\beta)\n+\\sum \\ln x_i; \\\\\n\\Rightarrow \\frac{\\partial \\mathcal{l}_{\\boldsymbol{x}_n}}{\\partial \\beta}(\\theta)& = -n \\psi_1(\\beta) + n \\psi_1(\\alpha + \\beta)\n+\\sum \\ln (1-x_i)\n\\end{aligned}\n\\] em que \\[\n\\psi_k(a) = \\frac{\\partial^k \\ln \\Gamma(a)}{\\partial a^k}\n\\] continuando, \\[\n\\begin{aligned}\n\\Rightarrow \\frac{\\partial^2 \\mathcal{l}_{\\boldsymbol{x}_n}}{\\partial \\alpha^2}(\\theta) = -n \\psi_2(\\alpha) + n \\psi_2(\\alpha + \\beta); \\\\\n\\Rightarrow \\frac{\\partial^2 \\mathcal{l}_{\\boldsymbol{x}_n}}{\\partial \\beta^2}(\\theta) = -n \\psi_2(\\beta) + n \\psi_2(\\alpha + \\beta); \\\\\n\\Rightarrow \\frac{\\partial^2 \\mathcal{l}_{\\boldsymbol{x}_n}}{\\partial \\beta \\partial \\alpha}(\\theta) = n \\psi_2(\\alpha + \\beta). \\\\ \\\\\n\\Rightarrow \\frac{\\partial^2 \\mathcal{l}_{\\boldsymbol{x}_n}}{\\partial \\theta \\partial \\theta^T}(\\theta) = -n\n\\begin{bmatrix}\n\\psi_2(\\alpha) - \\psi_2(\\alpha + \\beta) & -\\psi_2(\\alpha+\\beta) \\\\\n-\\psi_2(\\alpha+beta) & \\psi_2(\\beta) - \\psi_2(\\alpha+\\beta)\n\\end{bmatrix} \\\\\n\\Rightarrow I_n(\\theta) = n\n\\begin{bmatrix}\n\\psi_2(\\alpha) - \\psi_2(\\alpha + \\beta) & -\\psi_2(\\alpha+\\beta) \\\\\n-\\psi_2(\\alpha+beta) & \\psi_2(\\beta) - \\psi_2(\\alpha+\\beta)\n\\end{bmatrix}.\n\\end{aligned}\n\\]\nA estimativa de MV pode ser obtida numericamente via algoritmo de Newton-Raphson. Neste caso, o método é equivalente ao de escore de Fisher: \\[\n\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} + I_n(\\hat{\\theta}^{(j)})^{-1} U_n(\\boldsymbol{x}_n, \\hat{\\theta}^{(j)})\n\\] em que \\[\n\\begin{aligned}\nU_n(\\boldsymbol{x}_n, \\theta) = \\begin{bmatrix}\n-n \\psi_1(\\alpha) + n \\psi_1(\\alpha + \\beta) +\\sum \\ln x_i \\\\\n-n \\psi_1(\\beta) + n \\psi_1(\\alpha + \\beta) +\\sum \\ln(1-x_i)\n\\end{bmatrix}\n\\end{aligned}\n\\]\nAlém disso, \\[\n\\sqrt{n}(\\hat{\\theta}_{\\mathrm{MV}} - \\theta) \\stackrel{\\mathcal{D}}{\\rightarrow} N_2(\\boldsymbol{0}, I_1(\\theta)^{-1})\n\\] em que \\[\n\\begin{aligned}\nI_1(\\theta)^{-1} &= \\frac{1}{(\\psi_2(\\alpha) - \\psi_2(\\alpha+\\beta))(\\psi_2(\\beta)-\\psi_2(\\alpha+\\beta))-\\psi_2(\\alpha+\\beta)^2} \\\\\n&\\cdot\n\\begin{bmatrix}\n\\psi_2(\\alpha) - \\psi_2(\\alpha + \\beta) & \\psi_2(\\alpha+\\beta) \\\\\n\\psi_2(\\alpha+beta) & \\psi_2(\\beta) - \\psi_2(\\alpha+\\beta)\n\\end{bmatrix}.\n\\end{aligned}\n\\]",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Normal Multivariada</span>"
    ]
  },
  {
    "objectID": "intervalos-confianca.html",
    "href": "intervalos-confianca.html",
    "title": "38  Intervalos de Confiança - Aprofundamento",
    "section": "",
    "text": "38.1 Interpretação em termos de repetições:\nSeja \\(\\boldsymbol{X}_n = (X_1,\\dots,X_n)\\) amostra aleatória e \\(X \\sim f_\\theta, \\theta \\in \\Theta\\).\nDizemos que \\([I_1(\\boldsymbol{X}_n), I_2(\\boldsymbol{X}_n)]\\) é um intervalo de confiança exato para \\(g(\\theta)\\) com coeficiente de confiança \\(\\gamma \\in (0,1)\\) se, e somente se: \\[\nP_\\theta\\left(I_1(\\boldsymbol{X}_n) \\leq g(\\theta) \\leq I_2(\\boldsymbol{X}_n)\\right) = \\gamma, \\forall \\theta \\in \\Theta.\n\\] em que \\(I_1(\\boldsymbol{X}_n), I_2(\\boldsymbol{X}_n)\\) são estatísticas.\nObserve que \\(\\mathrm{IC}(g(\\theta),\\gamma)\\) é um intervalo aleatório que não depende de “\\(\\theta\\)”.\nNa prática, observamos a amostra \\(\\boldsymbol{x}_n = (x_1, \\dots, x_n)\\) e calculamos o IC observado\nPortanto, \\[\nP_\\theta\\left(I_1(\\boldsymbol{X}_n) \\leq g(\\theta) \\leq I_2(\\boldsymbol{X}_n)\\right) =\n\\begin{cases}\n1, g(\\theta) \\in \\mathrm{IC}_{\\mathrm{Obs}}(g(\\theta), \\gamma) \\\\\n0, \\mathrm{c.c.}\n\\end{cases}\n\\]\nSe repetirmos o experimento, mantendo as mesmas condições, então esperamos que em \\(\\gamma \\cdot 100\\%\\) dos experimentos os ICs contenham \\(g(\\theta)\\). Em outras palavras, \\[\n\\begin{aligned}\n\\#\\frac{(g(\\theta) \\in \\mathrm{IC}_{\\mathrm{Obs}})}{N} \\approx \\gamma \\\\\n\\left[\n\\frac{1}{N} \\sum \\mathbb{1}_{\\{\\mathrm{IC}_{\\mathrm{Obs}}^{(i)}\\}}(g(\\theta)) \\stackrel{N \\uparrow \\infty}{\\rightarrow} \\gamma\n\\right]\n\\end{aligned}\n\\]\nDizemos que \\[\n\\frac{1}{N} \\sum \\mathbb{1}_{\\{\\mathrm{IC}_{\\mathrm{Obs}}^{(i)}\\}}(g(\\theta))\n\\] é a cobertura de \\(\\mathrm{IC}(g(\\theta),\\gamma)\\)",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Intervalos de Confiança - Aprofundamento</span>"
    ]
  },
  {
    "objectID": "intervalos-confianca.html#interpretação-em-termos-de-repetições",
    "href": "intervalos-confianca.html#interpretação-em-termos-de-repetições",
    "title": "38  Intervalos de Confiança - Aprofundamento",
    "section": "",
    "text": "O que dizer sobre \\(\\mathrm{IC}_{\\mathrm{Obs}}\\) em relação a \\(g(\\theta)\\)?\n\n\n\nTemos uma confiança de \\(\\gamma\\cdot 100\\%\\) de que \\(g(\\theta) \\in \\mathrm{IC}_{\\mathrm{Obs}}\\).",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Intervalos de Confiança - Aprofundamento</span>"
    ]
  },
  {
    "objectID": "intervalos-confianca.html#exemplos",
    "href": "intervalos-confianca.html#exemplos",
    "title": "38  Intervalos de Confiança - Aprofundamento",
    "section": "38.2 Exemplos",
    "text": "38.2 Exemplos\n\n38.2.1 Exemplo Normal\nSeja \\(\\boldsymbol{X}_n\\) a.a. de \\(X \\sim N(\\mu,\\sigma^2), \\theta = (\\mu, \\sigma^2) \\in \\Theta = \\mathbb{R}\\times\\mathbb{R}^+\\). Encontre um IC com coeficiente de confiança \\(\\gamma = 95\\%\\)…\n…para \\(g(\\theta = \\mu)\\)\nSabemos que \\[\n\\bar{X} = \\frac{1}{n} \\sum X_i \\sim N(\\mu, \\sigma^2/n).\n\\] Além disso, \\[\n\\sum \\frac{(X_i - \\bar{X})^2}{\\sigma^2} \\sim \\chi^2_{n-1}\n\\]\nPor definição de t-student, com as V.As das distribuições independentes, \\[\n\\begin{aligned}\n\\frac{N(0,1)}{\\sqrt{\\frac{\\chi^2_k}{k}}} &\\sim t_k  \\\\\n\\Rightarrow\\frac{\\sqrt{n} \\frac{(\\bar{X} - \\mu)}{\\sigma}}{\\sqrt{\\frac{\\sum \\frac{(X_i - \\bar{X})^2}{\\sigma^2}}{n-1}}} &\\sim t_{(n-1)} \\\\\n\\Rightarrow \\frac{\\sqrt{n} (\\bar{X} - \\mu)}{\\sqrt{S^2_{n-1}(\\boldsymbol{X}_n)}} &\\sim t_{(n-1)}\n\\end{aligned}\n\\] logo, podemos sempre encontrar \\(c_{1, \\gamma}, c_{2, \\gamma}\\) tais que \\[\nP_\\theta\\left(c_{2, \\gamma} \\leq \\frac{\\sqrt{n} (\\bar{X} - \\mu)}{\\sqrt{S^2_{n-1}(\\boldsymbol{X}_n)}} \\leq c_{1, \\gamma}\\right) = \\gamma\n\\]\nNote que \\[\nc_{2, \\gamma} \\leq \\frac{\\sqrt{n} (\\bar{X} - \\mu)}{\\sqrt{S^2_{n-1}(\\boldsymbol{X}_n)}} \\leq c_{1, \\gamma} \\iff\n\\bar{X} - c_{2,\\gamma} \\sqrt{\\frac{S^2_{n-1}(\\boldsymbol{X}_n)}{n}} \\leq \\mu \\leq \\bar{X} - c_{1,\\gamma} \\sqrt{\\frac{S^2_{n-1}(\\boldsymbol{X}_n)}{n}}\n\\]\nPortanto, \\[\nP_\\theta\\left(\\bar{X} - c_{2,\\gamma} \\sqrt{\\frac{S^2_{n-1}(\\boldsymbol{X}_n)}{n}} \\leq g(\\theta) \\leq \\bar{X} - c_{1,\\gamma} \\sqrt{\\frac{S^2_{n-1}(\\boldsymbol{X}_n)}{n}}\\right) = \\gamma\n\\]\nPela definição de Intervalo de Confiança, \\[\n\\mathrm{IC}(\\mu,\\gamma) = \\left[\n\\bar{X} - c_{2,\\gamma} \\sqrt{\\frac{S^2_{n-1}(\\boldsymbol{X}_n)}{n}}, \\bar{X} - c_{1,\\gamma} \\sqrt{\\frac{S^2_{n-1}(\\boldsymbol{X}_n)}{n}}\n\\right]\n\\] em que \\[\nS^2_{n-1}(\\boldsymbol{X}_n) = \\frac{1}{n-1} \\sum  (X_i - \\bar{X})^2\n\\] e \\(c_{1,\\gamma}, c_{2,\\gamma}\\) são os quantis obtidos da distribuição t-student com \\(n-1\\) graus de liberdade que satisfaçam \\[\nP_\\theta\\left(\\bar{X} - c_{2,\\gamma} \\sqrt{\\frac{S^2_{n-1}(\\boldsymbol{X}_n)}{n}} \\leq g(\\theta) \\leq \\bar{X} -\nc_{1,\\gamma} \\sqrt{\\frac{S^2_{n-1}(\\boldsymbol{X}_n)}{n}}\\right) = \\gamma\n\\] no caso simétrico (minimiza o IC para distribuições como a Normal), \\(c_{2,\\gamma} = - c_{1,\\gamma}\\). Note que não é possível construir ICs simétricos dessa forma para distribuições estritamente positivas, como a qui-quadrado.",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Intervalos de Confiança - Aprofundamento</span>"
    ]
  },
  {
    "objectID": "intervalos-confianca.html#sec-quantpivot",
    "href": "intervalos-confianca.html#sec-quantpivot",
    "title": "38  Intervalos de Confiança - Aprofundamento",
    "section": "38.3 Quantidades Pivotais",
    "text": "38.3 Quantidades Pivotais\nDizemos que \\(Q(g(\\theta), \\boldsymbol{X}_n)\\) é uma quantidade pivotal para \\(g(\\theta)\\) se, e somente se,\n\n\\(Q(g(\\theta), \\boldsymbol{X}_n)\\) depende de \\(g(\\theta)\\)\nA distribuição de \\(Q(g(\\theta), \\boldsymbol{X}_n)\\) não depende de “\\(\\theta\\)”\nExistem \\(a_1, a_2\\), que não dependem de \\(g(\\theta)\\), tais que \\(c_1 \\leq Q(g(\\theta),\\boldsymbol{X}_n) \\leq c_2 \\iff a_1 \\leq g(\\theta) \\leq a_2\\)\n\n\n38.3.1 Exemplos\n\\(X \\sim N(\\mu, \\sigma^2)\\)\n\nSe \\(\\sigma^2\\) é conhecido e \\(\\theta = \\mu. g(\\theta) = \\mu\\), então uma quantidade pivotal é dada por \\[\nQ(\\mu, \\boldsymbol{X}_n) = \\sqrt{n}\\frac{\\bar{X} - \\mu}{\\sqrt{\\sigma^2}} \\sim N(0,1)\n\\]\n\n\\[\n\\mathrm{IC}(\\mu,\\gamma) = \\bar{X} \\mp c_{\\gamma} \\sqrt{\\frac{\\sigma^2}{n}}\n\\]\n\nSe \\(\\sigma^2\\) é desconhecido e \\(\\theta = (\\mu, \\sigma^2), g(\\theta) = \\mu\\), então uma quantidade pivotal é dada por \\[\nQ(\\mu, \\boldsymbol{X}_n) = \\sqrt{n}\\frac{\\bar{X}-\\mu}{\\sqrt{S^2_{n-1}(\\boldsymbol{X}_n)}} \\sim t_{(n-1)}\n\\]\n\n\\[\n\\mathrm{IC}(\\mu,\\gamma) = \\bar{X} \\mp c_{\\gamma} \\sqrt{\\frac{S^2_{n-1}(\\boldsymbol{X}_n}{n}}\n\\] essa é também uma quantidade pivotal para 1., mas o contrário não vale.\n\nSe \\(\\mu\\) é conhecido e \\(\\theta = \\sigma^2, g(\\theta) = \\sigma^2\\), então\n\n\\[\nQ(\\sigma^2, \\boldsymbol{X}_n) = \\sum \\frac{(X_i - \\bar{X})^2}{\\sigma^2} \\sim \\chi^2_{n}\n\\]\n\\[\n\\mathrm{IC}(\\sigma^2,\\gamma) = \\left[\\frac{\\sum (X_i - \\mu)^2}{c_{2,\\gamma}}, \\frac{\\sum (X_i - \\mu)^2}{c_{1,\\gamma}}\\right]\n\\]\n\nSe \\(\\mu\\) é desconhecido e \\(\\theta = (\\mu, \\sigma^2), g(\\theta) = \\sigma^2\\), então \\[\nQ(\\sigma^2, \\boldsymbol{X}_n) = \\sum \\frac{(X_i - \\bar{X})}{\\sigma^2} \\sim \\chi^2_{(n-1)}\n\\]\n\n\\[\n\\mathrm{IC}(\\sigma^2,\\gamma) = \\left[\\frac{\\sum (X_i - \\bar{X})^2}{c_{2,\\gamma}}, \\frac{\\sum (X_i - \\bar{X})^2}{c_{1,\\gamma}}\\right]\n\\]\n\n38.3.1.1 Exponencial\nSeja \\(X \\sim \\mathrm{Exp}(\\theta), \\theta &gt; 0\\). Encontre uma quantidade pivotal para \\(\\theta\\).\nNote que \\(\\sum X_i \\sim \\mathrm{Gama}(n,\\theta)\\) com F.G.M. dada por \\[\nM_{\\sum X_i}(t) = \\left(\\frac{\\theta}{\\theta-t}\\right)^n\n\\] note ainda que \\[\n\\begin{aligned}\nM_{\\sum X_i}(t) &= E(\\mathrm{e}^{t \\sum X_i}) =  \\left(\\frac{\\theta}{\\theta-t}\\right)^n \\\\\n\\Rightarrow M_{\\theta \\sum X_i}(t) &= E(\\mathrm{e}^{t\\theta \\sum X_i}) \\\\\n&=M_{\\sum X_i}(t \\theta) = \\left(\\frac{\\theta}{\\theta-\\theta t}\\right)^n = \\left(\\frac{1}{1-t}\\right)^n \\\\\n\\Rightarrow M_{2\\theta\\sum X_i}(t) &= \\left(\\frac{1}{1-2t}\\right)^n \\\\\n\\Rightarrow &2\\theta \\sum X_i \\sim \\chi^2_{(2n)}\n\\end{aligned}\n\\]\nPortanto, \\(Q(\\theta, \\boldsymbol{X}_n) = 2\\theta \\sum X_i\\) é uma quantidade de interesse para \\(\\theta\\)\n\\[\n\\begin{aligned}\nc_{1,\\gamma} &\\leq Q(\\theta, \\boldsymbol{X}_n) \\leq c_{2,\\gamma} \\\\\n\\iff c_{1,\\gamma} &\\leq 2\\theta \\sum X_i \\leq c_{2\\gamma} \\iff \\frac{c_{1,\\gamma}}{2\\sum X_i} \\leq \\theta \\leq \\frac{c_{2,\\gamma}}{2\\sum X_i}\n\\end{aligned}\n\\]\n\n\n38.3.1.2 Uniforme\nSeja \\(X \\sim \\mathrm{Unif}(0, \\theta), \\theta &gt; 0\\). Encontre uma quantidade pivotal para \\(\\theta\\).\nNote que \\(X_{(n)} = \\max \\boldsymbol{X}_n\\) é uma estatística suficiente cuja f.d.p. é dada por \\[\nf_(\\theta)^{\\boldsymbol{X}_{(n)}}(x) = \\frac{n x^{n-1}}{\\theta^n} \\mathrm{1}_{(0,\\theta]}(x)\n\\]\nSeja \\(Y = \\frac{X_{(n)}}{\\theta}\\), então \\[\nf_(\\theta)^{Y}(x) = f_(\\theta)^{\\boldsymbol{X}_{(n)}}(y \\theta) \\cdot |J|\n\\] em que \\(J = \\theta\\) (determinante jacobiano)\n\\[\nf_(\\theta)^{Y}(y) = \\frac{n (y\\theta)^{n-1}}{\\theta^n} \\theta \\mathrm{1}_{(0,\\theta]}(y \\theta) = n y^{n-1} \\mathrm{1}_{(0,1]}(y)\n\\] não depende de “\\(\\theta\\)”! Logo, \\(Q(\\theta, \\boldsymbol{X}_n) = \\frac{\\boldsymbol{X}_{(n)}}{\\theta}\\) é uma quantidade pivotal para \\(\\theta\\).\n\\[\n\\begin{aligned}\n\\frac{\\boldsymbol{X}_{(n)}}{c_{2,\\gamma}} \\leq \\theta \\leq \\frac{\\boldsymbol{X}_{(n)}}{c_{1,\\gamma}}\n\\Rightarrow \\mathrm{IC}(\\theta,\\gamma) = \\left[\n\\frac{\\boldsymbol{X}_{(n)}}{c_{2,\\gamma}}, \\frac{\\boldsymbol{X}_{(n)}}{c_{1,\\gamma}}\n\\right]\n\\end{aligned}\n\\] em que os quantis são obtidos da distribuição de \\(Y\\), nesse caso, \\(Y \\sim \\mathrm{Beta}(n,1)\\):\n\\[\n\\begin{aligned}\n\\int^{c_{1,\\gamma}}_0 ny^{n-1} dy = \\frac{1-\\gamma}{2}&\\ \\ \\ \\int_{c_{2,\\gamma}}^{1} ny^{n-1}dy = \\frac{1-\\gamma}{2} \\\\\n\\Rightarrow \\begin{cases}\ny^n \\rvert_0^{c_{1,\\gamma}} = \\frac{1-\\gamma}{2} \\\\\ny^n \\rvert^1_{c_{2,\\gamma}} = \\frac{1-\\gamma}{2} \\\\\n\\end{cases}\n\\Rightarrow c_{1,\\gamma} &= \\left(\\frac{1-\\gamma}{2}\\right)^{1/2}\\ \\ \\ \\ c_{2,\\gamma} = \\left(\\frac{1+\\gamma}{2}\\right)^{1/2}\n\\end{aligned}\n\\]\n\n\n\n\n\n\nQuantis que minimizam a amplitude do IC\n\n\n\nSeja \\(Q(g(\\theta), \\boldsymbol{X}_n)\\) uma quantidade pivotal com função densidade de probabilidade \\(f\\).\n\\(c_{1,\\gamma}, c_{2,\\gamma}\\) devem ser obtidos \\[\n\\int_{c_{1,\\gamma}}^{c_{2,\\gamma}} f(x) dx = \\gamma\n\\tag{38.1}\\]\nNote que em geral infinitas combinações desses quantis satisfazem (38.1). Podemos usar o par que satisfaz \\[\n\\int^{c_{1,\\gamma}}_{-\\infty} f(y) dy = \\frac{1-\\gamma}{2}\\ \\ \\mathrm{e} \\ \\ \\int_{c_{2,\\gamma}}^{\\infty} f(y)dy = \\frac{1-\\gamma}{2}\n\\]\nEsse método produz um intervalo de confiança simétrico, não necessariamente o de menor amplitude, mas é mais fácil de encontrar. O intervalo de confiança com menor amplitude com quantis que satisfaçam (38.1) é obtido minimizando \\(|c_{2,\\gamma} - c_{1,\\gamma}|\\) sujeito a (38.1).\nSe \\(f\\) for unimodal e bicaudal, pode-se demonstrar que \\(c_{1,\\gamma}, c_2{\\gamma}\\) que produzem amplitude mínima e satisfazem (38.1) são tais que \\[\nf(c_{1,\\gamma}) = f(c_{2,\\gamma})\n\\] ou seja, tem mesma densidade.\n\n\n\nusing Distributions, Random, StatsBase, LaTeXStrings\n\nRandom.seed!(8)\n\ntheta0 = 4\nn = 10\nMC = 10000\n\nI1 = []\nI2 = []\nfor _ in 1:MC\n  d = Exponential(1/theta0) # Parâmetro média =&gt; 1/theta parâmetro taxa\n  x = rand(d,n)\n\n  # Construindo um IC com 95% de confiança\n  gamma = 0.95\n  quiquadrado = Chisq(2*n)\n  c1 = quantile(quiquadrado, (1-gamma)/2)\n  c2 = quantile(quiquadrado, gamma + (1-gamma)/2)\n  push!(I1, round(c1/(2*sum(x)), digits=4))\n  push!(I2, round(c2/(2*sum(x)), digits=4))\nend\n\nacertos = [I1 .&lt;= theta0 .&lt;= I2]\ndisplay(L\"\\mathrm{IC}_{\\mathrm{Obs (1)}}(\\theta,0.95)=[%$(I1[1]), %$(I2[1])]\")\ndisplay(L\"\\text{ICs que contém}\\ \\theta: %$((sum(acertos[1]))/MC * 100)\\%\")\n\n\\(\\mathrm{IC}_{\\mathrm{Obs (1)}}(\\theta,0.95)=[2.7345, 9.7424]\\)\n\n\n\\(\\text{ICs que contém}\\ \\theta: 94.97\\%\\)\n\n\n\n\n\n38.3.2 Quantidades pivotais aproximadas ou assintóticas\nDizemos que \\(Q(g(\\theta), \\boldsymbol{X}_n)\\) é uma quantidade pivotal aproximada ou assintótica se, e somente se\n\n\\(Q(g(\\theta), \\boldsymbol{X}_n)\\) depende de \\(g(\\theta)\\);\nA distribuição assintótica de \\(Q(g(\\theta), \\boldsymbol{X}_n)\\) não depende de \\(g(\\theta)\\);\nExistem \\(a_1, a_2\\), que não dependem de \\(g(\\theta)\\), tais que \\(c_1 \\leq Q(g(\\theta),\\boldsymbol{X}_n) \\leq c_2 \\iff a_1 \\leq g(\\theta) \\leq a_2\\). Esses valores dependem apenas de \\(\\boldsymbol{X}_n\\).\n\nPodemos usar o TLC para estimadores:\nSe \\(T(\\boldsymbol{X}_n)\\) for um estimador para \\(\\theta\\) assintoticamente normal, então\n\\[\n\\sqrt{n} (T(\\boldsymbol{X}_n) - \\theta)  \\stackrel{\\mathcal{D}}{\\rightarrow} N_p(0, V_\\theta), \\forall \\theta \\in \\Theta.\n\\] em que \\(V_\\theta\\) é uma matriz positiva definida. Para \\(p=1\\), \\[\n\\sqrt{n} (T(\\boldsymbol{X}_n) - \\theta)  \\stackrel{\\mathcal{D}}{\\rightarrow} N_1(0, V_\\theta), \\forall \\theta \\in \\Theta.\n\\] em que \\(V_\\theta &gt; 0\\).\nSe \\(g: \\Theta \\rightarrow \\mathbb{R}\\) for uma função tal que \\(g'(\\theta) \\neq 0\\) \\(g'(\\theta)\\) é contínua, então, \\[\n\\sqrt{n} (g(T(\\boldsymbol{X}_n)) - g(\\theta))  \\stackrel{\\mathcal{D}}{\\rightarrow} N_1(0, g'(\\theta)^2V_\\theta), \\forall \\theta \\in \\Theta.\n\\]\nAlém disso,\n\n\\[\n\\frac{\\sqrt{n} (g(T(\\boldsymbol{X}_n)) - g(\\theta))}{\\sqrt{g'(\\theta)^{-2}V_\\theta}}  \\stackrel{\\mathcal{D}}{\\rightarrow} N_1(0, 1), \\forall \\theta \\in \\Theta.\n\\]\nPelo teorema de Slutsky \\[\n\\frac{\\sqrt{n} (g(T(\\boldsymbol{X}_n)) - g(\\theta))}{\\sqrt{g'(T(\\boldsymbol{X}_n))^{-2}V_{T(\\boldsymbol{X}_n)}}}  \\stackrel{\\mathcal{D}}{\\rightarrow} N_1(0, 1), \\forall \\theta \\in \\Theta.\n\\]\n\nOu seja, \\[\nQ(g(\\theta), \\boldsymbol{X}_n) = \\frac{\\sqrt{n} (g(T(\\boldsymbol{X}_n)) - g(\\theta))}{\\sqrt{g'(T(\\boldsymbol{X}_n))^{-2}V_{T(\\boldsymbol{X}_n)}}}\n\\] é uma quantidade pivotal aproximada para \\(g(\\theta)\\). Note que, com \\(T(\\boldsymbol{X}_n) = \\hat(\\theta)\\), \\[\n\\begin{aligned}\n&c_1 \\leq Q(g(\\theta), \\boldsymbol{X}_n) \\leq c_2 \\\\\n\\iff& c_1 \\leq \\frac{\\sqrt{n} (g(T(\\boldsymbol{X}_n)) - g(\\theta))}{\\sqrt{g'(T(\\boldsymbol{X}_n))^{-2}V_{T(\\boldsymbol{X}_n)}}} \\leq c_2 \\\\\n\\iff& c_1 \\sqrt{\\frac{g'(\\hat\\theta)^2}{n} V_{\\hat\\theta}} \\leq g(\\hat\\theta) - g(\\theta) \\leq c_2\\sqrt{\\frac{g'(\\hat\\theta)^2}{n} V_{\\hat\\theta}} \\\\\n\\iff& g(\\hat\\theta) - c_2 \\sqrt{\\frac{g'(\\hat\\theta)^2}{n} V_{\\hat\\theta}} \\leq g(\\theta) \\leq g(\\hat\\theta) - c_1\\sqrt{\\frac{g'(\\hat\\theta)^2}{n} V_{\\hat\\theta}} \\\\\n\\end{aligned}\n\\] Tomando \\(c_1 = -c_2\\), pois a distribuição assintótica é normal, temos \\[\n\\mathrm{IC}^a(g(\\theta),\\gamma) = \\left[\ng(\\hat\\theta) - c_2 \\sqrt{\\frac{g'(\\hat\\theta)^2}{n} V_{\\hat\\theta}}, g(\\hat\\theta) + c_2 \\sqrt{\\frac{g'(\\hat\\theta)^2}{n} V_{\\hat\\theta}}\n\\right]\n\\] em que \\(c_2\\) é obtido dos quantis da normal padrão tal que \\[\nP(-c_2 \\leq N(0,1) \\leq c_2) = \\gamma\n\\]\n\n\n\n\n\n\nErro padrão\n\n\n\nA quantidade \\[\n\\sqrt{\\frac{g'(\\hat\\theta)^2}{n} V_{\\hat\\theta}}\n\\] é o erro-padrão do estimador (seu desvio padrão).\n\n\nVamos conferir por simulações de monte carlo e o método de Newton-Raphson\na-) para θ\nb-) para \\(g(\\theta) = P(X &lt; 950) =&gt; g'(\\theta) = f_\\theta(950)\\)\nc-) para g(θ) = ln f_θ(x) g’(θ) = - ψ_1(θ) + ln(θ) - 1/θ\nd-) para g(θ) = f_(x)\n\nusing SpecialFunctions, Distributions, Random, LaTeXStrings\n# Modelo Gama(θ, 1)\n# IC assintótico\n# Calcule o IC\n#a-) para θ\n#b-) para g(θ) = P(X &lt; 950) =&gt; g'(θ) = f_θ(950)\n#c-) para g_x(θ) = ln f_θ(x) g'(θ) = - ψ_1(θ) + ln(θ) - 1/θ. x = θ\n#d-) para g_x(θ) = f_\\theta(x). x = θ\n\n\n# Sabemos que θ.MV ~a~ N(θ, I_n(θ)^-1)\n# =&gt; IC^a(θ,γ) = θ.MV ∓ c_2 * √(I_1(θ))\n\nRandom.seed!(13)\n\nfunction newton_raphson(x)\n\n  theta::Vector{Float64} = []\n  append!(theta, mean(x)) # Chute inicial = média\n  erromax = 10^(-5)\n  erro = Inf\n  i = 1\n  # iteracoesMax = 6 # Podemos também definir apenas um erro máximo\n  while erro &gt; erromax # && i &lt; iteracoesMax\n    append!(theta, theta[i] - (sum(log.(x)) - n * digamma(theta[i]))/\n            (-n*trigamma(theta[i])))\n    erro = abs(theta[i+1] - theta[i])\n    # println(\"Erro na iteração $i: $erro\")\n    i += 1\n  end\n  # println(\"Theta final: $(theta[length(theta)])\")\n  # println(\"Total de iterações: $i\")\n  return theta[end]\nend\n\n# Resolução do item a\nfunction monte_carlo_a(M, γ, n, theta0)\n  d = Gamma(theta0, 1)\n  function mc()\n    x = rand(d, n)\n    EMV = newton_raphson(x)\n    c2 = quantile(Normal(), γ + (1-γ)/2)\n    I1 = EMV - c2 * sqrt(1/(n*trigamma(EMV)))\n    I2 = EMV + c2 * sqrt(1/(n*trigamma(EMV)))\n    return I1, I2\n  end\n\n  inferiores = []; superiores = [] \n  for _ in 1:M\n    intervalo = mc()\n    push!(inferiores, intervalo[1])\n    push!(superiores, intervalo[2])\n  end\n\n  acertos = inferiores .&lt;= theta0 .&lt;= superiores\n  return mean(acertos)\n\nend\n\nRodando o código para o item a, temos\n\n\n\\(\\text{Confiança para } 10000 \\text{ simulações com alvo } 0.95, n = 100, \\theta_0 = 100:\\ 95.07\\%\\)\n\n\n\\(\\text{Confiança para } 10000 \\text{ simulações com alvo } 0.99, n = 100, \\theta_0 = 100:\\ 99.03\\%\\)\n\n\n\\(\\text{Confiança para } 10000 \\text{ simulações com alvo } 0.99, n = 5, \\theta_0 = 75:\\ 98.91\\%\\)\n\n\nPara o item b,\n\n# Resolução do item b\nfunction monte_carlo_b(M, γ, n, theta0)\n  d = Gamma(theta0, 1)\n  g(a) = cdf(Gamma(a, 1), 950)\n  g1(a) = pdf(Gamma(a,1), 950)\n  function mc()\n    x = rand(d, n)\n    EMV = newton_raphson(x)\n    c2 = quantile(Normal(), γ + (1-γ)/2)\n    I1 = g(EMV) - c2 * sqrt(g1(EMV)^2/(n*trigamma(EMV)))\n    I2 = g(EMV) + c2 * sqrt(g1(EMV)^2/(n*trigamma(EMV)))\n    return I1, I2\n  end\n\n  inferiores = []; superiores = [] \n  for _ in 1:M\n    intervalo = mc()\n    push!(inferiores, intervalo[1])\n    push!(superiores, intervalo[2])\n  end\n\n  acertos = inferiores .&lt;= g(theta0) .&lt;= superiores\n  return mean(acertos) \n\nend\n\nRodando o código para o item b, temos\n\n\n\\(\\text{Confiança para } 10000 \\text{ simulações com alvo } 0.95, n = 100, \\theta_0 = 1000:\\ 94.88\\%\\)\n\n\n\\(\\text{Confiança para } 10000 \\text{ simulações com alvo } 0.99, n = 100, \\theta_0 = 951:\\ 98.61\\%\\)\n\n\n\\(\\text{Confiança para } 10000 \\text{ simulações com alvo } 0.99, n = 5, \\theta_0 = 951:\\ 94.67\\%\\)\n\n\nPara o item c, precisaremos calcurar as derivadas: \\[\n\\begin{aligned}\ng(\\theta) &= \\ln f_\\theta(\\theta) = - \\ln \\Gamma(\\theta) + (\\theta-1) \\ln (\\theta) - \\theta \\\\\n\\Rightarrow g'(\\theta) &= -\\psi_1(\\theta) + \\ln(\\theta) + \\frac{\\theta -1}{\\theta} - 1 \\\\\n&= -\\psi_1(\\theta) + \\ln(\\theta) - \\frac{-1}{\\theta} = g1(\\theta)\n\\end{aligned}\n\\]\n\n# Resolução do item c\nfunction monte_carlo_c(M, γ, n, theta0)\n  d = Gamma(theta0, 1)\n  g(a) =  log(pdf(Gamma(a, 1), a))\n  g1(a) = -digamma(a) + log(a) - 1/a\n  function mc()\n    x = rand(d, n)\n    EMV = newton_raphson(x)\n    c2 = quantile(Normal(), γ + (1-γ)/2)\n    I1 = g(EMV) - c2 * sqrt(g1(EMV)^2/(n*trigamma(EMV)))\n    I2 = g(EMV) + c2 * sqrt(g1(EMV)^2/(n*trigamma(EMV)))\n    return I1, I2\n  end\n\n  inferiores = []; superiores = [] \n  for _ in 1:M\n    intervalo = mc()\n    push!(inferiores, intervalo[1])\n    push!(superiores, intervalo[2])\n  end\n  acertos = inferiores .&lt;= g(theta0) .&lt;= superiores\n  return mean(acertos) \nend\n\nRodando o código para o item c, temos\n\n\n\\(\\text{Confiança para } 10000 \\text{ simulações com alvo } 0.95, n = 100, \\theta_0 = 100:\\ 94.77\\%\\)\n\n\n\\(\\text{Confiança para } 10000 \\text{ simulações com alvo } 0.99, n = 100, \\theta_0 = 100:\\ 99.06\\%\\)\n\n\n\\(\\text{Confiança para } 10000 \\text{ simulações com alvo } 0.99, n = 5, \\theta_0 = 75:\\ 98.97\\%\\)\n\n\nPara o item d, note que \\[\ng'(\\theta) = \\mathrm{e}^{\\ln f_\\theta(x)} (-\\psi_1(\\theta) + \\ln(\\theta) - 1/\\theta)\n\\]\n\n# Resolução do item d\nfunction monte_carlo_d(M, γ, n, theta0)\n  d = Gamma(theta0, 1)\n  g(a) = exp(log(pdf(Gamma(a, 1), a))) # usado a expp(log), poderia usar direto\n  g1(a) = exp(log(pdf(Gamma(a,1), a))) * (-digamma(a) + log(a) - 1/a)\n  function mc()\n    x = rand(d, n)\n    EMV = newton_raphson(x)\n    c2 = quantile(Normal(), γ + (1-γ)/2)\n    I1 = g(EMV) - c2 * sqrt(g1(EMV)^2/(n*trigamma(EMV)))\n    I2 = g(EMV) + c2 * sqrt(g1(EMV)^2/(n*trigamma(EMV)))\n    return I1, I2\n  end\n\n  inferiores = []; superiores = [] \n  for _ in 1:M\n    intervalo = mc()\n    push!(inferiores, intervalo[1])\n    push!(superiores, intervalo[2])\n  end\n  acertos = inferiores .&lt;= g(theta0) .&lt;= superiores\n  return mean(acertos) \nend\n\nRodando o código para o item d, temos\n\n\n\\(\\text{Confiança para } 10000 \\text{ simulações com alvo } 0.95, n = 100, \\theta_0 = 100:\\ 95.28\\%\\)\n\n\n\\(\\text{Confiança para } 10000 \\text{ simulações com alvo } 0.99, n = 100, \\theta_0 = 100:\\ 99.02\\%\\)\n\n\n\\(\\text{Confiança para } 10000 \\text{ simulações com alvo } 0.99, n = 5, \\theta_0 = 75:\\ 98.83999999999999\\%\\)\n\n\n\n38.3.2.1 Comparação entre o exato e aproximado\n\nusing Plots, LaTeXStrings, Distributions, StatsBase, Random\n\nfunction monte_carlo_realxaprox()\n  theta0 = 10\n  M = 100_000\n  nn = 100\n\n  γ = 0.95\n\n  g(a) = a\n  g1(a) = 1\n\n  w = 0\n  cober = zeros(nn)\n  cobera = zeros(nn)\n  for n in 1:nn\n    w += 1\n    I1 = zeros(M)\n    Ia1 = zeros(M)\n    I2 = zeros(M)\n    Ia2 = zeros(M)\n\n    for i in 1:M\n      x = rand(Exponential(1/theta0), n)\n      c1 = quantile(Chisq(2n), (1-γ)/2)\n      c2 = quantile(Chisq(2n), γ + (1-γ)/2)\n      I1[i] = c1/(2*sum(x))\n      I2[i] = c2/(2*sum(x))\n\n      ca2 = quantile(Normal(), γ + (1-γ)/2)\n      EMV =  1/mean(x)\n      Ia1[i] = g(EMV) - ca2 * sqrt(g1(EMV)^2 * EMV^2 /n)\n      Ia2[i] = g(EMV) + ca2 * sqrt(g1(EMV)^2 * EMV^2 /n)\n    end\n    cober[w] = mean(I1 .&lt;= theta0 .&lt;= I2)\n    cobera[w] = mean(Ia1 .&lt;= g(theta0) .&lt;= Ia2)\n  end\n  preal = scatter(1:10:100, cober, color=\"blue\",\n                  label=\"Cobertura do IC real observado\",\n                  title=\"Comparação entre ICs: Cobertura\",\n                  ylims=(γ-0.1, γ+0.1),\n                  xlabel=\"Cobertura\",\n                  ylabel=L\"n\")\n  scatter!(1:10:100, cobera, color=\"red\", label=\"Cobertura IC aproximado\")\n  hline!([γ], label=L\"\\gamma\")\n  return preal\nend\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n38.3.2.2 Exemplo poisson\nNote que0 \\[\n\\begin{aligned}\n\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n) &= \\bar{X} \\\\\n\\sqrt{n}\\frac{\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n) - \\theta}{\\sqrt{\\theta}} &\\stackrel{\\mathcal{D}}{\\rightarrow} N(0,1) \\\\\n\\sqrt{n}\\frac{\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n) - \\theta}{\\sqrt{\\hat{\\theta}_{\\mathrm{MV}}(\\boldsymbol{X}_n)}} &\\stackrel{\\mathcal{D}}{\\rightarrow} N(0,1) \\\\\n\\end{aligned}\n\\]\n\nfunction monte_carlo_poiss()\n  run(`echo \\$QT_QPA_PLATFORM`)\n  theta0 = 10\n  d = Poisson(theta0)\n  g(a) = a\n  g1(a) = 1\n  nn = 1000\n  amplitude = zeros(nn)\n  cober = zeros(nn)\n  M = 10_000\n  for n in nn\n    I1 = zeros(M)\n    I2 = zeros(M)\n    for i in 1:M\n      x = rand(d, n)\n      EMV = mean(x)\n      c2 = quantile(Normal(), γ + (1-γ)/2)\n      I1[i] = EMV - c2 * sqrt(g1(EMV)^2 * EMV/n)\n      I2[i] = EMV + c2 * sqrt(g1(EMV)^2 * EMV/n)\n    end\n    amplitude[n] = mean(I2 .- I1)\n    cober[n] = mean(I1 .&lt;= g(theta0) .&lt;= I2)\n  end\n  amp = scatter(10:100:1000, amplitude, title=\"Amplitude dos ICs\", label=\"\",\n                color=:blue, xlabel=\"n\")\n  cob = scatter(10:100:1000, cober, title=\"Cobertura dos ICs\", label=\"\",\n                color=:red, xlabel=\"n\")\n  plt = plot(cob, amp)\n  return plt\nend\n\n\n\n$QT_QPA_PLATFORM",
    "crumbs": [
      "Inferência Frequentista",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Intervalos de Confiança - Aprofundamento</span>"
    ]
  }
]