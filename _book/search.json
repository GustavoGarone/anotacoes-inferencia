[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Anotaçõess de Inferência Frequentista",
    "section": "",
    "text": "Prefácio\nEste é um livrete feito na plataforma Quarto para minhas anotações das disciplinas de inferência frequentista ou clássica MAE0225 e MAE0301, ambas ministradas pelo Professor Alexandre Galvão Patriota e de sua autoria.\nDúvidas, sugestões, críticas ou erros por favor contactar em gustavo.garone arroba usp.br",
    "crumbs": [
      "Prefácio"
    ]
  },
  {
    "objectID": "modelo-estatistico.html",
    "href": "modelo-estatistico.html",
    "title": "1  Modelos Estatísticos na abordagem clássica",
    "section": "",
    "text": "1.1 Modelo Estatístico Paramétrico\nEm teoria de probabilidades, conhecemos a medida de probabilidade, logo, fazemos descrições probabilísticas. \\[(\\Omega, \\mathscr{A}, P)\\stackrel{\\text{X}}{\\rightarrow}(\\mathbb{R}, \\mathscr{B}, P_{X})\\] Na prática, contudo, não conhecemos a medida \\(P\\).\nDefinimos então uma família de medidas de probabilidades que possivelmente descrevem o comportamento aleatório dos dados.\nO Modelo Estatístico é definido pela trinca \\[(\\Omega, \\mathscr{A}, \\mathcal{P})\\] em que \\(\\Omega\\) é o espaço amostral (evento certo), \\(\\mathscr{A}\\) é a Sigma álgebra, uma família de subconjuntos ou eventos em \\(\\Omega\\), e \\(\\mathcal{P}\\) é uma família de medidas de probabilidade que possivelmente descrevem o comportamento aleatório dos dados ou eventos sob investigação.\nSe \\(\\mathcal{P} = \\{P_{\\theta} : \\theta \\in \\Theta\\}\\), em que \\(\\Theta \\subseteq \\mathbb{R}^{p}\\) e \\(p \\in \\mathbb{N}\\), então dizemos que \\((\\Omega, \\mathscr{A}, \\mathcal{P})\\) é um modelo estatístico Paramétrico.\nCaso não exista \\(\\Theta \\subseteq \\mathbb{R}^{p}\\) fixo, então dizemos que o modelo é não-paramétrico. Obs: \\(\\Theta\\) é o espaço paramétrico e \\(\\theta\\) é o vetor de parâmetros. \\(\\theta\\) Não* é variável aleatória, apenas indexa as medidas de probabilidade.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelos Estatísticos na abordagem clássica</span>"
    ]
  },
  {
    "objectID": "modelo-estatistico.html#modelo-estatístico-paramétrico",
    "href": "modelo-estatistico.html#modelo-estatístico-paramétrico",
    "title": "1  Modelos Estatísticos na abordagem clássica",
    "section": "",
    "text": "1.1.1 Exemplos\n\n1.1.1.1 Exemplo de Bernoulli\nConsidere um Ensaio de Bernoulli \\[\\Omega = \\{S,F \\}, \\mathscr{A} = 2^\\Omega\\] Temos algum conhecimento prévio que sugere que as probabilidades de sucesso podem ser \\(0.1, 0.5, 0.9\\)\nNesse caso, \\[\\mathcal{P} = \\{P_{1}, P_{2}, P_{3} \\}\\] em que \\[\\begin{cases}\nP_{1}(\\{S\\}) = 0.1;~ P_{1}(\\{F\\}) = 0.9;~ P_{1}(\\Omega) = 1  \\\\\nP_{2}(\\{S\\}) = 0.5;~ P_{2}(\\{F\\}) = 0.5;~ P_{2}(\\Omega) = 1  \\\\\nP_{3}(\\{S\\}) = 0.9;~ P_{3}(\\{F\\}) = 0.1;~ P_{3}(\\Omega) = 1\n\\end{cases}\\] Observe que \\[\\mathcal{P}=\\{P_{\\theta}: \\theta \\in \\Theta \\}\\] em que \\(\\Theta=\\{1,2,3 \\}\\subseteq \\mathbb{R}\\) Portanto, \\((\\Omega, \\mathscr{A}, \\mathcal{P})\\) é um modelo paramétrico.\nUsando de variáveis aleatórias,\nSeja \\(X : \\Omega \\rightarrow \\mathbb{R}\\) tal que \\[X(\\omega)=\\begin{cases}\n1, \\omega = \\text{S} \\\\\n0, c.c.\n\\end{cases}\\] \\[E_\\theta(X)=\\sum\\limits^{1}_{x=0}xP_\\theta(X=x)\\] \\[\\begin{cases}\n\\theta = 1 \\Rightarrow E_{1}(X)=0.1 \\\\\n\\theta = 2 \\Rightarrow E_{2}(X)=0.5 \\\\\n\\theta = 3 \\Rightarrow E_{3}(X)=0.9\n\\end{cases}\\]\n\n\n1.1.1.2 Exemplo de Exponencial\nSeja \\(\\Omega=(0,\\infty)\\) e \\(\\mathscr{A}\\) uma sigma-álgebra de \\(\\Omega\\) (Sigma-Álgebra de Borel) \\(\\Omega\\) representa o tempo até a ocorrência de um evento (uma reclamação, por exemplo) Temos conhecimento prévio de que as funções densidade de probabilidade que possivelmente descrevem esse evento são:\n\\[\n\\begin{aligned}\nf_{1}(\\omega) &=\n\\begin{cases}\n\\mathrm{e}^{-1\\omega}, \\omega&gt;0 \\\\\n0, c.c.\n\\end{cases}\\\\\nf_{2} (\\omega) &=\n\\begin{cases}\n2\\mathrm{e}^{-2\\omega}, \\omega&gt;0 \\\\\n0, c.c.\n\\end{cases}\\\\\nf_{3} (\\omega) &=\n\\begin{cases}\n\\frac{1}{2}\\mathrm{e}^{-\\frac{1}{2}\\omega}, \\omega&gt;0 \\\\\n0, c.c.\n\\end{cases}\\\\\nf_{4} (\\omega) &=\n\\begin{cases}\n\\frac{1}{10}\\mathrm{e}^{- \\frac{1}{10}\\omega}, \\omega&gt;0 \\\\\n0, c.c.\n\\end{cases}\n\\end{aligned}\n\\]\ne \\(P\\)s, \\[\n\\begin{aligned}\nP_{1}(A)&= \\int_{A} f_{1}(\\omega)d \\omega \\\\\nP_{2}(A)&= \\int_{A} f_{2}(\\omega)d \\omega \\\\\nP_{3}(A)&= \\int_{A} f_{3}(\\omega)d \\omega \\\\\nP_{4}(A)&= \\int_{A} f_{4}(\\omega)d \\omega \\\\\n\\end{aligned}\\] Dessa forma, \\[P_{\\theta}(A)= \\int_{A}f_\\theta(\\omega)d \\omega\\] e \\(\\Omega = \\{1,2,3,4 \\}\\) Seja \\(X: \\Omega \\rightarrow \\mathbb{R}\\) tal que \\[X(\\omega)= \\omega\\] Note que \\[E_\\theta(X)=\\int_{-\\infty}^{\\infty}xf_\\theta(x)dx, \\theta \\in \\{1,2,3,4 \\}\\] \\[E_\\theta(X)=\\begin{cases}\n1,~ \\theta=1 \\\\\n\\frac{1}{2},~ \\theta=2 \\\\\n2,~ \\theta=3 \\\\\n10,~ \\theta=4\n\\end{cases}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelos Estatísticos na abordagem clássica</span>"
    ]
  },
  {
    "objectID": "modelo-estatistico.html#principais-modelos-estatísticos",
    "href": "modelo-estatistico.html#principais-modelos-estatísticos",
    "title": "1  Modelos Estatísticos na abordagem clássica",
    "section": "1.2 Principais Modelos Estatísticos",
    "text": "1.2 Principais Modelos Estatísticos\n\n1.2.1 Modelo Estatístico de Bernoulli\nDizemos que \\(X\\) é uma variável aleatória com modelo de Bernoulli se, e somente se, \\[\nP_\\theta(X=x)=\\begin{cases}\n\\theta^{x} \\cdot(1-\\theta)^{1-x}, x \\in \\{0,1 \\} \\\\\n0, x \\not \\in \\{0,1 \\}\n\\end{cases}\\] O parâmetro é a probabilidade de sucesso \\[\\begin{cases}\nE_\\theta(X)=\\theta\\\\\n\\mathrm{Var}_{\\theta}(X) = \\theta(1-\\theta) \\\\\nP_\\theta(X=1)=\\theta, P_\\theta(X=0)=1-\\theta\n\\end{cases}\n\\]\nNotação: \\(\\mathrm{Ber}(\\theta)\\) em que \\(\\theta \\in \\Theta=(0,1)\\subseteq \\mathbb{R}\\)\n\n\n1.2.2 Modelo Estatístico Binomial\nDizemos que \\(X\\) é uma variável aleatória com modelo binomial se, e somente se, \\[\nP_\\theta(X=x)=\\begin{cases}\n{n \\choose x} \\cdot \\theta^{x}\\cdot(1-\\theta)^{n-x}, x \\in \\{0,1,\\dots,n \\} \\\\\n0, x \\not \\in \\{0,1,\\dots,n \\}\n\\end{cases}\\] \\[\\begin{cases}\nE_\\theta(X)=n\\theta\\\\\n\\mathrm{Var}_{\\theta}(X) = n\\theta(1-\\theta) \\\\\nP_\\theta(X=0)=(1-\\theta)^{n},\\dots, P_\\theta(X=n)=\\theta^n\n\\end{cases}\\]\nNotação: \\(\\mathrm{Bin}(n,\\theta)\\) em que \\(n\\) é conhecido e fixado previamente, \\(\\theta\\) é a probabilidade de sucesso (parâmetro do modelo) e \\(\\Theta =(0,1)\\) é o espaço paramétrico\n\n\n1.2.3 Modelo Estatístico Geométrico\nDizemos que \\(X\\), representando o número de fracassos até o primeiro sucesso, é uma variável aleatória com modelo estatístico geométrico se, e somente se, \\[P_\\theta(X=x)=\\begin{cases}\n\\theta (1-\\theta)^{x-1}, x \\in \\{1,\\dots\\} \\\\\n0, x \\not \\in \\{1,\\dots\\}\n\\end{cases}\n\\] \\[\n\\begin{cases}\nE_\\theta(X)=\\frac{1}{\\theta}\\\\\n\\mathrm{Var}_\\theta(X) = \\frac{1-\\theta}{\\theta^{2}}\n\\end{cases}\n\\] Notação: \\(\\mathrm{Geom}(\\theta)\\) em que \\(\\theta\\) é o parâmetro do modelo (probabilidade de sucesso) e \\(\\Theta=(0,1)\\) é o espaço paramétrico\n\n\n1.2.4 Modelo de Poisson\nDizemos que \\(X\\) é uma variável aleatória com modelo estatístico Poisson, se, e somente se, \\[\nP_\\theta(X=x)=\\begin{cases}\n\\mathrm{e}^{-\\theta}\\cdot \\frac{\\theta^{x}}{x!}, x \\in \\{0,1,\\dots\\} \\\\\n0, x \\not \\in \\{0,1,\\dots\\}\n\\end{cases}\n\\] \\[\n\\begin{cases}\nE_\\theta(X)=\\theta\\\\\n\\mathrm{Var}_\\theta(X) = \\theta\n\\end{cases}\n\\]\nNotação: \\(\\mathrm{Poisson}(\\theta)\\) em que \\(\\theta\\) é a taxa média de ocorrência do evento (parâmetro do modelo) e \\(\\Theta = (0, \\infty)\\), o espaço paramétrico.\n\n\n1.2.5 Modelo Multinomial\nDizemos que \\(\\pmb{X} = (X_1,\\dots,X_k)\\) é um Vetor Aleatório com modelo estatístico Multinomial se, e somente se a função de probabilidade é \\[\nP_{\\theta}(X_{1}=x_{1},\\dots,X_{k} = x_{k})=\n\\begin{cases}\n\\frac{n!}{x_{1}!x_{2}!\\dots x_{k}!} \\cdot \\theta^{x_{1}}_{1} \\cdot \\cdots \\cdot\\theta^{x_n}_{k} ~~~~ x_{1}+\\dots+x_{k}=n\\\\\n0, c.c\n\\end{cases}\n\\]\n\\[\n\\begin{cases}\nE_\\theta(X_{i})=n\\theta_{i}\\\\\n\\mathrm{Var}_\\theta(X_{i}) = n\\theta_{i} (1-\\theta_{i}) \\\\\n\\mathrm{Cov}(X_{i}X_{j})=-n\\theta_{i}\\theta_{j}\n\\end{cases}\n\\]\nNotação: \\(\\mathrm{Multinomial}(n, \\theta_1,\\theta_2,\\dots,\\theta_k)\\) em que \\(\\theta_{1}+\\dots+\\theta_{k}=1\\) e \\(0\\leq \\theta_{i} \\leq 1\\), \\(\\forall i = 1,2,\\dots,k\\), \\(\\Theta=\\{(\\theta_{1},,\\dots,\\theta_{k}) \\in \\mathbb{R}^{k} : 0\\leq \\theta_{i} \\leq 1, i=1,\\dots,k,\n\\theta_{1}+\\dots+\\theta_{k} = 1 \\}\\)\nEsse modelo tem aplicação em modelos de linguagem como o ChatGPT. (\\(k\\) como tamanho do vocabulário, \\(n=1\\), \\(\\theta_{1}=\\) probabilidade de escolher o primeiro elemento do vocabulário e assim por diante.)\n\n\n1.2.6 Modelo Uniforme contínuo\nDizemos que \\(X\\) é uma variável aleatória contínua com modelo estatístico Uniforme em \\((\\theta_{1}, \\theta_{2}), \\theta_{1}\\leq x \\leq\\theta_{2}\\), se, e somente se, a sua Função Densidade de Probabilidade é \\[\nf_\\theta(x)=\\begin{cases}\n\\frac{1}{\\theta_{2}-\\theta_{1}}, x \\in(\\theta_{1}, \\theta_{2}) \\\\\n0, c.c.\n\\end{cases}\n\\]\n\\[\n\\begin{cases}\nE_\\theta(X)=\\frac{\\theta_2+\\theta_1}{2}\\\\\n\\mathrm{Var}_\\theta(X) = \\frac{(\\theta_2-\\theta_1)^{2}}{12}\\end{cases}\n\\]\nNotação: \\(X \\sim U(\\theta_{1}, \\theta_{2})\\), em que \\(\\theta=(\\theta_{1}, \\theta_{2})\\) é o vetor de parâmetros e \\(\\Theta = \\{\\theta \\in \\mathbb{R}^{2} : \\theta_{2} &gt; \\theta_{1} \\}\\) é o espaço paramétrico.\n\n\n1.2.7 Modelo Exponencial\nDizemos que \\(X\\) é uma variável aleatória contínua com modelo estatístico Exponencial se, e somente se, a sua Função Densidade de Probabilidade é dada por \\[\nf_\\theta(x)=\\begin{cases}\n\\theta \\mathrm{e}^{-\\theta x}, x&gt;0\\\\\n0, c.c.\n\\end{cases}\n\\]\n\\[\n\\begin{cases}\nE_\\theta(X)=\\frac{1}{\\theta}\\\\\n\\mathrm{Var}_\\theta(X) = \\frac{1}{\\theta^{2}}\n\\end{cases}\n\\]\nNotação: \\(X\\sim \\mathrm{Exp}(\\theta), \\theta&gt; 0\\) em que \\(\\theta&gt;0, \\Theta=\\{\\theta \\in \\mathbb{R}: \\theta&gt;0 \\}\\)\n\n\n1.2.8 Modelo Normal\nDizemos que \\(X\\) é uma variável aleatória contínua com modelo estatístico Normal com média \\(\\mu\\) e variância \\(\\sigma^{2}\\) se, e somente se, a sua Função Densidade de Probabilidade é dada por \\[\nf_\\theta(x)=\n\\frac{1}{\\sqrt{2\\pi \\sigma^{2}}}\\cdot \\mathrm{e}^{-\\frac{1}{2\\sigma^{2}}(x-\\mu)^{2}}, x \\in \\mathbb{R}\n\\] em que \\(\\theta= (\\mu, \\sigma^{2}) \\in \\Theta= \\mathbb{R}\\times \\mathbb{R}^{+}\\)\n\\[\n\\begin{cases}\nE_\\theta(X)=\\mu\\\\\n\\mathrm{Var}_\\theta(X) = \\sigma^{2}\n\\end{cases}\n\\]\nNotação: \\(X \\sim N (\\mu, \\sigma^{2})\\), em que \\(\\theta=(\\mu, \\sigma^{2})\\) é o vetor de parâmetros.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelos Estatísticos na abordagem clássica</span>"
    ]
  },
  {
    "objectID": "populacao-e-amostra.html",
    "href": "populacao-e-amostra.html",
    "title": "2  População e Amostra",
    "section": "",
    "text": "2.1 Variável Populacional\nVeja: Modelo Estatístico para definições dos modelos estatísticos paramétricos.\nPela teoria estatística, população é o conjunto sob investigação de todos os potenciais elementos.\nA Variável Populacional representa os valores numéricos de cada elemento da população: \\[\nX\\sim f_{\\theta,}\\theta \\in \\Theta\n\\] em que \\(f_\\theta\\) é a Função Densidade de Probabilidade da Variável Aleatória populacional. \\(\\theta\\) é o vetor de parâmetros (desconhecido) e \\(\\Theta\\) é o espaço paramétrico",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>População e Amostra</span>"
    ]
  },
  {
    "objectID": "populacao-e-amostra.html#amostra-teórica",
    "href": "populacao-e-amostra.html#amostra-teórica",
    "title": "2  População e Amostra",
    "section": "2.2 Amostra (Teórica)",
    "text": "2.2 Amostra (Teórica)\nÉ uma parte ou subconjunto da população.\n\n2.2.1 Amostra Aleatória\nDizemos que \\((X_{1},\\dots,X_{n})\\) é uma amostra aleatória de X (v.a. populacional) se \\(X_{1},\\dots,X_{n}\\) forem independentes e identicamente distribuídas de acordo com a distribuição de \\(X\\) Ou seja, \\[\n\\text{Independentes = }\\begin{cases}X_{1}\\sim f_{\\theta,}\\theta \\in \\Theta \\\\\n. \\\\\n. \\\\\n. \\\\\nX_{n} \\sim f_{\\theta}, \\theta \\in \\Theta\n\\end{cases}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>População e Amostra</span>"
    ]
  },
  {
    "objectID": "populacao-e-amostra.html#sec-ao",
    "href": "populacao-e-amostra.html#sec-ao",
    "title": "2  População e Amostra",
    "section": "2.3 Amostra (Observada)",
    "text": "2.3 Amostra (Observada)\nÉ formada por valores numéricos após utilizar um procedimento de amostragem. \\[\nx_{1},\\dots,x_{n}\n\\]\nem que \\(n\\) é o tamanho amostral.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>População e Amostra</span>"
    ]
  },
  {
    "objectID": "quantidade-de-interesse.html",
    "href": "quantidade-de-interesse.html",
    "title": "3  Quantidade de Interesse",
    "section": "",
    "text": "É uma quantidade relacionada com a distribuição da variável aleatória populacional. \\[\ng(\\theta)\n\\]\nComo \\(g(\\theta) = E_\\theta(X), g(\\theta)= \\mathrm{Var}_\\theta(X), g(\\theta)=P_\\theta(X\\geq1)\\) e até \\(g(\\theta)=\\theta\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Quantidade de Interesse</span>"
    ]
  },
  {
    "objectID": "dist-amostral.html",
    "href": "dist-amostral.html",
    "title": "4  Distribuição Amostral",
    "section": "",
    "text": "4.1 Exemplos\nSeja \\((X_{1},\\dots,X_{n})\\) amostra aleatória (a.a.) de \\(X\\sim f_{\\theta,}\\theta \\in \\Theta\\)\na Função Densidade de Probabilidade conjunta de \\((X_{1}, X_{2},\\dots,X_{n})\\) é \\(\\forall \\theta \\in \\Theta\\), no caso discreto: \\[\nP_\\theta(X_{1}=k_{1}, X_{2}=k_{2},\\dots,X_{n}= k_{n})\\stackrel{\\text{ind}}{=}\\prod^{n}_{i=1}P_\\theta(X_{i}=k_{i})\n\\stackrel{\\text{id}}{=}\\prod^{n}_{i=1}P_\\theta(X=k_{i})\\stackrel{\\text{def}}{=}\\prod^{n}_{i=1}f_\\theta(k_{i})\n\\]\nno caso contínuo: \\[\nf_\\theta^{(x)}(k_{1},k_{2},\\dots,k_{n})\\stackrel{\\text{i.i.d}}{=}\\prod^{n}_{i=1}f_{\\theta}(k_{i})\n\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Distribuição Amostral</span>"
    ]
  },
  {
    "objectID": "dist-amostral.html#exemplos",
    "href": "dist-amostral.html#exemplos",
    "title": "4  Distribuição Amostral",
    "section": "",
    "text": "4.1.1 Exemplo um\nSeja \\((X_{1},X_{2},X_{3})\\) uma a.a de \\(X\\sim \\text{Ber}(\\theta), \\theta \\in (0,1)\\) 1. Especifique o espaço paramétrico 2. Calcule a função de probabilidade da amostra 3. Encontre as seguintes quantidade de interesses em função de \\(\\theta\\) 1. \\(g(\\theta)=E_\\theta(X)\\) 2. \\(g(\\theta)=P_\\theta(X=0)\\) 3. \\(g(\\theta)=\\mathrm{CV}_\\theta(X)\\) Resolução 1. \\(\\Theta=(0,1)\\) 2. Duas resoluções possíveis 1. Dando valores à amostra \\[\\begin{aligned}\n         &(X_{1}, X_{2},X_{3})  &P(X_{1}= k_{1},X_{2}= k_{2},X_{3}= k_{3})= \\prod ^{3}_{i=1}P_\\theta(X=k_{1})\\\\\n         &(0,0,0) &(1-\\theta)^{3}\\\\\n         &(0,0,1) &(1-\\theta)^{2}\\theta\\\\\n         &(0,1,0) &(1-\\theta)^{2}\\theta\\\\\n         &(1,0,0) &(1-\\theta)^{2}\\theta\\\\\n         &(0,1,1) &(1-\\theta)\\theta^2\\\\\n         &(1,0,1) &(1-\\theta)\\theta^{2}\\\\\n         &(1,1,0) &(1-\\theta)\\theta^{2}\\\\\n         &(1,1,1) &\\theta^{3}\\\\\n         \\end{aligned}\n    \\] 2. Enunciando a função Observe que, se \\(k \\in \\{0,1 \\}\\), \\[\\begin{aligned}\n       &P_\\theta(X=k)=\\theta^{k}(1-\\theta)^{1-k}\\cdot\\mathbb{1}_{\\{0,1 \\}}(k)\\Rightarrow\\\\\n       &P_\\theta(X_{1}=k,X_{2}=k_{2},X_{3}= k_{3})\\stackrel{\\text{i.i.d}}{=}\\prod^{3}_{i=1}\\{\\theta^{k_{i}}\n       (1-\\theta)^{1-k_{1}}\\mathbb{1}_{\\{0,1 \\}}(k_{1}) \\} = \\\\\n       &= \\theta^{\\sum\\limits^{3}_{i=1}k_{1}}(1-\\theta)^{3-\\sum\\limits^{3}_{i=1}k_{1}}\\prod^{3}_{i=1}\\mathbb{1}_{\\{0,1\\}}\n       (k_{i})\n       \\end{aligned}\n    \\] 3. Em função de \\(\\theta\\): 1. \\(g(\\theta)=E_\\theta(X)=\\theta\\) 2. \\(g(\\theta)=P_\\theta(X=0)=1-\\theta\\) 3. \\(g(\\theta)=\\mathrm{CV}_\\theta(X)=\\frac{\\sqrt{\\theta(1-\\theta)}}{\\theta}\\)\n\n\n4.1.2 Exemplo dois\nSeja \\((X_{1},\\dots,X_{n})\\) uma a.a de \\(X\\sim\\text{Ber}(\\theta), \\theta \\in(0,1)\\), encontre a f.p conjunta da amostra. \\[\n\\begin{aligned}\nP_\\theta(X_{1}=k_{1},\\dots,X_{n}=k_{n})\\stackrel{\\text{iid}}{=}\\prod ^{n}_{i=1}P_\\theta(X=k_{i})=\\prod^{n}_{i=1}\n\\{\\theta^{k_{i}}(1-\\theta)^{1-k_{i}}\\mathbb{1}_{\\{0,1 \\}}(k_{i}) \\}\\\\\n\\Rightarrow P_\\theta(X_{1}=k_{1},\\dots,X_{n}=k_{n})=\\theta^{\\sum\\limits^{n}_{i=1}k_{1}}\\cdot\n\\theta^{n-\\sum\\limits^{n}_{i=i}k_{i}}\\cdot\\mathbb{1}_{\\{0,1\\}}(k_{i})\n\\end{aligned}\n\\]\n\n\n4.1.3 Exemplo três\nSeja \\((X_{1},\\dots,X_{n})\\) uma a.a de \\(X\\sim\\text{Pois}(\\theta), \\theta \\in(0,\\infty)\\), encontre a f.p. conjunta da amostra. Como esse vetor é uma a.a. (ou seja, variáveis aleatórias independentes e identicamente distribuídas), temos que \\[\nP_\\theta(X_1=k_{1},\\dots,X_{n}=k_{n})\\stackrel{\\text{iid}}{=}\\prod^{n}_{i=1}P_\\theta(X=k_{i})=\\prod^{n}_{i=1}\\{\\mathrm{e}\n^{-\\theta} \\cdot \\frac{\\theta^{k_{i}}}{k_{i}!}\\}\n\\] Sempre que \\(k_{i}\\in\\{0,1,\\dots \\}, \\forall i = 1,\\dots,n\\)\n\\[\n\\Rightarrow P_\\theta(\\dots)=\\mathrm{e}^{-n \\theta} \\cdot \\frac{\\theta^{\\sum\\limits^{n}_{i=1}k_{i}}}{\\prod^{n}_{i=1}\n(k_{i})!}\n\\]\n\n\n4.1.4 Exemplo contínuo um\nSeja \\((X_{1},\\dots,X_{n})\\) uma a.a de \\(X\\sim\\text{Exp}(\\theta), \\theta \\in(0,\\infty)\\), encontre a função densidade de probabilidade (f.d.p.) conjunta da amostra. \\[\n\\begin{aligned}\nF_{\\theta}^{(*)}(k_{1},\\dots,k_{n})&\\stackrel{\\text{iid}}{=}\\prod^{n}_{i=1}f_\\theta(k_{i})=\\prod^{n}_{i=1}\n\\{\\theta \\mathrm{e}^{-\\theta k_{i}} \\cdot \\mathbb{1}_{(0,\\infty )}(k_{i}) \\}\n\\\\&\\Rightarrow f_\\theta^{(*)}(\\dots)=\\theta^{n}\n\\cdot \\mathrm{e}^{-\\theta\\sum\\limits^{n}_{i=1}k_{i}} \\cdot\\prod^{n}_{i=1}\\mathbb{1}_{(0,\\infty)}(k)\n\\end{aligned}\n\\]\n\n\n4.1.5 Exemplo contínuo dois\nSeja \\((X_{1},\\dots,X_{n})\\) uma a.a. (i.i.d) de \\(X\\sim N(\\mu,\\sigma^{2})\\) em que \\(\\theta=(\\mu, \\sigma^{2}) \\in \\Theta=\\mathbb{R}\\times \\mathbb{R}_{+}\\). Considere \\(\\stackrel{x}{\\sim}=(x_{1},\\dots,x_{n})\\) a amostra observada.\n\\[\n\\begin{aligned}\nL_{\\stackrel{X}{\\sim}} &\\stackrel{\\text{iid}}{=} \\prod^{n}_{i=1}f_\\theta(x_{i})=\n\\prod^{n}_{i=1}\\left\\{\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} \\mathrm{exp}\\{- \\frac{1}{2\\sigma^{2}}(x_{1}-\\mu)^{2}\\}\\right\\} \\\\\n&= \\frac{1}{(2 \\pi \\sigma^{2})^{\\frac{x}{2}}}\\cdot \\mathrm{exp}\\{- \\frac{1}{2 \\sigma^{2}} \\sum\\limits^{n}_{i=1}(x_{i}-\\mu)^{2}  \\}\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Distribuição Amostral</span>"
    ]
  },
  {
    "objectID": "funcao-verossimilhanca.html",
    "href": "funcao-verossimilhanca.html",
    "title": "5  Função de Verossimilhança",
    "section": "",
    "text": "5.1 Exemplo\nQuando analisamos a distribuição conjunta da amostra em função de \\(\\theta\\) nos valores da amostra observada, temos a Função de Verossimilhança\n\\[\n\\mathrm{L}_{\\stackrel{x}{\\sim}}(\\theta)=P_\\theta(X_{1}=x_{1},X_{2}=x_{2},\\dots,X_{n}=x_{n})\n\\]\nem que \\(\\stackrel{x}{\\sim}=(x_{1},x_{2},\\dots,x_{n})\\) é a amostra observada.\nObs: A função de verossimilhança, no caso discreto, é a probabilidade de observar a amostra observada.\nConsidere \\((X_{1},X_{2},X_{3},X_{4})\\) a.a de \\(X\\sim \\text{Ber}(\\theta), \\theta \\in \\{0.1, 0.5, 0.9 \\}\\). Note que o espaço paramétrico é \\(\\Theta=\\{0.1,0.5,0.9 \\}\\). Considere, ainda, que a amostra observada foi \\((0,1,1,1)\\). Encontre a função de verossimilhança.\n\\[\n\\begin{aligned}\nL_\\stackrel{X}{\\sim}(\\theta)&=P_\\theta(X_{1}=x_{1},X_{2}=x_{2},X_{3}=x_{3},X_{4}=x_{4}) \\\\\n&= \\theta^{\\sum\\limits^{4}_{i=1}x_{i}}\n(1-\\theta)^{4-\\sum\\limits^{4}_{i=1}x_{i}}\\cdot \\cancelto{1}{\\prod^{4}_{i=1}\\mathbb{1}_{\\{0,1 \\}}(x_{i})} =\n\\theta^{3}(1-\\theta)\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Função de Verossimilhança</span>"
    ]
  },
  {
    "objectID": "estatisticas.html",
    "href": "estatisticas.html",
    "title": "6  Estatísticas",
    "section": "",
    "text": "6.1 Exemplo\nFunções da amostra que não dependem de \\(\\theta \\in \\Theta\\)\nSeja \\((X_{1}, \\dots, X_{n})\\) a.a. de \\(X\\sim f_{\\theta,}\\theta \\in \\Theta\\). São estatísticas: 1. \\(T_{1}(X_{1},\\dots,X_{n})=X_{1}+\\dots+X_{n}\\) 2. \\(T_{2}=\\bar{X}= \\frac{X_{1}+\\dots+X_{n}}{n}\\) 3. \\(T_{3}=\\max\\{X_{1},\\dots,X_{n}\\}=X_{(n)}\\) 4. \\(T_{4}=\\min\\{X_{1},\\dots,X_{n}\\}=X_{(1)}\\) 5. \\(T_{5}(X_{1},\\dots,X_{n})=X_{(n})-X_{(1)}\\) 6. \\(T_{6}(X_{1},\\dots,X_{n})=X_{i}, \\text{para algum }i=1,\\dots,n\\) 7. \\(T_{7}(X_{1},\\dots,X_{n})=\\frac{1}{n}\\sum\\limits^{n}_{i=1}(X_{i}-\\bar{X})^{2}\\) 8. \\(T_{8}(X_{1},\\dots,X_{n})=\\frac{1}{n-1}\\sum\\limits^{n}_{i=1}(X_{i}-\\bar{X})^{2}\\) 9. \\(T_{9}(X_{1},\\dots,X_{n})=\\frac{1}{n}\\sum\\limits^{n}_{i=1}|X_{i}-\\bar{X}|\\) 10. \\(T_{7}(X_{1},\\dots,X_{n})=\\sqrt{\\frac{1}{n}\\sum\\limits^{n}_{i=1}(X_{i}-\\bar{X})^{2}}\\) etc.\nObservação: As estatísticas são variáveis aleatórias.\n\\[\n\\underset{\\sim}{X_n}=(X_{1},\\dots,X_{n}):\\Omega\\rightarrow \\mathbb{R}^{n}\n\\] \\[\nT(\\underset{\\sim}{X})=T \\circ \\underset{\\sim}{X}:\\Omega\\rightarrow \\mathbb{R}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Estatísticas</span>"
    ]
  },
  {
    "objectID": "estimadores.html",
    "href": "estimadores.html",
    "title": "7  Estimadores",
    "section": "",
    "text": "7.1 Estimativas\nSão estatísticas cujo objetivo é estimar uma quantidade de interesse. Portanto, estimadores são também variáveis aleatórias.\nSão os valores observados a partir da amostra observada dos estimadores. Portanto, estimativas são valores numéricos\nExemplos: 1. \\(\\bar{X}\\) é uma estatística, \\(\\bar{X}\\) é um estimador para \\(g(\\theta)=E_\\theta(X)\\) 2. Observando \\(\\bar{x}=\\frac{1}{n} \\sum\\limits^{n}_{i=1}x_{i}\\) é uma estimativa Seja \\((X_{1},X_{2})\\) a.a de \\(X\\sim \\text{Ber}(\\theta), \\theta \\in (0,1)\\). Considere as estatísticas e suas funções de probabilidade: \\[\n\\begin{aligned}\nT_{1}(X_{1},X_{2}) &= X_{1}\\\\\nT_{2}(X_{1},X_{2}) &= X_{2}\\\\\nT_{3}(X_{1},X_{2}) &= X_{1}+X_{2}\\\\\nT_{4}(X_{1},X_{2}) &= \\max\\{X_{1},X_{2}\\}\\\\\nT_{5}(X_{1},X_{2}) &= \\min\\{X_{1},X_{2}\\}\\\\\nT_{6}(X_{1},X_{2}) &= \\frac{1}{2}[(X_{1}-\\bar{X})^{2}+(X_{2}-\\bar{X})^{2}]=S^{2}_{n}=\\frac{1}{2}S_{n-1}^{2}\n\\end{aligned}\n\\]\n\\[\n\\begin{array}{cccccc}\n(X_{1},X_{2}) & P_{\\theta X_{1},X_{2}} & X_{1} & X_{2} & X_{1}+X_{2} & T_4 & T_5 & \\bar{X}& S^{2}_{n} & S^{2}_{n-1}\\\\\n\\hline\n(0,0) & (1-\\theta)^{2} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n(1,0) & \\theta(1-\\theta) & 1 & 0 & 1 & 1 & 0 & 0.5 & 0.25 & 0.5\\\\\n(0,1) & \\theta(1-\\theta) & 0 & 1 & 1 & 1 & 0 & 0.5 & 0.25 & 0.5\\\\\n(1,1) & \\theta^{2} & 1 & 1 & 2 & 1 & 1 & 1 & 0 & 0\\\\\n\\hline\n\\end{array}\n\\]\nCalcule para \\(T \\in \\{T_{1},T_{2},T_{3},T_{4},T_{5},T_{6} \\}\\)\na-) \\(E_\\theta(T)\\)\n\\(E_\\theta(T_{1}(X_{1},X_{2}))=E_\\theta(X_{1})=\\theta =0 (1-\\theta)^{2}+1 \\cdot\\theta(1-\\theta)+0 \\theta(1-\\theta)+1\n\\theta^{2}= \\theta\\)\nO mesmo vale para \\(T_2\\)\n\\(E_\\theta(T_{3}(X_{1},X_{2}))=E_\\theta(X_{1}+X_{2})=2 \\theta\\) \\(E_\\theta(T_{4}(X_{1},X_{2}))=E_\\theta(\\max\\{X_{1},X_{2}\\})=0 \\cdot (1-\\theta)^{2}+ 1\\cdot[2\\cdot \\theta(1-\\theta)+\n\\theta^{2}] = 2\\theta-\\theta^{2}\\) \\(E_{\\theta}(T_{5}(X_{1},X_{2}))=E_\\theta(\\min\\{X_{1},X_{2} \\})=\\theta^{2}\\) \\(E_\\theta(T_{6}(X_{1},X_{2}))=2 \\theta(1-\\theta)\\)\nb-) \\(\\mathrm{Var}_{\\theta}(T)\\)\nTermine com os mesmos raciocínios\nc-) \\(P_\\theta(T=0)\\)\nTermine com os mesmos raciocínios\nAlguns resultados importantes:\n\\[\n\\begin{aligned}\nE_\\theta(\\bar{X})&=E_\\Theta(\\frac{X_{1}+X_{2}}{2})=\\theta, \\theta \\in(0,1)\\\\\nE_{\\theta}(\\bar{X}^{2}) &= 0^{2}(1-\\theta)^{2} + \\frac{2}{4}  \\theta (1-\\theta) + 1^{2}\\theta^{2}=\\frac{1}{2}\\theta +\n\\frac{1}{2} \\theta^{2}, \\theta \\in(0,1)\\\\\n\\Rightarrow \\mathrm{Var}_\\theta(\\bar{X}) &= \\frac{\\theta+\\theta^{2}}{2} - \\theta^{2} =\\frac{\\theta (1-\\theta)}{2},\n\\theta \\in(0,1)\n\\end{aligned}\n\\]\nAplicando em nossa tabela (\\(S_{n-1}^{2}\\)):\n\\[\n\\begin{aligned}\nE_\\theta([(X_{1}-\\bar{X})^{2}+(X_{2}-\\bar{X})^{2}])&= \\theta(1-\\theta)\\\\\nE_\\theta([(X_{1}-\\bar{X})^{2}+(X_{2}-\\bar{X})^{2}]^{2}) &= \\frac{\\theta(1-\\theta)}{2}\\\\\n\\Rightarrow\\mathrm{Var}_{\\Theta}([(X_{1}-\\bar{X})^{2}+(X_{2}-\\bar{X})^{2}]) &= \\frac{1}{2}\\theta(1-\\theta)\n[1-2 \\cdot \\theta(1-\\theta)]\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Estimadores</span>"
    ]
  },
  {
    "objectID": "estimadores.html#propriedades-dos-estimadores-para-quantidades-de-interesse",
    "href": "estimadores.html#propriedades-dos-estimadores-para-quantidades-de-interesse",
    "title": "7  Estimadores",
    "section": "7.2 Propriedades dos estimadores para quantidades de interesse",
    "text": "7.2 Propriedades dos estimadores para quantidades de interesse\n\n7.2.1 Estimados não viciados ou enviesados\nSeja \\((X_{1},\\dots,X_{n})\\) a.a. de \\(X\\sim f_{\\theta,}\\theta \\in \\Theta\\) e considere \\(T(X_{1},\\dots,X_{n})=\\hat\\theta\\) um estimador para \\(\\theta\\). Dizemos que \\(\\hat \\theta\\) é não-enviesado para \\(\\theta\\) \\(\\Leftrightarrow\\) \\[\nE_\\theta(\\hat \\theta_{n}) = \\theta, \\forall \\theta \\in \\Theta\n\\] De forma geral, \\(T(X_{1},\\dots,X_{n})\\) é um estimador não-viciado para \\(g(\\theta) \\Leftrightarrow\\), \\[\nE_\\theta(T(X_{1},\\dots,X_{n}))=g(\\theta), \\forall \\theta \\in \\Theta\n\\] Caso contrário, dizemos que \\(T(X_{1},\\dots,X_{n})\\) é viciado ou enviesado para \\(g(\\theta)\\) Dizemos que \\(\\hat\\theta_{n}\\) é fracamente consistente para \\(\\theta \\Leftrightarrow\\) \\[\n\\lim_{n\\to \\infty}{P_\\theta(|\\hat \\theta_{n}- \\theta| &gt; \\epsilon)=0, \\forall \\theta \\in \\Theta}\n\\] e para cada \\(\\epsilon&gt;0\\) fixado.\n\n7.2.1.1 Estimadores não viciados assintoticamente\nDizemos que \\(T(X_{1},\\dots,X_{n})\\) é um estimador assintoticamente não viciado para \\(g(\\theta) \\Leftrightarrow\\) \\[\n\\lim_{n\\to \\infty}{E_\\theta(T(X_{1},\\dots,X_{n}))} = g(\\theta), \\forall \\theta \\in \\Theta\n\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Estimadores</span>"
    ]
  },
  {
    "objectID": "eqm-e-vies.html",
    "href": "eqm-e-vies.html",
    "title": "8  Erro quadrático médio (EQM)",
    "section": "",
    "text": "8.1 Propriedades do EQM\nO erro quadrático médio (EQM) do estimador \\(T(X_{1},\\dots,X_{n})\\) com respeito a \\(g(\\theta)\\) é definido por \\[\n\\mathrm{EQM}(T,g(\\theta))=E_{\\theta}((T(X_{1},\\dots,X_{n})-g(\\theta)^{2})\n\\] Obs.:\nSe \\(T(X_{1},\\dots,X_{n})\\) for não viciado para \\(g(\\theta)\\), então \\(\\mathrm{EQM}(T,g(\\theta))=\\mathrm{Var}_{\\theta}(T(X_{1},\\dots,X_{n})) \\forall \\theta \\in \\Theta\\)\nSeja \\(T(X_{1},\\dots,X_{n})\\) um estimador para \\(g(\\theta)\\), seja \\(\\mu_{t} = E_\\theta(T(X_{1},\\dots,X_{n}))\\) \\[\n\\begin{aligned}\n&\\mathrm{EQM}(T,g(\\theta))\\\\\n&=E_\\theta[(T(X_{1},\\dots,X_{n})-\\mu_{t}+\\mu_{t}-g(\\theta))^{2}] \\\\ \\\\\n&= E_\\theta[((T(X_{1},\\dots,X_{n})- \\mu_{t})+ (\\mu_{t}-g(\\theta)))^{2}] \\\\\n& = E_\\theta[(T(X_{1},\\dots,X_{n})-\\mu_{t})^{2}+2(T(X_{1},\\dots,X_{n})-\\mu_{t})(\\mu_{t}g(\\theta))+(\\mu_{t}- g(\\theta))^{2}] \\\\\n&= \\overbrace{E_\\theta[(T(X_{1},\\dots,X_{n})-\\mu_{t})^{2}]}^{\\mathrm{Var}_{\\theta}(T(X_{1},\\dots,X_{n}))} + 2(\\mu_{t} -\ng(\\theta))\\cancelto{0}{E_\\theta(T(X_{1},\\dots,X_{n})-\\mu_{t})} + (\\mu_{t}-g(\\theta))^{2} \\\\\n&=\\mathrm{Var}_\\theta(T(X_{1},\\dots,X_{n})) + (\\mu_{t}-g(\\theta))^{2}\n\\end{aligned}\n\\]\nPortanto, \\[\n\\mathrm{EQM}(T,g(\\theta)) = \\mathrm{Var}_\\theta(T(X_{1},\\dots,X_{n})) + (\\mu_{t}-g(\\theta))^{2}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Erro quadrático médio (EQM)</span>"
    ]
  },
  {
    "objectID": "eqm-e-vies.html#viés",
    "href": "eqm-e-vies.html#viés",
    "title": "8  Erro quadrático médio (EQM)",
    "section": "8.2 Viés",
    "text": "8.2 Viés\nDenotamos de viés de \\(T(X_{1},\\dots,X_{n})\\) com respeito a \\(g(\\theta)\\) por \\[\n\\mathrm{Viés}(T,g(\\theta)) = E_\\theta(T(X_{1},\\dots,X_{n}))-g(\\theta),\\forall \\theta \\in \\Theta\n\\]\nDessa forma, temos que \\[\n\\mathrm{EQM}(T,g(\\theta)) = \\mathrm{Var}_\\theta(T(X_{1},\\dots,X_{n})) + [\\mathrm{Viés}(T,g(\\theta))]^{2}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Erro quadrático médio (EQM)</span>"
    ]
  },
  {
    "objectID": "eqm-e-vies.html#exemplo",
    "href": "eqm-e-vies.html#exemplo",
    "title": "8  Erro quadrático médio (EQM)",
    "section": "8.3 Exemplo",
    "text": "8.3 Exemplo\nSeja \\((X_{1},\\dots,X_{n})\\) uma amostra aleatória, ou seja, independentes e identicamente distribuídas (i.i.d.), de \\(X\\sim \\mathrm{Ber}(\\theta)\\) em que \\(\\theta \\in \\Theta = (0,1)\\). Calcule o viés e o EQM de \\(\\bar{X}_{n}\\) com respeito a \\(g(\\theta)=P_\\theta(X=1)\\)\nO estimador é, então, \\(T(X_{1},\\dots,X_{n})=\\bar{X}_{n}= \\frac{X_{1}+\\dots+X_{n}}{n}\\) para \\(g(\\theta)=P_\\theta(X=1)=\\theta\\) (pelo modelo de Bernoulli). \\[\n\\begin{aligned}\nE_\\theta(\\bar{X}_{n}) &= E_\\theta\\left(\\frac{1}{n}\\sum\\limits^{n}_{i=1}X_{i}\\right)=\n\\frac{1}{n}\\sum\\limits^{n}_{i=1}E_\\theta(X_{i}) \\stackrel{id. dist.}{\\Rightarrow} \\\\\nE_\\theta(\\bar{X}_{n}) &= \\frac{1}{n} \\sum\\limits^{n}_{i=1} E_\\theta(X), \\forall \\theta \\in \\Theta \\\\\n& = \\frac{n}{n} \\theta = \\theta, \\forall \\theta \\in \\Theta\n\\end{aligned}\n\\]\nPortanto, \\(\\bar{X}_\\theta\\) é não enviesado para \\(g(\\theta) = \\theta\\). \\[\n\\Rightarrow \\mathrm{Viés}(\\bar{X}_{n}, g(\\theta)) = 0, \\forall \\theta \\in \\Theta\n\\]\nPara o EQM, \\[\n\\begin{aligned}\n\\mathrm{EQM}(\\bar{X}_{n},g(\\theta)) &= \\mathrm{Var}_\\theta(\\bar{X}_{n}) - 0^{2} = \\mathrm{Var}_\\theta\n\\left(\\frac{1}{n}\\sum\\limits^{n}_{i=1}X_{i}\\right)= \\frac{1}{n^{2}}\\mathrm{Var}_\\theta\\left(\\sum\\limits^{n}_{i=1}X_{i}\\right)\\\\\n& \\stackrel{\\text{ind}}{=} \\frac{1}{n^{2}}\\sum\\limits ^{n}_{i=1}\\mathrm{Var}_\\theta(X_{i})\n\\stackrel{\\text{ind. dist.}}{=} \\frac{1}{n^{2}} \\sum\\limits^{n}_{i=1}\\mathrm{Var}_{\\theta}(X), \\forall \\theta \\in \\Theta \\\\\n&= \\frac{n \\theta(1-\\theta)}{n^{2}} = \\frac{\\theta(1-\\theta)}{n^{2}}, \\forall \\theta \\in \\Theta\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Erro quadrático médio (EQM)</span>"
    ]
  },
  {
    "objectID": "monte-carlo.html",
    "href": "monte-carlo.html",
    "title": "9  Simulações de Monte Carlo",
    "section": "",
    "text": "9.1 Método\nTem como objetivo replicar artificialmente os dados de um modelo estatístico para estudar o comportamento de estatísticas e estimadores (ou qualquer procedimento estatístico)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Simulações de Monte Carlo</span>"
    ]
  },
  {
    "objectID": "monte-carlo.html#método",
    "href": "monte-carlo.html#método",
    "title": "9  Simulações de Monte Carlo",
    "section": "",
    "text": "Defina o modelo estatístico: “Seja \\((X_{1},\\dots,X_{n})\\)” a.a. de \\(X\\sim f_{\\theta,}\\theta \\in \\Theta\\).\nEscolha \\(\\theta_{0} \\in \\Theta\\) e considere-o fixado daqui em diante.\nPara \\(n\\) fixado, gere (\\(x_{1}, x_{2},\\dots,x_{n}\\)) a amostra observada de \\(X\\sim f_{\\theta_{0}}\\)\nArmazene a amostra observada\nRepita 3. e 4. \\(N=10000\\) vezes",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Simulações de Monte Carlo</span>"
    ]
  },
  {
    "objectID": "emv.html",
    "href": "emv.html",
    "title": "10  Estimação via máxima verossimilhança (EMV)",
    "section": "",
    "text": "10.1 Exemplo\nO valor numérico \\(\\hat\\theta_{n}\\) que maximiza a função de verossimilhança, ou seja, \\(L_{\\stackrel{x}{\\sim}}(\\hat\\theta_{n}) \\geq L_{\\stackrel{x}{\\sim}}(\\theta)\\forall \\theta \\in \\Theta\\) é dito ser uma estimativa de máxima verossimilhança (MV) para \\(\\theta\\). Observe que \\(\\hat\\theta_{n}\\) depende da amostra observada e portanto: \\(\\hat\\theta_{n} = \\hat\\theta_(x_{1},x_{2},\\dots,x_{n})\\).\nO estimador de máxima verossimilhança é obtido substituindo \\((x_{1},\\dots,x_{n})\\) por \\((X_{1},\\dots,X_{n})\\), ou seja, \\(\\hat\\theta_{(X_{1},\\dots,X_{n})}\\) é o Estimador de Máxima Verossimilhança (EMV)\nSeja \\((X_{1},\\dots,X_{n})\\) amostra aleatória (a.a.) de \\(X\\sim f_{\\theta}, \\theta \\in \\{\\frac{1}{3}, \\frac{1}{2} \\}\\) em que \\(f_\\theta\\) é uma função de probabilidade que satisfaz:\n\\[\n\\begin{array}{|c|c|c|c|}\n\\hline\nX=x: & 0 & 1 & 2\\\\\n\\hline\nf_{\\theta}(x): & \\theta & \\theta^{2} & 1-\\theta-\\theta^{2}\\\\\n\\hline\n\\end{array}\n\\] Considere que a amostra observada é \\(\\stackrel{x}{\\sim}=(0,0,1)\\).\na-) Encontre a estimativa da máxima verossimilhança\nSabemos que \\[\nf_{\\theta}(x)= \\theta^{\\mathbb{1}_{\\{0\\}}(x)} \\cdot (\\theta^{2})^{\\mathbb{1}_{\\{1\\}}(x)} \\cdot\n(1-\\theta-\\theta^{2})^{\\mathbb{1}_{\\{2\\}}(x)} \\forall \\theta \\in \\Theta\n\\] portanto,\n\\[\nL_{\\stackrel{x}{\\sim}}(\\theta)\\stackrel{\\text{iid}}{=}\\prod^{n}_{i=1}f_{\\theta}(x_{i})  =\n\\theta^{\\sum\\limits^{n}_{i=1}\\mathbb{1}_{\\{0\\}}(x_{i})} \\cdot (\\theta^{2})^{\\sum\\limits^{n}_{i=1}\\mathbb{1}_{\\{1\\}}(x_{i})}\n\\cdot (1-\\theta-\\theta^{2})^{\\sum\\limits^{n}_{i=1}\\mathbb{1}_{\\{2\\}}(x_{i})} \\forall \\theta \\in \\Theta\n\\]\nPara \\(\\stackrel{x}{\\sim}=(0,0,1)\\), \\[\nL_{\\stackrel{x}{\\sim}}(\\theta) = \\theta^{2} \\cdot \\theta^{2} \\cdot (1-\\theta -\\theta^{2})^{0} = \\theta^{4}\n\\forall \\theta \\in \\Theta\n\\]\nSubstituindo \\(\\forall \\theta \\in \\Theta\\): \\[\n\\theta = \\frac{1}{2}\\Rightarrow L_{\\stackrel{x}{\\sim}}\\left(\\frac{1}{2}\\right)=\\frac{1}{16} ~~~~ \\theta =\n\\frac{1}{3}\\Rightarrow L_{\\stackrel{x}{\\sim}}\\left(\\frac{1}{3}\\right)=\\frac{1}{81}\n\\]\nPortanto, \\(\\hat\\theta_{n}=\\frac{1}{2}\\) é a estimativa de máxima verossimilhança.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Estimação via máxima verossimilhança (EMV)</span>"
    ]
  },
  {
    "objectID": "emv.html#exemplo",
    "href": "emv.html#exemplo",
    "title": "10  Estimação via máxima verossimilhança (EMV)",
    "section": "",
    "text": "10.1.1 Invariância dos EMVs\nTeorema. Se \\(\\hat\\theta_{(X_{1},\\dots,X_{n})}\\) for EMV para \\(\\theta\\), então \\(g(\\hat\\theta_{(X_{1},\\dots,X_{n})})\\) é o EMV para \\(g(\\theta)\\), ou seja, \\(g(\\hat\\theta_n)\\) é a estimativa de máxima verossimilhança para \\(g(\\theta)\\)\nMais um exemplo:\nSeja \\((X_{1},\\dots,X_{n})\\) a.a. de \\(X\\sim N(\\mu, \\sigma^2)\\) em que \\(\\theta = (\\mu, \\sigma^{2}) \\in \\Theta=\\mathbb{R}\\times \\mathbb{R}^{+}\\) Assuma que \\(\\stackrel{x}{\\sim} = (x_{1},\\dots,x_{n})\\) é a amostra observada. Lembrando que estaremos chamando \\(\\theta=(\\mu, \\sigma^{2})\\), mas estes são parâmetros genéricos. Poderíamos, por exemplo, chamá-los de \\(\\theta=(\\theta_{1},\\theta_{2})\\), o que pode facilitar a visualizar algumas derivadas.\na-) Encontre as estimativas de máxima verossimilhança para \\(\\theta = (\\mu, \\sigma^{2})\\):\nA Função de Verossimilhança é: \\[\n\\begin{aligned}\nL_{\\underset{\\sim}{x}}(\\theta)&\\stackrel{\\text{iid}}{=}\\prod^{n}_{i=1}f_\\theta(x_{i}) = \\prod^{n}_{i=1}\n\\left\\{ \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}}\\cdot \\exp\\left\\{\\frac{-1}{2}\\cdot\\frac{(x_{i}-\\mu)^{2}}{\\sigma^{2}} \\right\\} \\right\\}\\\\\n&= \\frac{1}{(2\\pi \\sigma^{2})^{\\frac{n}{2}}} \\cdot \\exp\\left\\{ \\frac{-1}{2\\sigma^{2}}\\sum\\limits^{n}_{i=1}(x_{i}-\\mu)^{2}\\right\\}\n\\end{aligned}\n\\]\nPodemos derivar para encontrar o máximo da FMV. Para isso, derivaremos e igualamos a zero primeiro em relação a \\(\\mu\\), então a \\(\\sigma^{2}\\) (podemos aplicar o logaritmo para facilitar as operações.)\n\\[\n\\begin{aligned}\n\\frac{\\partial\\ln(L_{\\underset{\\sim}{x}})}{\\partial \\mu} &= \\frac{1}{\\sigma^{2}}\\sum\\limits^{n}_{i=1}(x_{i}-\\mu) =0\n\\Rightarrow \\hat \\mu =\\frac{1}{n} \\sum\\limits^{n}_{i=1}x_{i} \\\\\n\\frac{\\partial \\ln(L_{\\underset{\\sim}{x}})}{\\partial \\sigma^{2}} &= - \\frac{n}{2\\sigma^{2}} + \\frac{1}{2\\sigma^{4}}\n\\sum\\limits^{n}_{i=1}(x_{i}-\\mu)^{2} =0 \\\\\n\\therefore \\\\\n\\mathrm{Estimativas~ MV} & =\n\\begin{cases}\n\\mu = \\bar{x} \\\\\n-\\frac{n}{2\\sigma^{2}} + \\frac{1}{2 \\sigma^{4}} \\sum\\limits^{n}_{i=1}(x_{i}-\\mu)^{2}=0 \\\\\n\\end{cases} \\\\\n&\\Leftrightarrow \\begin{cases}\n\\hat{\\mu}=\\bar{x} \\\\\n\\hat{\\sigma}^{2}= \\frac{1}{n}\\sum\\limits^{n}_{i=1}(x_{i}-\\bar{x})^{2}\n\\end{cases}\n\\end{aligned}\n\\]\nEstes são os pontos que maximizam a Função de Máxima Verossimilhança. (Provados em cálculo), ou seja, são as estimativas de máxima verossimilhança para \\(\\mu, \\sigma^{2}\\) respectivamente, e \\(\\hat{\\mu}(X_{1},\\dots,X_{n})=\\bar{X}, \\sigma^{2}(X_{1},\\dots,X_{n})=\\frac{1}{n}\\sum\\limits^{n}_{i=1}(X_{i}-\\bar{X})^{2}\\) são os estimadores de máxima verossimilhança.\nPela propriedade de invariância podemos encontrar o EMV para \\(g(\\theta)= \\frac{\\sqrt{\\mathrm{Var}_\\theta(X)}}{E_{\\theta(X)}}\\): \\[\n\\widehat{g(\\theta)} = \\frac{\\sqrt{\\frac{1}{n}\\sum\\limits^{n}_{i=1}(X_{i}-\\bar{X})}}{\\bar{X}}\n\\]\nObservação: Seja \\((X_{1},\\dots,X_{n})\\) a.a. de \\(X\\sim N(\\mu, \\sigma^{2})\\). Então, 1. \\(\\bar{X} \\underset{\\text{Exata!}}{\\sim}N\\left(\\mu, \\frac{\\sigma^{2}}{n}\\right)\\forall \\mu, \\sigma \\in \\mathbb{R} : \\sigma^{2}&gt;0 \\text{ e } n\\geq 1\\) 2. \\(\\sum\\limits^{n}_{i=1} \\frac{(x_{1}-\\bar{X})^{2}}{\\sigma^{2}}\\underset{\\text{Exata!}}{\\sim}\\chi^{2}_{(n-1)}\\) em que \\(\\chi^{2}_{k}\\) representa a Distribuição Qui-Quadrado com \\(k\\) grau de liberdade, cuja função densidade de probabilidade é: \\[\nf(x) = \\frac{1}{\\Gamma(\\frac{k}{2})2^{\\frac{k}{2}}} \\cdot x^{\\frac{k}{2}-1} \\cdot \\exp\\left\\{\\frac{-x}{2}\\right\\} \\cdot \\mathbb{1}_{(0, \\infty)}(x)\n\\]\nPara qualquer outra distribuição, existe um resultado aproximado pelo Teorema do Limite Central",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Estimação via máxima verossimilhança (EMV)</span>"
    ]
  },
  {
    "objectID": "qui-quadrado.html",
    "href": "qui-quadrado.html",
    "title": "11  Distribuição Qui-Quadrado",
    "section": "",
    "text": "Se \\(Z\\sim N(0,1)\\), então \\(Z^{2}\\sim \\chi^2\\), como provado por transformação de variáveis aleatórias\nSe \\(W\\sim \\chi^{2}_{k}\\), então sua função densidade de probabilidade é dada por \\[\n\\begin{cases}\n\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)\\cdot 2 ^{\\frac{k}{2}}}\\cdot w^{\\frac{k}{2} -1} \\cdot e^{-\\frac{1}{2}w}, ~ ~~ w &gt; 0 \\\\\n0, cc\n\\end{cases}\n\\] Sendo assim, \\[\n\\begin{cases}\nE(W) = k \\\\\n\\mathrm{Var}(W) = 2k\n\\end{cases}\n\\] Se \\(Z_{1},Z_{2,}\\dots,Z_{N}\\stackrel{\\text{iid}}{\\sim} N(0,1)\\), então \\[Z_{1}^{2}+\\dots+Z_{n}^{2}\\sim \\chi^{2}_{n}\\] Prova por função característica\nSe \\(X_{1},\\dots,X_{n} \\stackrel{\\text{iid}}{\\sim}N(\\mu,\\sigma^{2}),\\) então \\[\n\\frac{(X_{1}-\\mu)^{2}+\\dots+(X_{n}-\\mu)^{2}}{\\sigma^{2}} \\sim \\chi^{2}_{n}\n\\] Se \\(X_{1},\\dots,X_{n} \\stackrel{\\text{iid}}{\\sim}N(\\mu,\\sigma^{2}),\\) então \\[\n\\frac{(X_{1}-\\bar{X})^{2}+\\dots+(X_{n}-\\bar{X})^{2}}{\\sigma^{2}} \\sim \\chi^{2}_{n-1}\n\\]\nAdemais, se \\(Y\\sim \\chi^2_{\\nu}\\), então \\[\n\\frac{Y-\\nu}{\\sqrt{2\\nu}} \\stackrel{a}{\\approx} N(0,1)\n\\] para \\(\\nu &gt; 30\\)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Distribuição Qui-Quadrado</span>"
    ]
  },
  {
    "objectID": "tlc.html",
    "href": "tlc.html",
    "title": "12  Teorema do Limite Central",
    "section": "",
    "text": "12.1 Exemplo\nTambém conhecido como Teorema Central do Limite, é fundamental para a teoria da probabilidade e a estatística.\nSeja \\(X_{1},\\dots,X_{n}\\) a.a de \\(X\\sim f_{\\theta},\\theta \\in \\Theta : E_{\\theta}(X^{2}) &lt; \\infty\\), então: \\[\n\\bar{X} \\stackrel{\\text{Aproximadamente}}{\\sim }N\\left(E_\\theta(X), \\frac{Var_{\\theta}{X}}{n}\\right)\n\\]\nFormalmente, temos o enunciado do Teorema do Limite Central: \\[\n\\frac{\\sqrt{n}(\\bar{X}-E_\\theta(x))}{\\sqrt{\\mathrm{Var}_\\theta(X)}} \\underset{n\\rightarrow \\infty}{\\stackrel{\\text{Distribuição}}{\\rightarrow}} N\\sim (0,1) \\forall \\theta \\in \\Theta\n\\]\nSe \\(X\\sim N(\\mu, \\sigma)\\), então a distribuição é exata.\nAdemais, seja \\(g\\) uma função contínua e diferenciável tal que \\(g'(\\theta)\\neq0\\). Então, \\[\ng(\\bar{X}) \\stackrel{\\text{Aproximadamente}}{\\sim}N\\left(g(E_\\theta(X)), \\frac{g'(E_\\theta(X))^{2}\\mathrm{Var}_\\theta(X)}{n}\\right)\n\\]\nSeja \\((X_{1},\\dots,X_{n})\\) a.a de \\(X\\sim Ber(\\theta), \\theta \\in (0,1)\\). Já vimos que EMV \\(p/ \\theta\\) é \\[\n\\bar{\\theta}(X_{1},\\dots,X_{n}) = \\bar{X}\n\\]\ne o EMV p/ \\(g(\\theta) = \\mathrm{Var}_{\\theta(x)}= \\theta(1-\\theta)\\) é: \\[\n\\widehat{g(\\theta)} = \\bar{X}(1-\\bar{X}).\n\\]\nAgora, \\[\n\\begin{aligned}\n\\bar{X} &\\stackrel{\\text{approx.}}{\\sim} N\\left(\\theta, \\frac{\\theta(1-\\theta)}{n}\\right)\\\\\ng(\\bar{X}) = \\bar{X}(1-\\bar{X}) & \\stackrel{\\text{approx}}{\\sim } N\\left(\\theta(1-\\theta),\n\\frac{[g'(\\theta)]^{2} \\theta(1-\\theta)}{n}\\right) \\\\\n&~~~\\Rightarrow~N\\left(\\theta(1-\\theta), \\frac{(1-2\\theta)^{2}\\theta(1-\\theta)}{n}\\right)\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Teorema do Limite Central</span>"
    ]
  },
  {
    "objectID": "estimador-intervalar.html",
    "href": "estimador-intervalar.html",
    "title": "13  Intervalo de Confiança ou Estimador Intervalar",
    "section": "",
    "text": "13.1 IC sob normalidade para \\(\\mu\\) com \\(\\sigma^2\\) conhecido\nSeja \\((X_1,\\ldots, X_n)\\) a.a. de \\(X\\sim f_\\theta, \\theta \\in \\Theta \\subseteq \\mathbb{R}\\).\nUm intervalo de confiança (IC) com coeficiente de confiança \\(\\gamma\\) é um intervalo aleatório que satisfaz: \\[\nP_{\\theta} (I_{1}(\\pmb{X}_{n}) \\leq \\theta \\leq I_{2} (\\pmb{X}_{n})) = \\gamma \\forall \\theta \\in \\Theta\n\\]\nem que \\(\\pmb{X}_{n} = (X_{1}\\ldots,X_{n})\\)\nObservação:\nSe o intervalo aleatório \\([I_{1} (\\pmb{X}_{n}),I_{2}(\\pmb{X}_{n})\\) satisfaz \\[\nP_{\\theta} (I_{1} (\\pmb{X}_{n}) \\leq \\theta \\leq I_{2} (\\pmb{X}_{n})) \\geq \\gamma \\forall \\theta \\in \\Theta\n\\]\nEntão o intervalo aleatório é um intervalo de confiança com pelo menos coeficiente de confiança \\(\\gamma\\)\nObservação: \\(I_{1} (\\pmb{X}_{n})\\) e \\(I_{2} (\\pmb{X}_{n})\\) são estatísticas e são tais que \\(I_{1} (\\pmb{X}_{n}) \\leq I_{2} (\\pmb{X}_{n})\\)\nObservação: Quando substituímos \\((\\pmb{X}_{n})\\) pela amostra observada \\((\\pmb{x}_{n})\\) temos que\n\\[\nP_{\\theta}(I_{1} (\\pmb{x}_{n}) \\leq \\theta \\leq I_{2} (\\pmb{x}_{n})) = \\begin{cases}\n1,  \\text{ se } \\theta \\in [I_{1}(\\pmb{x}_{n}), I_{2} (\\pmb{x}_{n})] \\\\\n0, \\text{ caso contrário }\n\\end{cases}\n\\]\nEm uma distribuição normal \\((\\theta, \\sigma^2)\\), por exemplo, conseguimos de forma genérica para qualquer \\(\\gamma \\in [0,1]\\) encontrar pela tabela um valor de \\(c_{\\gamma}\\) que satisfaça \\(P(-c_{\\gamma} \\leq Z \\leq c_{\\gamma})\\). Assim, \\[\n-c_{\\gamma} \\leq Z \\leq c_{\\gamma} \\Leftrightarrow -c_{\\gamma} \\leq \\frac{\\sqrt{n}(\\bar{X}-\\theta)}{\\sqrt{\\sigma^{2}}}\n\\leq c_{\\gamma} \\Leftrightarrow \\bar{X} - c_{\\gamma} \\frac{\\sqrt{\\sigma^2}}{\\sqrt{n}} \\leq \\theta \\leq \\bar{X} + c_{\\gamma}\n\\frac{\\sqrt{\\sigma^2}}{\\sqrt{n}}\n\\]\nPortanto, \\[\nP_{\\theta}\\left(\\bar{X} - c_{\\gamma} \\frac{\\sqrt{\\sigma^2}}{\\sqrt{n}} \\leq \\theta \\leq \\bar{X} + c_{\\gamma}\n\\frac{\\sqrt{\\sigma^2}}{\\sqrt{n}}\\right) = \\gamma \\forall \\theta \\in \\Theta\n\\]\nDessa forma, \\(\\bar{X} - c_{\\gamma} \\frac{\\sqrt{\\sigma^2}}{\\sqrt{n}} \\leq \\theta \\leq \\bar{X} + c_{\\gamma}\n\\frac{\\sqrt{\\sigma^2}}{\\sqrt{n}}\\) é um intervalo de confiança cujo coeficiente de confiança é \\(\\gamma\\).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intervalo de Confiança ou Estimador Intervalar</span>"
    ]
  },
  {
    "objectID": "estimador-intervalar.html#ic-sob-normalidade-para-mu-com-sigma2-conhecido",
    "href": "estimador-intervalar.html#ic-sob-normalidade-para-mu-com-sigma2-conhecido",
    "title": "13  Intervalo de Confiança ou Estimador Intervalar",
    "section": "",
    "text": "13.1.1 Exemplo\nConsidere que a amostra observada foi \\(1, 2.2, 3, 3.5\\) de uma distribuição \\(N(\\theta, 3)\\).\nEncontre o IC observado com coeficiente de confiança de \\(99\\%\\).\nPrimeiro encontramos a média: \\(\\bar{x} = 2.42\\).\nDessa forma, \\(c_{99\\%} = 2.58\\) e nosso intervalo de confiança é \\[\n\\left[2.42 - 2.58 \\frac{\\sqrt{3}}{\\sqrt{4}}, 2.42 + 2.58\\frac{\\sqrt{3}}{\\sqrt{4}}\\right] = [0.18, 4.65]\n\\]\nDessa forma, se repetirmos o experimento \\(N\\) vezes, esperamos que \\(\\gamma = 99\\%\\) dos ICs observados contenham a quantidade de interesse.\n\n13.1.1.1 Notação\n\\[\n\\mathrm{IC}(\\theta, \\gamma) = [I_{1} (\\pmb{X}_{n}), I_{2} (\\pmb{X}_{n})]\n\\] Denotará o intervalo de confiança teórico\n\\[\n\\mathrm{IC}_{\\mathrm{Obs}}(\\theta, \\gamma) = [I_{1} (\\pmb{x}_{n}), I_{2} (\\pmb{x}_{n})]\n\\] ## IC sob normalidade para \\(\\mu\\) quando \\(\\sigma^2\\) é desconhecido\nSeja uma a.a de \\(X\\sim N(\\mu, \\sigma^2)\\) em que \\(\\mu, \\sigma^2\\) são desconhecidos. Então, o IC para \\(\\mu\\) com coeficiente de confiança \\(\\gamma\\) é dado por\n\\[\n\\mathrm{IC}(\\mu,\\gamma) = \\left[\\bar{X} - t_{\\gamma, n-1} \\sqrt{\\frac{s^2 (\\pmb{X}_{n})}{n}}, \\bar{X} +t_{\\gamma, n-1}\n\\sqrt{\\frac{s^2 (\\pmb{X}_{n})}{n}}\\right]\n\\]\nEm que \\(s^2 (\\pmb{x}_{n}) = \\frac{1}{n-1}\\sum^{k}_{i=1}(X_i - \\bar{x})^2\\) e \\(t_{y,(n-1)}\\) deve ser calculado da tabela \\(P(-t_{\\gamma, n-1} \\leq T_{n-1} \\leq t_{\\gamma, n-1}) = \\gamma\\), \\(T_{n-1} \\sim \\mathrm{t-Student}(n-1)\\)\nSe a variância for desconhecida, substituímos \\(\\sigma^2\\) pelo estimador \\[\ns^2 = \\frac{1}{n-1}\\sum\\limits^{n}_{i=1} (X_{i} - \\bar{X})^2\n\\] e o valor de \\(c_{\\gamma}\\) obtido de uma t-Student com \\(n-1\\) graus de liberdade.\nJustificativa:\nSe uma a.a. de \\(X\\sim N(\\mu,\\sigma^2)\\), \\(\\theta = (\\mu, \\sigma^2) \\in \\mathbb{R} \\times\\mathbb{R_{+}}\\).\n\n\\(\\bar{X} \\sim N (\\mu, \\frac{\\sigma^2}{n})\\)\n\\(\\sqrt{n}\\frac{\\bar{X} - \\mu}{\\sqrt{\\sigma^2}}\\sim N(0,1)\\)\n\\(\\sum\\limits^{n}_{i=1}\\frac{(X_{i} - \\bar{X})^2}{\\sqrt{\\sigma^2}} \\sim \\chi^2_{n-1}\\)\n\\(\\sqrt{n}\\frac{\\bar{X} - \\mu}{\\sqrt{s^2}} = \\sqrt{n}\\frac{\\bar{X} - \\mu}{\\sqrt{s^2}} \\cdot\n\\frac{\\sqrt{\\sigma^2}}{\\sqrt{\\sigma^2}}  = \\frac{\\sqrt{n}\\frac{\\bar{X} - \\mu}{\\sqrt{\\sigma^2}}}\n{\\sqrt{\\frac{\\sum(X_i - \\bar{X})^2}{\\sigma^2} \\cdot \\frac{1}{n-1}}}\\)\nSe \\(Z \\sim N(0,1)\\) e \\(W \\sim \\chi^{2}_{k}\\), então \\(t = \\frac{Z}{\\sqrt{\\frac{W}{k}}} \\sim t_{k}\\)\n\\(\\sqrt{n} \\frac{(\\bar{X} - \\mu)}{\\sqrt{s^2}} \\sim t_{(n-1)}\\)\n\n\n\n\n13.1.2 Exercício\nConsidere uma a.a de \\(X\\sim N(\\mu, \\sigma^2)\\), ambos parâmetros desconhecidos, em que \\(X\\) é o retorno de um ativo específico. Considere que a seguinte amostra foi observada: \\[\n(-1.3\\%, 0.4\\%, -1.7\\%, 3.2\\%, 0.7\\%, -1.6\\%, 1.0\\%, 1.5\\%, 1.2\\%, -0.6\\%)\n\\]\nEncontre o IC para \\(\\mu\\), com coeficiente de confiança \\(\\gamma = 99\\%\\)\nTemos que \\(s^2(\\pmb{x}_{n}) = 2.48\\) e \\(\\bar{x} = 0.28\\). Da tabela, \\(t_{99\\%, 9} = 3.25\\)\nSendo assim,\n\\[\n\\mathrm{IC}_{\\mathrm{Obs}}(\\mu,99\\%) = \\left[0.28 - 3.25 \\sqrt{\\frac{2.48}{10}}, 0.28 + 3.25\\sqrt{\\frac{2.48}{10}}\\right] = [-1.34, 1.9]\n\\]",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intervalo de Confiança ou Estimador Intervalar</span>"
    ]
  },
  {
    "objectID": "estimador-intervalar.html#ic-sob-normalidade-para-sigma2-com-mu-desconhecido",
    "href": "estimador-intervalar.html#ic-sob-normalidade-para-sigma2-com-mu-desconhecido",
    "title": "13  Intervalo de Confiança ou Estimador Intervalar",
    "section": "13.2 IC sob normalidade para \\(\\sigma^2\\) com \\(\\mu\\) desconhecido",
    "text": "13.2 IC sob normalidade para \\(\\sigma^2\\) com \\(\\mu\\) desconhecido\nSeja uma a.a. de \\(X \\sim N(\\mu, \\sigma^2)\\) em que \\(\\mu, \\sigma^2\\) são desconhecidos.\nO intervalo de confiança para \\(\\sigma^2\\) com coeficiente de confiança \\(\\gamma\\) é dado por:\n\\[\n\\begin{aligned}\n\\mathrm{IC} (\\sigma^2, \\gamma) = \\left [\\frac{(n-1)s^2(\\pmb{X}_{n})}{q^{(2)}_{\\gamma, n-1}},\n\\frac{(n-1)s^2(\\pmb{X}_{n})}{q^{(1)}_{\\gamma, n-1}}\\right] \\\\\n\\mathrm{IC}_{\\mathrm{Obs}} (\\sigma^2, \\gamma) = \\left [\\frac{(n-1)s^2(\\pmb{x}_{n})}{q^{(2)}_{\\gamma, n-1}},\n\\frac{(n-1)s^2(\\pmb{x}_{n})}{q^{(1)}_{\\gamma, n-1}}\\right]\n\\end{aligned}\n\\]\nem que \\(s^2(\\pmb{X}_{n}) = \\frac{1}{n-1} \\sum^{n}_{i=1} (X_i - \\bar{X})^2\\) e \\(s^2(\\pmb{x}_{n}) =\n\\frac{1}{n-1} \\sum^{n}_{i=1} (x_i - \\bar{x})^2\\) E \\(q^{(1)}_{\\gamma, n-1}, q^{(2)}_{\\gamma, n-1}\\) são obtidos calculando \\(P(q^{(1)}_{\\gamma, n-1} \\leq W \\leq q^{(2)}_{\\gamma, n-1}) = \\gamma\\) no qual \\(W\\sim \\chi^{2}_{n-1}\\) e \\(P(\\chi^2_{n-1} \\leq q^{(1)}_{\\gamma, n-1}) = \\frac{1-\\gamma}{2} = P( \\chi^2_{n-1} \\geq q^{(2)}_{\\gamma,n-1})\\)\nDemonstração:\nVamos construir um IC para a variância.\nNote que \\[\\sum\\limits^{n}_{i=1}\\frac{(X_{i} - \\bar{X})^2}{\\sqrt{\\sigma^2}} \\sim \\chi^2_{n-1}\\]\nNote ainda que \\(W \\sim \\chi^{2}_{n-1}\\), ou seja, \\(P(q^{(1)}_{\\gamma, n-1} \\leq W \\leq q^{(2)}_{\\gamma, n-1}) = \\gamma\\). Assim, \\(W = \\frac{(n-1)s^2}{\\sigma^2}\\).\nDessa forma,\n\\[\n\\frac{1}{q^{(2)}_{\\gamma, n-1}} \\leq \\frac{\\sigma^2}{(n-1) s^2} \\leq \\frac{1}{q^{(1)}_{\\gamma, n-1}} \\Leftrightarrow\n\\frac{(n-1)s^2}{q^{(2)}_{\\gamma, n-1}} \\leq \\sigma^2 \\leq \\frac{(n-1)s^2}{q^{(1)}_{\\gamma, n-1}}\n\\]\nem que \\(s^2 = \\frac{1}{n-1} \\sum^{n}_{i=1} (X_i - \\bar{X})^2\\)\nPortanto, \\[ P_{\\theta}\\left (\\frac{(n-1)s^2}{q^{(2)}_{\\gamma, n-1}} \\leq \\sigma^2 \\leq \\frac{(n-1)s^2}{q^{(1)}_{\\gamma, n-1}}\\right)\n= \\gamma \\forall \\theta \\in \\Theta\\]\nExercício:\nConsidere uma a.a de \\(X\\sim N(0, \\theta), \\theta &gt; 0\\) em que \\(X\\) é o retorno de um ativo específico. Considere que a seguinte amostra foi observada: \\[\n(-1.3\\%, 0.4\\%, -1.7\\%, 3.2\\%, 0.7\\%, -1.6\\%, 1.0\\%, 1.5\\%, 1.2\\%, -0.6\\%)\n\\]\nConstrua um intervalo de confiança para a variância (populacional) \\(\\theta\\) com coeficiente de confiança \\(\\gamma = 95\\%\\).\nO IC para a variância é \\[\n\\mathrm{IC}(\\theta, \\gamma) = \\left[\\frac{(n-1)s^2 (\\pmb{x}_{n})}{q^2_{\\gamma,n-1}},\\frac{(n-1)s^2 (\\pmb{x}_{n})}{q^1_{\\gamma,n-1}}\\right]\n\\]\nem que \\(s^2 (\\pmb{x}_{n}) = \\frac{1}{n-1}\\sum^{k}_{i=1}(X_i - \\bar{x})^2\\) e \\(q^{1}_{\\gamma, n-1}, q^{2}_{\\gamma, n-1}\\) satisfazem as fronteiras que delimitam uma área de \\(\\gamma\\) em torno da média. Nesse caso, como \\(n = 10\\), \\(q^{1}_{9} = 2.7, q^{2}_{9} = 19.023\\). Dessa forma, \\(\\bar{x} = 0.28, \\sum_{i}x_{i}^2=2.308 \\Rightarrow s^2(\\pmb{x}_{n}) = \\frac{10}{9}(2.308 - 0.28^2) = 2.48\\)\nSendo assim \\[\n\\mathrm{IC}_{\\mathrm{Obs}}(\\theta, 95\\%) = \\left[\\frac{9 \\cdot 2.48}{19.023}, \\frac{9 \\cdot 2.48}{2.7}\\right] = [1.17, 8.27]\n\\]\nPodemos concluir que, com uma confiança de \\(95\\%\\), a variância está nesse intervalo.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intervalo de Confiança ou Estimador Intervalar</span>"
    ]
  },
  {
    "objectID": "estimador-intervalar.html#intervalo-de-confiança-para-a-proporção",
    "href": "estimador-intervalar.html#intervalo-de-confiança-para-a-proporção",
    "title": "13  Intervalo de Confiança ou Estimador Intervalar",
    "section": "13.3 Intervalo de Confiança para a proporção",
    "text": "13.3 Intervalo de Confiança para a proporção\nSeja \\((\\pmb{x}_{n})\\)uma amostra aleatória de \\(X\\sim Ber(\\theta)\\) em que \\(\\theta \\in [0,1]\\). Sabemos que, pelo Teorema do Limite Central\n\\[\n\\bar{X} \\stackrel{a}{\\approx} N\\left( \\theta, \\frac{\\theta (1-\\theta)}{n} \\right)\n\\]\nDessa forma, \\[\n\\frac{\\sqrt{ n }(\\bar{X}-\\theta)}{\\sqrt{ \\theta(1-\\theta) }} \\stackrel{a}{\\approx} N(0,1)\n\\]\nAlém disso, pelo Teorema de Slutsky \\[\n\\frac{\\sqrt{ n }(\\bar{X}-\\theta)}{\\sqrt{ \\bar{X}(1-\\bar{X}) }} \\stackrel{a}{\\approx} N(0,1)\n\\]\nObserve que, se \\(Z \\sim N(0,1)\\) então \\(-c_{\\gamma}\\leq Z \\leq c_{\\gamma} \\Leftrightarrow -c_{\\gamma }\n\\leq \\frac{\\sqrt{ n }(\\bar{X}-\\theta)}{\\sqrt{ \\bar{X}(1-\\bar{X}) }}\\leq c_{\\gamma}\\) \\[\n\\Leftrightarrow \\bar{X} - c_{\\gamma} \\sqrt{ \\frac{\\bar{X} (1-\\bar{X})}{n} } \\leq \\theta \\leq \\bar{X} + c_{\\gamma}\n\\sqrt{\\frac{\\bar{X} (1-\\bar{X})}{n}}\n\\]\nEntão, seja \\(c_{\\gamma}\\) tal que \\[\n\\begin{aligned}\n&P (-c_{\\gamma}\\leq N(0,1)\\leq c_{\\gamma }) = \\gamma \\\\\n\\Rightarrow &P_{\\theta}\\left(\\bar{X} - c_{\\gamma} \\sqrt{ \\frac{\\bar{X} (1-\\bar{X})}{n} } \\leq \\theta \\leq\n\\bar{X} + c_{\\gamma} \\sqrt{ \\frac{\\bar{X} (1-\\bar{X})}{n} }\\right) \\approx \\gamma~ \\forall \\theta \\in \\Theta = [0,1]\n\\end{aligned}\n\\]\nque melhora conforme \\(n \\rightarrow \\infty\\)\nLogo, um IC aproximado com coeficiente de confiança \\(\\gamma\\) é dado por \\[\nIC(\\theta, \\gamma) = \\left[\\bar{X} - c_{\\gamma} \\sqrt{ \\frac{\\bar{X} (1-\\bar{X})}{n} }, \\bar{X} + c_{\\gamma}\n\\sqrt{ \\frac{\\bar{X} (1-\\bar{X})}{n} }\\right] \\cap \\Theta\n\\]\n\n13.3.1 Exemplo\nSeja \\((\\pmb x_{n })\\) uma amostra aleatória de \\(X\\sim \\mathrm{Ber} (\\theta)\\) em que \\(\\theta \\in [0,1]\\). \\[\nX(w) = \\begin{cases}\n1, \\text{ se w disser que vota no candidato} \\\\ \\\\\n0, c.c\n\\end{cases}\n\\] A amostra observada foi \\((0,0,0,1,0,0,0,1)\\). Encontre o IC para a proporção de intenção de votos no candidato considerando \\(\\gamma = 99\\%\\)\n\\(\\bar{x} = \\frac{1}{4 }, \\bar{x}(1-\\bar{x}) = \\frac{3}{16}, n = 8, c_{\\gamma} = 2.58\\)\n\\[\n\\begin{aligned}\n\\mathrm{IC}_{\\mathrm{Obs}}(\\theta, 99\\%) &= \\left[0.25- 2.58 \\frac{\\sqrt{3}}{4\\sqrt{8}}, 0.25 - 2.58\n\\frac{\\sqrt{3}}{4 \\sqrt{8}}\\right] \\cap [0,1] \\\\\n&= [0.25 - 0.39, 0.25 + 0.39] \\cap [0,1]  \\\\\n&= [0,0.64]\n\\end{aligned}\n\\]\n\n\n13.3.2 Intervalos conservadores e otimistas\nUm intervalo de confiança de proporção é dito ser conservador quando o calculamos tomando \\(\\theta\\) Na variância (\\(\\theta(1-\\theta)\\)) como o valor mais alto possível. No exemplo Bernoulli, o valor máximo para \\(\\theta\\) (derivando \\(\\theta(1-\\theta)\\) e igualando a 0, \\(1-2 \\theta = 0\\)) é \\(\\theta=0.5\\). Dessa forma, o IC conservador é calculado usando \\(\\theta=0.5\\).\nPor sua vez, um IC otimista é calculado usando o valor de \\(\\theta\\) obtido através do EMV para \\(\\theta\\), no caso Bernoulli, usaríamos essa variância como \\(\\bar{X}(1-\\bar{X})\\)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intervalo de Confiança ou Estimador Intervalar</span>"
    ]
  },
  {
    "objectID": "estimador-intervalar.html#como-interpretar-intervalos-de-confiança",
    "href": "estimador-intervalar.html#como-interpretar-intervalos-de-confiança",
    "title": "13  Intervalo de Confiança ou Estimador Intervalar",
    "section": "13.4 Como interpretar intervalos de confiança",
    "text": "13.4 Como interpretar intervalos de confiança\nImportante:\nNa estatística clássica (frequentista), devemos interpretar um intervalo de confiança \\([a,b]\\) com \\(\\gamma=0.95\\) da seguinte forma:\n“Com \\(95\\%\\) de confiança, o intervalo \\([a,b]\\) conterá o valor da quantidade de interesse”.\nIsso é importante para diferenciar a interpretação frequentista (Theta do espaço paramétrico) da Bayesiana (Theta como variável aleatória). Dessa forma, estaria incorreto na estatística clássica dizer que\n“O intervalo \\([a,b]\\) conterá a quantidade de interesse com probabilidade \\(95\\%\\)” ou “O intervalo \\([a,b]\\) conterá a quantidade de interesse \\(95\\%\\) das vezes”",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intervalo de Confiança ou Estimador Intervalar</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html",
    "href": "teste-hipotese.html",
    "title": "14  Teste de Hipótese simples",
    "section": "",
    "text": "14.1 Etapas de um teste de hipótese\nUm dos principais objetivos da estática é testar hipóteses. Veja algumas dessas hipóteses potenciais: 1. A moeda é honesta? 2. O medicamento proposto é melhor que o vendido no mercado? 3. O número médio de acidentes aumentou em relação ao ano passado? 4. A altura interfere na performance num determinado esporte? 5. Um suspeito é culpado? 6. O mercado financeiro está em equilíbrio? 7. Dona Maria terá dinheiro para comprar o pão do próximo mês?",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#etapas-de-um-teste-de-hipótese",
    "href": "teste-hipotese.html#etapas-de-um-teste-de-hipótese",
    "title": "14  Teste de Hipótese simples",
    "section": "",
    "text": "Formular as hipóteses de interesse;\n\nNa estatística clássica, pela abordagem Fisheriana (uma hipótese) ou Neyman-Pearson (mais de uma hipótese).\n\nObservar dados experimentais do estudo relacionado ao problema;\nElaborar uma conclusão utilizando um procedimento estatístico.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#exemplo-da-hipótese-5.",
    "href": "teste-hipotese.html#exemplo-da-hipótese-5.",
    "title": "14  Teste de Hipótese simples",
    "section": "14.2 Exemplo da hipótese 5.",
    "text": "14.2 Exemplo da hipótese 5.\nConsidere uma pessoa que está sendo acusada de ter cometido um crime.\nAs duas hipóteses envolvidas aqui são (abordagem de Neyman-Pearson): 1. “O suspeito não é culpado” -&gt; \\(h_{0}\\) Hipótese Nula ou de não-efeito; 2. “O suspeito é culpado” -&gt; \\(h_{1}\\) Hipótese alternativa ou hipótese que contém o efeito.\n\nApós coletar as evidências, dizemos que, se houver evidências de que o suspeito cometeu o crime, a pessoa é culpada.\nSe não, concluímos que não é culpado.\n\nContudo, devemos nos atentar aos erros de decisão\n\n14.2.1 Erros de decisão\n\\[\n\\begin{array}{c|cc}\n& H_{0} & H_{1} \\\\\n\\hline\n\\text{Decisão}  & \\text{Não cometeu o crime}  & \\text{Cometeu o crime}\\\\\n\\hline\n\\text{Inocente} & \\text{Acerto} & \\text{Erro Tipo II}\\\\\n\\text{Culpado} & \\text{Erro Tipo I} & \\text{Acerto}\\\\\n\\hline\n\\end{array}\n\\] - Erro Tipo I: Decidir que o acusado é culpado quando na verdade é inocente (Rejeitar \\(H_{0}\\)). - Erro Tipo II: Decidir que o acusado é inocente quando na verdade é culpado (Rejeitar \\(H_{1}\\)).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#exemplo-da-hipótese-1.",
    "href": "teste-hipotese.html#exemplo-da-hipótese-1.",
    "title": "14  Teste de Hipótese simples",
    "section": "14.3 Exemplo da hipótese 1.",
    "text": "14.3 Exemplo da hipótese 1.\nEstamos interessados em verificar se uma moeda é honesta. Executaremos \\(n\\) experimentos de Bernoulli e verificaremos se a face voltada para cima após o lançamento é cara. Dessa forma, sendo \\(X\\) o resultado dum lançamento, teremos a a.a \\((\\pmb{X}_{n})\\) de \\(X\\sim \\mathrm{Ber}(\\theta), \\theta \\in \\Theta = [0,1]\\). Suspeitamos que a moeda é honesta ou que \\(\\theta = 0.9\\).\nNossas hipóteses são 1. \\(H_{0}\\) -&gt; \\(\\theta = 0.5\\) 2. \\(H_{1}\\) -&gt; \\(\\theta = 0.9\\)\n\n14.3.1 Erros Tipo I e II\nNote que \\(\\bar{X}\\) é um estimador para \\(\\theta\\) e \\(\\bar{x}\\) é uma estimativa. - Se \\(\\bar{x}&gt;0.7\\), rejeitaremos a hipótese nula \\(h_{0}\\), a moeda não seria honesta e haveria um viés agindo sobre seus lançamentos. - Se \\(\\bar{x}\\leq 0.7\\), concluiremos que a moeda é honesta. \\[\n\\begin{array}{c|cc}\n& H_{0} & H_{1} \\\\\n\\hline\n\\text{Decisão}  & \\text{Honesta} & \\text{Viesada}\\\\\n\\hline\n\\bar{x} &lt; 0.7 & \\text{Acerto} & \\text{Erro Tipo II}\\\\\n\\bar{x} \\geq 0.7& \\text{Erro Tipo I} & \\text{Acerto}\\\\\n\\hline\n\\end{array}\n\\]\n\nErro Tipo I: Rejeitar que a moeda é honesta (rejeitar \\(h_{0}\\)) quando na verdade é.\nErro Tipo II: Rejeitar que a moeda é enviesada (rejeitar \\(h_{1}\\)) quando na verdade é.\n\nCalculando a probabilidade dos erros \\[\n\\begin{aligned}\nP(\\text{Erro Tipo I}) &= P(\\text{Probabilidade de rejeitar $h_{0}$}|\\text{$h_{0}$ é verdadeiro}) \\\\\n\\text{Incorreto na Estatística Clássica} &= P(\\bar{X} &gt; 0.7 | \\theta = 0.5) \\\\\n\\text{Correto na Estatística Clássica} & = P_{0.5}(\\bar{X} &gt; 0.7)\n\\end{aligned}\n\\]\nNa segunda notação, correta na estatística frequentista, \\(P\\) está sob a hipótese nula \\(h_{0}\\) verdadeira. Dessa forma \\[\nP(\\text{Erro Tipo II}) = P_{0.9}(\\bar{X}\\leq 0.7)\n\\]\nCalcule as probabilidades dos erros considerando \\(n=10\\) e a aproximação pela distribuição normal.\n\n14.3.1.1 Calculando Exato e pela aproximação do Teorema do Limite Central\nSabemos que \\(\\sum ^{n}_{i=1}X_{i}\\sim \\mathrm{Bin}(n, \\theta)\\), logo \\[\n\\begin{aligned}\n\\alpha &= P(\\text{Erro Tipo I}) \\stackrel{\\text{Sob $h_{0}$}}{=} P_{0.5}\\left( \\frac{1}{n} \\sum^n_{i=1}X_{i}&gt; 0.7\\right)\n= P_{0.5}\\left( \\sum^{10}_{i=1}X_{i} &gt; 7 \\right) \\\\\n&= P_{0.5}\\left( \\sum^{10}_{i=1} X_{i} \\geq 8 \\right) \\\\\n&= \\binom{10}{8} 0.5^{8} \\cdot 0.5^{2} + \\binom{10}{9} 0.5^{9} \\cdot 0.5^{1} + \\binom{10}{10} 0.5^{10} \\approx 0.05469 \\\\\n\\alpha & = P_{0.5}\\left( \\sqrt{ \\frac{n}{0.25}}(\\bar{X}-0.5) &gt;\\sqrt{ \\frac{n}{0.25}}(0.7-0.5) \\right) \\\\\n& \\approx P(N(0,1) &gt; 1.26) \\approx 0.103\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\beta &= P(\\text{Erro Tipo II}) \\stackrel{\\text{Sob $h_{1}$}}{=} P_{0.9}\\left( \\frac{1}{n} \\sum^n_{i=1}X_{i}\\leq 0.7\\right)\n= P_{0.9}\\left( \\sum^{10}_{i=1}X_{i} \\leq 7 \\right) \\\\\n&= 1-P_{0.9}\\left( \\sum^{10}_{i=1} X_{i} \\geq 8 \\right) \\\\\n&= 1-\\left(\\binom{10}{8} 0.9^{8} \\cdot 0.9^{2} + \\binom{10}{9} 0.9^{9} \\cdot 0.9^{1} + \\binom{10}{10} 0.9^{10}\\right)\n\\approx 0.0702 \\\\\n\\alpha & = P_{0.9}\\left( \\sqrt{ \\frac{n}{0.09}}(\\bar{X}-0.9) \\leq \\sqrt{ \\frac{n}{0.09}}(0.7-0.9) \\right) \\\\\n& \\approx P(N(0,1) \\leq -2.1) \\approx 0.018\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#poder-do-teste",
    "href": "teste-hipotese.html#poder-do-teste",
    "title": "14  Teste de Hipótese simples",
    "section": "14.4 Poder do teste",
    "text": "14.4 Poder do teste\nChamamos de poder do teste a probabilidade de rejeitar \\(h_{0}\\) quando este é falso.\nNo exemplo anterior, \\[\n\\pi = P_{0.9}(\\bar{X} &gt; 0.7) = 1-P_{0.9}(\\bar{X}\\leq 0.7) = 1 - \\beta = 92.92\\%\n\\]\nConsidere nesse exemplo uma a.a do lançamento de quatro moedas: \\((\\pmb{x}_{10}) = (1, 0, 1, 0,0,1,1,0,0,0)\\). Como \\(\\bar{x}=0.4 \\leq 0.7\\), não rejeitamos a hipótese nula \\(h_{0}\\).\nEm uma outra amostra, \\((\\pmb{x}_{10}) = (0,0,1,1,1,1,1,1,1,1)\\). Como \\(\\bar{x}=0.8 &gt; 0.7\\), rejeitamos a hipótese nula \\(h_{0}\\).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#um-exemplo-mais-amplo-diferença",
    "href": "teste-hipotese.html#um-exemplo-mais-amplo-diferença",
    "title": "14  Teste de Hipótese simples",
    "section": "14.5 Um exemplo mais amplo (Diferença)",
    "text": "14.5 Um exemplo mais amplo (Diferença)\nSeja \\((X_{n})\\) uma a.a. de \\(X\\sim\\mathrm{Ber}(\\theta)\\), em que \\(\\theta \\in (0,1)\\). Considere as hipóteses: \\[\n\\begin{cases}\nH_{0}: \\theta =0.5 \\\\\nH_{1}: \\theta \\neq 0.5\n\\end{cases}\n\\]\nDecisões elaboradas:\n\nSe \\(\\bar{x}&lt;0.3\\) ou \\(\\bar{x} &gt; 0.7\\), rejeitamos \\(H_{0}\\)\nCaso contrário, não rejeitamos \\(H_0\\)\n\nRelembrando: \\(\\alpha\\) = Probabilidade do Erro Tipo I (Rejeitar um \\(H_0\\) verdadeiro). \\(\\beta\\) Probabilidade do Erro Tipo II (Rejeitar um \\(H1\\) verdadeiro). \\(\\pi =\\) Poder do Teste. Lembre-se que \\(\\sum^n_{i=1}X_{i}\\sim \\mathrm{Bin}(n,\\theta)\\). Tome \\(n =10\\).\n\\[\n\\begin{aligned}\n\\alpha &= P_{\\theta=0.5}(\\bar{X} &lt; 0.3 \\text{ ou } \\bar{X} &gt; 0.7) = P_{0.5}(\\bar{X}&lt;0.3) + P_{0.5}(\\bar{X}&gt;0.7) \\\\\n&=P(\\mathrm{Bin}(n,0.5) &lt; 3) + P(\\mathrm{Bin}(10,0.5)&gt;7) \\\\\n&=P(\\mathrm{Bin}(n,0.5) \\leq 2) + P(\\mathrm{Bin}(10,0.5)\\geq 8) \\\\\n&= 0.055 +0.055 = 0.11 \\\\\n\\end{aligned}\n\\]\nNote que para o Erro Tipo II, não existe uma única probabilidade para o erro sob \\(H_1\\). Optaremos por tentar calcular seu máximo. \\(\\beta_{\\max}\\) \\[\n\\begin{aligned}\n\\beta &= P_{\\theta} (0.3\\leq \\bar{X} \\leq 0.7) \\theta \\in \\Theta \\setminus \\{ 0.5 \\} \\\\\n\\beta_{\\max} &= \\sup_{\\theta \\in \\Theta \\setminus \\{ 0.5 \\}} \\beta(\\theta)\n\\end{aligned}\n\\]\nPara \\(n=10\\), \\[\n\\beta(\\theta) = P_{\\theta}\\left( 3\\leq \\sum^{n=10}_{i=1} X_{i} \\leq 7 \\right) = P\\left( 3\\leq \\mathrm{Bin}(10,\\theta)\n\\leq 7 \\right), \\theta \\in \\Theta \\setminus \\{ 0.5 \\}\n\\]\nPodemos encontrar o valor que maximiza \\(\\beta(\\theta)\\), \\(\\theta =0.5\\) derivando.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#hipóteses-como-subconjuntos-do-espaço-paramétrico",
    "href": "teste-hipotese.html#hipóteses-como-subconjuntos-do-espaço-paramétrico",
    "title": "14  Teste de Hipótese simples",
    "section": "14.6 Hipóteses como subconjuntos do espaço paramétrico",
    "text": "14.6 Hipóteses como subconjuntos do espaço paramétrico\nSeja \\((X_{n})\\) uma a.a. de \\(X\\sim\\mathrm{Ber}(\\theta)\\), em que \\(\\theta \\in (0,1)\\). Considere as hipóteses: \\[\n\\begin{cases}\nH_{0}: \\theta \\in \\Theta_{0} \\\\\nH_{1}: \\theta \\in \\Theta_{1}\n\\end{cases}\n\\]\nem que \\(\\Theta_{0} \\cup \\Theta_{1} = \\Theta, \\Theta_{0},\\Theta_{1} \\neq \\emptyset, \\Theta_{0}\\cap\\Theta_{1}= \\emptyset\\).\nExemplos de decisões elaboráveis: \\[\n\\begin{aligned}\n&\\begin{cases}\n\\Theta_{0}=\\{ 0.5 \\} \\\\\n\\Theta_{1} = \\left( 0, \\frac{1}{2} \\right) \\cup \\left( \\frac{1}{2}, 1 \\right)\n\\end{cases} \\Rightarrow\n\\begin{cases}\nH_{0}: \\theta=0.5 \\\\\nH_{1}: \\theta \\neq 0.5 \\\\\n\\end{cases} \\text{ Hipótese alternativa bilateral} \\\\\n&\\begin{cases}\n\\Theta_{0}= \\left(  0, \\frac{1}{2} \\right] \\\\\n\\Theta_{1} = \\left( \\frac{1}{2}, 1 \\right)\n\\end{cases} \\Rightarrow\n\\begin{cases}\nH_{0}: \\theta \\leq 0.5\\\\\nH_{1}: \\theta &gt; 0.5 \\\\\n\\end{cases} \\text{ Hipótese alternativa unilateral} \\\\\n&\\begin{cases}\n\\Theta_{0}= \\left[ \\frac{1}{2}, 1\\right)   \\\\\n\\Theta_{1} = \\left( 0, \\frac{1}{2} \\right)\n\\end{cases} \\Rightarrow\n\\begin{cases}\nH_{0}: \\theta \\geq 0.5 \\\\\nH_{1}: \\theta &lt; 0.5 \\\\\n\\end{cases} \\text{ Hipótese alternativa uniteral}\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#função-poder",
    "href": "teste-hipotese.html#função-poder",
    "title": "14  Teste de Hipótese simples",
    "section": "14.7 Função poder",
    "text": "14.7 Função poder\nNo caso geral, calculamos a função poder definida por \\[\n\\pi (\\theta) = P_{\\theta}(\\{ \\text{Rejeitar } H_{0} \\}), \\theta \\in \\Theta\n\\]\nem que “Rejeitar \\(H_0\\)” é o procedimento de decisão para rejeitar \\(H_0\\).\nA partir da função poder conseguimos calcular as probabilidades máximas de cometer os erros tipo I e II.\nProbabilidade Máxima do Erro Tipo I: \\[\n\\alpha_{\\max} = \\sup_{\\theta \\in \\Theta_{0}}(\\pi(\\theta))\n\\]\nProbabilidade Máxima do Erro Tipo II: \\[\n\\beta_{\\max} = \\sup_{\\theta \\in \\Theta_{1}}[1-\\pi(\\theta)]\n\\]",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#um-exemplo-do-cálculo-de-erros-com-hipótese-unilateral",
    "href": "teste-hipotese.html#um-exemplo-do-cálculo-de-erros-com-hipótese-unilateral",
    "title": "14  Teste de Hipótese simples",
    "section": "14.8 Um exemplo do cálculo de erros com hipótese unilateral",
    "text": "14.8 Um exemplo do cálculo de erros com hipótese unilateral\nSeja \\((X_{n})\\) uma a.a. de \\(X\\sim\\mathrm{Ber}(\\theta)\\), em que \\(\\theta \\in (0,1) = \\Theta\\). Considere as hipóteses: \\[\n\\begin{cases}\nH_{0}: \\theta \\geq 0.6\\\\\nH_{1}: \\theta &lt; 0.6\n\\end{cases}\n\\]\nPrecisamos de decisões que fazem sentido. Uma delas seria\n\nSe \\(\\bar{x}&lt;0.4\\), rejeitamos \\(H_{0}\\)\nSe \\(\\bar{x} \\geq 0.4\\), não rejeitamos \\(H_{0}\\)\n\nVamos calcular as probabilidades máximas dos erros I e II.\nPrimeiro, encontramos a função poder \\[\n\\pi(\\theta) = P_{\\theta}(\\bar{X}&lt;0.4)\n\\]\nComo \\(\\sum^n_{i=1}X_{i} \\sim \\mathrm{Bin}(n,\\theta)\\), temos que \\[\n\\pi(\\theta) = P_{\\theta} \\left( \\sum^n_{i=1}X_{i} &lt; 0.4 \\cdot n \\right) = P(\\mathrm{Bin}(n,\\theta) &lt; 0.4 \\cdot n)\n\\]\nRelembrando: \\[\n\\begin{aligned}\n\\alpha_{\\max} &= \\sup_{\\theta \\in [0.6,1)} \\pi(\\theta) \\\\\n&= \\sup_{\\theta \\in [0.6,1)} P(\\mathrm{Bin}(n, \\theta) &lt; 0.4 \\cdot n) \\\\\n\\beta_{\\max} &= \\sup_{\\theta \\in (0,0.6)} (1-\\pi(\\theta)) \\\\\n&= \\sup_{\\theta \\in (0,0.6)}P(\\mathrm{Bin}(n,\\theta)\\geq 0.4 \\cdot n)\n\\end{aligned}\n\\]\nPara \\(n = 2\\), \\[\n\\begin{aligned}\n\\alpha_{\\max} &= \\sup_{\\theta \\in [0.6,1)} \\pi(\\theta) \\\\\n&= \\sup_{\\theta \\in [0.6,1)} P(\\mathrm{Bin}(2, \\theta) &lt; 0.4 \\cdot 2) \\\\\n&= \\sup_{\\theta \\in [0.6,1)} P(\\mathrm{Bin(2,\\theta)}  = 0)\\\\\n\\beta_{\\max} &= \\sup_{\\theta \\in (0,0.6)} (1-\\pi(\\theta)) \\\\\n&= \\sup_{\\theta \\in (0,0.6)}P(\\mathrm{Bin}(2,\\theta)\\geq 0.8 \\cdot n) \\\\\n&= \\sup_{\\theta \\in (0,0.6)}P(\\mathrm{Bin}(2,\\theta) \\geq 1) \\\\\n&= \\sup_{\\theta \\in (0,0.6)}[1-P(\\mathrm{Bin}(2,\\theta) = 0)]\n\\end{aligned} ~~~ \\Rightarrow ~~~~~\n\\begin{aligned}\n\\alpha_{\\max} &= \\sup_{\\theta \\in [0.6,1]}\\left[\\binom{2}{0} \\theta^0 (1-\\theta)^2\\right] \\\\\n&=\\sup_{\\theta \\in [0.6,1]} (1-\\theta)^2 \\\\\n\\beta_{\\max} &= \\sup_{\\theta \\in (0,0.6)} (1-(1-\\theta)^2)\n\\end{aligned}\n\\]\nPodemos analisar os gráficos:\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComo é uma função decrescente, seu supremo está no ponto \\(0.6\\)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComo é uma função crescente, seu supremo está também no \\(0.6\\) Portanto, \\[\n\\begin{aligned}\n\\alpha_{\\max}&=(1-0.6)^2 = 0.16\\\\\n\\beta_{\\max} &= (1-(1-0.6)^2) = 0.84\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#teste-sob-normalidade",
    "href": "teste-hipotese.html#teste-sob-normalidade",
    "title": "14  Teste de Hipótese simples",
    "section": "14.9 Teste sob Normalidade",
    "text": "14.9 Teste sob Normalidade\nSeja \\((x_{n})\\) amostra aleatória de \\(X\\sim N(\\mu,\\sigma^2)\\) em que \\(\\sigma^2\\) é conhecido. Considere as hipóteses \\[\n\\begin{cases}\nH_{0}: \\mu = \\mu_{0}\\\\\nH_{1}: \\mu \\neq \\mu_{0}\n\\end{cases}\n\\] com \\(\\mu_{0} \\in \\mathbb{R}\\) e fixado.\nCalcule as probabilidades (máximas) dos erros tipo I e II, para as seguintes decisões\n\nSe \\(\\bar{x} &lt; \\mu_{0} - 1.96\\sqrt{\\frac{\\sigma^2}{n}}\\) ou \\(\\bar{x} &gt; \\mu+1.96 \\sqrt{ \\frac{\\sigma^2}{n} }\\), então rejeitamos \\(H_{0}\\)\nCaso contrário, não rejeitamos \\(H_0\\)\n\nTemos a função poder \\[\n\\pi(\\theta) = P_{\\theta}(\\mathrm{Rejeitar} H_{0}) = P_{\\theta}\\left(\\bar{X}&lt;\\mu_{0} - 1.96 \\sqrt{\\frac{\\sigma^2}{n}}\\right)\n+P_{\\theta}\\left( \\bar{X} &gt; \\mu_{0} + 1.96 \\sqrt{  \\frac{\\sigma^2}{n} } \\right)\n\\]\nem que \\(theta = \\mu \\in \\mathbb{R}\\). Portanto, \\[\n\\alpha_{\\max} = \\sup_{\\theta \\in \\Theta_{0}} \\pi(\\theta)\n\\]\nComo \\(H_{0}=\\mu=\\mu_{0} \\Leftrightarrow H_{0}: \\theta \\in \\Theta\\), em que \\(\\Theta_{0}=\\{ \\mu_{0} \\}\\), logo, \\(\\sup_{\\theta \\in \\Theta_{0}} = \\mu_{0}\\) Portanto, temos que \\[\n\\alpha_{\\max} = \\pi(\\mu_{0}) = P_{\\mu_{0}}\\left( \\bar{X}&lt;\\mu_{0}-1.96\\sqrt{ \\frac{\\sigma^2}{n} } \\right) +\nP_{\\mu_{0}}\\left( \\bar{X}&gt;\\mu_{0} + 1.96 \\sqrt{  \\frac{\\sigma^2}{n} } \\right)\n\\]\nSabemos que, pelo enunciado \\(\\bar{X} \\sim N\\left( \\mu, \\frac{\\sigma^2}{n} \\right) \\forall \\mu \\in \\mathbb{R}\\) sob \\(H_{0}\\), ou seja, quando \\(\\mu= \\mu_{0}\\) temos que \\(\\bar{X}\\sim N\\left( \\mu_{0}, \\frac{\\sigma^2}{n} \\right)\\). Note que \\(P_{\\mu_{0}}\\left( \\bar{X}&lt;\\mu_{0}-1.96 \\sqrt{ \\frac{\\sigma^2}{n} } \\right) =\nP_{\\mu_{0}}\\left(\\frac{\\bar{X}-\\mu_{0}}{\\sqrt{ \\frac{\\sigma^2}{n} }} \\right) = 2.5\\%\\) Pela simetria da distribuição normal, \\(P_{\\mu_{0}}\\left( \\bar{X}&gt;\\mu_{0}+1.96 \\sqrt{ \\frac{\\sigma^2}{n} } \\right) = 2.5\\%\\) Portanto a probabilidade máxima do erro tipo 1 é \\[\n\\alpha_{\\max} = 2.5\\% + 2.5\\% = 5.0\\%\n\\]\nComo \\(H_{1}: \\mu \\neq \\mu_{0} \\Leftrightarrow H_{1}: \\theta \\in \\Theta_{1}\\), em que \\(\\Theta_{1} = \\mathbb{R} \\setminus \\{\\mu_{0}\\}\\), temos que\n\\[\n\\begin{aligned}\n\\beta_{\\max} &= \\sup_{\\theta \\in \\Theta_{1}}[1-\\pi(\\theta)] \\\\\n\\pi(\\theta) &= P_{\\theta}\\left( \\bar{X} &lt; \\mu_{0} - 1.96 \\sqrt{\\frac{\\sigma^2}{n}} \\right) +\nP_{\\theta}\\left(\\bar{X}&gt;\\mu_{0}+1.96\\sqrt{\\frac{\\sigma^2}{n}}\\right)\n\\end{aligned}\n\\]\nSabemos que \\(\\bar{X} \\sim \\mathrm{N}\\left( \\mu, \\frac{\\sigma^2}{n} \\right)\\) para todo \\(\\mu \\in \\mathbb{R}\\). Assim, \\[\n\\begin{aligned}\n\\pi(\\theta) &= P_{\\theta}\\left( \\bar{X}&lt;\\mu_{0}-1.96 \\sqrt{  \\frac{\\sigma^2}{n} } \\right) +\nP_{\\theta}\\left( \\bar{X} &gt; \\mu_{0} + 1.96 \\sqrt{  \\frac{\\sigma^2}{n} } \\right) \\\\\n&= P_{\\theta}\\left( \\frac{\\bar{X}-\\theta}{\\sqrt{ \\frac{\\sigma^2}{n} }} &lt;\n\\frac{\\mu_{0}-\\theta-1.96 \\sqrt{  \\frac{\\sigma^2}{n} } }{\\sqrt{  \\frac{\\sigma^2}{n} }}\\right) +\nP_{\\theta}\\left( \\frac{\\bar{X}-\\theta}{\\sqrt{ \\frac{\\sigma^2}{n} }} &gt;\n\\frac{\\mu_{0}-\\theta+1.96 \\sqrt{\\frac{\\sigma^2}{n}}}{\\sqrt{\\frac{\\sigma^2}{n}}}\\right)\n\\end{aligned}\n\\]\nDessa forma, \\[\n\\beta_{max} = \\sup_{\\theta \\in \\Theta_{1}}[1-\\pi(\\theta)] = 1 - \\inf_{\\theta \\in \\Theta_{1}} \\pi(\\theta)\n\\]\nOu seja, o supremo dessa expressão é dado por 1 - o ínfimo da função poder, o que significa que queremos encontrar o valor de \\(\\theta\\) para o qual \\(P_{\\theta}\\left( \\frac{\\bar{X}-\\theta}{\\sqrt{ \\frac{\\sigma^2}{n} }} &lt;\n\\frac{\\mu_{0}-\\theta-1.96 \\sqrt{\\frac{\\sigma^2}{n} } }{\\sqrt{  \\frac{\\sigma^2}{n} }}\\right) +\nP_{\\theta}\\left(\\frac{\\bar{X}-\\theta}{\\sqrt{ \\frac{\\sigma^2}{n} }} &gt;\n\\frac{\\mu_{0}-\\theta+1.96 \\sqrt{\\frac{\\sigma^2}{n}}}{\\sqrt{\\frac{\\sigma^2}{n}}}\\right)\\) é o menor possível.\nVamos usar \\(\\mu_0 = 7\\) e \\(\\sigma^2 = 5\\) para visualizarmos o comportamento de \\(\\alpha\\) e \\(\\beta\\)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n14.9.1 Um outro exemplo\nSeja \\((X_{n})\\) amostra aleatória de \\(X\\sim \\mathrm{N}(\\mu,\\sigma^2)\\) com \\(\\sigma^2 = 5, n = 10\\) Com as hipóteses \\[\n\\begin{cases}\nH_{0} : \\mu = 10 \\\\\nH_{1}: \\mu \\neq 10\n\\end{cases}\n\\] Com as decisões 1. Rejeitamos \\(H_{0}\\) se \\(\\bar{x} &gt; 10 + 1.96 \\sqrt{  \\frac{5}{10} }\\) ou \\(\\bar{x} &lt; 10 - 1.96 \\sqrt{  \\frac{5}{10} }\\) Foram observados os seguintes valores \\[\n\\begin{array}{c}\n7.1 & 8.9 & 12 & 13 & 11.7 \\\\\n6.1 & 2.5 & 3.1 & 5.2 & 7\n\\end{array}\n\\] Temos então que \\(\\bar{x} = 7.66\\) que, como é abaixo de \\(8.6\\), rejeitamos a hipótese nula de que \\(\\mu = 10\\)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#exemplos",
    "href": "teste-hipotese.html#exemplos",
    "title": "14  Teste de Hipótese simples",
    "section": "15.1 Exemplos",
    "text": "15.1 Exemplos\n\n15.1.1 Eexemplo um\nSeja \\((X_{n})\\) a.a de \\(X\\sim \\mathrm{N}(\\mu,\\sigma^2)\\) em que \\(\\sigma^2\\) é conhecido. Considere (\\(\\mu_{0}\\) fixado)\n\\[\n\\begin{cases}\nH_{0} : \\mu = \\mu_{0} \\\\\nH_{1}: \\mu \\neq \\mu_{0}\n\\end{cases}\n\\]\n\nConstrua uma decisão para rejeitar \\(H_0\\) que produza no máximo \\(\\alpha = 5\\%\\) (que tenha nível de significância de \\(5\\%\\))\nComo a hipótese alternativa é bilateral, \\(H_{1}:\\mu\\neq \\mu_{0}\\) e \\(\\bar{x}\\) é a EMV para o parâmetro \\(\\mu\\) - a esperança da distribuição Normal - definimos a regra: Se \\(\\bar{x}&lt; x_{a}\\) ou \\(\\bar{x}&gt; x_{b}\\), rejeitamos \\(H_{0}\\). Caso contrário, não rejeitamos. \\[\n\\begin{aligned}\n     \\alpha_{\\max} &= \\sup_{\\theta \\in \\Theta_{0}} P_{\\theta}(\\text{Rejeitar }H_{0}), \\Theta_{0} = \\{ \\mu_{0} \\} \\\\\n     &= P_{\\mu_{0}}(\\bar{X}&lt;x_{a}) + P_{\\mu_{0}}(\\bar{X}&gt;x_{b}) \\leq 5\\%\n\\end{aligned}\n\\]\nNote que \\(\\bar{X} \\sim \\mathrm{N}\\left(\\mu_{0}, \\frac{\\sigma^{2}}{n} \\right)\\), sob \\(H_{0}\\)\nLogo, \\[\n\\begin{aligned}\n     \\alpha_{\\max} &= P\\left( \\mathrm{N}(0,1) &lt; \\frac{{x_{a}-\\mu_{0}}}{\\sqrt{ \\frac{\\sigma^2}{n} }} \\right)+\nP\\left( \\mathrm{N}(0,1) &gt; \\frac{{x_{a}-\\mu_{0}}}{\\sqrt{ \\frac{\\sigma^2}{n} }} \\right)\n\\end{aligned}\n\\]\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTomando \\(\\frac{{x_{a}-\\mu_{0}}}{\\sqrt{\\frac{\\sigma^2}{n}}} = -1.96\\) e \\(\\frac{{x_{b}-\\mu_{0}}}{\\sqrt{ \\frac{\\sigma^2}{n}}}\n= 1.96\\) (tabela normal padrão simétrica), temos que \\(\\alpha_{\\max}=5\\%\\). Assim, resolvendo as equações, \\[\n\\begin{cases}\n     x_{a} = \\mu - 1.96 \\sqrt{\\frac{\\sigma^2}{n}} \\\\\n     x_{b} = \\mu + 1.96 \\sqrt{\\frac{\\sigma^2}{n}}\n\\end{cases}\n\\]\n\nConsidere \\(n=100\\), \\(\\mu_{0}=1\\), \\(\\sigma^2=0.1\\) e \\(\\bar{x}=0.99\\). Conclua o teste considerando o mesmo nível de significância \\(\\alpha=5\\%\\).\nO pontos pontos de corte são \\(x_{a} = 0.93\\) e \\(x_{b} = 1.069\\). Como \\(0.93\\leq 0.99 \\leq 1.069\\), concluímos que não há evidências para rejeitarmos \\(H_{0}\\) a \\(5\\%\\) de significância.\nRefaça considerando \\(15\\%\\) de significância estatística.\nUsando os mesmos argumentos do item 1, podemos encontrar novos valores para \\(x_{a}, x_{b}\\) através da tabela da Normal-Padrão: \\[\n\\begin{cases}\n    x_{a} = \\mu - 1.44 \\sqrt{\\frac{\\sigma^2}{n}} \\\\\n    x_{b} = \\mu + 1.44 \\sqrt{\\frac{\\sigma^2}{n}}\n\\end{cases}\n\\]\nSubstituindo esses valores para os fornecidos em 2, temos que \\(0.95 \\leq 0.99 \\leq 1.045\\). Portanto, continuaríamos a dizer que não há evidências para rejeitarmos \\(H_{0}\\) a \\(15\\%\\) de significância.\n\n\n\n15.1.2 2\nSeja \\((X_{n})\\) a.a de \\(X\\sim \\mathrm{N}(\\mu,\\sigma^2)\\) em que \\(\\sigma^2\\) é conhecido.\nConsidere (\\(\\mu_{0}\\) fixado)\n\\[\n\\begin{cases}\nH_{0} : \\mu \\geq \\mu_{0} \\\\\nH_{1}: \\mu &lt; \\mu_{0}\n\\end{cases}\n\\]\n\nConstrua uma decisão para rejeitar \\(H_0\\) que produza no máximo \\(\\alpha = 5\\%\\) (que tenha nível de significância de \\(5\\%\\))\nPelos parâmetros e hipóteses envolvidos, \\((\\mu, \\text{unilateral})\\), podemos considerar a seguinte decisão:\nSe \\(\\bar{x} &lt; x_{c}\\), rejeitamos \\(H_{0}\\). Caso contrário, não rejeitamos.\n\\[\n\\begin{aligned}\n     \\alpha_{\\max} &= \\sup_{\\mu \\geq \\mu_{0}} P_{\\theta}(\\text{Rejeitar }H_{0}) \\\\\n     &= \\sup_{\\mu\\geq \\mu_{0}}P_{\\mu}(\\bar{X}&lt;x_{c}) \\\\\n     \\Rightarrow \\alpha_{\\max} &=\\sup_{\\mu\\geq \\mu_{0}}P_{\\mu}\\left( \\mathrm{N}(0,1)&lt; \\frac{{x_{c}-\\mu}}\n{\\sqrt{\\frac{\\sigma^2}{n} }} \\right)\n  \\end{aligned}\n\\]\nComo essa função (acumulada) é decrescente em \\(\\mu\\), temos que \\[\n\\begin{aligned}\n     \\alpha_{\\max} &= P_{\\mu_{0}}(\\text{Rejeitar }H_{0}) \\\\\n     &= P_{\\mu_{0}}(\\bar{X}&lt;x_{c}) \\\\\n     \\Rightarrow \\alpha_{\\max} &=P_{\\mu_{0}}\\left( \\mathrm{N}(0,1)&lt; \\frac{{x_{c}-\\mu_{0}}}\n{\\sqrt{\\frac{\\sigma^2}{n} }} \\right) \\leq 5\\%\n  \\end{aligned}\n\\]\nLogo, para encontrarmos \\(x_{c}\\) tal que \\(\\frac{{x_{c}-\\mu_{0}}}{\\sqrt{ \\frac{\\sigma^2}{n} }} = -1.64\\) (da tabela da normal padrão) \\(\\Rightarrow x_{c} = \\mu_{0}-1.64 \\sqrt{ \\frac{\\sigma^2}{n} }\\)\nConsidere \\(n=100, \\mu_{0} = 1, \\sigma^2 = 0.1, \\bar{x}=0.99\\). Conclua o teste anterior a \\(\\alpha = 5\\%\\) de significância.\nO ponto de corte é \\(x_{c} = 0.9836\\). Como \\(0.99 \\geq 0.9836\\), concluímos que não há evidências para rejeitar a hipótese nula a \\(5\\%\\) de significância.\n\n\n\n15.1.3 3\nSeja \\((X_{n})\\) amostra aleatória de \\(X \\sim \\mathrm{N}(\\mu,\\sigma^2)\\) em que \\(\\theta = (\\mu, \\sigma^2) = \\mathrm{R} \\times \\mathrm{R}^+\\), ou seja, ambos parâmetros são desconhecidos.\n\\[\n\\begin{cases}\nH_{0} : \\sigma^2 = \\sigma^2_{0} \\\\\nH_{1}: \\sigma^2 \\neq \\sigma^2_{0}\n\\end{cases} \\Rightarrow\n\\begin{aligned}\n& \\text{Decisão com significância } \\alpha \\\\\n&\\text{Rejeita $H_{0}$ se} \\\\\n&\\begin{cases}\ns^2 &lt; c_{1c} \\\\\ns^2 &gt; c_{2c} \\\\\n\\end{cases} \\\\\n& \\text{Em que $c_{1c},c_{2c}$ são tais que} \\\\\n& \\sup_{\\theta\\in\\Theta_{0}}P_{\\theta}(\\text{Erro Tipo I}) = \\alpha_{\\max} = \\alpha \\\\\n&\\text{E } s^2 = \\frac{1}{n-1} \\sum^n_{i=1}(x_{i}-\\bar{x})^2\n\\end{aligned} \\\\ \\\\\n\\]\nSabemos que \\(\\frac{\\sum_{i=1}^n(X_{i}-\\bar{X})^2}{\\sigma^2}\\sim \\chi^2_{n-1},\\forall \\mu \\in \\mathbb{R}, \\sigma^2 &gt; 0\\). Em particular, sob \\(H_{0}\\) \\[\n\\begin{aligned}\n&\\frac{(n-1)s^2(\\underset{\\sim}{X_{n}})}{\\sigma^2_{0}} \\sim \\chi^2_{n-1}\n\\\\ \\Rightarrow& \\alpha_{\\max} = \\sup_{\\theta \\in \\Theta_{0}}\\left\\{ P_{\\theta}(s^2(\\underset{\\sim}{X_{n}}) &lt; c_{1c}) +\nP_{\\theta}(s^2(\\underset{\\sim}{X_{n}})&gt; c_{2c}) \\right\\}\n\\end{aligned}\n\\]\nAlém disso, note que \\(\\Theta_{0}=\\{ (\\mu,\\sigma^2)\\in \\Theta:\\sigma^2=\\sigma^2_{0} \\}\\). Portanto, temos que \\[\n\\begin{aligned}\n& \\alpha_{\\max} = \\sup_{\\theta \\in \\Theta_{0}} \\underbracket{ \\left\\{  P\\left( \\chi^2_{n-1} &lt;\n\\frac{c_{1c}(n-1)}{\\sigma^2_{0}}\\right) + P\\left( \\chi^2_{n-1} &gt;\n\\frac{c_{2c}(n-1)}{\\sigma^2_{0}} \\right) \\right\\}}_{\\text{Não depende de $\\theta$}}\\\\\n\\Rightarrow & \\alpha_{\\max} =  P\\left( \\chi^2_{n-1} &lt; \\frac{c_{1c}(n-1)}{\\sigma^2_{0}} \\right) + P\\left( \\chi^2_{n-1} &gt;\n\\frac{c_{2c}(n-1)}{\\sigma^2_{0}} \\right)\n\\end{aligned}\n\\]\nFixando \\(\\alpha_{\\max}=\\alpha\\) (significância), encontramos pela tabela os valores de \\(q_{\\frac{\\alpha}{2},n-1}^{(1)}, q_{\\frac{\\alpha}{2},n-1}^{(2)}\\) tais que dividam a distribuição \\(\\chi^2_{n-1}\\) criando duas seções de \\(\\frac{\\alpha}{2}\\) de área. Portanto,\n\\[\n\\begin{cases}\nH_{0} : \\sigma^2 = \\sigma^2_{0} \\\\\nH_{1}: \\sigma^2 \\neq \\sigma^2_{0}\n\\end{cases} \\Rightarrow\n\\begin{aligned}\n& \\text{Decisão com significância } \\alpha \\\\\n&\\text{Rejeita $H_{0}$ se} \\\\\n&\\begin{cases}\ns^2 &lt; q_{\\frac{\\alpha}{2},n-1}^{(1)} \\cdot \\frac{\\sigma^2_{0}}{(n-1)} \\\\\ns^2 &gt; q_{\\frac{\\alpha}{2},n-1}^{(2)} \\cdot \\frac{\\sigma^2_{0}}{(n-1)}\\\\\n\\end{cases}\n\\end{aligned}\n\\]\n\n\n15.1.4 Exemplo\nSeja \\((X_{n})\\) amostra aleatória de \\(X\\sim N(\\mu,\\sigma^2)\\) em que \\(X\\) é o peso do pacote de café. Colheu-se uma amostra de \\(n=16\\) pacotes e observou-se uma variância de \\(s^2 =169g^2\\). O processo de fabricação diz que a média dos pacotes é \\(500g\\) e desvio-padrão 10 gramas (\\(\\sigma^2_{0}=100g^2\\)).\nQueremos verificar se há alguma evidência de que o processo não esteja sendo cumprido com \\(\\alpha=5\\%\\) de significância \\[\n\\begin{cases}\nH_{0} : \\sigma^2 = 100 \\\\\nH_{1}: \\sigma^2 \\neq 100\n\\end{cases} \\Rightarrow\n\\begin{aligned}\n& \\text{Decisão com significância } 5\\% \\\\\n&\\text{Rejeita $H_{0}$ se} \\\\\n&\\begin{cases}\ns^2 &lt; q_{2.5\\%,15}^{(1)} \\cdot \\frac{100}{15} \\\\\ns^2 &gt; q_{2.5\\%,15}^{(2)} \\cdot \\frac{100}{15}\\\n\\end{cases}\n\\end{aligned}\n\\] Da tabela Qui-quadrado, temos \\(q_{2.5\\%,15}^{(1)} = 6.26\\) e \\(q_{2.5\\%,15}^{(2)} =27.49\\).\nComo \\(41.73&lt;100&lt;183.26\\), concluímos que não há evidências para rejeitar a hipótese nula a \\(5\\%\\) de significância",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#sob-normalidade-variância-conhecida",
    "href": "teste-hipotese.html#sob-normalidade-variância-conhecida",
    "title": "14  Teste de Hipótese simples",
    "section": "16.1 Sob normalidade, variância conhecida",
    "text": "16.1 Sob normalidade, variância conhecida\nSeja \\((X_{n})\\) amostra aleatória de \\(X\\sim \\mathrm{N}(\\mu,\\sigma^2)\\) em que \\(\\sigma^2\\) é conhecido. \\[\n\\begin{aligned}\n&1.\n\\begin{cases}\nH_{0} : \\mu = \\mu_{0} \\\\\nH_{1}: \\mu \\neq \\mu_{0}\n\\end{cases} \\Rightarrow\n\\begin{aligned}\n& \\text{Decisão com significância } \\alpha \\\\\n&\\text{Rejeita $H_{0}$ se} \\\\\n&\\begin{cases}\n\\bar{x} &lt; \\mu_{0} - z_{\\frac{\\alpha}{2}} \\sqrt{ \\frac{\\sigma^2}{n} } \\\\\n\\bar{x} &gt; \\mu_{0} + z_{\\frac{\\alpha}{2}} \\sqrt{ \\frac{\\sigma^2}{n} }\n\\end{cases} \\\\\n& \\text{Em que $z_{\\frac{\\alpha}{2}}$ é tal que} \\\\\n& P\\left( \\mathrm{N(0,1)} &lt; z_{\\frac{\\alpha}{2}} \\right) = \\frac{\\alpha}{2}\n\\end{aligned} \\\\ \\\\\n& 2.\n\\begin{cases}\nH_{0} : \\mu \\geq \\mu_{0} \\\\\nH_{1}: \\mu &lt; \\mu_{0}\n\\end{cases} \\Rightarrow\n\\begin{aligned}\n&\\text{Rejeita $H_{0}$ se} \\\\\n&\\begin{cases}\n\\bar{x} &lt; \\mu_{0} - z_{\\alpha} \\sqrt{ \\frac{\\sigma^2}{n} } \\\\\n\\end{cases} \\\\\n& \\text{Em que $z_{\\alpha}$ é tal que} \\\\\n& P\\left( \\mathrm{N(0,1)} \\leq z_{\\alpha} \\right) = \\alpha\n\\end{aligned} \\\\ \\\\\n& 3.\n\\begin{cases}\nH_{0} : \\mu \\leq \\mu_{0} \\\\\nH_{1}: \\mu &gt; \\mu_{0}\n\\end{cases} \\Rightarrow\n\\begin{aligned}\n&\\text{Rejeita $H_{0}$ se} \\\\\n&\\begin{cases}\n\\bar{x} &gt; \\mu_{0} + z_{\\alpha} \\sqrt{ \\frac{\\sigma^2}{n} } \\\\\n\\end{cases} \\\\\n& \\text{Em que $z_{\\alpha}$ é tal que} \\\\\n& P\\left( \\mathrm{N(0,1)} \\geq z_{\\alpha} \\right) = \\alpha\n\\end{aligned} \\\\\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#sec-normalidade-vardesc",
    "href": "teste-hipotese.html#sec-normalidade-vardesc",
    "title": "14  Teste de Hipótese simples",
    "section": "16.2 Sob normalidade, variância desconhecida",
    "text": "16.2 Sob normalidade, variância desconhecida\nSeja \\((X_{n})\\) amostra aleatória de \\(X \\sim \\mathrm{N}(\\mu,\\sigma^2)\\) em que \\(\\theta = (\\mu, \\sigma^2) = \\mathrm{R}\n\\times \\mathrm{R}^+\\), ou seja, ambos parâmetros são desconhecidos. \\[\n\\begin{aligned}\n&1.\n\\begin{cases}\nH_{0} : \\mu = \\mu_{0}  \\\\\nH_{1} : \\mu \\neq \\mu_{0} \\\\\n\\Rightarrow \\Theta_{0}=\\{ (\\mu,\\sigma^2) \\in \\Theta : \\mu = \\mu_{0} \\}\\\\\n\\Rightarrow \\Theta_{1}=\\{ (\\mu,\\sigma^2) \\in \\Theta : \\mu \\neq \\mu_{0} \\}\\\\\n\\end{cases} \\Rightarrow\n\\begin{aligned}\n& \\text{Decisão com significância } \\alpha \\\\\n&\\text{Rejeita $H_{0}$ se} \\\\\n&\\begin{cases}\n\\frac{\\bar{x}-\\mu_{0}}{\\sqrt{ \\frac{s^2}{n} }} &lt; - t_{\\frac{\\alpha}{2},n-1} \\\\\n\\frac{\\bar{x}-\\mu_{0}}{\\sqrt{ \\frac{s^2}{n} }} &gt; t_{\\frac{\\alpha}{2},n-1} \\\\\n\\end{cases} \\\\\n& \\text{Em que $t_{\\frac{\\alpha}{2},n-1}$ é tal que} \\\\\n& P\\left( t_{n-1} &lt; - t_{\\frac{\\alpha}{2},n-1} \\right) = \\frac{\\alpha}{2}\n\\end{aligned} \\\\ \\\\\n& 2.\n\\begin{cases}\nH_{0} : \\mu \\geq \\mu_{0} \\\\\nH_{1}: \\mu &lt; \\mu_{0}\n\\end{cases} \\Rightarrow\n\\begin{aligned}\n&\\text{Rejeita $H_{0}$ se} \\\\\n&\\begin{cases}\n\\frac{\\bar{x}-\\mu_{0}}{\\sqrt{ \\frac{s^2}{n} }} &lt; - t_{\\alpha,n-1} \\\\\n\\end{cases} \\\\\n& \\text{Em que $t_{\\alpha,n-1}$ é tal que} \\\\\n& P\\left( t_{n-1}\\leq -t_{\\alpha,n-1} \\right) = \\alpha\n\\end{aligned} \\\\ \\\\\n& 3.\n\\begin{cases}\nH_{0} : \\mu \\leq \\mu_{0} \\\\\nH_{1}: \\mu &gt; \\mu_{0}\n\\end{cases} \\Rightarrow\n\\begin{aligned}\n&\\text{Rejeita $H_{0}$ se} \\\\\n&\\begin{cases}\n\\frac{\\bar{x}-\\mu_{0}}{\\sqrt{ \\frac{s^2}{n} }} &gt; - t_{\\alpha,n-1} \\\\\n\\end{cases} \\\\\n& \\text{Em que $t_{\\alpha,n-1}$ é tal que} \\\\\n& P\\left( t_{n-1} &gt; t_{\\alpha,n-1} \\right) = \\alpha\n\\end{aligned} \\\\\n\\end{aligned}\n\\]\nEm que \\(s^2 = \\frac{1}{n-1} \\sum^n_{i=1}(x_{i}-\\bar{x})^2\\) é a variância amostral (não enviesada)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese.html#sob-normalidade-para-a-variância.",
    "href": "teste-hipotese.html#sob-normalidade-para-a-variância.",
    "title": "14  Teste de Hipótese simples",
    "section": "16.3 Sob normalidade, para a variância.",
    "text": "16.3 Sob normalidade, para a variância.\nSeja \\((X_{n})\\) amostra aleatória de \\(X \\sim \\mathrm{N}(\\mu,\\sigma^2)\\) em que \\(\\theta = (\\mu, \\sigma^2) = \\mathrm{R} \\times \\mathrm{R}^+\\), ou seja, ambos parâmetros são desconhecidos. \\[\n\\begin{cases}\nH_{0} : \\sigma^2 = \\sigma^2_{0} \\\\\nH_{1}: \\sigma^2 \\neq \\sigma^2_{0}\n\\end{cases} \\Rightarrow\n\\begin{aligned}\n& \\text{Decisão com significância } \\alpha \\\\\n&\\text{Rejeita $H_{0}$ se} \\\\\n&\\begin{cases}\ns^2 &lt; q_{\\frac{\\alpha}{2},n-1}^{(1)} \\cdot \\frac{\\sigma^2_{0}}{(n-1)} \\\\\ns^2 &gt; q_{\\frac{\\alpha}{2},n-1}^{(2)} \\cdot \\frac{\\sigma^2_{0}}{(n-1)}\\\\\n\\end{cases}\n\\end{aligned}\n\\]\nEm que \\(s^2 = \\frac{1}{n-1} \\sum^n_{i=1}(x_{i}-\\bar{x})^2\\) é a variância amostral (não enviesada) e \\(q_{\\frac{\\alpha}{2},n-1}^{(1)}, q_{\\frac{\\alpha}{2},n-1}^{(2)}\\) tais que dividam a distribuição \\(\\chi^2_{n-1}\\) criando duas seções de \\(\\frac{\\alpha}{2}\\) de área.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Teste de Hipótese simples</span>"
    ]
  },
  {
    "objectID": "teste-hipotese-duas-pops.html",
    "href": "teste-hipotese-duas-pops.html",
    "title": "15  Teste de Hipótese para duas populações",
    "section": "",
    "text": "15.1 Dados independentes e dependentes\nSejam \\(X, Y\\) duas variáveis de interesse representando duas sub-populações. Estamos interessados em verificar se a média populacional de \\(X\\) é menor, maior ou igual à de \\(Y\\). Sendo assim, precisamos considerar os casos em que \\(X\\) é independente de \\(Y\\) e o caso em que não são independentes (pareados).\nUm pesquisador propôs um novo método de investimento para aumentar o rendimento mensal. Selecionou 20 investidores aleatoriamente de um universo de investidores cadastrados. Em um primeiro momento, o pesquisador deixou os investidores investirem do jeito que sabem e ao final verificou a renda obtida. \\(X\\) é o rendimento dos investidores sem ter o conhecimento do método.\nEntão, o pesquisador ensinou seu método aos investidores, onde \\(Y\\) passou a ser o rendimento dos investidores após a aplicação do método ensinado.\nClaramente, \\(X,Y\\) são dependentes.\nO mesmo pesquisador testará o mesmo método de forma diferente. Para testar o seu método, o pesquisador selecionou 20 indivíduos com características similares do universo de investidores, dos quais\nNesse caso, as variáveis \\(X,Y\\) são independentes.\nEm ambas abordagens, temos as mesmas hipóteses de interesse: \\[\n1.\n\\begin{cases}\nH_{0}:\\mu_{X}\\geq \\mu_{y} \\\\\nH_{1}: \\mu_{X} &lt; \\mu_{Y}\n\\end{cases} ~~~2.\n\\begin{cases}\nH_{0}:\\mu_{X}\\leq \\mu_{Y} \\\\\nH_{1}: \\mu_{X} &gt; \\mu_{Y}\n\\end{cases} ~~~3.\n\\begin{cases}\nH_{0}:\\mu_{X}= \\mu_{Y} \\\\\nH_{1}: \\mu_{X} \\neq \\mu_{Y}\n\\end{cases}\n\\]\nPodemos definir \\(\\mu_{D}=\\mu_{X}-\\mu_{Y}\\) e reescrever as hipóteses \\[\n1.\n\\begin{cases}\nH_{0}:\\mu_{D} \\geq 0 \\\\\nH_{1}: \\mu_{D} &lt; 0\n\\end{cases} ~~~2.\n\\begin{cases}\nH_{0}:\\mu_{D} \\leq 0 \\\\\nH_{1}: \\mu_{D} &gt; 0\n\\end{cases} ~~~3.\n\\begin{cases}\nH_{0}:\\mu_{D} = 0 \\\\\nH_{1}: \\mu_{D} \\neq 0\n\\end{cases}\n\\]\nPara os próximos exemplos, assumiremos normalidade para \\(X,Y\\)",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Teste de Hipótese para duas populações</span>"
    ]
  },
  {
    "objectID": "teste-hipotese-duas-pops.html#dados-independentes-e-dependentes",
    "href": "teste-hipotese-duas-pops.html#dados-independentes-e-dependentes",
    "title": "15  Teste de Hipótese para duas populações",
    "section": "",
    "text": "10 foram designados aleatoriamente a não receber o método \\(X:\\) rendimento de um indivíduo que não recebeu o método\n10 foram designados aleatoriamente a receberem o método \\(Y:\\) rendimento de um indivíduo que recebeu o método\n\n\n\n\n\n\n15.1.1 Caso pareado (variáveis dependentes)\nNo caso em que \\(X,Y\\) são dependentes, as amostras são \\((X_{n})\\) amostras aleatórias de \\(X\\) e \\((Y_{n})\\) a.a de \\(Y\\) tais que \\(X_{i},Y_{i}\\) são dependentes.\nPara este caso, fazemos \\(D_{i}=X_{i}-Y_{i},i=1,2,\\dots, n\\). Temos que \\((D_{n})\\) é uma amostra aleatória de \\(D=X-Y \\sim N(\\mu_{D},\\sigma^2_{D})=N(\\mu_{X}-\\mu_{Y},\\sigma^2_{X}+\\sigma^2_{Y}-2 \\rho\\sigma_{X}\\sigma _{Y})\\)\nObserve ainda que \\[\n\\bar{D}_{\\mathrm{Par}}=\\sum^{n}_{i=1} \\frac{D_{i}}{n} \\sim N\\left( \\mu_{D}, \\frac{\\sigma^2_{D}}{n} \\right)\n\\] Note ainda que as variância e covariância de \\(X,Y\\) estão embutidas em \\(\\sigma^2_{D}\\). Podemos usar \\[\ns^2_{D}(\\underset{\\sim}{D})=\\frac{1}{n-1}\\sum^n_{i=1}(D_{i}-\\bar{D}_{\\mathrm{Par}})^2\n\\] Para estimar \\(\\sigma^2_{D}\\) Podemos construir as decisões como já vimos anteriormente em testes sob normalidade com variância desconhecida.\n\n\n15.1.2 Exemplo\nForam coletados os rendimentos (em mil reais) antes e após a aplicação o método para 12 investidores. Queremos verificar se o método aumentou o rendimento médio. Chamaremos de \\(X\\) o rendimento anterior ao treinamento e \\(Y\\) o rendimento após. Isto é, queremos verificar se \\(\\mu_{D}=\\mu_{X}-\\mu_{Y}\\leq 0\\). Portanto, nossa hipótese nula é de que o treinamento não tem efeito positivo no rendimento:\n\\[\n\\begin{cases}\nH_{0}:\\mu_{D} \\geq 0 \\\\\nH_{1}: \\mu_{D} &lt; 0\n\\end{cases}\n\\]\n\\[\n\\begin{array}{c|c|c|c}\n\\mathrm{Indíce}  & \\mathrm{Antes}(X)  &  \\mathrm{Depois}(Y) & \\mathrm{Dif}(D) & \\mathrm{Dif^2}(D^2)\\\\\n\\hline\n1 & 2.4 &  4.3 & -1.9 & 3.61\\\\\n2 & 2.8  & 3.4  & -0.6 & 0.36\\\\\n3 & 4.6  & 3.2  & 1.4 & 1.96\\\\\n4 & 3.1  & 3.3 & -0.2 & 0.04\\\\\n5 & 3.1  & 3.3 & -0.2 & 0,04\\\\\n6 & 4.7 &  5.8 & -1.1 & 1.21\\\\\n7 & 3.5  & 3.8 & -0.3 & 0.09\\\\\n8 & 1.7  & 3.5 & -1.8 & 3.24\\\\\n9 & 2.3  & 3.2 & -0.9 & 0.81\\\\\n10 & 2.6  & 3.9 & -1.3 & 1.69\\\\\n11 & 4.2  & 3.6 & 0.6 & 0.36\\\\\n12 & 3.4  & 4.3  & -0.9  & 0.81\\\\\n\\hline\n\\mathrm{Média}  & 3.2  & 3.8  & -0.6 \\\\\ns^2 & 0.87  & 0.55  &  0.9\n\\end{array}\n\\]\nTemos que \\(s^2(\\underset{\\sim}{D})=0.9\\). (Podemos calcular diretamente ou usando a coluna \\(D^2\\) e substituindo no somatório \\(s^2_{D}=\\left( \\frac{\\sum^n_{i=1}d_{i}^2}{n}-\\bar{d}^2 \\right) \\frac{n}{n-1}\\) ) Rejeitamos \\(H_{0}\\) se \\(\\frac{\\bar{d}_{\\mathrm{Par}}}{\\sqrt{ \\frac{s^2_{D}}{n}}}&lt;-t_{\\alpha,n-1}\\) onde \\(t_{\\alpha,n-1}\\) é tal que \\(P(t_{n-1}&lt;-t_{\\alpha,n-1})=\\alpha\\), onde \\(t_{n-1}\\) é a distribuição T de Student com \\(n-1\\) graus de liberdade.\nTemos que \\(\\frac{\\bar{d}_{\\mathrm{Par}}}{\\sqrt{ \\frac{s^2_{D}}{n}}} =-2.19\\) e, a \\(\\alpha=0.05\\), \\(-t_{0.05,11}=-1.796\\). Como \\(-2.19&lt; -1.796\\), podemos dizer que há evidências de que o método aumenta o rendimento médio dos investidores a \\(5\\%\\) de significância. Por outro lado, com \\(\\alpha=0.01, -t_{0.01,11}=-2.718\\) e, por \\(-2.19\\geq -2.718\\), dizemos que não há evidências para rejeitar a hipótese de que o método não aumenta o rendimento (rejeitar a hipótese nula) a 1% de significância estatística.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Teste de Hipótese para duas populações</span>"
    ]
  },
  {
    "objectID": "teste-hipotese-duas-pops.html#caso-de-independência",
    "href": "teste-hipotese-duas-pops.html#caso-de-independência",
    "title": "15  Teste de Hipótese para duas populações",
    "section": "15.2 Caso de independência",
    "text": "15.2 Caso de independência\nSejam \\(X, Y\\) variáveis aleatórias independentes tais que \\[ \\begin{cases}\nX\\sim N(\\mu_{X},\\sigma^2_{X}) \\\\\nY\\sim (\\mu_{Y},\\sigma^2_{Y})\n\\end{cases} \\] Considere \\((X_{n})\\) amostra aleatória de \\(X\\) e \\((Y_{m})\\). Estamos interessados em testar as hipóteses: \\[\n1.\n\\begin{cases}\nH_{0}:\\mu_{X}\\geq \\mu_{Y} \\\\\nH_{1}: \\mu_{X} &lt; \\mu_{Y}\n\\end{cases} ~~~2.\n\\begin{cases}\nH_{0}:\\mu_{X}\\leq \\mu_{Y} \\\\\nH_{1}: \\mu_{X} &gt; \\mu_{Y}\n\\end{cases} ~~~3.\n\\begin{cases}\nH_{0}:\\mu_{X}= \\mu_{Y} \\\\\nH_{1}: \\mu_{X} \\neq \\mu_{Y}\n\\end{cases}\n\\] Podemos definir \\(\\mu_{D}=\\mu_{X}-\\mu_{Y}\\) e obter a equivalência dessas hipóteses \\[\n1.\n\\begin{cases}\nH_{0}:\\mu_{D} \\geq 0 \\\\\nH_{1}: \\mu_{D} &lt; 0\n\\end{cases} ~~~2.\n\\begin{cases}\nH_{0}:\\mu_{D} \\leq 0 \\\\\nH_{1}: \\mu_{D} &gt; 0\n\\end{cases} ~~~3.\n\\begin{cases}\nH_{0}:\\mu_{D} = 0 \\\\\nH_{1}: \\mu_{D} \\neq 0\n\\end{cases}\n\\] Considere \\[\n\\bar{D}_{\\mathrm{NPar}}=\\bar{X}-\\bar{Y}\n\\]\nComo \\(\\bar{X}\\sim N\\left( \\mu_{X}, \\frac{\\sigma^2_{X}}{n} \\right), \\bar{Y} \\sim N\\left( \\mu_{Y}, \\frac{\\sigma^2_{Y}}{m} \\right)\\), temos que \\(\\bar{D}_{\\mathrm{NPar}} \\sim N\\left( \\mu_{D}, \\frac{\\sigma^2_{X}}{n} + \\frac{\\sigma^2_{Y}}{m} \\right)\\)\n\n15.2.1 Ambas variâncias conhecidas\nPodemos substituir o valor numérico das variâncias na distribuição de \\(\\bar{D}_{\\mathrm{NPar}}\\), obtendo uma distribuição normal com pontos de corte para as decisões:\n\\[\n\\begin{aligned}\n1. &\\text{ Rejeita $H_{0}$ se } \\bar{d}_{\\mathrm{NPar}} &lt; -z_{\\alpha} \\underbrace{\\sqrt{ \\frac{\\sigma^2}{n} +\n\\frac{\\sigma^2}{m} }}_{\\mathrm{Var}(\\bar{D}_{\\mathrm{NPar}})}, \\sigma_{2}=\\sigma^2_{X}=\\sigma^2_{Y} \\\\\n2. & \\text{ Rejeita $H_{0}$ se } \\bar{d}_{\\mathrm{NPar}} &gt; z_{\\alpha} \\sqrt{ \\frac{\\sigma^2}{n} + \\frac{\\sigma^2}{m} } \\\\\n3. & \\text{ Rejeita $H_{0}$ se } \\bar{d}_{\\mathrm{NPar}} &lt; -z_{\\frac{\\alpha}{2}} \\sqrt{ \\frac{\\sigma^2}{n} +\n\\frac{\\sigma^2}{m} } \\text{ ou } \\bar{d}_{NPar} &gt; z_{\\alpha} \\sqrt{ \\frac{\\sigma^2}{n} + \\frac{\\sigma^2}{m} }\n\\end{aligned}\n\\]\n\n\n15.2.2 Variâncias desconhecidas e iguais\nTemos que \\(\\sigma^2_{X}=\\sigma^2_{Y}=\\sigma^2\\) é conhecido.\n\n15.2.2.1 Estimando via t-Student\nAtravés da distribuição t-Student com \\(n+m-2\\) graus de liberdade, podemos estimar os pontos de corte. Temos nossas decisões: \\[\n\\begin{aligned}\n1. &\\text{ Rejeita $H_{0}$ se } \\bar{d}_{\\mathrm{NPar}} &lt; -t_{n+m-2,\\alpha} \\sqrt{ \\frac{s_{p}^2}{n} + \\frac{s_{p}^2}{m} } \\\\\n2. & \\text{ Rejeita $H_{0}$ se } \\bar{d}_{\\mathrm{NPar}} &gt; t_{n+m-2,\\alpha} \\sqrt{ \\frac{s^2_{p}}{n} + \\frac{s^2_{p}}{m} } \\\\\n3. & \\text{ Rejeita $H_{0}$ se } \\bar{d}_{\\mathrm{NPar}} &lt; -t_{n+m-2,\\frac{\\alpha}{2}} \\sqrt{ \\frac{s^2_{p}}{n} +\n\\frac{s^2_{p}}{m} } \\text{ ou } \\bar{d}_{NPar} &gt; t_{n+m-2, \\frac{\\alpha}{2}} \\sqrt{ \\frac{s^2_{p}}{n} + \\frac{s^2_{p}}{m} }\n\\end{aligned}\n\\] Onde \\(s^2_{p}= \\frac{(n-1)s^2_{X}+(m-1)s^2_{Y}}{n+m-2}\\), com \\(s^2_{X}, s^2_{Y}\\) sendo os estimadores não enviesados para as variâncias de \\(X\\) e \\(Y\\), respectivamente. (Ponderamos os estimadores com base no tamanho de sua amostra, assim favorecendo os estimadores mais precisos)\n\n\n\n15.2.3 Variâncias desconhecidas e diferentes (caso geral)\nTemos que \\(\\sigma^2_{X},\\sigma^2_{Y}\\) são desconhecidos e diferentes.\n\n15.2.3.1 Estimando via t-Student\nusaremos a distribuição t-Student com \\(n'\\) graus de liberdade. Temos nossas decisões:\n\\[\n\\begin{aligned}\n1. &\\text{ Rejeita $H_{0}$ se } \\bar{d}_{\\mathrm{NPar}} &lt; -t_{n',\\alpha} \\sqrt{ \\frac{s_{p}^2}{n} + \\frac{s_{p}^2}{m} } \\\\\n2. & \\text{ Rejeita $H_{0}$ se } \\bar{d}_{\\mathrm{NPar}} &gt; t_{n',\\alpha} \\sqrt{ \\frac{s^2_{p}}{n} + \\frac{s^2_{p}}{m} } \\\\\n3. & \\text{ Rejeita $H_{0}$ se } \\bar{d}_{\\mathrm{NPar}} &lt; -t_{n',\\frac{\\alpha}{2}} \\sqrt{\\frac{s^2_{p}}{n} + \\frac{s^2_{p}}{m}}\n\\text{ ou } \\bar{d}_{NPar} &gt; t_{n', \\frac{\\alpha}{2}} \\sqrt{ \\frac{s^2_{p}}{n} + \\frac{s^2_{p}}{m} }\n\\end{aligned}\n\\]\nOnde \\(s^2_{p}= \\frac{(n-1)s^2_{X}+(m-1)s^2_{Y}}{n+m-2}\\).\n\n15.2.3.1.1 Encontrando \\(n’\\)\nNa fórmula acima, temos os graus de liberdade da t-Student dado por \\[\nn' \\approx \\frac{\\left(\\frac{s^2_{X}}{n}+\\frac{s^2_{Y}}{m}\\right)^2}{\\frac{\\left(\\frac{s^2_{X}}{n}\\right)^2}\n{n-1}+\\frac{\\left(\\frac{s^2_{Y}}{m}\\right)^2}{m-1}}\n\\] Esse valor, caso não inteiro, deverá ser arredondado.\n\n\n\n15.2.3.2 Exemplo (Importante)\nQueremos testar a resistência de dois tipos de viga de aço, \\(A\\) e \\(B\\). Tomando-se \\(n=15\\) vigas do tipo \\(A\\) e \\(m=20\\) vigas do tipo \\(B\\). de um teste \\(f\\), conseguimos com \\(10\\%\\) de significância que as variâncias não são iguais. Obtemos os valores da tabela: \\[\n\\begin{array}{c|c}\n\\text{Tipo}  & \\text{Média}  & \\text{Variância} (s^2) \\\\\n\\hline\nA & 70.5 & 81.6 \\\\\nB  & 84.3  & 210.8 \\\\\n\\bar{d}_{\\mathrm{NPar}}  & -13.8 & --\n\\end{array}\n\\] Teste a hipótese \\[\n\\begin{cases}\nH_{0} : \\mu_{X} = \\mu_{Y} \\\\\nH_{1}:\\mu_{X} \\neq \\mu_{Y}\n\\end{cases}\n\\] Com significância \\(\\alpha = 0.05\\) para os casos\n\n15.2.3.2.1 Caso 1. Variâncias conhecidas\nTemos do produtor que \\(\\sigma^2_{X}=81, \\sigma^2_{Y}=209\\) \\(\\bar{d}_{\\mathrm{NPar}}=70.5-84.3 = -13.8\\). Logo, \\(z_{\\frac{\\alpha}{2}}\\sqrt{ \\frac{81}{15} + \\frac{209}{20}}=7.8\\). Como \\(-13.8 &lt; -7.8\\), concluímos que há evidências para rejeitarmos a hipótese nula de que as resistências médias das vigas \\(A,B\\) são iguais a \\(\\alpha = 5\\%\\) de significância estatística\n\n\n15.2.3.2.2 Caso 2. Variâncias desconhecidas e iguais.\nPara \\(\\alpha=0.05\\), \\(t_{33,0.025}=2.03\\). Encontrando \\(s^2_{P} = 155.988\\). Finalmente, \\(t_{33,0.025}\n\\sqrt{  \\frac{s^2_{P}}{n} + \\frac{s^2_{P}}{m} }=8.65\\). Como \\(-13.8 &lt; -8.65\\), concluímos que há evidências para rejeitarmos a hipótese nula a \\(\\alpha=5\\%\\) de significância estatística.\n\n\n15.2.3.2.3 Caso 3. Variâncias desconhecidas e diferentes\nPrimeiro calculamos \\(n'= 32.08 \\stackrel{\\text{Arrendonda}}{=}32\\). Assim, \\(t_{32,0.025} = 2.037\\). Portanto, \\(t_{32,0.025} \\sqrt{  \\frac{s^2_{X}}{n} + \\frac{s^2_{Y}}{m} }=8.14\\). Como \\(-13.8 &lt; -8.15\\), concluímos que há evidências para rejeitarmos a hipótese nula a \\(\\alpha=5\\%\\) de significância estatística.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Teste de Hipótese para duas populações</span>"
    ]
  },
  {
    "objectID": "tabela-frequencias.html",
    "href": "tabela-frequencias.html",
    "title": "16  Tabela de frequências",
    "section": "",
    "text": "Sejam \\(X,Y\\) variáveis aleatórias cujos valores observados são \\(B_{1},B_{2},\\dots,B_{l}\\) e \\(A_{1},A_{2}, A_{k}\\), respectivamente. Observam-se os seguintes dados \\[\n\\begin{array}{ccc}\n\\mathrm{ind.}  & X & Y \\\\\n1  & B_{2} & A_{1} \\\\\n2 & B_{7} & A_{3} \\\\\n\\vdots & \\vdots  & \\vdots \\\\\nn  & B_{1} & A_{5}\n\\end{array}\n\\] Colocamos nossos dados numa tabela de frequências absolutas observadas \\[\n\\begin{array}{c|cccc|c}\nX\\setminus Y  & A_{1} & A_{2} & \\dots & A_{k} & \\mathrm{Total}~X \\\\\n\\hline\nB_{1} & O_{11} & O_{12} & \\dots & O_{1k} & O_{1\\cdot} \\\\\nB_{2} & O_{11} & O_{12} & \\dots & O_{1k} & O_{2\\cdot} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\nB_{l} & O_{l1} & O_{l2} & \\dots & O_{lk} & O_{l\\cdot} \\\\\n\\hline\n\\mathrm{Total}~Y  & O_{\\cdot_{1}} & O_{\\cdot_{2}}  & \\dots  & O_{\\cdot k}  & n\n\\end{array}\n\\]\nTemos nossa tabela de frequências esperadas [[Teste de Hipótese|sob]] \\(H_{0}\\) (Independência) \\[\n\\begin{array}{c|cccc|c}\nX\\setminus Y  & A_{1} & A_{2} & \\dots & A_{k} & \\mathrm{Total}~X \\\\\n\\hline\nB_{1} & E_{11} & E_{12} & \\dots & E_{1k} & O_{1\\cdot} \\\\\nB_{2} & E_{11} & E_{12} & \\dots & E_{1k} & O_{2\\cdot} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\nB_{l} & E_{l1} & E_{l2} & \\dots & E_{lk} & O_{l\\cdot} \\\\\n\\hline\n\\mathrm{Total}~Y  & O_{\\cdot_{1}} & O_{\\cdot_{2}}  & \\dots  & O_{\\cdot k}  & n\n\\end{array}\n\\] Em que \\[\nE_{ij} = \\frac{O_{i \\cdot} \\cdot O_{\\cdot j}}{n}\n\\] Note que, sob independência \\[\n\\begin{aligned}\nP(B_{i}\\cap A_{j}) &= P(B_{i}) \\cdot P(A_{j}) \\\\\nE_{ij} &= n \\cdot P(B_{i}\\cap A_{j}) \\stackrel{\\mathrm{ind.}}{=} n P(B_{i}) \\cdot P_{A_{j}}\n\\end{aligned}\n\\] Estimando \\(P(B_{i}), P(A_{j})\\) temos \\[\n\\widehat{P(B_{i})} = \\frac{O_{i\\cdot}}{n}, \\widehat{P(A_{j})}= \\frac{O_{\\cdot j}}{n}\n\\] Logo, o valor esperado estimado é \\[\n\\widehat{E_{ij}}=n \\cdot \\widehat{P(B_{i})}\\cdot\\widehat{P(A_{j})} = \\frac{O_{i \\cdot} \\cdot O_{\\cdot j}}{n}\n\\]\nEm ambos testes, usaremos a seguinte estatística para testar suas hipóteses (independência e homogeneidade) \\[\n\\chi^2 = \\sum^k_{i=1}\\sum^l_{j=1} \\frac{(O_{ij}-E_{ij})^2}{E_{ij}}\n\\] Sob \\(H_{0}\\), ou seja, \\[\n\\chi^2_{obs}\\sim \\chi^2_{(k-1)(l-1)}\n\\] Dessa forma, rejeitamos a hipótese \\(H_{0}\\) a \\(\\alpha\\) graus de liberdade se \\[\n\\chi^2_{obs} &gt; c_{p}\n\\] em que \\(c_{p}\\) satisfaz \\(P(\\chi^2_{(k-1)(l-1)} &gt; c_{p})=\\alpha\\).\nObservação Essa aproximação com a \\(\\chi^2\\) só funciona de modo razoável quando cada \\(E_{ij}&gt;5\\)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Tabela de frequências</span>"
    ]
  },
  {
    "objectID": "analise-aderencia.html",
    "href": "analise-aderencia.html",
    "title": "17  Teste Qui-Quadrado e análise de aderência",
    "section": "",
    "text": "17.1 Exemplo\nA análise de aderência testa a distribuição dos dados: \\[\n\\begin{cases}\nH_{0}: P= P_{0} \\\\\nH_{1}: P \\neq P_{0}\n\\end{cases}\n\\] Em que \\(P_{0}\\) é a medida de probabilidade especificada que governaria (sob \\(H_0\\)) os eventos observados.\nNeste teste co,paramos a frequência observada com a frequência esperada em \\(k\\) eventos disjuntos e distintos observáveis. \\[\n\\begin{array}{c|cc}\n\\text{Eventos}  & 1  & 2 & \\dots & k \\\\\n\\hline\nP_{0} & P_{01} & P_{02} & \\dots & P_{0k} \\\\\nE_{i}  & E_{1} & E_{2} & \\dots & E_{k} \\\\\nO_{i} & O_{1} & O_{2} & \\dots & O_{k}\n\\end{array}\n\\]\nEm que observou-se uma amostra de tamanho \\(n\\). Temos também que \\(E_{i}\\) é o valor esperado do número de eventos \\(i\\) sob \\(H_{0}\\) \\[\n\\mathrm{Freq. Esperada} = E_{i} = P_{0i} \\cdot n\n\\]\ne \\(\\mathrm{Freq. Observada} = O_{i}\\) é o numero real de eventos \\(i\\) observados na amostra. A estatística para testar \\(H_{0}\\) é \\[\n\\chi^2 = \\sum^k_{i=1} \\frac{(E_{i}-O_{i})^2}{E_{i}}\n\\]\nque, sob \\(H_0\\) - ou seja, sob a hipótese de que \\(P_{0}\\) é de fato a medida de probabilidade que governa o comportamento probabilístico do evento - é aproximadamente \\[\n\\underbracket{\\chi^2 \\sim \\chi^2_{(k-1)}}_{\\mathrm{Sob}~H_{0}}\n\\]\n*Esse procedimento é confiável sempre que \\(E_{i}&gt;5 \\forall i \\in \\{ 1,\\dots,k \\}\\)\nConsidere que queremos verificar se os números sorteados nos concursos da Mega Sena são de fato uniformemente distribuídos. Nesse caso, analisaremos 60 eventos, cuja probabilidade de cada um seria, caso uniformemente distribuídos, \\(\\frac{1}{60}\\). \\[\n\\begin{cases}\nH_{0}: P = P_{0} \\\\\nH_{1}: P \\neq P_{0}\n\\end{cases}\n\\]\nEm que \\(P_{0}(\\{ i \\}) = \\frac{1}{60} \\forall i \\in \\{ 1,2,\\dots 60 \\}\\)\nVamos criar a tabela para as frequências. Consideraremos a primeira bola de todos os \\(2800\\) sorteios da Mega. \\[\n\\begin{array}{c|ccc}\n\\mathrm{Eventos}  &  1 & 2 & \\dots & 60\\\\\n\\hline\nP_{0} & \\frac{1}{60} & \\frac{1}{60} & \\dots & \\frac{1}{60} \\\\\nE_{i} & \\frac{2800}{60}  & \\frac{2800}{60}  & \\dots  & \\frac{2800}{60} \\\\\nO_{i} & 42 & 48 & \\dots  & 55\n\\end{array}\n\\] Portanto, \\[\n\\chi^2 = \\sum^{60}_{i} \\frac{(46.7 - O_{i})^2}{46.7} \\stackrel{a}{\\sim} \\chi^2_{59}\n\\] Considerando um nível de significância de \\(\\alpha=5\\%\\), calculamos o ponto crítico \\(c\\) tal que \\[\nP(\\chi^2_{59}&gt;c) = 0.05\n\\]\nPelo computador, encontramos \\(c = 77.93\\) Logo, como \\(\\chi^2=56.68 &lt; 77.93\\), concluímos que, sob \\(H_{0}\\), não há evidências de que o modelo não seja equiprovável a \\(5\\%\\) de significância de estatística.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Teste Qui-Quadrado e análise de aderência</span>"
    ]
  },
  {
    "objectID": "analise-aderencia.html#k-grupos",
    "href": "analise-aderencia.html#k-grupos",
    "title": "17  Teste Qui-Quadrado e análise de aderência",
    "section": "17.2 K-Grupos",
    "text": "17.2 K-Grupos\n\n(Morettin, Pag.404 E.7) Considere os \\(n=30\\) dados abaixo que supostamente seguem uma distribuição normal \\(N(10,25)\\). (usando os dados do livro já em ordem) \\[\n\\begin{array}{}\n1.01 & 1.73 & 3.93 & 4.44 & 6.37 & 6.51 \\\\\n\\vdots  & \\vdots  & \\vdots & \\vdots  & \\vdots & \\vdots \\\\\n14.11 & 14.6 & 14.64 & 14.75 & 16.68 & 22.14\n\\end{array}\n\\] Queremos testar se os dados de fato se distribuem de acordo com \\(N(10,25)\\). \\[\n\\begin{cases}\nH_{0}:P=N(10,25) \\\\\nH_{1}:P\\neq N(10,25)\n\\end{cases}\n\\] Sob \\(H_{0}\\), podemos dividir a distribuição normal em \\(k\\) blocos. Escolheremos \\(k=4\\) delimitado pelos quartis teóricos dessa distribuição normal. (Primeiro padronizamos, encontramos os valores pela tabela, então voltamos para nossa normal) \\[\n\\begin{cases}\nq_{1} = 6.63 \\\\\nq_{2} = 10 \\\\\nq_{3} = 13.3\n\\end{cases} \\stackrel{\\mathrm{Intervalos}}{\\Rightarrow}\n\\begin{cases}\n1.(-\\infty, q_{1}) \\\\\n2.[q_{1},q_{2}] \\\\\n3.(q_{2},q_{3}] \\\\\n4.(q_{3},\\infty)\n\\end{cases}\n\\] Podemos produzir uma tabela com as frequências por intervalo \\[\n\\begin{array}{c|cc}\n\\mathrm{Eventos}  &  1.  & 2. & 3. & 4.\\\\\n\\hline\nE_{i} & 0.25 \\cdot 30=7.5  & 7.5 & 7.5 & 7.5 \\\\\nO_{i} & 6  & 9 & 9 & 6 \\\\\n\\end{array}\n\\] \\[\n\\chi^2 = \\sum^4_{i=1} \\frac{(7.5 - O_{i})^2}{7.5} = 1.2\n\\] Na \\(\\chi^2_{3}\\) (número de nichos), com nível de significância \\(\\alpha=0.10\\), \\(c = 6.25\\). Como \\(\\chi^2=1.2&lt;6.25\\), concluímos que não há evidências de que a distribuição dos dados difere de uma \\(N(10,25)\\) a \\(\\alpha=10\\%\\) de significância estatística",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Teste Qui-Quadrado e análise de aderência</span>"
    ]
  },
  {
    "objectID": "testes-ind-homo.html",
    "href": "testes-ind-homo.html",
    "title": "18  Testes de Independência e Homogeneidade",
    "section": "",
    "text": "18.1 Teste de Homogeneidade\nCom a ajuda da tabela de frequências, conseguimos testar independência entre eventos e homogeneidade em distribuição de eventos. Por mais que utilizem o mesmo mecanismo, os dois testes são interpretados de forma diferentes e, portanto, também apresentados individualmente nesta seção.\nUsamos esse teste para verificar se as medidas de probabilidade de vários grupos diferentes são iguais (seguem uma mesma distribuição).\nOs totais marginais para cada grupo devem ser fixados antes de executarmos o experimento. \\[\n\\begin{cases}\nH_{0}: \\text{Os grupos são independentes} \\\\\nH_{1} : \\text{Pelo menos um dos grupos não é indepndente}\n\\end{cases}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Testes de Independência e Homogeneidade</span>"
    ]
  },
  {
    "objectID": "testes-ind-homo.html#teste-de-independência",
    "href": "testes-ind-homo.html#teste-de-independência",
    "title": "18  Testes de Independência e Homogeneidade",
    "section": "18.2 Teste de independência",
    "text": "18.2 Teste de independência\nUsamos esse teste para verificar se os eventos são independentes.\nAqui, apenas o tamanho amostral (total dos totais) é fixado.\n\\[\n\\begin{cases}\nH_{0}: \\text{Os grupos se distribuem de forma equivalente} \\\\\nH_{1} : \\text{Pelo menos um dos grupos não se distribui de forma equivalente}\n\\end{cases}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Testes de Independência e Homogeneidade</span>"
    ]
  },
  {
    "objectID": "testes-ind-homo.html#exemplos",
    "href": "testes-ind-homo.html#exemplos",
    "title": "18  Testes de Independência e Homogeneidade",
    "section": "18.3 Exemplos",
    "text": "18.3 Exemplos\n\n18.3.1 Primeiro exemplo (homogeniedade)\n510 segurados foram amostrados, sendo 200 de São Paulo, 100 do Ceará e 210 de Pernambuco. O objetivo é verificar se o número de acidentes se distribui igualmente entre os estados. \\[\n\\begin{array}{ccc}\n\\mathrm{Indivíduos}  & \\mathrm{Estado}  & \\mathrm{Sinistralidade}\\\\\n1 & \\mathrm{SP} & 1 \\\\\n\\vdots  & \\vdots & \\vdots \\\\\n200  &  \\mathrm{SP}  & 0 \\\\\n1 & \\mathrm{CE} & 1 \\\\\n\\vdots  & \\vdots & \\vdots \\\\\n100  &  \\mathrm{CE}  & 0 \\\\\n1 & \\mathrm{PE} & 1 \\\\\n\\vdots  & \\vdots & \\vdots \\\\\n210  &  \\mathrm{PE}  & 0\n\\end{array}\n\\] Tabela Observada \\[\n\\begin{array}{c|cc|c}\n&     \\mathrm{Sinistralidade}   &  \\\\\n\\mathrm{Estado}  & 1 & 0 &  \\mathrm{Total} \\\\\n\\hline\n\\mathrm{SP} & 60 & 140 & 200 \\\\\n\\mathrm{CE} & 10 & 90 & 100 \\\\\n\\mathrm{PE} & 50 & 160 & 210 \\\\\n\\hline\n\\mathrm{Total} & 120 & 390 & 510\n\\end{array}\n\\] Tabela esperada \\[\n\\begin{array}{c|cc|c}\n&     \\mathrm{Sinistralidade}   &  \\\\\n\\mathrm{Estado}  & 1 & 0 &  \\mathrm{Total} \\\\\n\\hline\n\\mathrm{SP} & 47 & 153 & 200 \\\\\n\\mathrm{CE} & 24 & 76 & 100 \\\\\n\\mathrm{PE} & 49 & 161 & 210 \\\\\n\\hline\n\\mathrm{Total} & 120 & 390 & 510\n\\end{array}\n\\] Temos nossa estatística qui-quadrado \\[\n\\begin{aligned}\n\\chi^2_{obs} &= \\frac{(60-47)^2}{47} + \\frac{(140-153)^2}{153} + \\frac{(10-24)^2}{24} + \\frac{(90-76)^2}{76} \\\\\n&+ \\frac{(50-49)^2}{49} + \\frac{(160-161)^2}{161} = 15.47\n\\end{aligned}\n\\] Concluiremos o teste tomando \\(\\alpha = 1\\%\\) de significância estatística Sabemos que, sob \\(H_{0}\\), \\[\n\\chi^2_{obs} \\sim \\chi^2_{(3-1)(2-1)}\n\\] Logo, devemos encontrar \\(c_{p}\\) tal que \\[\nP(\\chi^2_{2}&gt; c_{p}) = 1\\%\n\\] Pela tabela, \\(c_{p}=9.21\\) Como \\(15.47 &gt; 9.21\\), concluímos que, a sinistralidade não se distribui de forma homogênea entre os estados de SP, CE e PE a \\(1\\%\\) de significância.\n\n\n18.3.2 Outro Exemplo (independência)\nTemos nossa tabela de valores observados: \\[\n\\begin{array}{c|ccc|c}\n\\mathrm{Opinião}  & \\mathrm{1ª~Tent}  & \\mathrm{2ª~Tent}  & \\mathrm{3ª~Tent}  & \\mathrm{Total} \\\\\n\\hline\n\\mathrm{Excelente}  &  62  & 36 & 12 & 110\\\\\n\\mathrm{Satisfatório}  & 84 & 42 & 14 & 140\\\\\n\\mathrm{Insatisfatório}  & 24 & 22 & 24 & 70 \\\\\n\\hline\n\\mathrm{Total}  & 170 & 100 & 50 & 320\n\\end{array}\n\\] Nossa tabela de valores esperados (arredondados): \\[\n\\begin{array}{c|ccc|c}\n\\mathrm{Opinião}  & \\mathrm{1ª~Tent}  & \\mathrm{2ª~Tent}  & \\mathrm{3ª~Tent}  & \\mathrm{Total} \\\\\n\\hline\n\\mathrm{Excelente}  &  58 & 34 & 17 & 110 \\\\\n\\mathrm{Satisfatório}  & 74 & 44 & 22 & 140\\\\\n\\mathrm{Insatisfatório}  & 37 & 22 & 11 & 70 \\\\\n\\hline\n\\mathrm{Total}  & 170 & 100 & 50 & 320\n\\end{array}\n\\] \\[\n\\begin{aligned}\n\\chi^2_{obs} &= \\frac{(62-58)^2}{58} + \\frac{(36-34)^2}{34} + \\frac{(12-17)^2}{17} + \\frac{(84-74)^2}{74} +\n\\frac{(42-44)^2}{44} \\\\\n&+ \\frac{(14-22)^2}{22} + \\frac{(24-37)^2}{37} + \\frac{(22-22)^2}{22} + \\frac{(24-11)^2}{11} = 26.14\n\\end{aligned}\n\\] Considerando \\(\\alpha= 5\\%\\), precisamos encontrar \\(c_{p}\\) tal que \\[\nP(\\chi^2_{4}&gt;c_{p})=5\\% \\Rightarrow c_{p} = 9.49\n\\] Como \\(26.14 &gt; 9.49\\), podemos concluir que, a \\(5\\%\\) de significância estatística, existem evidências que o número da tentativa tem influência sobre a opinião do cliente.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Testes de Independência e Homogeneidade</span>"
    ]
  }
]