```{julia}
#| output: false
using Pkg; Pkg.add(["Distributions", "StatsBase", "Plots", "StatsPlots", "LaTeXStrings"])
```
# Teste de Hipótese - Aprofundamento

Seja $\boldsymbol{X}_n = (X_1, \dots, X_n)$ [amostra aleatória](populacao-e-amostra.qmd#sec-aa) de $X \sim f_\theta, \theta \in \Theta$.

Note que, no caso discreto:
$$
P_\theta(X \in A) = \sum_{x \in A} f_\theta(x), \forall \theta \in \Theta.
$$
já no caso contínuo,
$$
P_\theta(X \in A) = \int_{A} f_\theta(x), \forall \theta \in \Theta.
$$

Temos uma família $\mathcal{P} = \{P_\theta : \theta \in \Theta\}$ de probabilidades que podem explicar o comportamento
dos dados. Um dos objetivos do teste de hipótese é verificar se podemos reduzir $\mathcal{P}$ para uma família menor $\mathcal{P}_0 \subseteq \mathcal{P}$:
$$
\mathcal{P}_0 = \{P_\theta : \theta \in \Theta_0\}, \Theta_0 \subseteq \Theta.
$$

Definimos uma *hipótese estatística*
$$
\mathcal{H}_0 : \theta \in \Theta_0 \iff \mathcal{H}_0 : P_\theta \in \mathcal{P}_0
$$
$\mathcal{H}_0$ afirma que

> "A medida de probabilidade que explica os dados está em $\mathcal{P}_0$"

No caso em que $\Theta_0 = \{\theta_0\}$, então a hipótese
$\mathcal{H}_0 : \theta = \theta_0 ( \iff \mathcal{H}_0 : P_{\theta} = P_{\theta_0})$ afirma que $P_{\theta_0}$ explica o
comportamento probabilístico dos dados observados.

$\mathcal{H}_0$ é chamada de *hipótese nula*.

Note que a negação de $\mathcal{H}_0$ é

> "Não é o caso que a família $\mathcal{P}_0$ contenha a medida que explica os dados".

Ou seja, a negação de $\mathcal{H}_0$ sugere que a medida que explica os dados pode, inclusive, não estar contida em $\mathcal{P}$.

Definimos também uma hipótese alternativa,
$$
\mathcal{H}_1: \theta \in (\Theta \setminus \Theta_0) \iff \mathcal{H}_1 : P_\theta \in (\mathcal{P} \setminus \mathcal{P}_0)
$$

<!-- Em termos de espaço paramétrico, -->
<!-- TODO: Desenho do Theta maior e theta menor e visualização dos conjuntos e relação com hipóteses e os thetas -->

<!-- Em termos de família de probabilidades,  -->
<!-- TODO: Desenho do mathcal P maior e P0 e visualização dos conjuntos e relação com hipóteses e os P_thetas -->

Observe que $\mathcal{H}_0$ e $\mathcal{H}_1$ não são exaustivas: na prática, ambas podem ser falsas.

Se o analista considerar que $\mathcal{P}$ contém a medida que explica os dados, então, condicional a essa crença, $\mathrm{\mathcal{H}_0}$ e
$\mathrm{\mathcal{H}_1}$ são exaustivas.

## Exemplo

Seja $\boldsymbol{X}_n$ a.a. de $X\sim N(\mu, \sigma^2), \theta = (\mu, \sigma^2) \in \mathbb{R}\times\mathbb{R}_+$.

Considere as seguintes hipóteses
$$
\begin{cases}
\mathcal{H}_0 : \mu = 0 \\
\mathcal{H}_1 : \mu \neq 0.
\end{cases}
$$
Note que
$$
\begin{cases}
\mathcal{H}_0 : \theta \in \Theta_0 \\
\mathcal{H}_1 : \theta \in \Theta \setminus \Theta_0,
\end{cases}
$$
em que
$$
\begin{aligned}
\Theta_0 &= \{(\mu, \sigma^2) \in \mathbb{R} \times \mathbb{R}_+ : \mu = 0\} \\
\Theta_1 &= \Theta \setminus \Theta_0 = \{(\mu, \sigma^2) \in \mathbb{R} \times \mathbb{R}_+ : \mu \neq 0\}.
\end{aligned}
$$
Disso, temos que
$$
\begin{aligned}
\Theta &= \Theta_0 \cup \Theta_1 \\
\Theta_0 &\cap \Theta_1 = \emptyset.
\end{aligned}
$$


As hipóteses acima podem ser reescritas em termos das suas respectivas famílias de medidas de probabilidades da seguinte forma:
$$
\begin{cases}
\mathcal{H}_0: P \in \mathcal{P}_0 \\
\mathcal{H}_1: P \in \mathcal{P}_1
\end{cases}
$$

em que $\mathcal{P}_0 = \{N(\mu, \sigma^2): \ \mu = 0, \ \sigma^2 > 0 \}$ e
$\mathcal{P}_1 = (\mathcal{P} - \mathcal{P}_0) = \{ N(\mu, \sigma^2): \ \mu \neq 0, \ \sigma^2 > 0 \}$

Observe que, se assumirmos que a distribuição normal explica os dados, então $\mathcal{H}_0$ e $\mathcal{H}_1$ são exaustivas.
Caso contrário, existe uma terceira opção: $\neg(\mathcal{H}_0 \lor \mathcal{H}_1)$.

## Hipóteses estatísticas

Toda hipótese estatística é uma interpretação de uma hipótese científica.

| Hipótese científica |     Hipótese estatística   | Hipótese paramétrica |
|:-------------------:|:--------------------------:|:--------------------:|
| "A moeda é honesta" | $X \sim \mathrm{Ber}(0.5)$ |    $\theta = 0.5$    | 

As hipótese estatísticas são escritas em termos de medidas de probabilidade, mas podem também ser representadas em termos
do espaço paramétrico:

$$ 
\begin{cases}
\mathcal{H}_0: P_\theta \in \mathcal{P}_0 \\
\mathcal{H}_1: P_\theta \in \mathcal{P}_1 =  (\mathcal{P} \setminus \mathcal{P}_0),
\end{cases}
$$

em que $\mathcal{P}_0 = \{P_\theta : \theta \in \Theta_0\}$ e  $\mathcal{P}_1 = \{P_\theta : \theta \in \Theta_1\}$ e
$\Theta_1 = \Theta \setminus \Theta_0$.

Observe que $\mathcal{H}_0, \mathcal{H}_1$ estão sempre restritas a modelo estatístico. Entretanto, a negação de
$\mathcal{H}_0, \neg \mathcal{H}_0$, não está restrita ao modelo adotado.

:::{.callout-note title="Algumas considerações lógicas"}
<!-- TODO: desenho das famílias e tal igual da outra aula com o P0 no cantinho e P1 grandão no resto do quadrado uau h0 no p0 e 1 no p1 e ta mas pode ta fora, terceira via... -->

1. Se o analista considerar que $\mathcal{H}_0$ ou $\mathcal{H}_1$ são verdadeiros, então ele está considerando que a medida que *explica* ou *gera*
os dados está em $\mathcal{P}$. Está é a suposição de que o universo de possibilidades é fechado (suposição de Neyman-Pearson; Teoria da Decisão).

2. $\mathcal{H}_0$ e $\mathcal{H}_1$ não pode ser simultaneamente verdadeiras.

3. Se $\neg \mathcal{H}_0$ e $\neg \mathcal{H}_1$, então a medida que explica/gera os dados não está em $\mathcal{P}$.

4. $\neg \mathcal{H}_0 \not\Rightarrow \mathcal{H}_1$. Isto é, se provarmos que $\mathcal{H}_0$ é falsa, não necessariamente
$\mathcal{H}_1$ é verdadeira.

5. $\mathcal{H}_1 \Rightarrow \neg \mathcal{H}_0$.
:::

## Tipos de hipótese

Sejam $\mathcal{H}_0, \mathcal{H}_1$ as hipóteses nula e alternativa, respectivamente, tais que
$$
\begin{cases}
\mathcal{H}_0 : \theta \in \Theta_0 \\
\mathcal{H}_1 : \theta \in \Theta_1
\end{cases}
$$
em que $\Theta_0 \neq \emptyset, \Theta_0 \cup \Theta 1 = \Theta, \Theta_0 \cap \Theta_1 = \emptyset$. Dizemos que $\mathcal{H}_0$ é
uma **hipótese simples** se $\# \Theta_0 = 1$. Caso contrário, dizemos que é uma **hipótese composta**.

### Exemplos

1. Seja $\boldsymbol{X}_n = (X_1, \dots, X_n)$ a.a. de $X\sim \mathrm{Ber}(\theta), \theta \in \{0.5, 0.9\}$. Considere as hipóteses
$$
\begin{cases}
\mathcal{H}_0 : \theta = 0.5 \\
\mathcal{H}_1 : \theta = 0.9
\end{cases}\ \ \ \text{Ambas são simples!}
$$

2. Seja $\boldsymbol{X}_n = (X_1, \dots, X_n)$ a.a. de $X\sim \mathrm{Ber}(\theta), \theta \in (0,1)$. Considere as hipóteses
$$
\begin{cases}
\mathcal{H}_0 : \theta = 0.5\ \  \text{Hipótese simples} \\
\mathcal{H}_1 : \theta \neq 0.5\ \ \text{Hipótese composta}
\end{cases}
$$
Note que
$$
\begin{cases}
\Theta_0 = \{0.5\} \\
\Theta_1 = (0,0.5) \cup (0.5, 1) \\
\end{cases}
$$

3. Seja $\boldsymbol{X}_n$ a.a. de $X \sim N(\mu, \sigma^2) \in \mathbb{R}\times\mathbb{R}_+$
$$
a)\ \begin{cases}
\mathcal{H}_0 : \mu = 0.5\ \  \text{Hipótese composta!} \\
\mathcal{H}_1 : \mu \neq 0.5\ \ \text{Hipótese composta}
\end{cases}
$$
Note que como
$$
\begin{cases}
\Theta_0 = \{(\mu, \sigma^2) \in \mathbb{R}\times\mathbb{R}_+ : \mu = 0\}
\Theta_0 = \{(\mu, \sigma^2) \in \mathbb{R}\times\mathbb{R}_+ : \mu \neq 0\}
\end{cases}
$$
têm mais de um elemento, concluímos que ambas hipóteses são compostas.

$$
b)\ \begin{cases}
\mathcal{H}_0 : (\mu, \sigma^2) = (0,1)\ \  \text{Hipótese simples!} \\
\mathcal{H}_1 : (\mu, \sigma^2) \neq (0,1)\ \ \text{Hipótese composta}
\end{cases}
$$

### Tipos de decisão sobre as hipóteses

1. Se o espaço de possibilidades for fechado, isto é, $\mathcal{H}_0$ ou $\mathcal{H}_1$ é verdadeira, então:
a) Aceitamos $\mathcal{H}_0$ (rejeitamos $\mathcal{H}_1$)
b) Aceitamos $\mathcal{H}_1$ (rejeitamos $\mathcal{H}_0$)

Observe que, neste caso, não há terceira opção (abordagem de Neyman-Pearson).

2. Se o espaço de possibilidades for aberto, isto é, temos a terceira opção de que o modelo está equivocado, então:
a) Não rejeitamos $\mathcal{H}_0$ (**Não** significa que aceitamos $\mathcal{H}_0$)
b) Rejeitamos $\mathcal{H}_0$ (**Não** significa aceitar $\mathcal{H}_1$, pela existência da terceira opção.)

Neste caso temos uma terceira opção (abordagem de Fisher).

Atualmente, dizemos apenas que há ou não há evidências para rejeitarmos $\mathcal{H}_0$. **Não** se diz aceitar
"$\mathcal{H}_0$"

### Tipos de Erro

Se o universo for aberto:

*Erro tipo I*: "Rejeitar $\mathcal{H}_0$ quando este é verdadeiro"

*Erro tipo II*: "Não rejeitar $\mathcal{H}_0$ quando este é falso"


| $\mathcal{H}_0$ | Não rejeitar | Rejeitar |
|:--------:|:-----------:|:---------------:|
Verdadeiro |    Acerto   | Erro tipo I |
   Falso   | Erro tipo II|    Acerto   |

Se o universo for fechado:

Erro tipo I: "Rejeitar $\mathcal{H}_0$ (ou aceitar $\mathcal{H}_1$) quando este é verdadeiro"

Erro tipo II: "Aceitar $\mathcal{H}_0$ (ou rejeitar $\mathcal{H}_1$ quando este é falso"

## Teste de Hipótese de Neyman-Pearson

Considere o universo fechado.

### Função teste

*Definição*. Seja $\delta : \mathfrak{X}^{(n)} \rightarrow \{0,1\}$ uma função tal que
$$
\delta(\boldsymbol{X}_n) = \begin{cases}
0,\ \text{Se não rejeitamos}\ \mathcal{H}_0 \\
1,\ \text{Se rejeitamos}\ \mathcal{H}_0 \\
\end{cases}
$$
em que $\mathfrak{X}^{(n)} = \{x \in \mathbb{R}^n : f_\theta^{\boldsymbol{X}_n}(\boldsymbol{x}) > 0 \}$, $\mathcal{H}_0 : \theta \in \Theta_0$,
$\mathcal{H}_1 : \theta \in \Theta_1$, $\Theta_0 \neq \emptyset, \Theta_1 \neq \emptyset, \Theta_0 \cap \Theta_1 = \emptyset, \Theta_0 \cup \Theta_1 = \Theta$.

Dizemos que $\delta(\cdot)$ é uma *função teste*.

Região de rejeição de $\mathcal{H}_0$:
$$
S_{\delta} = \{\boldsymbol{x} \in \mathfrak{X}^{(n)} : \delta(\boldsymbol{x}) = 1 \}
$$

Região de não rejeição de $\mathcal{H}_0$:
$$
S_{\delta}^c = \{\boldsymbol{x} \in \mathfrak{X}^{(n)} : \delta(\boldsymbol{x}) = 0 \}
$$

<!-- TODO: desenho das regiões dentro do $\mathfrak{X}$ -->

#### Exemplo

Seja $\boldsymbol{X}_n$ a.a. de $X\sim\mathrm{Ber}(\theta), \theta \in \{0.1, 0.9\}$ e as hipóteses
$$
\begin{cases}
\mathcal{H}_0 : \theta = 0.1 \\
\mathcal{H}_1 : \theta = 0.9
\end{cases}
$$

Defina $\delta_i : \{0,1\}^n \rightarrow \{0, 1\}, i = 1, 2$ tais que

$$
\begin{aligned}
\delta_1(\boldsymbol{x}_n) &= \begin{cases}
0, \bar{x} \leq 0.5 \\
1, \bar{x} > 0.5 \\
\end{cases} \\
\delta_2(\boldsymbol{x}_n) &= \begin{cases}
0, \bar{x} \leq 0.8 \\
1, \bar{x} > 0.8 \\
\end{cases}
\end{aligned}
$$

### Função Poder do teste $\delta$ {#sec-funpoder}

*Definição*. A função poder do teste $\delta$ é
$\pi_\delta(\theta) = P_\theta(S_\delta)$

:::{.callout-note title="Observações"}
1.
$$
\pi_\delta(\theta), \forall \theta \in \Theta_0,
$$
são probabilidades de rejeitar $\mathcal{H}_0$ quando esta é verdadeira.

2.
$$
\pi_\delta(\theta), \forall \theta \in \Theta_1,
$$
são probabilidades de rejeitar $\mathcal{H}_0$ quando esta é falsa.
:::

*Definição*. O *nível de significância* é qualquer valor $\alpha \in (0,1)$ tal que:
$$
P_\theta(S_\delta) \leq \alpha, \forall \theta \in \Theta_0.
$$
em termos de função poder,
$$
\pi_\delta(\theta) \leq \alpha, \forall \theta \in \Theta_0
$$


*Definição*. O tamanho do teste $\delta$ é
$$
\tau_\delta = \sup_{\theta \in \Theta_0} P_\theta(S_\delta) \leq \alpha
$$
em termos de função poder,
$$
\tau_\delta = \sup_{\theta \in \Theta_0} \pi_\delta(\theta)
$$

#### Probabilidade dos Erros I, II

:::{.callout-tip title="Tipos de Erro"}
Relembrando os tipos de erro:
$$
\begin{cases}
\text{Erro tipo I:}\ \text{Rejeitar $\mathcal{H}_0$ quando esta é verdadeira} \\
\text{Erro tipo II:}\ \text{Não rejeitar $\mathcal{H}_0$ quando esta é falsa}
\end{cases}
$$
:::

$$
\alpha_{\delta, \mathrm{max}} = \tau_\delta = \sup_{\theta \in \Theta_0} \pi_\delta(\theta)
$$
é a probabilidade máxima de cometer o Erro tipo I e
$$
\beta_{\delta, \mathrm{max}} = \sup_{\theta \in \Theta_1} (1-\pi_\delta(\theta))
$$
é a probabilidade máxima de cometer o Erro tipo II.

Quando $\mathcal{H}_0, \mathcal{H}_1$ são simples, temos
$$
\begin{aligned}
\alpha_{\delta, \max} &= \pi_\delta(\theta_0) = P_\theta(S_\delta) \\
\beta_{\delta, \max} &= 1 - \pi_\delta(\theta_1),
\end{aligned}
$$
com
$$
\begin{aligned}
\mathcal{H}_0 : \theta = \theta_0 \\
\mathcal{H}_1 : \theta \neq \theta_1.
\end{aligned}
$$

:::{.callout-warning title="Notação (equivocada) em alguns materiais"}
Alguns livros, especialmente para iniciantes ou profissionais de outras áreas, podem escrever:
$$
\begin{aligned}
\alpha_{\delta, \max} &= P(\text{Erro tipo I}) \\
&= P(\text{Rejeitar} | \text{$\mathcal{H}_0$ é verdadeira}) \\
&= P(\delta(\boldsymbol{X}_n) = 1 | \text{$\mathcal{H}_0$ é verdadeira}),
\end{aligned}
$$
$$
\begin{aligned}
\beta_{\delta, \max} &= P(\text{Erro tipo II}) \\
&= P(\text{Não rejeitar} | \text{$\mathcal{H}_0$ é falsa}) \\
&= P(\delta(\boldsymbol{X}_n) = 0 | \text{$\mathcal{H}_0$ é falsa}).
\end{aligned}
$$
Note que, formalmente, isso **não** faz sentido na estatística clássica! O parâmetro em hipótese, $\theta$, é desconhecido
mas **não** aleatório.

Note que, na estatística clássica, as hipóteses são afirmações epistêmicas, e não experimentais (eventos do experimento).
Portanto, não são elementos da $\sigma$-álgebra do modelo estatístico. Logo, a notação
$$
P(S_\delta | \text{$\mathcal{H}_0$ é verdadeira})
$$
não está bem definida.
:::

#### Poder do Teste {#sec-poderteste}

Se $\mathcal{H}_1$ for simples, isto é, $\mathcal{H}_1 : \theta = \theta_1$, então o poder do teste é definido
$$
1 - \beta_{\delta, \max} = \pi_\delta(\theta_1)
$$

:::{.callout-warning title="Notação (equivocada) em alguns materiais"}
Alguns livros escrevem
$$
\mathrm{poder} = P(\text{Rejeitar $\mathcal{H}_0$} | \text{$\mathcal{H}_0$ é falsa})
$$
Isto está incorreto na interpretação clássica de estatística!
:::


## Lema de Neyman-Pearson
Estamos interessados em um teste $\delta^*$ que produza menor $\alpha_{\delta^*, \max}$ e maior poder possível. O Lema
de Neyman-Pearson apresenta um teste que, dentre os testes de tamanho $\alpha$, tem maior poder.

Seja $\boldsymbol{X}_n$ a.a. de $X \sim f_\theta, \theta \in \Theta = \{\theta_0, \theta_1\}$. Considere as hipóteses
$$
\begin{cases}
\mathcal{H}_0 : \theta = \theta_0 \\
\mathcal{H}_1 : \theta = \theta_1
\end{cases}
$$
nula e alternativa. A função teste $\delta^* : \mathfrak{X}^{(n)} \rightarrow \{0,1\}$ tal que
$$
\delta^*(\boldsymbol{x}_n) = \begin{cases}
0,\ \text{se}\ \mathcal{L}_{\boldsymbol{x}_n}(\theta_1) < \eta \cdot \mathcal{L}_{\boldsymbol{x}_n}(\theta_0) \\
1,\ \text{se}\ \mathcal{L}_{\boldsymbol{x}_n}(\theta_1) \geq \eta \cdot \mathcal{L}_{\boldsymbol{x}_n}(\theta_0),
\end{cases}
$$
em que $\eta$ satisfaz $\underbracket{P_{\theta_0}(\delta^*(\boldsymbol{X}_n) = 1)}_{\pi_{\delta^*}(\theta_0)} = \alpha$ para um $\alpha$ fixado apriori, é o teste mais
poderoso dentre todo os testes de tamanho $\alpha$.

### Exemplo (Normal)

Seja $\boldsymbol{X}_n$ a.a. de $X \sim N(\mu, \sigma^2)$ em que $\theta = (\mu, \sigma^2) \in \{(0,1), (0,2)\}$. Considere
$$
\begin{cases}
\mathcal{H}_0 : \theta = (0,1) \\
\mathcal{H}_1 : \theta = (0,2)
\end{cases}
$$
as hipóteses nula e alternativa.

Encontre o teste mais poderoso de tamanho $\alpha = 5\%$.

#### Solução:

De acordo com o Lema de Neyman-Pearson, o teste mais poderoso de tamanho $\alpha = 5\%$ é
$$
\delta^*(\boldsymbol{x}_n) = \begin{cases}
0,\ \text{se}\ \mathcal{L}_{\boldsymbol{x}_n}((0,2)) < \eta \cdot \mathcal{L}_{\boldsymbol{x}_n}((0,1)), \\
1,\ \text{se}\ \mathcal{L}_{\boldsymbol{x}_n}((0,2)) \geq \eta \cdot \mathcal{L}_{\boldsymbol{x}_n}((0,1)),
\end{cases}
$$
em que $\eta$ satisfaz $\pi_{\delta^*}((0,1)) = 5\%$.

A [função de verossimilhança](funcao-verossimilhanca.qmd) é
$$
\begin{aligned}
\mathcal{L}_{\boldsymbol{x}_n}(\theta) &= \frac{1}{(\sqrt{2\pi\sigma^2})^n} \cdot \mathrm{Exp}\left\{
-\frac{1}{2} \sum \frac{(x_i - \mu)^2}{\sigma^2}\right\}, \theta \in \{(0,1), (0,2)\} \\
&= \frac{1}{(\sqrt{2\pi\sigma^2})^n} \cdot \mathrm{Exp}\left\{
-\frac{1}{2} \sum \frac{x_i^2}{\sigma^2} \right\}, \theta \in \{(0,1), (0,2)\} \\
\Rightarrow  \mathcal{L}_{\boldsymbol{x}_n}(\theta_0) 
&= \frac{1}{(\sqrt{2\pi})^n} \cdot \mathrm{Exp}\left\{
-\frac{1}{2} \sum x_i^2 \right\} \\
\mathcal{L}_{\boldsymbol{x}_n}(\theta_1) 
&= \frac{1}{(\sqrt{4\pi})^n} \cdot \mathrm{Exp}\left\{
-\frac{1}{2} \sum \frac{x_i^2}{2} \right\} \\
\Rightarrow \mathcal{L}_{\boldsymbol{x}_n}((0,2)) &\geq \eta \cdot \mathcal{L}_{\boldsymbol{x}_n}((0,1)) \\
\iff \frac{\mathcal{L}_{\boldsymbol{x}_n}((0,2))}{\mathcal{L}_{\boldsymbol{x}_n}((0,1))} &\geq \eta \\
\Rightarrow &\frac{\frac{1}{(\sqrt{4\pi})^n} \cdot \mathrm{Exp}\left\{
-\frac{1}{2} \sum \frac{x_i^2}{2} \right\}}{\frac{1}{(\sqrt{2\pi})^n} \cdot \mathrm{Exp}\left\{
-\frac{1}{2} \sum x_i^2 \right\}} \geq \eta \\
\iff &\frac{1}{2^{\frac{n}{2}}} \mathrm{e}^{\frac{1}{4}\sum x_i^2} \geq \eta \\
\iff &\mathrm{e}^{\frac{1}{4}\sum x_i^2} \geq 2^{n/2}\eta \\
\iff &\frac{1}{4}\sum x_i^2 \geq \ln(2^{n/2}\eta) \\
\iff &\sum x_i^2 \geq 4 \ln(2^{n/2}\eta).
\end{aligned}
$$

Logo, $\sum x_i^2 \geq 4 \ln(2^{n/2}\eta)$ é equivalente a $\frac{\frac{1}{(\sqrt{4\pi})^n} \cdot \mathrm{Exp}\left\{
-\frac{1}{2} \sum \frac{x_i^2}{2} \right\}}{\frac{1}{(\sqrt{2\pi})^n} \cdot \mathrm{Exp}\left\{
-\frac{1}{2} \sum x_i^2 \right\}}$.

Observe ainda que
$$
\begin{aligned}
\pi_{\delta^*}((0,1)) &= P_{(0,1)}(\delta^*(\boldsymbol{X}_n) = 1) \\
&\stackrel{\text{Def.}}{=} P_{(0,1)}\left(\frac{\mathcal{L}_{\boldsymbol{x}_n}((0,2))}{\mathcal{L}_{\boldsymbol{x}_n}((0,1))} \geq \eta\right) \\
&\stackrel{\text{Desenv.}}{=} P_{(0,1)}\left(\sum X_i^2 \geq \eta^*\right),
\end{aligned}
$$
em que $\eta^* = 4\ln(2^{n/2} \eta)$. Como, sob $\mathcal{H}_0, \sum X_i^2 \sim \chi^2_n$, temos que
$$
\pi_{\delta^*}((0,1)) = P_{(0,1)}(\chi^2_{n} \geq \eta^*) = 5\%.
$$

Com $n=2$, temos que
$$
\pi_{\delta^*}((0,1)) = P_{(0,1)}(\chi^2_{n} \geq \eta^*) = 5\%. \Rightarrow \eta^* = 5.991.
$$

Portanto, o teste mais poderoso é
$$
\delta^*(\boldsymbol{x}_n) = \begin{cases}
0,\ \text{se}\ \sum x_i^2 < 5.991 \\
1,\ \text{se}\ \sum x_i^2 \geq 5.991.
\end{cases}
$$

### Exemplo (Bernoulli)
Seja $\boldsymbol{X}_n$ a.a. de $X \sim \mathrm{Ber}(\theta), \theta \in \{0.1, 0.9\}$. Considere
$$
\begin{cases}
\mathcal{H}_0 : \theta = 0.9 \\
\mathcal{H}_1 : \theta = 0.1
\end{cases}
$$
as hipóteses nula e alternativa e $n=10$. Encontre o Teste Mais Poderoso (TMP) de tamanho $\alpha = 10\%$.

#### Resposta

De acordo com o Lema de Neyman-Pearson, o teste mais poderoso de tamanho $\alpha = 5\%$ é
$$
\delta^*(\boldsymbol{x}_n) = \begin{cases}
0,\ \text{se}\ \mathcal{L}_{\boldsymbol{x}_n}(0.1) < \eta \cdot \mathcal{L}_{\boldsymbol{x}_n}(0.9), \\
1,\ \text{se}\ \mathcal{L}_{\boldsymbol{x}_n}(0.1) \geq \eta \cdot \mathcal{L}_{\boldsymbol{x}_n}(0.9),
\end{cases}
$$
em que $\eta$ satisfaz $\pi_{\delta^*}(0.9) = 10\%$.

Note que
$$
\begin{aligned}
\mathcal{L}_{\boldsymbol{x}_n}(\theta) &= \theta^{\sum x_i} (1-\theta)^{n - \sum x_i} \\
\Rightarrow \frac{\mathcal{L}_{\boldsymbol{x}_n}(0.1)}{\mathcal{L}_{\boldsymbol{x}_n}(0.9)} &= \frac{0.1^{\sum x_i} (0.9)^{n - \sum x_i}}{0.9^{\sum x_i} (0.1)^{n - \sum x_i}} \\
&= \frac{0.1^{\sum x_i} \cdot 0.9^{n} \cdot 0.1^{\sum x_i}}{0.9^{\sum x_i} \cdot 0.9^{\sum x_i} \cdot 0.1^n} \\
&= \left(\frac{0.1 \cdot 0.1}{0.9 \cdot 0.9}\right)^{\sum x_i} \cdot \left(\frac{0.9}{0.1}\right)^{n} \geq \eta \\
\iff &(\sum x_i) \ln \left(\frac{0.01}{0.81}\right) + n \ln (9) \geq \ln \eta \\
\iff &(\sum x_i) \ln \left(\frac{1}{81}\right) \geq \ln \eta - n \ln (9) \\
\iff &(-\sum x_i) \ln (81) \geq \ln \eta - n \ln (9) \\
\iff &-\sum x_i \geq \frac{\ln \eta - n \ln (9)}{\ln (81)} \\
\iff &\sum x_i \leq \frac{-\ln \eta + n \ln (9)}{\ln (81)}
\end{aligned}
$$

Logo, $\left(\frac{0.1 \cdot 0.1}{0.9 \cdot 0.9}\right)^{\sum x_i} \cdot \left(\frac{0.9}{0.1}\right)^{n} \geq \eta \iff \sum x_i \leq \frac{-\ln \eta + n \ln (9)}{\ln (81)}$.
$$
\pi_{\delta^*}(0.1) = P_{0.1}(\delta^*(\boldsymbol{X}_n) = 1) = P_{0.1}(\sum X_i \leq \eta^*) = \alpha = 10\%
$$

Note que, sob $\mathcal{H}_0, \sum^10_{i=1} X_i \sim \mathrm{Bin}(10, 0.9)$. Ademais,
$$
P_{0.9}(\sum X \leq 7) = 0.0702,\ \ \
P_{0.9}(\sum X \leq 8) = 0.2639.
$$
Logo, não é possível encontrar um valor para $\eta$ exato com esse tamanho do teste.

## Testes uniformemente mais poderosos (TUMP)

Considere
$$
\begin{cases}
\mathcal{H}_0 : \theta \in \Theta_0 \\
\mathcal{H}_1 : \theta \in \Theta_1
\end{cases}
$$
as hipóteses nula e alternativa em que $\Theta_0 \cup \Theta_1 = \Theta, \Theta_0 \cap \Theta_1 = \emptyset, \#(\Theta_0) \geq 1, \#(\Theta_1) \geq 1$.
O teste de tamanho $\alpha$ $\delta^* : \mathfrak{X}^{(n)} \rightarrow \{0,1\}$ é uniformemente mais poderoso se, e somente se,

1. $\sup\limits_{\theta \in \Theta_0} \pi_{\delta^*}(\theta) = \alpha$;

2. Para qualquer outro teste $\delta$ tal que $\pi_{\delta}(\theta) \leq \alpha, \underbrace{\forall \theta \in \Theta_0}_{\text{Sob}\ \mathcal{H}_0}$, então
$$
\pi_{\delta^*}(\theta) \geq \pi_{\delta}(\theta), \underbrace{\forall \theta \in \Theta_1}_{\text{Sob}\ \mathcal{H}_1}.
$$

### Teorema de Karlin-Rubin (I)
*Uma generalização do Lema de Neyman-Pearson.*

Seja $\boldsymbol{X}_n$ a.a. de $X \sim f_\theta, \theta \in \Theta \subseteq \mathbb{R}$. Considere
$$
\begin{cases}
\mathcal{H}_0 : \theta \leq \theta_0\\
\mathcal{H}_1 : \theta > \theta_0
\end{cases}
$$
em que $\theta_0$ é um valor dado. Se a *razão de verossimilhanças*
$$
\mathrm{R}_{\theta',\theta''}(T(\boldsymbol{x}_n)) = \frac{\mathcal{L}_{\boldsymbol{x}_n}(\theta')}{\mathcal{L}_{\boldsymbol{x}_n}(\theta'')}
$$
for *monótona não-decrescente* e $\theta' \geq \theta''$, em que $T(\boldsymbol{x}_n)$ é uma [estatística suficiente](estatisticas-suficientes.qmd) para o [modelo](modelo-estatistico.qmd), então
o teste $\delta^* : \mathfrak{X}^{(n)} \rightarrow \{0, 1\}$ tal que
$$
\delta^* (\boldsymbol{x}_n) = \begin{cases}
0, T(\boldsymbol{x}_n) < c \\
1, T(\boldsymbol{x}_n) \geq c,
\end{cases}
$$
e $c$ satisfaz
$$
\pi_{\delta^*}(\theta_0) = P_{\theta_0}(\delta^*(\boldsymbol{x}_n) = 1) = \alpha,
$$
é o teste uniformemente mais poderoso de tamanho $\alpha$.

#### Exemplo (Normal)

Seja $\boldsymbol{X}_n$ a.a. de $X\sim N(\theta, 2), \theta \in \Theta \subseteq \mathbb{R}$. Considere
$$
\begin{cases}
\mathcal{H}_0 : \theta \leq 1\\
\mathcal{H}_1 : \theta > 1.
\end{cases}
$$
Encontre o TUMP de tamanho $\alpha = 10\%$.

##### Resposta

A função de verossimilhança é dada por
$$
\begin{aligned}
\mathcal{L}_{\boldsymbol{x}_n}(\theta) &= \left(\frac{1}{\sqrt{4\pi}}\right)^n \mathrm{Exp}\left\{
-\frac{1}{4} \sum (x_i - \theta)^{2} \right\} \\ &= 
\left(\frac{1}{\sqrt{4\pi}}\right)^n \mathrm{Exp}\left\{
-\frac{1}{4} \sum x_i^2 - 2\theta\sum x_i + n\theta^2 \right\}.
\end{aligned}
$$
Pelo [Critério de Fatoração de Fisher](estatisticas-suficientes.qmd#sec-crit-fat), $T(\boldsymbol{X}_n) = \sum x_i$ é uma estatística suficiente.
Note que
$$
\begin{aligned}
\mathrm{R}_{\theta',\theta''}(T(\boldsymbol{X}_n)) &= \frac{\left(\frac{1}{\sqrt{4\pi}}\right)^n \mathrm{Exp}\left\{
-\frac{1}{4} \sum x_i^2 - 2\theta'\sum x_i + n\theta'^2 \right\}}{\left(\frac{1}{\sqrt{4\pi}}\right)^n \mathrm{Exp}\left\{
-\frac{1}{4} \sum x_i^2 - 2\theta''\sum x_i + n\theta''^2 \right\}} \\
&= \mathrm{Exp}\left\{-\frac{1}{4}(-\theta'T(\boldsymbol{x}_n) + n\theta'^2 + 2\theta''T(\boldsymbol{x}_n) - n\theta''^2\right\} \\
&= \mathrm{Exp}\left\{\frac{1}{2}(\theta' - \theta'')T(\boldsymbol{x}_n)-\frac{1}{4} n\theta''^2 +\frac{1}{4}\theta''^2\right\}.
\end{aligned}
$$
Como $\theta' \geq \theta''$, temos que $\theta' - \theta'' \geq 0$. Logo, $\mathrm{R}_{\theta',\theta''}(T(\boldsymbol{x}_n))$ é uma função
monótona *não-decrescente* em $T(\boldsymbol{x}_n)$. Logo, pelo Teorema de Karlin-Rubin (I), temos que
$$
\delta^*(\boldsymbol{x}_n) = \begin{cases}
0, \sum x_i < c \\
1, \sum x_i \geq c
\end{cases}
$$
em que $c$ satisfaz
$$
\pi_{\delta^*}(1) = P_{1}(\sum X_i \geq c) = \alpha.
$$

Com $n = 5$, temos que, quando $\theta = 1$, $\sum X_i \sim N(1 \cdot 5, 5 \cdot 2)$. Assim,
$$
\begin{aligned}
P_{1}(\sum X_i \geq c) &= P_1\left(\frac{\sum x_i - 5}{\sqrt{10}} \geq \frac{c - 5}{\sqrt{10}}\right) = 0.1 \\
\Rightarrow \frac{c - 5}{\sqrt{10}} &= 1.28 \\
\Rightarrow c &= 1.28 \cdot \sqrt{10} + 5 = 9.064 \\
\Rightarrow  \delta^*(\boldsymbol{x}_n) &= \begin{cases}
0, \sum x_i < 9.064 \\
1, \sum x_i \geq 9.064
\end{cases}
\end{aligned}
$$
Portanto, se $\sum x_i < 9.064$, dizemos que *não há evidencias* para rejeitarmos $\mathcal{H}_0$ a $10\%$ de significância
estatística. Se $\sum x_i \geq 9.064$, dizemos que *há evidencias* para rejeitarmos $\mathcal{H}_0$ a $10\%$ de significância
estatística.

#### Exemplo (Exponencial)

Seja $\boldsymbol{X}_n$ a.a. de $X\sim \mathrm{Exp}(\theta), \theta \in \Theta \subseteq \mathbb{R}_+$. Considere
$$
\begin{cases}
\mathcal{H}_0 : \theta \leq 2\\
\mathcal{H}_1 : \theta > 2.
\end{cases}
$$
Encontre o TUMP de tamanho $\alpha = 5\%$.

##### Resposta

A função de verossimilhança é dada por
$$
\begin{aligned}
\mathcal{L}_{\boldsymbol{x}_n}(\theta) &= \theta^n \cdot \mathrm{e}^{-\theta \sum x_i}.
\end{aligned}
$$

Pelo [Critério de Fatoração de Fisher](estatisticas-suficientes.qmd#sec-crit-fat), $T(\boldsymbol{X}_n) = \sum X_i$ é uma estatística suficiente.
$$
\begin{aligned}
\mathrm{R}_{\theta',\theta''}(T(\boldsymbol{X}_n)) &= \frac{\theta'^n \cdot \mathrm{e}^{-\theta' \sum x_i}}{\theta''^n \cdot \mathrm{e}^{-\theta'' \sum x_i}} \\
&= \left(\frac{\theta'}{\theta''}\right)^n \mathrm{Exp} \left\{ -\theta' T(\boldsymbol{x}_n) + \theta''T(\boldsymbol{x}_n) \right\} \\
&= \left(\frac{\theta'}{\theta''}\right)^n \mathrm{Exp} \left\{ (\theta'' - \theta') T(\boldsymbol{x}_n)\right\}, \theta' \geq \theta''.
\end{aligned}
$$
Como $\theta' \geq \theta''$, temos que $\mathrm{R}_{\theta',\theta''}(T(\boldsymbol{X}_n))$ é decrescente em $T(\boldsymbol{x}_n)$.
Tome $T'(\boldsymbol{x}_n) = - \sum x_i$, logo
$$
\mathrm{R}_{\theta',\theta''}(T(\boldsymbol{X}_n)) = \left(\frac{\theta'}{\theta''}\right)^n \mathrm{Exp}
\left\{(\theta' - \theta'') T(\boldsymbol{x}_n)\right\}, \theta' \geq \theta''.
$$
é monótona não-decrescente em $T'(\boldsymbol{x}_n)$. Portanto, pelo Teorema de Karlin-Rubin (I),
$$
\delta^*(\boldsymbol{x}_n) = \begin{cases}
0, \sum -x_i < c \\
1, \sum -x_i \geq c
\end{cases} \Rightarrow
\delta^*(\boldsymbol{x}_n) = \begin{cases}
0, \sum x_i > -c \\
1, \sum x_i \leq -c
\end{cases}
$$
em que $c$ satisfaz
$$
\pi_{\delta^*}(1) = P_{1}(\sum X_i \leq -c) = \alpha.
$$
Para $n=10$, sob $\theta = 2, \sum X_i \sim \mathrm{Gama}(10,2)$. Computacionalmente, o quantil $0.05$ dessa distribuição
é $2.7127$. Logo, $-c = 2.7127 \Rightarrow c = -2.7127$. Dessa forma, se $\sum x_i > 2.7127$, dizemos que *não há evidencias* para rejeitarmos $\mathcal{H}_0$ a $5\%$ de significância
estatística. Se $\sum x_i \leq 2.7127$, dizemos que *há evidencias* para rejeitarmos $\mathcal{H}_0$ a $5\%$ de significância estatística.

### Teorema de Karlin-Rubin (II)
Seja $\boldsymbol{X}_n$ a.a. de $X \sim f_\theta, \theta \in \Theta \subseteq \mathbb{R}$. Considere
$$
\begin{cases}
\mathcal{H}_0 : \theta \geq \theta_0\\
\mathcal{H}_1 : \theta < \theta_0
\end{cases}
$$
em que $\theta_0$ é um valor dado. Se a *razão de verossimilhanças*
$$
\mathrm{R}_{\theta',\theta''}(T(\boldsymbol{x}_n)) = \frac{\mathcal{L}_{\boldsymbol{x}_n}(\theta')}{\mathcal{L}_{\boldsymbol{x}_n}(\theta'')}
$$
for *monótona não-crescente* e $\theta' \leq \theta''$, em que $T(\boldsymbol{x}_n)$ é uma [estatística suficiente](estatisticas-suficientes.qmd) para o [modelo](modelo-estatistico.qmd), então
o teste $\delta^* : \mathfrak{X}^{(n)} \rightarrow \{0, 1\}$ tal que
$$
\delta^* (\boldsymbol{x}_n) = \begin{cases}
0, T(\boldsymbol{x}_n) > c \\
1, T(\boldsymbol{x}_n) \leq c,
\end{cases}
$$
e $c$ satisfaz
$$
\pi_{\delta^*}(\theta_0) = P_{\theta_0}(\delta^*(\boldsymbol{x}_n) = 1) = \alpha,
$$
é o teste uniformemente mais poderoso de tamanho $\alpha$.

#### Exemplo (Normal)

Seja $\boldsymbol{X}_n$ a.a. de $X\sim N(\theta, 2), \theta \in \Theta \subseteq \mathbb{R}$. Considere
$$
\begin{cases}
\mathcal{H}_0 : \theta \geq 1\\
\mathcal{H}_1 : \theta < 1.
\end{cases}
$$
Encontre o TUMP de tamanho $\alpha$.

##### Resposta

A função de verossimilhança é dada por
$$
\begin{aligned}
\mathcal{L}_{\boldsymbol{x}_n}(\theta) &= \left(\frac{1}{\sqrt{4\pi}}\right)^n \mathrm{Exp}\left\{
-\frac{1}{4} \sum (x_i - \theta)^{2} \right\} \\ &= 
\left(\frac{1}{\sqrt{4\pi}}\right)^n \mathrm{Exp}\left\{
-\frac{1}{4} \sum x_i^2 - 2\theta\sum x_i + n\theta^2 \right\}.
\end{aligned}
$$
Pelo [Critério de Fatoração de Fisher](estatisticas-suficientes.qmd#sec-crit-fat), $T(\boldsymbol{X}_n) = \sum x_i$ é uma estatística suficiente.
Note que
$$
\begin{aligned}
\mathrm{R}_{\theta',\theta''}(T(\boldsymbol{X}_n)) &= \frac{\left(\frac{1}{\sqrt{4\pi}}\right)^n \mathrm{Exp}\left\{
-\frac{1}{4} \sum x_i^2 - 2\theta'\sum x_i + n\theta'^2 \right\}}{\left(\frac{1}{\sqrt{4\pi}}\right)^n \mathrm{Exp}\left\{
-\frac{1}{4} \sum x_i^2 - 2\theta''\sum x_i + n\theta''^2 \right\}} \\
&= \mathrm{Exp}\left\{-\frac{1}{4}(-\theta'T(\boldsymbol{x}_n) + n\theta'^2 + 2\theta''T(\boldsymbol{x}_n) - n\theta''^2\right\} \\
&= \mathrm{Exp}\left\{\frac{1}{2}(\theta' - \theta'')T(\boldsymbol{x}_n)-\frac{1}{4} n'\theta^2 +\frac{1}{4}\theta''^2\right\}.
\end{aligned}
$$
Como $\theta' \leq \theta''$, temos que $\theta' - \theta'' \leq 0$. Logo, $\mathrm{R}_{\theta',\theta''}(T(\boldsymbol{x}_n))$ é uma função
monótona *não-crescente* em $T(\boldsymbol{x}_n)$. Logo, pelo Teorema de Karlin-Rubin (II), temos que
$$
\delta^*(\boldsymbol{x}_n) = \begin{cases}
0, \sum x_i > c \\
1, \sum x_i \leq c
\end{cases}
$$
em que $c$ satisfaz
$$
\pi_{\delta^*}(1) = P_{1}(\sum X_i \leq c) = \alpha.
$$

### Simulações

```{julia}
#| echo: true
# Seja X a.a. de X ~ Exp()
# H_0 θ ≤ 2; H_1 θ > 2
# δ(x) = 1 ⟺ ∑ x ≤ 2.71
using Distributions, Random, StatsBase
Random.seed!(24)

# Borda do Θ_0
θ_0 = 2
# Valores do teste e amostra
n = 100
α = 0.05
MC = 10_000

function h0()
    # Sob H_0
    θ_00 = rand(Uniform(0, θ_0))


    d = Exponential(1 / θ_00)
    dsoma = Gamma(n, 1 / θ_00)
    c = quantile(dsoma, α)

    δ = zeros(MC)
    for i in 1:MC
        x = rand(d, n)
        δ[i] = sum(x) ≤ c
    end
    return println("Testes rejeitados sob H_0: $(mean(δ) * 100)%")
end

function h1()
    # Sob H_0
    θ_11 = rand(Uniform(θ_0, θ_0 + 10))


    d = Exponential(1 / θ_11)
    dsoma = Gamma(n, 1 / θ_11)
    c = quantile(dsoma, α)

    δ = zeros(MC)
    for i in 1:MC
        x = rand(d, n)
        δ[i] = sum(x) ≥ c
    end
    return println("Testes rejeitados sob H_1 (Poder do teste): $(mean(δ) * 100)")
end

h0()
h1()
```

## Testes de hipótese gerais
Considere
$$
\begin{cases}
\mathcal{H}_0 : \theta \in \Theta_0 \\
\mathcal{H}_1 : \theta \in \Theta_1
\end{cases}
$$
as hipóteses nula e alternativa em que $\Theta_0 \cup \Theta_1 = \Theta, \Theta_0 \cap \Theta_1 = \emptyset, \#(\Theta_0) \geq 1, \#(\Theta_1) \geq 1$.

O teste da razão de [verossimilhanças](funcao-verossimilhanca.qmd) generalizada de tamanho $\alpha$ é $\delta:\mathfrak{X}^{(n)}\rightarrow \{0,1\}$ tal que

$$
\delta(\boldsymbol{X}_n) = \begin{cases}
0,\ \boldsymbol{x}_n \not\in A_c
1,\ \boldsymbol{x}_n \in A_c
\end{cases}
$$
e $\sup\limits_{\theta \in \Theta_0}\pi_\delta(\theta) = \alpha$, em que

$$
A_c = \left\{
\boldsymbol{x}_n \in \mathfrak{X}^{(n)} : \frac{\sup\limits_{\theta \in \Theta_1} \mathcal{L}_{\boldsymbol{x}_n}(\theta)}
{\sup\limits_{\theta \in \Theta_0} \mathcal{L}_{\boldsymbol{x}_n}(\theta)} \geq c
\right\}
$$

:::{.callout-tip title="Constante $c$"}
A constante $c$ é obtida resolvendo $\sup\limits_{\theta \in \Theta_0}\pi_\delta(\theta) = \alpha$
:::

:::{.callout-note title="Teste de Neyman-Pearson"}
Se $\Theta_0 = \{\theta_0\}, \Theta_1 = \{\theta_1\}$, temos o teste mais poderoso de Neyman-Pearson.
:::

:::{.callout-note title="TUMP de Karlin-Rubin I e II"}
Se $\Theta_0 = (-\infty \theta_0] \cap \Theta, \Theta_1 = (\theta_1, \infty) \cap \Theta$, temos o teste uniformemente
mais poderoso de Karlin-Rubin (I). Por sua vez,
se $\Theta_0 = [\theta_0, \infty) \cap \Theta, \Theta_1 = (-\infty \theta_1) \cap \Theta$, temos o teste uniformemente
mais poderoso de Karlin-Rubin (II)
:::

### [Estatística](estatisticas.qmd) da razão de verossimilhanças generalizada
Se $\dim(\Theta) = \dim(\Theta_1) > \dim(\Theta_0)$ e $\mathcal{L}_{\boldsymbol{x}_n}(\theta)$ for contínua para todo $\theta$ em $\Theta$, então:
$$
\sup\limits_{\theta \in \Theta} \mathcal{L}_{\boldsymbol{x}_n}(\theta) = \sup\limits_{\theta \in \Theta_1} \mathcal{L}_{\boldsymbol{x}_n}(\theta),\ \mathrm{q.c.}
$$
Portanto,
$$
\begin{aligned}
\frac{\sup\limits_{\theta \in \Theta_1} \mathcal{L}_{\boldsymbol{x}_n}(\theta)}
{\sup\limits_{\theta \in \Theta_0} \mathcal{L}_{\boldsymbol{x}_n}(\theta)} =
\frac{\sup\limits_{\theta \in \Theta} \mathcal{L}_{\boldsymbol{x}_n}(\theta)}
{\sup\limits_{\theta \in \Theta} \mathcal{L}_{\boldsymbol{x}_n}(\theta)} \cdot
\frac{\sup\limits_{\theta \in \Theta_1} \mathcal{L}_{\boldsymbol{x}_n}(\theta)}
{\sup\limits_{\theta \in \Theta_0} \mathcal{L}_{\boldsymbol{x}_n}(\theta)} &= 
\frac{\sup\limits_{\theta \in \Theta} \mathcal{L}_{\boldsymbol{x}_n}(\theta)}
{\sup\limits_{\theta \in \Theta_0} \mathcal{L}_{\boldsymbol{x}_n}(\theta)} \geq c \\
&\iff
\frac{\sup\limits_{\theta \in \Theta_0} \mathcal{L}_{\boldsymbol{x}_n}(\theta)}
{\sup\limits_{\theta \in \Theta} \mathcal{L}_{\boldsymbol{x}_n}(\theta)} \leq \frac{1}{c} \\
\Rightarrow
A_c &= \left\{
\frac{\sup\limits_{\theta \in \Theta_0} \mathcal{L}_{\boldsymbol{x}_n}(\theta)}
{\sup\limits_{\theta \in \Theta} \mathcal{L}_{\boldsymbol{x}_n}(\theta)} \leq \frac{1}{c}
\right\}
\end{aligned}
$$

Dizemos que

$$
\lambda(\boldsymbol{x}_n; \Theta_0) = 
\frac{\sup\limits_{\theta \in \Theta_0} \mathcal{L}_{\boldsymbol{x}_n}(\theta)}
{\sup\limits_{\theta \in \Theta} \mathcal{L}_{\boldsymbol{x}_n}(\theta)}
$$ {#eq-estatrazao}

é a estatística da razão de verossimilhanças generalizada.

Denote por $\hat{\theta}_0$ o estimador para "$\theta$" sob $\mathcal{H}_0$, ou seja,
$$
\hat{\theta}_0 = \mathrm{argmax}_{\theta \in \Theta_0} \mathcal{L}_{\boldsymbol{x}_n}(\theta)
$$
sempre que existir. Denote também
$$
\hat{\theta}_{\mathrm{MV}} = \mathrm{argmax}_{\theta \in \Theta} \mathcal{L}_{\boldsymbol{x}_n}(\theta)
$$
o [estimador de máxima verossimilhança](emv2.qmd) não restrito a $\mathcal{H}_0$. A estatística da razão de verossimilhança
generalizada pode ser reescrita:

$$
\lambda(\boldsymbol{x}_n; \Theta_0) = 
\frac{\mathcal{L}_{\boldsymbol{x}_n}(\hat{\theta}_0)}
{\mathcal{L}_{\boldsymbol{x}_n}(\hat{\theta}_{\mathrm{MV}})}
$$ {#eq-estatrazao2}

Note que a @eq-estatrazao é bem definida sempre que $0 < \sup\limits_{\theta \in \Theta} \mathcal{L}_{\boldsymbol{x}_n}(\theta) < \infty$.
Em alguns casos, a @eq-estatrazao2 não pode ser resolvida por não existir um argumento que maximize a função de verossimilhança sob
a hipótese nula.

Observe que $\mathcal{H}_0 : \theta = \theta_0$ contra $\mathcal{H}_1 : \theta = \theta_1$ e $\Theta \subseteq \mathbb{R}$,
$$
\lambda(\boldsymbol{x}_n; \Theta_0) = 
\frac{\mathcal{L}_{\boldsymbol{x}_n}(\theta_0)}
{\mathcal{L}_{\boldsymbol{x}_n}(\hat{\theta}_{\mathrm{MV}})}
$$

*Teorema*. Sob as [condições de regularidade](cond-regular.qmd) e com $\dim(\Theta_0) < \dim(\Theta)$, temos que
$$
-2\ln \lambda(\boldsymbol{X}_n, \Theta_0) \stackrel{\mathcal{D}}{\rightarrow} \chi^2_{s},
$$
em que $s = \dim(\Theta) - \dim(\Theta_0)$. Supõe-se que o conjunto $\Theta_0$ *não* contém singularidades.

O teste da Razão de Verossimilhança Generalizada (RVG) pode ser reescrito:
$$
\begin{aligned}
\delta(\boldsymbol{x}_n) &= \begin{cases}
0,\ \  \frac{\mathcal{L}_{\boldsymbol{x}_n}(\hat{\theta}_0)}
{\mathcal{L}_{\boldsymbol{x}_n}(\hat{\theta}_{\mathrm{MV}})} > \frac{1}{c} \\
1,\ \ \frac{\mathcal{L}_{\boldsymbol{x}_n}(\hat{\theta}_0)}
{\mathcal{L}_{\boldsymbol{x}_n}(\hat{\theta}_{\mathrm{MV}})} \leq \frac{1}{c}
\end{cases} \\
\iff
\delta(\boldsymbol{x}_n) &= \begin{cases}
0,\ \  \lambda(\boldsymbol{x}_n, \Theta_0) > \frac{1}{c} \\
1,\ \  \lambda(\boldsymbol{x}_n, \Theta_0) \leq \frac{1}{c}
\end{cases} \\
\iff
\delta(\boldsymbol{x}_n) &= \begin{cases}
0,\ \  -2 \ln \lambda(\boldsymbol{x}_n, \Theta_0) < 2\ln c \\
1,\ \  -2 \ln \lambda(\boldsymbol{x}_n, \Theta_0) \geq 2 \ln c
\end{cases} \\
\end{aligned}
$$
em que $c$ deve satisfazer

$$
\begin{aligned}
\sup\limits_{\theta \in \Theta_0} \pi_{\delta}(\theta) &= \alpha \\
\iff \sup\limits_{\theta \in \Theta_0} P_\theta(\delta(\boldsymbol{X}_n) = 1) &= \alpha \\
\iff \sup\limits_{\theta \in \Theta_0} P_\theta(-2\ln \lambda(\boldsymbol{X}_n, \Theta_0) \geq 2 \ln c) &= \alpha
\end{aligned}
$$ {#eq-testeestatrazao}

1. Se a distribuição exata de $-2\ln \lambda(\boldsymbol{X}_n, \Theta_0)$ for conhecida, entã basta encontrar $2\ln c$ que satisfaça
a @eq-testeestatrazao.

2. Caso contrário, utilizamos o teorema anterior:
$$
-2\ln \lambda(\boldsymbol{X}_n, \Theta_0) \stackrel{\mathcal{D}}{\rightarrow} \chi^2_{s},
$$
em que $s = \dim(\Theta) - \dim(\Theta_0)$

### Exemplo (Normal)

Seja $\boldsymbol{X}_n$ a.a. de $X \sim N(\theta,1), \theta \in \Theta \subseteq \mathbb{R}$, e considere
$$
\begin{cases}
\mathcal{H}_0 : \theta = 0 \\
\mathcal{H}_1 : \mu \neq 0,
\end{cases}
$$
as hipóteses nula e alternativa. Encontre o teste da razão de verossimilhanças generalizada de tamanho $\alpha$.

#### Resposta

Já sabemos que $\hat{\theta}_{\mathrm{MV}} = \bar{X}$ e
$$
\begin{aligned}
\mathcal{L}_{\boldsymbol{x}_n}(\theta) &= \left(\frac{1}{\sqrt{2\pi}}\right)^n \mathrm{Exp}\left\{
-\frac{1}{2} \sum (x_i - \theta)^{2} \right\} \\ &= 
\left(\frac{1}{\sqrt{2\pi}}\right)^n \mathrm{Exp}\left\{
-\frac{1}{2} \sum x_i^2 - 2\theta\sum x_i + n\theta^2 \right\}.
\end{aligned}
$$.

Assim,
$$
\begin{aligned}
\lambda(\boldsymbol{x}_n; \Theta_0) &= 
\frac{\sup\limits_{\theta \in \Theta_0} \mathcal{L}_{\boldsymbol{x}_n}(\theta)}
{\sup\limits_{\theta \in \Theta} \mathcal{L}_{\boldsymbol{x}_n}(\theta)}\\
&= \frac{\mathcal{L}_{\boldsymbol{x}_n}(0)} {\mathcal{L}_{\boldsymbol{x}_n}(\bar{x})} \\
&= \mathrm{Exp}\left\{
-\frac{1}{2} \sum x_i^2 + \frac{1}{2}\left(\sum x_i^2 - 2n \bar{x}^2 + n \bar{x}^2\right)
\right\} = \mathrm{e}^{-\frac{1}{2}n\bar{x}^2}
\end{aligned}
$$
Portanto,
$$
-2\ln \lambda(\boldsymbol{x}_n, \Theta_0) = n \bar{x}^2.
$$
Sob $\mathcal{H}_0$, $\theta =0, \bar{X} \sim N(0, 1/n), \sqrt{n}\bar{X} \sim N(0,1) \Rightarrow n \bar{X}^2 \sim \chi^2_1$.
Logo, o teste de RVG é
$$
\delta(\boldsymbol{x}_n) = \begin{cases}
0,\ \  n\bar{x}^2 < 2\ln c
1,\ \  n\bar{x}^2 \geq 2\ln c
\end{cases}
$$
em que $c$ deve satisfazer
$$
\sup\limits_{\theta \in \Theta_0} P_\theta(n\bar{X}^2 \geq 2 \ln c) = \alpha.
$$
Note que $\forall \theta \in \Theta_0$, $n\bar{X}^2 \sim \chi^2_1$ (é [ancilar](estatisticas-ancilares.qmd) ao modelo reduzido). Portanto,
$$
\sup\limits_{\theta \in \Theta_0} P_{0}(n\bar{X}^2 \geq c^*) = P(\chi^2_1 \geq c^*) = \alpha.
$$
Tomando $c^* = 2\ln c$, podemos obter o valor para $c^*$ de uma qui-quadradado com $1$ grau de liberdade para qualquer valor de $\alpha$.
Por exemplo, se $\alpha = 10\%, c^* = 2.70$. Se $\alpha = 5\%, c^* = 3.84$, etc.

### Exemplo (Poisson)
Seja $\boldsymbol{X}_n$ a.a. de $X \sim \mathrm{Poiss}(\theta), \theta \in \Theta = (0, \infty)$. Considere
$$
\begin{cases}
\mathcal{H}_0 : \theta = \theta_0 \\
\mathcal{H}_1 : \mu \neq \theta_0,
\end{cases}
$$
as hipóteses nula e alternativa. Encontre o teste da razão de verossimilhanças generalizada de tamanho $\alpha$. Use o resultado
assintótico.

#### Resposta
Já sabemos que $\hat{\theta}_{\mathrm{MV}} = \bar{X}$ e
$$
\begin{aligned}
\mathcal{L}_{\boldsymbol{x}_n}(\theta) &= \mathrm{e}^{n\theta} \cdot \frac{\theta^{\sum x_i}}{\prod (x_i!)} \\
\Rightarrow \lambda(\boldsymbol{x}_n, \Theta_0) &= \frac{\mathrm{e}^{n\theta_0} \cdot \frac{\theta_0^{\sum x_i}}{\prod (x_i!)}}
{\mathrm{e}^{n\bar{x}} \cdot \frac{\bar{x}^{\sum x_i}}{\prod (x_i!)}} \\
&= \mathrm{e}^{n\theta_0+n\bar{x}} \cdot \left(\frac{\theta_0}{\bar{x}}\right)^{\sum x_i}
\end{aligned}
$$
Sabemos que, sob $\mathcal{H}_0, \ (\mathrm{i.e.}\ \forall \theta \in \Theta_0)$, $-2\ln \lambda(\boldsymbol{X}_n, \Theta_0) \stackrel{\mathcal{D}}{\rightarrow} \chi^2_1$.
Assim,
$$
\begin{aligned}
2n\theta_0 - 2n\bar{X} - 2n\bar{X}\ln\left(\frac{\theta_0}{\bar{X}}\right) = \underbrace{2n\theta_0 -2n\bar{X} -2n\bar{X} \ln \theta_0 + 2n\bar{X} \ln \bar{X}}_{T(\boldsymbol{X}_n)}
\stackrel{\mathcal{D}}{\rightarrow} \chi^2_1.
\end{aligned}
$$
Logo, o teste de RVG é
$$
\delta(\boldsymbol{x}_n) = \begin{cases}
0,\ \  T(\boldsymbol{x}_n) < c^*
1,\ \  T(\boldsymbol{x}_n) \geq c^*
\end{cases}
$$
em que $c$ deve satisfazer
$$
\sup\limits_{\theta \in \Theta_0} P_\theta(n\bar{X}^2 \geq c^*) = P_{\theta_0}(T(\boldsymbol{X}_n) \geq c^*) = \alpha.
$$
Pela aproximação,
$$
P_{\theta_0}(T(\boldsymbol{X}_n) \geq c^*) \approx P(\chi^2_1 \geq c^*) \alpha.
$$

```{julia}
#| echo: true
# Seja Xn a.a. de X ~ Pois(θ)
# H0: θ = θ_0
# H1: θ ≠ θ_0
using Random, Distributions, StatsBase, Plots, StatsPlots, LaTeXStrings
n = 100
θ_0 = 3

# Gerando sob H_0
d = Poisson(θ_0)
MC = 10_000
TX = zeros(MC)
for i in 1:MC
    x = rand(d, n)
    TX[i] = 2 * n * θ_0 - 2 * n * mean(x) - 2 * n * mean(x) * log(θ_0) + 2 * n * mean(x) * log(mean(x))
end
p = histogram(TX, normalize = true, title = "Histograma de T(X)", label = "", ylims = (0, 1), bins = 20, xlabel = "T(X)", ylabel = "Densidade")
plot!(Chisq(1), label = L"\chi^2_1", color = :tomato)

## Probabilidade do erro tipo I, α = 10%

quantil = quantile(Chisq(1), 0.9)
rejeita = TX .> quantil
println("Probilidade do Erro Tipo I com α = 10%: $(mean(rejeita) * 100)%")
display(p)
```

### Hipótese Linear Geral

Seja $\boldsymbol{X}_n$ a.a. de $X \sim f_\theta, \theta \in \Theta \subseteq \mathbb{R}^p$. Considere
$$
\begin{cases}
\mathcal{H}_0 : C \cdot \theta = d \\
\mathcal{H}_1 : C \cdot \theta \neq d
\end{cases}
$$

em que $C$ é uma matriz $s \times p$ conhecida e $d$ é um vetor de dimensão $s$ conhecido. Podemos escrever as hipóteses
em termos de $\Theta_0$ e $\Theta_1$:

$$
\begin{cases}
\mathcal{H}_0 : \theta \in \Theta_0\\
\mathcal{H}_1 : \theta \in \Theta_1
\end{cases}
$$
em que $\Theta_0 = \{\theta \in \Theta : C\cdot \theta = d\}$ e  $\Theta_1 = \{\theta \in \Theta : C\cdot \theta \neq d\}$.

A hipótese $\mathcal{H}_0 : C \cdot \theta = d$ é conhecida como *hipótese linear geral*.

Note que, podemos testar as seguintes hipóteses como casos particulares:
1.$$
\begin{cases}
\mathcal{H}_0 : \theta = \theta_0\\
\mathcal{H}_1 : \theta \neq \theta_0
\end{cases}\ \ \ \iff C=I, d = \theta_0.
$$
2.$$
\begin{cases}
\mathcal{H}_0 : \theta_1 = \theta_2\\
\mathcal{H}_1 : \theta_1 \neq \theta_2
\end{cases}\ \ \ \iff C=\begin{pmatrix}1  & -1 & 0 & \dots & 0\end{pmatrix}, d = 0,\ \ \theta = (\theta_1, \theta_ 2\dots, \theta_p).
$$
3.$$
\begin{aligned}
\begin{cases}
\mathcal{H}_0 : \theta_1 = \theta_3\ \text{e}\ \theta_2 = \theta_4\\
\mathcal{H}_1 : \text{Ao menos um diferente}
\end{cases}\ \ \ \iff C&=\begin{bmatrix} 1 & 0 & -1 & 0 & \cdots & 0 \\ 0 & 1 & 0 & -1 & \cdots & 0 \end{bmatrix},\\
d &= \begin{bmatrix}0 \\ 0\end{bmatrix}.
\end{aligned}
$$
4.$$
\begin{cases}
\mathcal{H}_0 : \sum^p_{i=1} c_i \theta_i = d \\
\mathcal{H}_1 : \sum^p_{i=1} c_i \theta_i \neq d
\end{cases}\ \ \ \iff C=\begin{pmatrix} c_1 & c_2 & \dots & c_p \end{pmatrix}, d = d.
$$
5.$$
\begin{aligned}
\begin{cases}
\mathcal{H}_0 : \sum^p_{i=1} c_i^{(1)} \theta_i = d^{(1)}, \dots, \sum^p_{i=1} c_i^{(p)} \theta_i = d^{(p)} \\
\mathcal{H}_1 : \text{Ao menos um diferente}
\end{cases}\ \ \ \iff 
C &=\begin{bmatrix} 
c_1^{(1)} & c_2^{(1)} & \dots & c_{p}^{(1)} \\
c_1^{(2)} & c_2^{(2)} & \dots & c_{p}^{(2)} \\
\vdots & \vdots & \ddots & \vdots \\
c_1^{(p)} & c_2^{(p)} & \dots & c_{p}^{(p)} \\
\end{bmatrix},\\
d &= \begin{bmatrix}d^{(1)}\\ d^{(2)} \\ \vdots \\ d^{(p)}\end{bmatrix}.
\end{aligned}
$$

### Estatística de Wald

Seja $\hat{\theta}$ um [estimador](estimadores.qmd) para "$\theta$" [assintoticamente normal](prop-est.qmd#sec-assinorm), ou seja,
$$
\sqrt{n}(\hat{\theta} - \theta) \stackrel{\mathcal{D}}{\rightarrow} N_s(\boldsymbol{0}, V_{\theta})
$$
Se $C$ tiver posto linha completo, ou seja, todas suas linhas são linearmente independentes, então
$$
\begin{aligned}
\sqrt{n}(C\hat{\theta} - C\theta) &\stackrel{\mathcal{D}}{\rightarrow} N_s(\boldsymbol{0}, C V_\theta C) \\
\Rightarrow  n(C\hat{\theta} - C\theta)^T [C V_{\hat{\theta}} C^T]^{-1} (C\hat{\theta} - C\theta) &\stackrel{\mathcal{D}}{\rightarrow} \chi^2_{s}, \forall \theta \in \Theta.
\end{aligned}
$$
Sob $\mathcal{H}_0, C\theta = d$. Disso, temos que
$$
n(C\hat{\theta} - d)^T [C V_{\hat{\theta}} C^T]^{-1} (C\hat{\theta} - d) \stackrel{\mathcal{D}}{\rightarrow} \chi^2_{s}.
$$

A estatística de Wald é definida por
$$
W(\boldsymbol{X}_n) = n(C\hat{\theta}(\boldsymbol{X}_n) - d)^T [C V_{\hat{\theta}(\boldsymbol{X}_n)} C^T]^{-1} (C\hat{\theta}(\boldsymbol{X}_n) - d).
$$

Sob $\mathcal{H}_0$, $W \stackrel{\mathcal{D}}{\rightarrow} \chi^2_s$, em que $s$ é o número de linhas de $C$.

#### Exemplos

Considere as hipóteses com $\theta_{1,0}$ conhecido
$$
\begin{aligned}
\begin{cases}
\mathcal{H}_0 : \theta_1 = \theta_{1,0} \\
\mathcal{H}_1 : \theta_1 \neq \theta_{1,0}
\end{cases}&\ \ \ \Rightarrow C=\begin{pmatrix}1 & 0 & \dots & 0\end{pmatrix}, d = \theta_{1,0} \\
W &= n(\hat{\theta}_1 - \theta_{1,0})(V_{\hat{\theta}}^{(1,1)})^{-1}(\hat{\theta}_1 - \theta_{1,0}) \\
&= \frac{(\hat{\theta}_1 - \theta_{1,0})^2}{(V_{\hat{\theta}}^{(1,1)}/n)} \stackrel{\mathcal{D}}{\rightarrow} \chi^2_1,\ \text{Sob}\ \mathcal{H}_0
\end{aligned}
$$
Logo,
$$
C V_{\hat{\theta}}C^T = \begin{pmatrix} 1 & 0 & \dots & 0 \end{pmatrix}
\begin{bmatrix}
V_{\hat{\theta}}^{(1,1)} &  V_{\hat{\theta}}^{(1,2)} & \dots & V_{\hat{\theta}}^{(1,p)} \\
\cdot & V_{\hat{\theta}}^{(2,2)} & \dots & V_{\hat{\theta}}^{(2,p)} \\
\vdots & \vdots & \ddots & \cdot \\
\cdot & \cdot & \cdot  & V_{\hat{\theta}}^{(p,p)}
\end{bmatrix}\begin{pmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{pmatrix} = V_{\hat{\theta}}^{(1,1)}
$$

Considere as hipóteses
$$
\begin{aligned}
\begin{cases}
\mathcal{H}_0 : \theta_1 = \theta_{3} \\
\mathcal{H}_1 : \theta_1 \neq \theta_{3}
\end{cases}&\ \ \ \Rightarrow C=\begin{pmatrix}1 & 0 & -1 & 0 & \dots & 0\end{pmatrix}, d = 0 \\
W &= n(\hat{\theta}_1 - \hat{\theta}_3)(C V_{\hat{\theta}} C^T)^{-1}(\hat{\theta}_1 - \hat{\theta}_3) \\
&= \frac{n(\hat{\theta}_1 - \hat{\theta}_3)^2}{C V_{\hat{\theta}} C^T}
\end{aligned}
$$
Logo,
$$
\begin{aligned}
C V_{\hat{\theta}}C^T &= \begin{pmatrix}1 & 0 & -1 & 0 & \dots & 0\end{pmatrix}
\begin{bmatrix}
V_{\hat{\theta}}^{(1,1)} &  V_{\hat{\theta}}^{(1,2)} & \dots & V_{\hat{\theta}}^{(1,p)} \\
V_{\hat{\theta}}^{(2,1)}& V_{\hat{\theta}}^{(2,2)} & \dots & V_{\hat{\theta}}^{(2,p)} \\
\vdots & \vdots & \ddots & \cdot \\
V_{\hat{\theta}}^{(p,1)}& V_{\hat{\theta}}^{(p,2)}& \cdot  & V_{\hat{\theta}}^{(p,p)}
\end{bmatrix}\begin{pmatrix}1 \\ 0 \\ -1 \\ 0 \\ \vdots \\ 0\end{pmatrix} \\
&= \begin{pmatrix}
V_{\hat{\theta}}^{(1,1)} -V_{\hat{\theta}}^{(3,1)} & 
V_{\hat{\theta}}^{(1,2)} - V_{\hat{\theta}}^{(3,2)} &
\dots &
V_{\hat{\theta}}^{(1,p)} - V_{\hat{\theta}}^{(3,p)}
\end{pmatrix} \begin{pmatrix}1 \\ 0 \\ -1 \\ 0 \\ \vdots \\ 0\end{pmatrix} \\
&= V_{\hat{\theta}}^{(1,1)} - V_{\hat{\theta}}^{(3,1)} + V_{\hat{\theta}}^{(3,3)} - V_{\hat{\theta}}^{(1,3)} \\
& = V_{\hat{\theta}}^{(1,1)} + V_{\hat{\theta}}^{(3,3)} - 2 V_{\hat{\theta}}^{(1,3)}
\end{aligned}
$$

O teste usando a estatística de Wald é definido por
$$
\delta_{W}(\boldsymbol{x}_n) = \begin{cases}
0,\ \ \ W(\boldsymbol{x}_n) < \eta \\
1,\ \ \ W(\boldsymbol{x}_n) \geq \eta
\end{cases}
$$
em que $\eta$ é obtido de
$$
P(\chi^2_s \geq \eta) = \alpha
$$
para um teste de Wald de tamanho $\alpha$.

:::{.callout-tip title="Relação com as hipóteses"}
Sob $\mathcal{H}_0$, esperamos que $W$ seja "pequeno", enquanto
sob $\mathcal{H}_1$, esperamos que $W$ seja "grande".
:::

### Estatística Escore (ou de Rao)

Considere as hipóteses
$$
\begin{cases}
\mathcal{H}_0 : \theta = \theta_0 \\
\mathcal{H}_1 : \theta \neq \theta_0
\end{cases}
$$
em que $\theta_0$ é conhecido e $\theta \in \theta \subseteq \mathbb{R}^p$.
A estatística escore é definida por

$$
R(\boldsymbol{X}_n) = U_n(\boldsymbol{X}_n, \theta_))^T I_n(\theta_0)^{-1} U_n(\boldsymbol{X}_n, \theta_0)
$$
em que $U_n$ é o [escore da amostra](escore.qmd) e $I_n$ é a [informação de fisher total](infofisher.qmd).

Pode-se demonstrar que
$$
R \stackrel{\mathcal{D}}{\rightarrow} \chi^2_p,\ \text{sob}\ \mathcal{H}_0
$$

O teste usando a estatística escore é definido por
$$
\delta_{R}(\boldsymbol{x}_n) = \begin{cases}
0,\ \ \ R(\boldsymbol{x}_n) < \eta \\
1,\ \ \ R(\boldsymbol{x}_n) \geq \eta
\end{cases}
$$
em que $\eta$ é obtido de
$$
P(\chi^2_p \geq \eta) = \alpha
$$
para um teste escore de tamanho $\alpha$.

### Comparando as estatísticas

<!-- TODO: adicionar probabilidades dos erros tipo II, gráfico das funções poder -->

```{julia}
#| echo: true
#| output: false
# Razão de verossimilhanças
# Seja Xn a.a. de X ~ Pois(θ)
# H0: θ = θ_0
# H1: θ ≠ θ_0
using Random, Distributions, StatsBase, Plots, StatsPlots, LaTeXStrings
Random.seed!(96)
n = 100
α = 0.1

MC = 10_000

# Razão de verossimilhanças
function razao(dist, θ_0)
    TX = zeros(MC)
    for i in 1:MC
        x = rand(dist, n)
        TX[i] = 2 * n * θ_0 - 2 * n * mean(x) - 2 * n * mean(x) * log(θ_0) +
            2 * n * mean(x) * log(mean(x))
    end
    return TX
end

# Estatística de Wald
function wald(dist, θ_0)
    C = 1
    d = θ_0
    W = zeros(MC)
    for i in 1:MC
        x = rand(dist, n)
        V = mean(x) # theta =emv> mean(x)
        W[i] = n * (C * mean(x) - d)^2 / V
    end
    return W
end

# Estatística de Escore
function escore(dist, θ_0)
    I(θ) = n / θ
    R = zeros(MC)
    for i in 1:MC
        x = rand(dist, n)
        U(θ) = sum(x) / θ - n
        R[i] = U(θ_0)^2 / I(θ_0)
    end
    return R
end
```

Geraremos sob $\mathcal{H}_0$:
```{julia}
#| echo: true
# Gerando sob H_0
θ_0 = 3
dist = Poisson(θ_0)
distassin = Chisq(1)
quantil = quantile(distassin, 1 - α)
## Probabilidade do erro tipo I, α = 10%, Razão

TX = razao(dist, θ_0)
rejeita = TX .> quantil
println("Probilidade do Erro Tipo I com α = 10% (Razão de Ver.):
        $(mean(rejeita) * 100)%")


## Probabilidade do erro tipo I, α = 10%, Wald
W = wald(dist, θ_0)
rejeita = W .> quantil
println("Probilidade do Erro Tipo I com α = 10% (Wald):
        $(mean(rejeita) * 100)%")

## Probabilidade do erro tipo I, α = 10%, Wald
R = escore(dist, θ_0)
rejeita = R .> quantil
println("Probilidade do Erro Tipo I com α = 10% (Escore):
        $(mean(rejeita) * 100)%")

pv = histogram(
    TX, normalize = true, title = "Histograma da Razão de Ver.",
    label = "", ylims = (0, 1), bins = 20, xlabel = "T(X)",
    ylabel = "Densidade"
)
plot!(Chisq(1), label = L"\chi^2_1", color = :tomato)
pw = histogram(
    W, normalize = true, title = "Histograma de Wald", label = "",
    ylims = (0, 1), bins = 20, xlabel = "W(X)", ylabel = "Densidade"
)
plot!(Chisq(1), label = L"\chi^2_1", color = :tomato)
pr = histogram(
    R, normalize = true, title = "Histograma de Escore", label = "",
    ylims = (0, 1), bins = 20, xlabel = "R(X)", ylabel = "Densidade"
)
plot!(Chisq(1), label = L"\chi^2_1", color = :tomato)
l = @layout [pv pw; pr]
plot(pv, pw, pr, layout = l)
```
<!-- TODO: melhorar performance -->
Para comparar, podemos também gerar sob $\mathcal{H}_1$:
```{julia}
#| echo: true
#| eval: false
# Gerando sob H_1

rejeitaTX(θ) = mean(razao(dist, θ) .> quantil)
rejeitaW(θ) = mean(wald(dist, θ) .> quantil)
rejeitaR(θ) = mean(escore(dist, θ) .> quantil)
Δ = θ_0 * 0.3
# alance = length(Θ_0-Δ:0.1:Θ_0+Δ)
# poderTX = zeros(alcance)
# poderW = zeros(alcance)
# poderR = zeros(alcance)
# for i in Θ_0-Δ:0.1:Θ_0+Δ
#   poderTX[i] = rejeitaTX(i)
# end
pv = plot(
    rejeitaTX,
    xlims = (θ_0 - Δ, θ_0 + Δ),
    ylims = (0, 1),
    label = "",
    ylabel = "Poder Razão de ver.",
    xlabel = L"θ_1"
)
pw = plot(
    rejeitaW,
    xlims = (θ_0 - Δ, θ_0 + Δ),
    ylims = (0, 1),
    label = "",
    ylabel = "Poder Wald",
    xlabel = L"θ_1"
)
pr = plot(
    rejeitaR,
    xlims = (θ_0 - Δ, θ_0 + Δ),
    ylims = (0, 1),
    label = "",
    ylabel = "Poder Escore",
    xlabel = L"θ_1"
)
l = @layout [pv pw; pr]
plot(pv, pw, pr, layout = l)
```

### Valor-$p$ ou nível descritivo do teste

Seja $\boldsymbol{X}_n$ a.a. de $X \sim f_\theta, \theta \in \Theta \subseteq \mathbb{R}^p$. Considere
$$
\mathcal{H}_0: \theta \in \Theta_0
$$

:::{.callout-note title="Suficiência da hipótese nula para o valor-$p$"}
A hipótese alternativa, $\mathcal{H}_1$, não é necessária para formular o valor-$p$.
:::

Seja $T_{\mathcal{H}_0}(\boldsymbol{X}_n)$ tal que, quanto maior for seu valor observado, maior é a discrepância entre 
$\mathcal{H}_0$ e os dados observados. Podemos usar a estatística da razão de verossimilhanças, Wald, ou de Escore. O valor-$p$ é definido por

$$
\mathrm{valor-}p(\boldsymbol{x}_n, \mathcal{H}_0) - \sup_{\theta \in \Theta_0}
P_\theta(T_{\mathcal{H}_0}(\boldsymbol{X}_n) \geq T_{\mathcal{H}_0}(\boldsymbol{x}_n))
$$

*O valor-$p$ é a probabilidade de observar uma estatística tão ou mais extrema que a observada sob o cenário mais favorável
à $\mathcal{H}_0$.*

:::{.callout-warning title="Notação em alguns textos"}
Alguns livros, para fim didáticos, definem o valor-$p$ como probabilidade condicional:
$$
\mathrm{valor-}p(\boldsymbol{x}_n, \mathcal{H}_0) - \sup_{\theta \in \Theta_0}
P_\theta(T_{\mathcal{H}_0}(\boldsymbol{X}_n) \geq T_{\mathcal{H}_0}(\boldsymbol{x}_n) | \mathcal{H}_0)
$$

Isto, na estatística clássica, está equivocado uma vez que $\mathcal{H}_0$ é uma hipótese científica, não um evento definido
na $\sigma$-álgebra, isto é, $P(A|B)$ só está bem definido se, e somente se, $A$ e $B$ estiverem na mesma $\sigma$-álgebra.
:::

1. Observe que, quanto menor for o valor-$p$, há mais evidencias de que $\mathcal{H}_0$ é falsa.

:::{.callout-note title="Interpretação com níveis de significância"}
Caso $\mathrm{valor-}p(\boldsymbol{x}_n, \mathcal{H}_0) \leq \alpha$, então dizemos que há evidências para rejeitar
$\mathcal{H}_0$ a $\alpha \cdot 100\%$ de significância estatística.
:::

2. Se $T_{\mathcal{H}_0} \sim G$, sob $\mathcal{H}_0$, em que $G$ não depende de "$\theta$", então
$$
\mathrm{valor-}p (\boldsymbol{x}_n, \mathcal{H}_0) = P(G \geq T_{\mathcal{H}_0}(\boldsymbol{x}_n))
$$

3. Se $T_{\mathcal{H}_0} \sim G$, sob $\mathcal{H}_0$, em que $G$ não depende de "$\theta$", então
$$
\mathrm{valor-}p(\boldsymbol{X}_n,\mathcal{H}_0) \stackrel{\mathrm{Sob}\ \mathcal{H}_0}{\sim} \mathrm{Unif}(0,1)
$$

#### Valor-$p$ assintótico

Seja $T_{\mathcal{H}_0}$ uma estatística tal que
$$
T_{\mathcal{H}_0} \stackrel{\mathcal{D}}{\rightarrow} G,\ \ \text{Sob}\ \mathcal{H}_0.
$$
então, o valor-$p$ assintótico é definido por
$$
\mathrm{valor-}p^{(a)} (\boldsymbol{x}_n, \mathcal{H}_0) = P(G \geq T_{\mathcal{H}_0}(\boldsymbol{x}_n)
$$

Pode-se conduzir [simulações de Monte Carlo](monte-carlo.qmd) para verificar se o tamanho amostral é grande o suficiente
para fazer a aproximação assintótica:

1. Gerar os dados sob $\mathcal{H}_0$;

2. Calcular $\mathrm{valor-}p^{(a)}$ e armazenar;

3. Repetir 1-2 $M$ (tradicionalmente 10 mil) vezes;

4. Fazer um histograma do valor-$p$ assintótico e comparar com $\mathrm{Unif}(0,1)$.


#### Exemplo (Normal)

Seja $\boldsymbol{X}_n$ a.a. de $X \sim N(\theta, 1), \theta \in \Theta \subseteq \mathbb{R}$. Considere
$$
\mathcal{H}_0 : \theta = 0.
$$

Encontre um valor-$p$.

##### Resposta

Observe que $T_{\boldsymbol{H}_0}(\boldsymbol{x}_n) = n\bar{X}^2$ (essa também é a estatística de Wald) satisfaz o critério
que, quanto maior o valor observado de $T_{\boldsymbol{H}_0}$, mais evidencias contra $\mathcal{H}_0$. Além disso, sob $\mathcal{H}_0$,

$$
T_{\mathcal{H}_0} \sim \chi^2_1 \Rightarrow \mathrm{valor-}p(\boldsymbol{x}_n, \mathcal{H}_0) = P(\chi^2_1 \geq n \bar{x}^2)
$$

Se $n = 10$ e $\bar{x} = 1$, então
$$
\mathrm{valor-}p(\boldsymbol{x}_n, \mathcal{H}_0) = P(\chi^2_1 \geq 10 \cdot 1^2) = 0.001
$$

#### Exemplo (Poisson)

Seja $\boldsymbol{X}_n$ a.a. de $X \sim \mathrm{Pois}(\theta), \theta \in \Theta = (0,\infty)$. Considere
$$
\mathcal{H}_0: \theta = 1.
$$

Encontre o valor-$p$ assintótico utilizando as três estatísticas (Razão de Verossimilhanças, Wald e Escore,
$T'_{\mathcal{H}_0}(\boldsymbol{X}_n), T''_{\mathcal{H}_0}(\boldsymbol{X}_n), T'''_{\mathcal{H}_0}(\boldsymbol{X}_n)$).

##### Resposta

Para a razão de verossimilhanças,
$$
\begin{aligned}
T'_{\mathcal{H}_0}(\boldsymbol{X}_n) &= 2 n \cdot 1 - 2 n \bar{X} - 2 n \bar{X} \ln \frac{1}{\bar{X}} \\
&= 2n - 2n\bar{X} + 2n\bar{X}\ln\frac{1}{\bar{X}} \stackrel{\mathcal{D}}{\rightarrow} \chi^2_1,\ \ \theta = 1.
\end{aligned}
$$


Para a estatística de Wald, usaremos $\hat{\theta} = \bar{X}$ (que é o [EMV](emv2.qmd), [EMM](metodo-momentos.qmd) e [ENVVUM](envvum.qmd))

$$
\begin{aligned}
T''_{\mathcal{H}_0}(\boldsymbol{X}_n) &= \frac{n (\bar{X} - 1)^2}{\bar{X}} \stackrel{\mathcal{D}}{\rightarrow} \chi^2_1,\ \ \theta = 1.
\end{aligned}
$$
pois $\bar{X} \stackrel{a}{\approx} N(\theta, \theta / n)$, sob $\mathcal{H}_0$, então, pelo [Teorema de Slutsky](slutsky.qmd),
$$
\frac{\bar{X} - \theta}{\sqrt{\frac{\bar{X}}{n}}} \stackrel{a}{\approx} N(0, 1)
\Rightarrow n\left(\frac{\bar{X} - \theta}{\sqrt{\bar{X}}}\right)^2 \stackrel{a}{\approx} \chi^2_1, \forall \theta \in \Theta.
$$
Logo, sob $\mathcal{H}_0 : \theta = 1$,
$$
\frac{n(\bar{X} - 1)^2}{\bar{X}} \approx \chi^2_1
$$

Para a estatística de escore, temos que o [escore](escore.qmd) é dado por
$$
U_n(\boldsymbol{X}_n, \theta) = -n + \frac{1}{\theta_0} \sum X_i
$$
e a [Informação de Fisher Total](infofisher.qmd) por
$$
I_n(\theta_0) = -E_\theta\left(-\sum \frac{X_i}{\theta_0} \right) = \frac{n}{\theta_0},
$$
portanto,
$$
\begin{aligned}
T'''_{\mathcal{H}_0} (\boldsymbol{X}_n) &= \left(\frac{\sum X_i}{\theta_0} - n \right)^2 \cdot \frac{\theta_0}{n} \\
&= n \frac{(\bar{X} - \theta_0)^2}{\theta_0} = n(\bar{X} - 1)^2.
\end{aligned}
$$

O valor-$p$ assintótico é, portanto,
$$
\begin{aligned}
\mathrm{valor-}p_1 (\boldsymbol{x}_n, \mathcal{H}_0) &= P_1(T'_{\mathcal{H}_0}(\boldsymbol{X}_n) \geq T'_{H_0}(\boldsymbol{x}_n)) \stackrel{a}{\approx}
\mathrm{val.-}p^{(a)}_1 (\boldsymbol{x}_n, \mathcal{H}_0) = P(\chi^2_1 \geq T'_{H_0}(\boldsymbol{x}_n)) \\
\mathrm{valor-}p_2 (\boldsymbol{x}_n, \mathcal{H}_0) &= P_1(T''_{\mathcal{H}_0}(\boldsymbol{X}_n) \geq T''_{H_0}(\boldsymbol{x}_n)) \stackrel{a}{\approx}
\mathrm{val.-}p^{(a)}_2 (\boldsymbol{x}_n, \mathcal{H}_0) = P(\chi^2_1 \geq T''_{H_0}(\boldsymbol{x}_n)) \\
\mathrm{valor-}p_3 (\boldsymbol{x}_n, \mathcal{H}_0) &= P_1(T'''_{\mathcal{H}_0}(\boldsymbol{X}_n) \geq T'''_{H_0}(\boldsymbol{x}_n)) \stackrel{a}{\approx}
\mathrm{val.-}p^{(a)}_3 (\boldsymbol{x}_n, \mathcal{H}_0) = P(\chi^2_1 \geq T'''_{H_0}(\boldsymbol{x}_n))
\end{aligned}
$$

Consideraremos $\bar{x} = 2$ e $n=10$ para encontramos os valores numéricos dos valor-$p$:
$$
\begin{aligned}
T'_{H_0}(\boldsymbol{x}_n) &= 2 \cdot 10 - 2 \cdot 10 \cdot 2 + 2 \cdot 10 \cdot 2 \ln 2 = 7.73 \\
\Rightarrow \mathrm{valor-}p^{(a)}_1 (\boldsymbol{x}_n, \mathcal{H}_0) &= 0.005, \\ \\
T''_{H_0}(\boldsymbol{x}_n) &= 10 \cdot \frac{(2-1)^2}{2} = 5 \\
\Rightarrow \mathrm{valor-}p^{(a)}_2 (\boldsymbol{x}_n, \mathcal{H}_0) &= 0.025, \\ \\
T'''_{H_0}(\boldsymbol{x}_n) &= 10 \cdot 1^2 = 10 \\
\Rightarrow \mathrm{valor-}p^{(a)}_3 (\boldsymbol{x}_n, \mathcal{H}_0) &= 0.002.
\end{aligned}
$$

Note que, com $n=10$, a aproximação pode não ser tão boa:
```{julia}
#| echo: true
n = 10
p1 = @. ccdf(Chisq(1), razao(Poisson(1), 1))
p2 = @. ccdf(Chisq(1), wald(Poisson(1), 1))
p3 = @. ccdf(Chisq(1), escore(Poisson(1), 1))

hist1 = histogram(
    p1, title = "Valores-P da Razão de Ver.", label = "",
    normalize = true
)
hist2 = histogram(
    p2, title = "Valores-P da Wald", label = "",
    normalize = true
)
hist3 = histogram(
    p3, title = "Valores-P da Escore", label = "",
    normalize = true
)

plot(hist1, hist2, hist3, layout = l)
```
(Lembre-se que, assintoticamente, esperamos um histograma uniforme). Vamos medir:

```{julia}
#| output: asis
L"\hat P(\mathrm{val.-}p_1 < 0.05) = %$(mean(p1 .< 0.05)), \hat P(\mathrm{val.-}p_2 < 0.05) = %$(mean(p2 .< 0.05)), \hat P(\mathrm{val.-}p_3 < 0.05) = %$(mean(p3 .< 0.05))"
```

Na distribuição Poisson, espera-se que, com valores pequenos de $\theta$, a aproximação piore pela assimetria. Vejamos com $\theta_0 = 0.01, n=100$:
```{julia}
#| echo: true
n = 100
p1 = @. ccdf(Chisq(1), razao(Poisson(0.01), 0.01))
p2 = @. ccdf(Chisq(1), wald(Poisson(0.01), 0.01))
p3 = @. ccdf(Chisq(1), escore(Poisson(0.01), 0.01))

hist1 = histogram(
    p1, title = "Valores-P da Razão de Ver.", label = "",
    normalize = true
)
hist2 = histogram(
    p2, title = "Valores-P da Wald", label = "",
    normalize = true
)
hist3 = histogram(
    p3, title = "Valores-P da Escore", label = "",
    normalize = true
)

plot(hist1, hist2, hist3, layout = l)
```

```{julia}
#| output: asis
L"\hat P(\mathrm{val.-}p_1 < 0.05) = %$(mean(p1 .< 0.05)), \hat P(\mathrm{val.-}p_2 < 0.05) = %$(mean(p2 .< 0.05)), \hat P(\mathrm{val.-}p_3 < 0.05) = %$(mean(p3 .< 0.05))"
```

Mesmo com uma amostra maior, o efeito da assimetria fez com que a convergência assintótica fosse muito lenta.

Se gerarmos dados fora da hipótese nula, como $\theta_1 = 2$, será possível visualizar o erro no histograma:

```{julia}
#| echo: true
n = 10
p1 = @. ccdf(Chisq(1), razao(Poisson(2), 1))
p2 = @. ccdf(Chisq(1), wald(Poisson(2), 1))
p3 = @. ccdf(Chisq(1), escore(Poisson(2), 1))

hist1 = histogram(
    p1, title = "Valores-P da Razão de Ver.", label = "",
    normalize = :pdf
)
hist2 = histogram(
    p2, title = "Valores-P da Wald", label = "",
    normalize = :pdf
)
hist3 = histogram(
    p3, title = "Valores-P da Escore", label = "",
    normalize = :pdf
)

plot(hist1, hist2, hist3, layout = l)
```

```{julia}
#| output: asis
L"\hat P(\mathrm{val.-}p_1 < 0.05) = %$(mean(p1 .< 0.05)), \hat P(\mathrm{val.-}p_2 < 0.05) = %$(mean(p2 .< 0.05)), \hat P(\mathrm{val.-}p_3 < 0.05) = %$(mean(p3 .< 0.05))"
```

### Teste de hipótese para o vetor de médias sob normalidade

Seja $\boldsymbol{X}_n^*$ a.a. de $\boldsymbol{X} \sim N_p(\boldsymbol{\mu},
\Sigma)$. Considere as hipóteses

$$
\begin{cases}
\mathcal{H}_0 : \boldsymbol{\mu} = \boldsymbol{\mu}_0 \\
\mathcal{H}_1 : \boldsymbol{\mu} \neq \boldsymbol{\mu}_0
\end{cases}
$$

em que $\boldsymbol{\mu}_0$ é um vetor de números fixado e conhecido.

Se o $\Sigma$ for desconhecido, o vetor de parâmetros é
$$
\sigma = \begin{pmatrix}
\boldsymbol{\mu} \\
\mathrm{vech}(\Sigma)
\end{pmatrix}
= \begin{pmatrix}
\mu_1 \\
\vdots \\
\mu_p \\
\sigma_{11} \\
\sigma_{21} \\
\vdots \\
\sigma_{p1} \\
\sigma_{p2} \\
\vdots \\
\sigma_{pp}
\end{pmatrix}
$$

em que 

$$
\mathrm{vech} \begin{bmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{bmatrix} = \begin{\bmatrix}
a_{11} \\ a_{21} \\ a_{22}
\end{bmatrix}
$$

Se $\Sigma$ for conhecido o vetor de parâmetros é $\theta = \boldsymbol{\mu}$.

Pode-se utilizar as estatísticas  de razão de verossimilhanças, Wald e Escore
para testar as hipóteses. Todas essas estatísticas convergem para $\chi^2_p$.o

Para a razão de verossimilhanças,

$$
\frac{\sup\limits_{\theta \in \Theta_0} \mathcal{L}_{\boldsymbol{x}_n}(\theta)}
{\sup\limits_{\theta \in \Theta} \mathcal{L}_{\boldsymbol{x}_n}(\theta)}
$$

Para a estatística de Wald,

$$
\begin{cases}
C = (I_p, \boldsymbol{0}) \\
d = \boldsymbol{\mu}_0
\end{cases}
$$

Para a Escore ($\Sigma$ conhecido)

$$
R = U_n(\theta_n, \boldsymbol{X}_n^*)^T I_n(\theta)^{-1}
U_n(\theta_n, \boldsymbol{X}_n^*)
$$

Para encontrar a distribuição exata, precisaremos introduzir as distribuições de
[Wishart](wishart.qmd) e [$T^2$-Hotelling](hotelling.qmd).


#### Caso geral

Seja $\boldsymbol{X}_n^*$ a.a. de $X \sim N_p(\boldsymbol{\mu}, \Sigma)$.
Considere

$$
\begin{cases}
\mathcal{H}_0: C \boldsymbol{\mu} = d \\
\mathcal{H}_1: C \boldsymbol{\mu} \neq d
\end{cases}
$$

Definimos a função teste por

$$
\delta(\boldsymbol{x}^*) = \begin{cases}
0,\ \ \text{se}\ W(\boldsymbol{x}^*) < \eta \\
1,\ \ \text{se}\ W(\boldsymbol{x}^*) \geq \eta
\end{cases}
$$

em que

$$
W(\boldsymbol{X}_n^*) = 
\frac{n-s}{s}(C\bar{X} - C\boldsymbol{\mu})^T [CS_n^2C^{-1}]^{-1}(C\boldsymbol{X}_n -
C\boldsymbol{\mu}) \sim F_{(s,n-s)},\ \text{Sob}\ \mathcal{H}_0
$$

pela propriedade da [distribuição de hotelling](hotelling.qmd) (Teorema 8)
e $\eta$ deve ser obtido fixando o tamanho do teste igua a $\alpha$ e
calculando:

$$
P(F_{(s, n-s)} \geq \eta) = \alpha
$$

e o valor-$p$ para testar $\mathcal{H}_0$ é

$$
\mathrm{valor-}p(\boldsymbol{x}^*, \mathcal{H}_0) = 
P(F_{(s, n-s)} \geq W(\boldsymbol{x}^*))
$$
