# Estatísticas Suficientes

Seja $\boldsymbol{X}_n(X_1,\dots,X_n)$ uma [amostra aleatória](populacao-e-amostra.qmd#sec-aa) de $X\sim f_\theta,
\theta \in \Theta$. Dizemos que uma [estatística](estatisticas.qmd) $T(\boldsymbol{X}_n)$ é suficiente para o 
[modelo estatístico](modelo-estatistico.qmd) se, e somente se, a distribuição da amostra dado que $T(\boldsymbol{X}_n) = t$
não depende de "$\theta$". Ou seja,
$$
\begin{aligned}
P_\theta(X_1\leq y_1,\dots,X_n\leq y_n \lvert T(\boldsymbol{X}_n)=t)\
\text{Não depende de}\ \theta, \forall y_1,\dots,y_n \in \mathbb{R} \\
\text{E para todo valor de}\ t \ \text{para o quais a distribuição de}\ T(\boldsymbol{X}_n)\ \text{exista}
\end{aligned}
$$

Em outras palavras, a informação probabilística sobre "$\theta$" da amostra aleatória está inteiramente contida no modelo
induzido pela estatística.

No caso discreto, basta mostrar que $P_\theta(X_1=y_1,\dots,X_n=y_n\lvert T(\boldsymbol{X}_n)=t)$ não depende de "$\theta$"
para todo $y_1,\dots,y_n \in \mathbb{R}$ e valores de $t$ para os quais a distribuição de $T(\boldsymbol{X}_n)$ exista.

No caso contínuo, basta mostrar que $f_\theta^{(n)}(y_1,\dots,y_n\lvert t)$ não depende de $\theta$ para todo
$y_1,\dots,y_n \in \mathbb{R}$ e valores de $t$ para os quais a função densidade de probabilidade de $T(\boldsymbol{X}_n)$ exista.

Como discutiremos [adiante](estatisticas-suficientes.qmd#sec-problema), podemos substituir a restrição "$\in \mathbb{R}$"
pelo termo "quase certamente" ($\mathrm{q.c.}$), isto é, para todos exceto um conjunto enumerável (de medida de probabilidade nula).

Também, $f_\theta^{(n)}$ será reescrita por $f_\theta^{\boldsymbol{X}_n}$ para diferenciar a função (densidade) da amostra, da estatística e das condicionais.

Usaremos $y$ no lugar de $x$ para distinguir que $y$ representa valores observados *em potencial*, e não realmente observados
de uma amostra real colhida, como poderia estar subtendido com o uso de $x$.

## Caso discreto

### Exemplos

#### Exemplo 1

Seja $\boldsymbol{X}_n = (X_1, \dots, X_n)$ uma a.a. de $X\sim\mathrm{Ber}(\theta), \theta \in \Theta = (0,1)$.
Verifique se $T(\boldsymbol{X}_n)=\sum^n_{i=1}X_i$ é suficiente para o modelo estatístico.

##### Resposta

Por definição,
$$
P_\theta(X_1=y_1,\dots,X_n=y_n\lvert T(\boldsymbol{X}_n) = t) = \frac{P_\theta(X_1=y_1,\dots,X_n=y_n,T(\boldsymbol{X}_n)=t)}
{P_\theta(T(\boldsymbol{X}_n)=t)}
$$

Sabemos que $T(\boldsymbol{X}_n)=\sum^n_{i=1}X_i \sim \mathrm{Bin}(n,\theta)$ (por função geradora de momentos). Portanto,
$$
P_\theta(T(\boldsymbol{X}_n)=t) = \left\{ \begin{array}{ll}
\binom{n}{t} \cdot \theta^t\cdot(1-\theta)^{n-t},&\text{se}\ t \in \{0,1,\dots,n\} \\
0, & \mathrm{c.c.}
\end{array} \right.
$$

Logo, $P_\theta(X_1=y_1,\dots,X_n=y_n\lvert T(\boldsymbol{X}_n) = t)$ só está bem definida se $t \in \{0,1,\dots,n\}$.

(Numerador)
$$
\begin{aligned}
&P_\theta(X_1=y_1,\dots,X_n=y_n,T(\boldsymbol{X}_n)=t) \\
&\stackrel{\mathrm{TPT}}{=}
\overbracket{P_\theta(X_1=y_1,\dots,X_n=y_n)}^{A} \cdot \overbracket{P_\theta(T(\boldsymbol{X}_n)=t\lvert X_1=y_1,\dots,X_n=y_n)}^B
\end{aligned}
$$

Observe que $A$ é a função probabilidade da amostra e
$$
B = \left \{ \begin{array}{ll}
1, & \text{se}\ t = \sum^n_{i=1} y_i \\
0, & \mathrm{c.c}
\end{array}\right.
$$

Para $t \in \{0,1,\dots,n\}$

$$
P_\theta(X_1=y_1,\dots,X_n=y_n\lvert T(\boldsymbol{X}_n) = t) =
\left \{ \begin{array}{ll}
\frac{\prod^n_{i=1}\theta^{y_i}(1-\theta)^{1-y_i}}{\binom{n}{t}\cdot\theta^t\cdot(1-\theta)^{n-t}}, & \text{se}\ t = \sum^n_{i=1}y_i\\
0, & \mathrm{c.c}
\end{array}\right. \forall \theta \in \Theta
$$

Portanto,
$$
P_\theta(X_1=y_1,\dots,X_n=y_n\lvert T(\boldsymbol{X}_n) = t) =
\left \{ \begin{array}{ll}
\frac{\theta^{\sum^n_{i=1}y_i}(1-\theta)^{\sum^n_{i=1}1-y_i}}{\binom{n}{t}\cdot\theta^t\cdot(1-\theta)^{n-t}}, & \text{se}\ t = \sum^n_{i=1}y_i \\
0, & \mathrm{c.c}
\end{array}\right. \forall \theta \in \Theta
$$

Concluímos que
$$
\begin{aligned}
&P_\theta(X_1=y_1,\dots,X_n=y_n\lvert T(\boldsymbol{X}_n) = t) =
\left \{ \begin{array}{ll}
\frac{\prod_{i=1}^n \mathbb{1}_{\{0,1\}}(y_i)}{\binom{n}{t}}, & \text{se}\ t = \sum^n_{i=1}y_i \\
0, & \mathrm{c.c}
\end{array}\right.\\
&\forall \theta \in \Theta, t \in \{0,1,\dots,n\}, \forall y_1,\dots,y_n \in \mathbb{R}
\end{aligned}
$$

A estatística $T(\boldsymbol{X}_n)$ é suficiente para o modelo estatístico Bernoulli.

#### Exemplo 2

Seja $\boldsymbol{X}_n=(X_1,\dots,X_n)$ uma amostra aleatória de $X\sim\mathrm{Pois}(\theta), \theta \in \Theta = (0,\infty)$.
Verifique se $T(\boldsymbol{X}_n)= \frac{1}{n}\sum^n_{i=1}X_i$ é uma estatística suficiente para o modelo estatístico.

##### Resposta

$$
P_\theta(X_1=y_1,\dots,X_n=y_n\lvert T(\boldsymbol{X}_n) = t) =
\frac{P_\theta(X_1=y_1,\dots,X_n=y_n,T(\boldsymbol{X}_n) = t)}
{P_\theta(T(\boldsymbol{X}_n)=t)}
$$

(Numerador)
$$
\begin{aligned}
&P_\theta(X_1=y_1,\dots,X_n=y_n,T(\boldsymbol{X}_n)=t) \\
&\stackrel{\mathrm{TPT}}{=}
\overbracket{P_\theta(X_1=y_1,\dots,X_n=y_n)}^{A} \cdot \overbracket{P_\theta(T(\boldsymbol{X}_n)=t\lvert X_1=y_1,\dots,X_n=y_n)}^B
\end{aligned}
$$

Observe que $A$ é a função probabilidade da amostra e
$$
B = \left \{ \begin{array}{ll}
1, & \text{Se}\ t =\frac{1}{n} \sum^n_{i=1} y_i \\
0, & \mathrm{c.c}
\end{array}\right.
$$

Já sabemos que $\sum^n_{i=1}X_i\sim \mathrm{Pois}(n\theta)$ (por função geradora de momentos)

$$
P_\theta\left(\frac{1}{n}\sum^n_{i=1}X_i=\frac{k}{n}\right) = \left\{ \begin{array}{ll}
\mathrm{e}^{-n\theta}\cdot \frac{(n\theta)^k}{k!}, & k \in \{0,1,2,\dots\} \\
0, & \mathrm{c.c.}
\end{array}\right.
$$

Tome $t = \frac{k}{n}$, então $k=nt$
$$
P_\theta\left(\sum^n_{i=1}X_i=k\right) = \left\{ \begin{array}{ll}
\mathrm{e}^{-n\theta}\cdot \frac{(n\theta)^{nt}}{(nt)!}, & t \in \{0,\frac{1}{n},\frac{2}{n},\dots\} \\
0, & \mathrm{c.c.}
\end{array}\right.
$$

Portanto, para $t \in \{0, \frac{1}{n}, \frac{2}{n},\dots\}$, temos que
$$
\begin{aligned}
&P_\theta(X_1=y_1,\dots,X_n=y_n\lvert T(\boldsymbol{X}_n)=t) = \\ &= 
\left\{ \begin{array}{ll}
\frac{\prod^n_{i=1}\left\{\mathrm{e}^{-\theta}\cdot\frac{\theta^{y_i}}{y_i!}\cdot \mathbb{1}_{\{0,1,\dots\}}(y_i)\right\}} 
{\mathrm{e}^{-n\theta}\cdot \frac{(n\theta)^{nt}}{(nt)!}}, & t=\frac{1}{n} \sum^n_{i=1} y_i \\
0, & \mathrm{c.c.}
\end{array}\right. \\
&= \left\{\begin{array}{ll}
\frac{\mathrm{e}^{-n\theta}\cdot\theta^{\sum^n_{i=1}y_i}\cdot \prod^n_{i=1}\mathbb{1}_{\{0,1,\dots\}}(y_i)(nt!)} 
{\prod^n_{i=1}(y_i!)\cdot\mathrm{e}^{-n\theta}\cdot (n\theta)^{nt}}, & t=\frac{1}{n} \sum^n_{i=1} y_i \\
0, & \mathrm{c.c.}
\end{array}\right. \\
&= \left\{\begin{array}{ll}
\frac{\prod^n_{i=1}\mathbb{1}_{\{0,1,\dots\}}(y_i)(nt!)} 
{\prod^n_{i=1}(y_i!)(n)^{nt}}, & t=\frac{1}{n} \sum^n_{i=1} y_i \\
0, & \mathrm{c.c.}
\end{array}\right. \\
\forall \theta \in \Theta
\end{aligned} 
$$

Não depende de "$\theta$" para todo $y_1,\dots,y_n \in \mathbb{R}$ e $t \in \{0,\frac{1}{n},\frac{2}{n},\dots\}$.

## Caso Contínuo

### Exemplo (Normal)
Seja $\boldsymbol{X}_n = (X_1,\dots,X_n)$ a.a. de $X\sim N(\theta,1), \theta \in \mathbb{R}$. Verifique se
$T(\boldsymbol{X}_n) = \sum^n_{i=1}X_i$ é suficiente para o modelo estatístico.

#### Resposta
Por definição
$$
f_\theta^{\boldsymbol{X}_n\lvert T(\boldsymbol{X}_n) = t}(y_1,\dots,y_n) = \frac{f_\theta^{\boldsymbol{X}_n, T(\boldsymbol{X}_n)}(y_1,\dots,y_n,t)}
{f_\theta^{T(\boldsymbol{X}_n)}(t)}
$$

(Denominador)
Já sabemos que $T(\boldsymbol{X}_n) = \sum^n_{i=1} X_i \sim N(n\theta, n)$
$$
\Rightarrow f_\theta^{T(\boldsymbol{X}_n)}(t) = \frac{1}{\sqrt{2\pi n}} \cdot \mathrm{e}^{-\frac{1}{2n}\cdot (t-n\theta)^2}, t \in \mathbb{R}
$$
(Numerador)
$$
f_\theta^{\boldsymbol{X}_n,T(\boldsymbol{X}_n)}(y_1,\dots,y_n,t) = 
f_\theta^{\boldsymbol{X}_n}(y_1,\dots,y_n) \cdot f_\theta^{T(\boldsymbol{X}_n)\lvert \boldsymbol{X}_n=(y_1,\dots,y_n)}(t)
$$

Note que
$$
\begin{aligned}
&f_\theta^{T(\boldsymbol{X}_n)\lvert \boldsymbol{X}_n = (y_1,\dots,y_n} = \left\{ \begin{array}{ll}
1, & t = \sum^n_{i=1} y_1 \\
0, & \mathrm{c.c.}
\end{array}\right. \\
\Rightarrow&
f_\theta^{\boldsymbol{X}_n\lvert T(\boldsymbol{X}_n) = t}(y_1,\dots,y_n) = \left\{ \begin{array}{ll}
\frac{f_\theta^{\boldsymbol{X}_n}(y_1,\dots,y_n)}
{f_\theta^{T(\boldsymbol{X}_n)}(t)},& t = \sum^n_{i=1}y_i \\
0, & \mathrm{c.c.}
\end{array} \right.
\end{aligned}
$$

Logo
$$
f_\theta^{T(\boldsymbol{X}_n)\lvert \boldsymbol{X}_n = (y_1,\dots,y_n)} = \left\{ \begin{array}{ll}
\frac{\frac{1}{\sqrt{2\pi\cdot1}^n} \cdot \mathrm{exp}\left\{-\frac{1}{2} \sum^n_i=1 (y_i - \theta)^2\right\}}
{\frac{1}{\sqrt{2\pi\cdot n}}\cdot \mathrm{exp}\left\{-\frac{1}{2n}(t-n\theta)^2\right\}}, & t = \sum y_i \\
0, & \mathrm{c.c.}
\end{array}\right.
$$

Note que 
$$
-\frac{1}{2}\sum(y_i-\theta)^2=-\frac{1}{2}\left(\frac{t^2}{n}-2t\theta+n\theta^2\right)
$$
logo $f_\theta^{\boldsymbol{X}_n\lvert T(\boldsymbol{X}_n)=t}$ não depende de $\theta$ e $\sum X_i$ é suficiente para o modelo
estatístico.

### Problema das funções densidade de probabilidade {#sec-problema}

*A função densidade de probabilidade não é única.* Entretanto, é única para quase todo ponto (quase certamente).

Por exemplo, $X\sim\mathrm{Exp}(\theta), \theta \in (0,\infty)$
$$
\begin{aligned}
f_\theta(x) &= \left\{\begin{array}{ll}
\theta \mathrm{e}^{-\theta x},& x \in (0, \infty) \\
0, & x \not \in (0,\infty)
\end{array}\right.\ \ \theta \in \Theta \\
P_\theta(X>2)&=\int^\infty_2 \theta \cdot \mathrm{e}^{-\theta x} dx \\ 
&\text{Se $A$ é enumerável e defina} \\
f_{\theta}^A(x) &= \left\{\begin{array}{ll}
\theta \mathrm{e}^{-\theta x},& x \in (0, \infty) \setminus A \\
10, & x \in \mathrm{A} \\
0, & x \not \in (0,\infty)
\end{array}\right. \ \ \theta \in \Theta \\
\end{aligned}
$$

Temos que
$$
\begin{aligned}
f_\theta(x) &= f_\theta^A(x), \forall x \in \mathbb{R} \setminus A, \forall \theta \in \Theta \\
\text{e} \\
f_\theta(x) &\neq f_\theta^A(x), \forall x \in A, \forall \theta \in \Theta
\end{aligned}
$$


Note que $f_\theta$ e $f_\theta^A$ são diferentes, mas produzem as mesmas probabilidades. Dizemos portanto que $f_\theta$
e $f_\theta^A$ são iguais *quase certamente*, ou seja,
$$
P_\theta\left(f_\theta(x)=f_\theta^A(x)\right) = 1, \forall \theta \in \Theta
$$
Ou, de outra forma,
$$
P_\theta\left(f_\theta(x)\neq f_\theta^A(x)\right) = 0, \forall \theta \in \Theta
$$

Notação:
$$
f_\theta(x) = f_\theta^A(x)\ \mathrm{q.c.}\ \forall \theta \in \Theta
$$

No caso contínuo, portanto, a estatística $T(\boldsymbol{X}_n)$ será suficiente mesmo se
$f_\theta^{\boldsymbol{X}_n\lvert T(\boldsymbol{X}_n)=t}(y_1,\dots,y_n)$
depende de "$\theta$" para $(y_1,\dots,y_n) \in A$, **DESDE QUE** $P_\theta(X \in A) = 0, \forall \theta \in \Theta$. Ou seja,
pode depender de "$\theta$" em um conjunto com probabilidade zero.

## Critério da Fatoração de Neyman-Fisher (Caso Simples) {#sec-crit-fat}

Seja $\boldsymbol{X}_n=(X_1,\dots,X_n)$ uma [amostra aleatória](populacao-e-amostra.qmd#sec-aa) de $X \sim f_\theta, \theta \in \Theta \subseteq \mathbb{R}^p$
(sendo as $f_\theta, \theta \in \Theta$ do mesmo "tipo", formalmente, dominadas pela mesma medida), em que $p \in \{1,2,\dots\}$.
Uma estatística $T(\boldsymbol{X}_n)$ é suficiente para o modelo estatístico se, e somente se, existirem funções
$h(\cdot): \mathbb{R}^n\rightarrow \mathbb{R}, m(\cdot,\cdot): \mathrm{Im}(T)\times\theta\rightarrow\mathbb{R}$
(mensuráveis) tais que:
$$
f_\theta^{\boldsymbol{X}_n}(y_1,\dots,y_n) = h(y_1,\dots,y_n)\cdot m\left(T(y_1,\dots,y_n),\theta\right), \forall \theta \in \Theta\; \mathrm{q.c.}
$$

Obs:

1. $h$ não depende de "$\theta$";

2. $m$ depende de valores amostrais por meio da estatística $T(\boldsymbol{X}_n)$;

3. $f_\theta^{\boldsymbol{X}_n} = f_\theta^{(n)}$ é a função (densidade) de probabilidade da amostra aleatória.

4. Note que a [função de verossimilhança](funcao-verossimilhanca.qmd) é obtida calculando $f_\theta^{\boldsymbol{X}_n}$ na
amostra observada, ou seja,
$$
L_{\boldsymbol{X}_n}(\theta) = f_\theta^{\boldsymbol{X}_n}\underbracket{(x_1,\dots,x_n)}^{\boldsymbol{X}_n}
$$

5. Alguns livros usam a função de verossimilhança no critério da fatoração
$$
L_{\boldsymbol{X}_n}(\theta) =  h(\boldsymbol{X}_n) \cdot m(T(\boldsymbol{X}_n),\theta)\; \mathrm{q.c.} \forall \theta \in \Theta
$$
em que $\boldsymbol{X}_n = (x_1,\dots,x_n)$ da [amostra observada](populacao-e-amostra.qmd#sec-ao)

### Prova (caso discreto)

#### $\Rightarrow$

Assuma que $T(\boldsymbol{X}_n)$ seja suficiente. Por definição, $P_\theta(X_1=y_1,\dots,X_n=y_n\lvert T(\boldsymbol{X}_n) = t)$
não depende de "$\theta$". Logo, podemos escrever:
$$
P_\theta(X_1=y_1,\dots,X_n=y_n\lvert T(\boldsymbol{X}_n)=t) = h^*(y_1,\dots,y_n,t) \forall \theta \in \Theta
$$ {#eq-podemos}
Note também que,
$$
P_\theta(X_1=y_1,\dots,X_n=y_n\lvert T(\boldsymbol{X}_n)=t) =
\frac{P_\theta(X_1=y_1,\dots,X_n=y_n, T(\boldsymbol{X}_n)=t)}
{P_\theta(T(\boldsymbol{X}_n)=t)}
$$
para valores de $t$ em que a probabilidade condicional exista.

$$
\begin{aligned}
&P_\theta(X_1=y_1,\dots,X_n=y_n, T(\boldsymbol{X}_n)=t) \\
&=
P_\theta(X_1=y_1,\dots,X_n=y_n)\cdot P_\theta(T(\boldsymbol{X}_n)=t\lvert X_1=y_1,\dots,X_n=y_n).
\end{aligned}
$$

Como, com $\boldsymbol{y}_n = y_1,\dots,y_n$,
$$
P_\theta(T(\boldsymbol{X}_n)=t\lvert X_1=y_1,\dots,X_n=y_n) = \left\{\begin{array}{ll}
1, & T(\boldsymbol{y}_n) = t \\
0, & \mathrm{cc}
\end{array}\right.
$$
então
$$
P_\theta(X_1=y_1,\dots,X_n=y_n\lvert T(\boldsymbol{X}_n)=t) =
\frac{P_\theta(X_1=y_1,\dots,X_n=y_n)\cdot \mathbb{1}(T(\boldsymbol{y}_n)=t)}
{P_\theta(T(\boldsymbol{X}_n) =t)}
$${#eq-aberto}

Por ([-@eq-podemos]) e ([-@eq-aberto]), temos que
$$
h^*(y_1,\dots,y_n, t) \cdot P_\theta(T(\boldsymbol{X}_n)=t) = P_\theta(X_1=y_1,\dots,X_n=y_n\lvert \mathbb{1}(T(\boldsymbol{X}_n)=t)).
$$

Para $T(\boldsymbol{X}_n)=t$, temos que
$$
P_\theta(X_1=y_1,\dots,X_n=y_n) = h^*(y_1,\dots,y_n, T(\boldsymbol{y}_n)) \cdot P_\theta(T(\boldsymbol{X}_n)=T(\boldsymbol{y}_n))
$$

#### $\Leftarrow$
Assuma que existam $h, m$ tais que
$$
P_\theta(X_1=y_1,\dots,X_n=y_n) = h(\boldsymbol{y}_n) m(T(\boldsymbol{y}_n,\theta))
$$
Note que
$$
P_\theta(X_1=y_1,\dots,X_n=y_n\lvert T(\boldsymbol{X}_n) = t) = \left\{ \begin{array}{ll}
\frac{P_\theta(X_1=y_1,\dots,X_n=y_n)}{P_\theta(T(\boldsymbol{X}_n)=t)}, & T(\boldsymbol{y}_n)=t \\
0, & \mathrm{c.c.}
\end{array}\right.
$$

Observe que
$$
P_\theta(T(\boldsymbol{X}_n) = t) = \sum_{(y_1,\dots,y_n) : T(\boldsymbol{y})_n=t} P_\theta(X_1=y_1,\dots,X_n=y_n)
$$
Por suposição
$$
\begin{aligned}
P_\theta(T(\boldsymbol{X}_n) =t) &= \sum_{\boldsymbol{y}_n : T(\boldsymbol{y}_n)=t} h(\boldsymbol{y}_n) \cdot m (T(\boldsymbol{y}_n), \theta) \\
&= \sum_{\boldsymbol{y}_n : T(\boldsymbol{y}_n)=t} h(\boldsymbol{y}_n) \cdot m(t,\theta) \\
&= m(t,\theta) \cdot \sum_{\boldsymbol{y}_n : T(\boldsymbol{y}_n)=t} h(\boldsymbol{y}_n)
\end{aligned}
$$
Portanto
$$
P_\theta(X_1=y_1,\dots,X_n=y_n\lvert T(\boldsymbol{X}_n)=t) = \left\{\begin{array}{ll}
\frac{h(\boldsymbol{y}_n)\cdot m(T(\boldsymbol{y}_n),\theta)}{m(t,\theta) \cdot \sum_{\boldsymbol{y}_n: T(\boldsymbol{y}_n) = t} h(\boldsymbol{y}_n)}, & T(\boldsymbol{y}_n) = t \\
0, & \mathrm{c.c.}
\end{array}\right.
$$

Logo,
$$
P_\theta(X_1=y_1,\dots,X_n=y_n\lvert T(\boldsymbol{X}_n)=t) = \left\{\begin{array}{ll}
\frac{h(\boldsymbol{y}_n)}{\sum_{\boldsymbol{y}_n: T(\boldsymbol{y}_n) = t} h(\boldsymbol{y}_n)}, & T(\boldsymbol{y}_n) = t \\
0, & \mathrm{c.c.}
\end{array}\right.
$$


### Exemplo (1 do caso discreto) {#sec-ex-crit-fat}

Seja $\boldsymbol{X}_n=(X_1,\dots,X_n)$ amostra aleatória de $X\sim\mathrm{Ber}(\theta), \theta \in \Theta = (0,1)$. Verifique
se $T(\boldsymbol{X}_n)=\sum^n_{i=1}X_i$ é suficiente para o modelo estatístico.

#### Resposta
Observe que
$$
\begin{aligned}
f_\theta^{\boldsymbol{X}_n}(y_1,\dots,y_n) &= \left\{ \begin{array}{ll}
\prod^n_{i=1}\left\{\theta^{y_i}\cdot(1-\theta)^{1-y_i}\right\}, & y_i \in \{0,1\}, \forall i = 1,\dots,n \\
0, & \mathrm{c.c.}
\end{array}\right. \\
\Rightarrow
f_\theta^{\boldsymbol{X}_n}(y_1,\dots,y_n) &= \theta^{\sum y_i} \cdot (1-\theta)^{n-\sum y_i} \cdot \prod^n_{i=1}\mathbb{1}_{\{0,1\}} (y_1)
\end{aligned}
$$

Tome $h(y_1,\dots,y_n) = \prod^n_{i=1}\mathbb{1}_{\{0,1\}}(y_1)$ e $m(T(y_1,\dots,y_n),\theta) = \theta^{\sum y_i} \cdot (1-\theta)^{n-\sum y_i}$
em que $T(y_1,\dots,y_n) = \sum^n_{i=1}y_i$. Temos que
$$
f_\theta^{\boldsymbol{X}_n}(y_1,\dots,y_n) = h(y_1,\dots,y_n) m(T(y_1,\dots,y_n),\theta),\forall \theta \in \Theta
$$

Pelo critério da fatoração, $T(\boldsymbol{X}_n) = \sum^n_{i=1}X_i$ é suficiente para o modelo de Bernoulli.

### Mais Exemplos

#### Exemplo a

Seja $\boldsymbol{X}_n$ amostra aleatória de $X\sim\mathrm{Beta}(a,b), \theta = (a,b) \in \Theta = \mathbb{R}^2_+$. Encontre
uma estatística suficiente para o modelo.

##### Resposta
A função densidade de probabilidade da amostra aleatória é

$$
f^{\boldsymbol{X}_n}_\theta(y_1,\dots,y_n) \stackrel{\mathrm{a.a.}}{=}
\prod^n_{i=1} f_\theta(y_i) \stackrel{\mathrm{Beta}}{=}
\prod^n_{i=1}
\left\{
\frac{1}{\beta(a,b)}y_{i}^{a-1}\cdot(1-y_i)^{b-1}
\right\}
$$

Em que
$$
\begin{aligned}
\beta(a,b) &= \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)} \\
\Gamma(a) &= \int^\infty_0 x^{a-1}\mathrm{e}^{-x}dx
\end{aligned}
$$

$$
\Rightarrow f_\theta^{\boldsymbol{X}_n}(\boldsymbol{y}_n) = \left\{\begin{array}{ll}
\frac{1}{\beta(a,b)^n}
\left(\prod^n_{i=1}y_1\right)^{a-1} \cdot \left( \prod^n_{i=1} (1-y_1)\right)^{b-1}, & \boldsymbol{y}_n \in (0,1)^n \\
0, & \mathrm{c.c.}
\end{array}\right.
$$

Tome $h(\boldsymbol{y}_n) = \prod^n_{i=1}\mathbb{1}(y_i)_{(0,1)}$ e
$$
m(t,\theta) = \frac{1}{\beta(a,b)^n} \cdot t_1^{a-1} \cdot t_2^{b-1}
$$
em que
$t=(t_1,t_2)$ e $t_1 = \prod^n_{i=1}y_i, t_2 = \prod^n_{i=1}(1-y_i)$. Ou seja,
$$
T(\boldsymbol{X}_n) = \left(\prod^n_{i=1}X_i, \prod^n_{i=1}(1-X_i)\right)
$$
é uma estatística suficiente para o modelo pois
$$
f_\theta^{\boldsymbol{X}_n}(\boldsymbol{y}_n) m(T(\boldsymbol{y}_n),\theta)\ \mathrm{q.c.}\ \forall \theta \in \Theta.
$$

#### Exemplo b
Seja $\boldsymbol{X}_n$ amostra aleatória de $X\sim\mathrm{N}(\mu,\sigma^2), \theta = (\mu,\sigma^2) \in \Theta = \mathbb{R}\times\mathbb{R}_+$. Encontre
uma estatística suficiente para o modelo.

##### Resposta

A função densidade de probabilidade da amostra aleatória é
$$
\begin{aligned}
f_\theta^{\boldsymbol{X}_n} (y_1,\dots,y_n) &\stackrel{\mathrm{a.a.}}{=} \prod_{i=1}^n f_\theta(y_i), \forall \boldsymbol{y}_n \in \mathbb{R}^n \\
&= \prod_{i=1}^n \left\{
\frac{1}{\sqrt{2\pi\sigma^2}} \cdot \mathrm{exp}\left\{-\frac{1}{2\sigma^2}(y_i-\mu)^2\right\}
\right\} \\
&= \frac{1}{(2\pi\sigma^2)^{\frac{n}{2}}} \cdot \mathrm{exp} \left\{-\frac{1}{2\sigma^2}\sum^n_{i=1}(y_1-\mu)^2\right\}
\end{aligned}
$$

Note que $(y_1-\mu)^2 =y_i^2 -2y_i\mu + \mu^2$, logo,

$$
f_\theta^{\boldsymbol{X}_n} = \frac{1}{(2\pi\sigma^2)^{\frac{n}{2}}} \cdot \mathrm{exp} \left\{
-\frac{1}{\sigma^2}\left(
\sum^n_{i=1} y_i^2-2\mu\sum^n_{i=1}y_i + n\mu^2
\right)
\right\}
$$

Tome $h(\boldsymbol{y}_n) = \frac{1}{(2\pi\sigma^2)}$ e
$$
m(t,\theta) = \frac{1}{(\sigma^2)^{\frac{n}{2}}}\cdot \mathrm{exp}\left\{
-\frac{1}{2\sigma^2} \left(t_2-2\mu t_1-\mu^2\right)
\right\}.
$$

Em que $t=(t_1,t_2)$ e $t_1 = \sum^n_{i=1}y_i, t_2 = \sum^n_{i=1}y^2_i$

Ou seja, $T(\boldsymbol{X}_n) = \left(\sum^n_{i=1}X_i,\sum^n_{i=1}X^2\right)$ é uma estatística suficiente para o modelo pois
$$
f_\theta^{\boldsymbol{X}_n}(\boldsymbol{y}_n) \cdot m(T(\boldsymbol{X}_n),\theta)\ \mathrm{q.c.}\ \forall \theta \in \Theta.
$$

#### Exemplo c
Seja $\boldsymbol{X}_n$ amostra aleatória de $X\sim\mathrm{Unif}(0,\theta), \theta = (a,b) \in \Theta = \mathbb{R}_+$. Encontre
uma estatística suficiente para o modelo.

##### Respostas
A função densidade de probabilidade da amostra aleatória

$$
f_\theta^{\boldsymbol{X}_n}(y_1,\dots,y_n) \stackrel{a.a.}{=}
\prod^n_{i=1} f_\theta(y_1) \forall \boldsymbol{y}_n \in \mathbb{R}^n
$$

Note que
$$
f_\theta (x)= \left\{\begin{array}{ll}
\frac{1}{\theta}, & x \in (0,\theta] \\
0, & \mathrm{c.c.}
\end{array}\right. = \frac{1}{\theta}\mathbb{1}(x)_{(0,\theta]}
$$

Logo,
$$
f_\theta^{\boldsymbol{X}_n}(\boldsymbol{y}_n) \stackrel{\mathrm{Unif}}{=}
\frac{1}{\theta} \mathbb{1}_{(0,\theta]}(y_i) \\
= \frac{1}{\theta^n} \cdot \prod^n_{i=1} \mathbb{1}_{(0,\theta]}(y_1)
$$

Note que
$$
\prod^n_{i=1} \mathbb{1}_{(0,\theta]}(y_i)=1 \Leftrightarrow
\left\{\begin{array}{ll}
0 < y_1 \leq \theta \\
\vdots \\
0 < y_n \leq \theta
\end{array}\right. \Leftrightarrow
\left\{\begin{array}{ll}
\min(\boldsymbol{y}_n) > 0 \\
\max(\boldsymbol{y}_n) \leq \theta
\end{array}\right.
$$

Logo
$$
\prod^n_{i=1}\mathbb{1}(y_i)_{(0,\theta]} = 1 \Leftrightarrow
\mathbb{1}_{(0,\infty)}(\min(\boldsymbol{y}_n)) \cdot
\mathbb{1}_{(0,\theta]}(\max(\boldsymbol{y}_n)) = 1
$$

e

$$
f_\theta^{\boldsymbol{X}_n}(\boldsymbol{y}_n) = \frac{1}{\theta^n} 
\mathbb{1}_{(0,\infty)}(\min(\boldsymbol{y}_n)) \cdot
\mathbb{1}_{(0,\theta]}(\max(\boldsymbol{y}_n))
$$

Tome $h(\boldsymbol{y}_n) = \mathbb{1}_{(0,\infty)}(\min(\boldsymbol{y}_n))$ e $m(t,\theta) = \frac{1}{\theta^n} \mathbb{1}_{(0,\theta]}(t)$,
em que $t=\max(\boldsymbol{y}_n)$

Portanto, pelo critério da fatoração, $T(\boldsymbol{X}_n) = \max(X_1,\dots,X_n)$ é uma estatística suficiente para o modelo
em questão.

## Teorema ("Invariância" da estatística suficiente)
Seja $T(\boldsymbol{X}_n)$ uma estatística suficiente para o modelo estatístico. Então
$$
G(\boldsymbol{X}_n) = s(T(\boldsymbol{X}_n))
$$
é uma estatistica suficiente se $s(\cdot)$ for bijetora (só precisa ser injetora)

### Prova

Como $T(\boldsymbol{X}_n)$ é suficiente para o modelo temos, pelo critério da fatoração, que
$$
f_\theta^{\boldsymbol{X}_n}(\boldsymbol{y}_n) = h(\boldsymbol{y}_n) \cdot m(T(\boldsymbol{y}_n),\theta)\ \mathrm{q.c.}\ \forall \theta \in \Theta
$$

Como $s(\cdot)$ é bijetora, temos que sua inversa existe:
$$
T(\boldsymbol{X}_n) = s^{-1}(G(\boldsymbol{X}_n)) \Rightarrow 
T(\boldsymbol{y}_n) = s^{-1}(G(\boldsymbol{y}_n))
$$

Portanto,
$$
f_\theta^{\boldsymbol{X}_n} (\boldsymbol{y}_n)= h(\boldsymbol{y}_n) \cdot m(s^{-1}(G(\boldsymbol{X}_n)), \theta)\ \mathrm{q.c.}\ \forall \theta \in \Theta
$$

Tome $m^*(\cdot,\theta) = m(s^{-1}(\cdot),\theta)$. Substituindo, temos que

$$
f_\theta^{\boldsymbol{X}_n} (\boldsymbol{y}_n)= h(\boldsymbol{y}_n) \cdot m^*((G(\boldsymbol{X}_n)), \theta)\ \mathrm{q.c.}\ \forall \theta \in \Theta
$$

Logo, pelo [critério da fatoração](estatisticas-sufucientes.qmd#sec-crit-fat), $G(\boldsymbol{X}_n)$ é suficiente para o
modelo estatístico.

### Exemplos

a) Se $T(\boldsymbol{X}_n) = \sum^n_{i=1}X_i$ é suficiente para o modelo, então:

1. $G(\boldsymbol{X}_n)=\frac{1}{n}\sum^n_{i=1}X_i$ é suficiente para o modelo;

2. $G(\boldsymbol{X}_n)=\mathrm{e}^{\sum^n_{i=1}X_i}$ é suficiente para o modelo;

3. Para $\sum X_i \neq 0\ \mathrm{q.c.}$, $G(\boldsymbol{X}_n)=\frac{1}{\sum^n_{i=1}X_i}$ é suficiente para o modelo;

4. $G(\boldsymbol{X}_n)=\left(\sum^n_{i=1}X_i\right)^2$ não é necessariamente suficiente para o modelo uma vez que $f(x) = x^2$
não é injetora.

b) Se $T(\boldsymbol{X}_n)$ for suficiente para o modelo estatístico, então

1. $G(\boldsymbol{X}_n) = (T(\boldsymbol{X}_n), T(\boldsymbol{X}_n))$ é suficiente para o modelo;

2. $G(\boldsymbol{X}_n) = (T(\boldsymbol{X}_n), X_1)$ é suficiente para o modelo;

3. $G(\boldsymbol{X}_n) = (T(\boldsymbol{X}_n), \boldsymbol{X}_n)$ é suficiente para o modelo pois $\boldsymbol{X}_n$ é suficiente
para o modelo;

4. $G(\boldsymbol{X}_n) = (X_1\dots,X_n)$ "quase" nunca será suficiente para o modelo;

5. $G(\boldsymbol{X}_n) = (\boldsymbol{X}_n,X_1)$ é suficiente para o modelo.

6. A amostra ordenada $X_{(1)} \leq \dots \leq X_{(n)}$, denotada por $T^*(\boldsymbol{X}_n)=(X_{(1)},\dots,X_{(n)})$ é uma
estatística suficiente para o modelo pelo critério da fatoração, no caso de variáveis aleatórias independentes e identicamente
distribuídas. $T^*(\boldsymbol{X}_n)$ é dita ser a *estatística de ordem*;

7. Se $f_\theta$ pertencer à [família exponencial k-dimensional](familia-exponencial.qmd#sec-fek), então
$T(\boldsymbol{X}_n) = \left(\sum^n_{i=1}T_1(X_i),\dots,\sum^n_{i=1}T_k(X_i)\right)$ é uma estatística suficiente para o modelo.
Prova:
$$
\begin{aligned}
f_\theta^{\boldsymbol{X}_n}(\boldsymbol{y}_n) &\stackrel{\mathrm{iid}}{=}
\prod^n_{i=1} f_\theta(y_i) = \prod^n_{i=1}\left\{
\mathrm{exp}\left\{
\sum_{j=1}^n c_j(\theta)T_j(y_i)+d(\theta)+S(y_i)\cdot\mathbb{1}_{\mathfrak{X}}(y_i)
\right\}
\right\} \\
&= \mathrm{exp}\left\{
\sum^n_{j=1} c_j(\theta) \cdot \sum^n_{i}T_j(y_i) + nd(\theta) + \sum^n_{i=1}S(y_i) \cdot \prod^n_{i=1}\mathbb{1}_{\mathfrak{X}}(y_i)
\right\}
\end{aligned}
$$

Tome $h(\boldsymbol{y}_n) = \prod \mathbb{1}_{\mathfrak{X}}(y_i)\cdot \mathrm{e}^{\sum S(y_i)}$,
$m(t,\theta) = \mathrm{exp}\left\{c_1(\theta)t_1+\dots+c_k(\theta)t_k +nd(\theta)\right\}$, em que $t=(t_1,\dots,t_k)$ e
$$
\begin{aligned}
t_1 &= \sum T_1(y_i) \\
\vdots \\
t_k & = \sum T_k(y_i)
\end{aligned}
$$

Pelo critério da fatoração,
$$
T(\boldsymbol{X}_n) = \left(\sum^n_{i=1} T_1(X_i),\dots,\sum^n_{i=1}T_k(X_i)\right)
$$

## Estatísticas Suficientes Minimais (SM)

Dizemos que $T(\boldsymbol{X}_n)$ é uma estatística suficiente minimal para o modelo se, e somente se:

1. $T(\boldsymbol{X}_n)$ é suficiente para o modelo

2. Para qualquer outra estatística suficiente $U(\boldsymbol{X}_n)$, existe uma função $H$ tal que
$$
T(\boldsymbol{X}_n) = H(U(\boldsymbol{X}_n)),\ \mathrm{q.c.}
$$

Obs: A $\sigma$-álgebra associada à estatística suficiente minimal é a menor $\sigma$-álgebra dentre aquelas associadas
às estatísticas suficientes.

### Teorema (1 das estatísticas SM) {#sec-teo-um}

Seja $\boldsymbol{X}_n = (X_1,\dots,X_n)$ de $X\sim f_\theta, \theta \in \Theta_0=\{\theta_0,\theta_1,\dots,\theta_p\}$ em que
$\mathfrak{X} = \{x:f_\theta(x)>0\}$ não depende de "$\theta$". Então, 
$$
T(\boldsymbol{x}) = \left(
\frac{f_{\theta_1}^{\boldsymbol{X}}(\boldsymbol{x})}{f_{\theta_0}^{\boldsymbol{X}_n}(\boldsymbol{x})}, \dots,
\frac{f_{\theta_p}^{\boldsymbol{X}}(\boldsymbol{x})}{f_{\theta_0}^{\boldsymbol{X}_n}(\boldsymbol{x})}
\right)
$$
em que $T:\mathfrak{X} \rightarrow \mathbb{R}^p$ é uma estatística suficiente minimal ($T(\boldsymbol{X}_n)$) para o modelo estatístico.

Isso trata de *razões entre [funções verossimilhança](funcao-verossimilhanca.qmd)*.

#### Prova

Note que, $\forall \boldsymbol{y}_n \in \mathfrak{X}$, temos que
$$
f_{\theta_j}^{\boldsymbol{X}_n}(\boldsymbol{y}_n) = 
f_{\theta_0}^{\boldsymbol{X}_n}(\boldsymbol{y}_n) \cdot 
\frac{f_{\theta_j}^{\boldsymbol{X}_n}(\boldsymbol{y}_n)}{f_{\theta_0}^{\boldsymbol{X}_n}(\boldsymbol{y}_n)}
$$
Tome $h(\boldsymbol{y}_n) = f_{\theta_0}^{\boldsymbol{X}_n}(\boldsymbol{y}_n)$, não depende dos diferentes valores de "$\theta$" e

$$
m(T(\boldsymbol{X}_n), \theta) = 
\left\{\begin{array}{ll}
T_1(\boldsymbol{y}_n), & \theta = \theta_1 \\
T_2(\boldsymbol{y}_n), & \theta = \theta_2 \\
\vdots \\
T_p(\boldsymbol{y}_n), & \theta = \theta_p \\
\end{array}\right.
$$

Em que $T(\boldsymbol{x}) = (T_1(\boldsymbol{x}), \dots, T_p(\boldsymbol{x})$ e 
$T_j = \frac{f_{\theta_j}^{\boldsymbol{X}_n}(\boldsymbol{y}_n)}{f_{\theta_0}^{\boldsymbol{X}_n}(\boldsymbol{y}_n)}$.

Logo, $T(\boldsymbol{X}_n)$ é suficiente para o modelo pelo [critério da fatoração](estatisticas-suficientes.qmd#sec-crit-fat).

Seja $U(\boldsymbol{X}_n)$ uma estatística suficiente para o modelo. Então pelo critério da fatoração,

$$
f_\theta^{\boldsymbol{X}_n}(\boldsymbol{y}_n) = h'(\boldsymbol{y}_n) \cdot m'(U(\boldsymbol{y}_n),\theta)\ \mathrm{q.c.}\ \forall \theta \in \Theta_0
$$

Observe que $\forall \boldsymbol{y}_n \in \mathfrak{X}$,
$$
\frac{f_{\theta_j}^{\boldsymbol{X}_n}(\boldsymbol{y}_n)}{f_{\theta_0}^{\boldsymbol{X}_n}(\boldsymbol{y}_n)} = 
\frac{h'(\boldsymbol{y}_n)\cdot m'(U(\boldsymbol{y}_n),\theta_j)}{h'(\boldsymbol{y}_n)\cdot m'(U(\boldsymbol{y}_n),\theta_0)}\ \mathrm{q.c.}\ \forall \theta \in \Theta_0
$$

Logo, 

$$
T_j(\boldsymbol{y}_n) = 
\frac{f_{\theta_j}^{\boldsymbol{X}_n}(\boldsymbol{y}_n)}{f_{\theta_0}^{\boldsymbol{X}_n}(\boldsymbol{y}_n)} = 
\frac{m'(U(\boldsymbol{y}_n),\theta_j)}{m'(U(\boldsymbol{y}_n),\theta_0)},\ j=1,\dots,p
$$

Portanto, existe $H$ tal que
$$
T_j(\boldsymbol{X}_n) = H(U(\boldsymbol{X}_n))
$$
basta tomar
$$
H(u) = \left(
\frac{m'(u,\theta_1)}{m'(u,\theta_0)}, \dots,
\frac{m'(u\theta_p)}{m'(u,\theta_0)}
\right)
$$

#### Exemplo (Bernoulli) {#sec-ex-teo-um}

Seja $\boldsymbol{X}_n = (X_1,\dots,X_n)$ [amostra aleatória](populacao-e-amostra.qmd#sec-aa) de
$X\sim\mathrm{Ber}(\theta), \theta \in \Theta = \{0.1,0,5\}$. Encontre uma estatística suficiente minimal.

##### Resposta

Pelo teorema anterior,
$$
T(\boldsymbol{y}_n) = \frac{f_{\theta_1}^{\boldsymbol{X}_n}(\boldsymbol{y}_n)}{f_{\theta_0}^{\boldsymbol{X}_n}(\boldsymbol{y}_n)} 
$$
é suficiente minimal, em que $\theta_0=0.1,\theta_1=0.5$ e

$$
\begin{aligned}
f_{0.1}^{\boldsymbol{X}_n} (\boldsymbol{y}_n) &= 0.1^{\sum y_i} \cdot 0.9^{n-\sum y_i} \\
f_{0.5}^{\boldsymbol{X}_n} (\boldsymbol{y}_n) &= 0.5^{\sum y_i} \cdot 0.5^{n-\sum y_i} \\
&\forall \boldsymbol{y}_n \in \mathfrak{X} = \{0,1\}^n
\end{aligned}
$$
Logo 
$$
\begin{aligned}
T(\boldsymbol{y}_n) &= \frac{0.5^n}{0.1^{\sum y_i} \cdot 0.9^{n-\sum y_i}} \\
&= \frac{0.5^n}{\left(\frac{0.1}{0.9}\right)^{\sum y_i} \cdot 0.9^{\sum y_i}} \\
&= 9^{\sum y_i} \cdot \left(\frac{5}{9}\right)^n
\end{aligned}
$$

Note que $T(\boldsymbol{y}_n)$ é função 1:1 de $T'(\boldsymbol{y}_n) = \sum^n_{i=1} y_i$. Logo,
$$
T'(\boldsymbol{X}_n) = \sum^n_{i=1} X_i
$$
é também uma estatística suficiente minimal para o modelo.

#### Exemplo (Normal) {#sec-ex-norm-teo-um}

Seja $\boldsymbol{X}_n = (X_1,\dots,X_n)$ [amostra aleatória](populacao-e-amostra.qmd#sec-aa) de
$X\sim\mathrm{N}(\mu,\sigma^2), \theta = (\mu,\sigma^2) \in \Theta = \{(0,1),(1,1),(0,2)\}$. 
Encontre uma estatística suficiente minimal.

##### Resposta

Pelo teorema,
$$
T(\boldsymbol{y}_n) = \left(
\frac{f_{\theta_1}^{\boldsymbol{X}_n}(\boldsymbol{y}_n)}{f_{\theta_0}^{\boldsymbol{X}_n}(\boldsymbol{y}_n)},
\frac{f_{\theta_2}^{\boldsymbol{X}_n}(\boldsymbol{y}_n)}{f_{\theta_0}^{\boldsymbol{X}_n}(\boldsymbol{y}_n)} 
\right)
$$

Tomando $\theta_0 = (0,1), (1,1), (0,2)$, temos

$$
\begin{aligned}
f_{\theta_0}^{\boldsymbol{X}_n}(\boldsymbol{y}_n) &=
\frac{\mathrm{exp}\left\{-\frac{1}{2}\sum y^2_i\right\}}{(\sqrt{2\pi})^n} \\
f_{\theta_1}^{\boldsymbol{X}_n}(\boldsymbol{y}_n) &=
\frac{\mathrm{exp}\left\{-\frac{1}{2}(\sum y^2_i - 2\sum y_i + n)\right\}}{(\sqrt{2\pi})^n} \\
f_{\theta_2}^{\boldsymbol{X}_n}(\boldsymbol{y}_n) &=
\frac{\mathrm{exp}\left\{-\frac{1}{4}\sum y^2_i \right\}}{(\sqrt{4\pi})^n} \\
\end{aligned}
$$

Portanto,
$$
\begin{aligned}
T(\boldsymbol{y}_n) &= \left(
\mathrm{exp}\left\{-\frac{1}{2}(n-2\sum y_i)\right\}, 
\frac{\mathrm{exp}\left\{-\frac{1}{4}\sum y_i^2 + \frac{1}{2}\sum y_i^2)\right\}}{2^{\frac{n}{2}}}, 
\right)
\\
&= \left(
\mathrm{exp}\left\{-\frac{1}{2}(n-2\sum y_i)\right\}, 
\frac{\mathrm{exp}\left\{\frac{1}{4}\sum y_i^2)\right\}}{2^{\frac{n}{2}}}, 
\right)
\end{aligned}
$$
dessa forma, $T(\boldsymbol{X}_n)$ é SM para o modelo.

Note que $T(\boldsymbol{y}_n)$ é função 1:1 de $(\sum y_i, \sum y_i^2)$. Portanto,
$$
T'(\boldsymbol{X}_n) = \left(
\sum^n_{i=1} X_i, \sum^n_{i=1} X_i^2
\right)
$$
é também SM para o modelo.

### Teorema (2 das estatísticas SM)

Seja $\boldsymbol{X}_n=(X_1,\dots,X_n)$ [amostra aleatória](populacao-e-amostra.qmd#sec-aa) de $X\sim f_\theta, \theta \in \Theta$.
Considere $\Theta_0 \subseteq \Theta$ não vazio. Então, se $T(\boldsymbol{X}_n)$ for SM para o modelo reduzido a $\Theta_0$ e
suficiente para $\Theta$, então será suficiente minimal para $\Theta$.

#### Prova
Como $T(\boldsymbol{X}_n)$ é SM para o modelo restrito a $\Theta_0 \subseteq \Theta$, para qualquer estatística $U(\boldsymbol{X}_n)$
suficiente para o modelo restrito a $\Theta_0$, existe $H$ tal que

$$
T(\boldsymbol{X}_n) = H(U(\boldsymbol{X}_n))\ \mathrm{q.c.}
$$

Observe que todas as estatísticas suficientes *para o modelo completo* $\Theta$ são também suficientes *para os modelos
restritos*. Como $T(\boldsymbol{X}_n)$ também é, por hipótese, suficiente para o modelo completo, então é função de qualquer
estatística suficiente minimal para o modelo completo.

#### Exemplo (Bernoulli)

Seja $\boldsymbol{X}_n = (X_1,\dots,X_n)$ [amostra aleatória](populacao-e-amostra.qmd#sec-aa) de
$X\sim\mathrm{Ber}(\theta), \theta \in \Theta = (0,1)$. Mostre que $T(\boldsymbol{X}_n) = \sum X_i$
é uma estatística suficiente minimal para o modelo.

##### Resposta

Tome $\Theta_0 = \{0.1,0.5\} \subseteq \Theta$. [Já mostramos](estatisticas-suficientes.qmd#sec-ex-teo-um) que
$T'(\boldsymbol{X}_n) = \sum^n_{i=1} X_i$ é suficiente minimal para o modelo reduzido a $\Theta_0$. Além disso,
$T'(\boldsymbol{X}_n) = \sum X_i$ [é suficiente](estatisticas-suficientes.qmd#sec-ex-crit-fat) para o modelo completo. Logo,
pelo Teorema 2 para estatísticas suficientes minimais, concluímos que $T'(\boldsymbol{X}_n) = \sum X_i$ é também SM para o modelo
completo.

#### Exemplo (Normal)

Seja $\boldsymbol{X}_n = (X_1,\dots,X_n)$ amostra aleatória de
$X\sim\mathrm{N}(\mu,\sigma^2), \theta = (\mu,\sigma^2) \in \Theta = \mathbb{R}\times\mathbb{R}^+$. Mostre que 
$T(\boldsymbol{X}_n) = (\sum X_i, \sum X_i^2)$ é suficiente minimal para o modelo.

##### Resposta correta

Tome $\Theta_0 = \{(0,1),(1,1),(0,2)\}$. [Já mostramos](estatisticas-suficientes.qmd#sec-ex-norm-teo-um) que essa
estatística é suficiente minimal para o modelo reduzido a $\Theta_0$. Além disso, já mostramos (lista) que é suficiente
para o modelo completo. Logo, pelo teorema 2, é SM para o modelo completo.

##### Tentativa alternativa (frustrada)

Tome $\Theta_0 = \{(0,1),(1,1)\}$. Pelo [teorema 1](estatisticas-suficientes.qmd#sec-teo-um), temos que
$$
T(\boldsymbol{X}_n) = \frac{f_{\theta_1}^{\boldsymbol{X}_n}(\boldsymbol{X}_n)}{f_{\theta_0}^{\boldsymbol{X}_n}(\boldsymbol{X}_n)}
$$
é SM para o modelo reduzido.

$$
T(\boldsymbol{X}_n) = \frac{\frac{1}{(2\pi)^{\frac{n}{2}}}\mathrm{exp}\left\{-\frac{1}{2} (\sum X_i^2 -2\sum X_i + n)\right\}}
{\frac{1}{(2\pi)^{\frac{n}{2}}}\mathrm{exp}\left\{-\frac{1}{2} \sum X_i^2\right\}} = \mathrm{e}^{\sum X_i -\frac{1}{2} n}
$$

Como $\sum X_i$ é uma função 1:1 da estatística, temos que $\sum X_i$ é também SM para o modelo reduzido. Entretanto,
$\sum X_i$ não é suficiente para o modelo completo.

##### Tentativa 2

Tome $\Theta_0 = \{(0,1),(1,1), (2,1)\}$. Então, pelo [teorema 1](estatisticas-suficientes.qmd#sec-teo-um), temos que

$$
T(\boldsymbol{X}_n) = \left(\frac{f_{\theta_1}^{\boldsymbol{X}_n}(\boldsymbol{X}_n)}{f_{\theta_0}^{\boldsymbol{X}_n}(\boldsymbol{X}_n)}, 
\frac{f_{\theta_2}^{\boldsymbol{X}_n}(\boldsymbol{X}_n)}{f_{\theta_0}^{\boldsymbol{X}_n}(\boldsymbol{X}_n)}\right) \stackrel{\text{Tent. Ant.}}{=}
\left(
\mathrm{e}^{\sum X_i - \frac{1}{2}n}, 
\frac{f_{\theta_2}^{\boldsymbol{X}_n}(\boldsymbol{X}_n)}{f_{\theta_0}^{\boldsymbol{X}_n}(\boldsymbol{X}_n)}\right) 
$$

Temos que

$$
\frac{f_{\theta_2}^{\boldsymbol{X}_n}(\boldsymbol{X}_n)}{f_{\theta_0}^{\boldsymbol{X}_n}(\boldsymbol{X}_n)} =
\frac{\frac{1}{(2\pi)^{\frac{n}{2}}}\mathrm{exp}\left\{-\frac{1}{2} (\sum X_i^2 -4\sum X_i + 4n)\right\}}
{\frac{1}{(2\pi)^{\frac{n}{2}}}\mathrm{exp}\left\{-\frac{1}{2} \sum X_i^2\right\}} = \mathrm{e}^{2\sum X_i -2 n}
$$
Logo, 
$$
T(\boldsymbol{X}_n) = 
\left(
\mathrm{e}^{\sum X_i - \frac{1}{2}n},
\mathrm{e}^{2\sum X_i -2 n}\right)
$$

Como $\sum X_i$ é função 1:1 de $T(\boldsymbol{X}_n)$, temos que é SM para o modelo reduzido a $\Theta_0$. Entretanto,
não é suficiente para o modelo completo.

Observe que o espaço paramétrico do modelo completo é $\mathbb{R}\times\mathbb{R}^+$

#### Exemplo (Uniforme)

Seja $\boldsymbol{X}_n=(X_1,\dots,X_n)$ [amostra aleatória](populacao-e-amostra.qmd#sec-aa) de 
$X\sim U(0,\theta), \theta \in \Theta = (0,\infty)$. Verifique se $\boldsymbol{X}_{(n)} = \max\{X_1,\dots,X_n\}$ é SM
para o modelo.

##### Resposta

Já sabemos que é uma estatística suficiente para o modelo. Com o suporte $\mathfrak{X}_\theta = (0,\theta]$ depende de
"$\theta$", não podemos usar o teorema anterior. Precisamos mostras que, para qualquer estatística suficiente para
o modelo $U(\boldsymbol{X}_n)$ existe $H(\cdot)$ tal que

$$
X_{(n)} = H(U(\boldsymbol{X}_n))\ \mathrm{q.c.}
$$

Seja $f_\theta^{\boldsymbol{X}_n}$ a função densidade probabilidade da amostra aleatória
$$
\begin{aligned}
f_\theta^{\boldsymbol{X}_n}(\boldsymbol{y}_n) &= \frac{1}{\theta^n} \prod \mathbb{1}_{(0,\theta]}(y_i)  \\
&= \frac{1}{\theta^n} \mathbb{1}_{(0,\infty)} (\min\{y_1,\dots,y_n\}) \cdot \mathbb{1}_{(0,\theta]}(\max\{y_1,\dots,y_n\})
\end{aligned}
$$

<!-- TODO: add gráfico da foto -->

Note que podemos escrever
$$
\max\{y_1,\dots,y_n\} = \inf\left\{ \theta \in (0,\infty) : f_\theta^{\boldsymbol{X}_n}(\boldsymbol{y}_n) > 0\right\}
$$

Logo, a estatística $X_{(n)}$ pode ser reescrita por
$$
X_{(n)} = \inf\left\{\theta \in (0,\infty) : f^{\boldsymbol{X}_n}_\theta (\boldsymbol{X}_n) > 0\right\}
$$

Seja $U(\boldsymbol{X}_n)$ uma estatística suficiente qualquer. Então, pelo [CF](estatisticas-suficientes.qmd#sec-crit-fat),
temos que existem $h'$ e $m'$ tais que
$$
f_\theta{\boldsymbol{y}_n}^{\boldsymbol{X}_n} = h'(\boldsymbol{y}_n) \cdot m'(U(\boldsymbol{y}_n), \theta),\ \mathrm{q.c.}\ \forall \theta >0
$$

Logo,
$$
X_{(n)} = \inf\left\{\theta \in (0,\infty) : h'(\boldsymbol{X}_n) \cdot m'(U(\boldsymbol{X}_n),\theta) > 0\right\} \mathrm{q.c.}
$$

Como $h'(\boldsymbol{y}_n)> 0\ \mathrm{q.c.}$ temos que

$$
X_{(n)} = \inf\left\{\theta \in (0,\infty) : m'(U(\boldsymbol{X}_n),\theta)>0 \right\} \mathrm{q.c.}
$$

Portanto, $X_{(n)}$ é suficiente minimal para o modelo, pois existe $H(\cdot)$ tal que
$$
X_{(n)} = H(U(\boldsymbol{X}_n))\ \mathrm{q.c.}
$$

Basta tomar
$$
H(u) = \inf\left\{
\theta \in \Theta : m'(u,\theta)> 0
\right\}
$$

#### Observação

$$
f_\theta(x) = \mathrm{e}^{-\theta} \cdot \mathbb{1}_{(0,\theta)}(x)
$$

Defina $g:\mathbb{R}^+\rightarrow\mathbb{R}$
$$
\begin{aligned}
g(u) &= \inf\left\{\theta \in (0,\infty) : f_\theta(u) > 0\right\}\\
&= \inf\left\{\theta \in (0,\infty) : \mathrm{e}^{-\theta}\cdot \mathbb{1}_{(0,\theta]}(u) > 0\right\}\\
\Rightarrow g(u) &= u
\end{aligned}
$$

#### Exemplo (Normal Curvada)

Seja $\boldsymbol{X}_n = (X_1,\dots,X_n)$ amostra aleatória de $X\sim\mathrm{N}(\theta,\theta^2), \theta \in \Theta = \mathbb{R}^+$.
Encontre uma estatística SM para o modelo.

##### Resposta

Como $(\sum X_i, \sum X_i^2)$ é suficiente para o modelo completo, é suficiente para o modelo restrito a $\Theta = \mathbb{R}^+$
