# Estatísticas Suficientes

Seja $\mathbb{X}_n(x_1,\dots,x_n)$ uma [amostra aleatória](populacao-e-amostra.qmd#sec-aa) de $X\sim f_\theta,
\theta \in \Theta$. Dizemos que uma [estatística](estatisticas.qmd) $T(\mathbb{X}_n$ é suficinete para o 
[modelo estatístico](modelo-estatistico.qmd) se, e somente se, a distribuição da amosta dado que $T(\mathbb{X}_n = t$
não depende de "$\theta$". Ou seja,
$$
\begin{aligned}
P_\theta(X_1\leq y_1,\dots,X_n\leq y_n \lvert T(\mathbb{X}_n)=t)\
\text{Não depende de}\ \theta, \forall y_1,\dots,y_n \in \mathbb{R} \\
\text{E para todo valor de}\ t \ \text{para o quais a distribuição de}\ T(\mathbb{X}_n)\ \text{exista}
\end{aligned}
$$

Em outras palavras, a informação probabilística sobre "$\theta$" da amostra aleatória está inteiramente contida no modelo
induzido pela estatística.

No caso discreto, basta mostrar que $P_\theta(X_1=y_1,\dots,X_n=y_n\lvert T(\mathbb{X}_n)=t)$ não depende de "$\theta$"
para todo $y_1,\dots,y_n \in \mathbb{R}$ e valores de $t$ para os quais a distribuição de $T(\mathbb{X}_n)$ exista.

No caso contínuo, basta mostrar que $f_\theta^{(n)}(y_1,\dots,y_n\lvert t)$ não depende de $\theta$ para todo
$y_1,\dots,y_n \in \mathbb{R}$ e valores de t para os quais a função densidade de probabilidade de $T(\mathbb{X}_n)$ exista.

## Caso discreto

### Exemplos

#### Exemplo 1

Seja $\mathbb{X}_n = (X_1, \dots, X_n)$ uma a.a. de $X\sim\mathrm{Ber}(\theta), \theta \in \Theta = (0,1)$.
Verifique se $T(\mathbb{X}_n=\sum^n_{i=1}X_i$ é suficiente para o modelo estatístico.

##### Resposta

Por definição,
$$
P_\theta(X_1=y_1,\dots,X_n=y_n\lvert T(\mathbb{X}_n) = t) = \frac{P_\theta(X_1=y_1,\dots,X_n=y_n,T(\mathbb{X}_n)=t)}
{P_\theta(T(\mathbb{X}_n)=t}
$$

Sabemos que $T(\mathbb{X}_n)=\sum^n_{i=1}X_i \sim \mathrm{Bin}(n,\theta)$ (por função geradora de momentos). Portanto,
$$
P_\theta(T(\mathbb{X}_n=t) = \left\{ \begin{array}{ll}
\binom{n}{t} \cdot \theta^t\cdot(1-\theta)^{n-t},&\text{Se}\ t \in \{0,1,\dots,n\} \\
0, & \mathrm{c.c.}
\end{array} \right.
$$

Logo, $P_\theta(X_1=y_1,\dots,X_n=y_n\lvert T(\mathbb{X}_n) = t)$ só está bem definida se $t \in \{0,1,\dots,n\}$

(Numerador)
$$
\begin{aligned}
&P_\theta(X_1=y_1,\dots,X_n=y_n,T(\mathbb{X}_n)=t) \\
&\stackrel{\mathrm{TPT}}{=}
\overbracket{P_\theta(X_1=y_1,\dots,X_n=y_n)}^{A} \cdot \overbracket{P_\theta(T(\mathbb{X}_n)=t\lvert X_1=y_1,\dots,X_n=y_n)}^B
\end{aligned}
$$

Observe que $A$ é a função probabilidade da amostra e
$$
B = \left \{ \begin{array}{ll}
1, & \text{Se}\ t = \sum^n_{i=1} y_i \\
0, & \mathrm{c.c}
\end{array}\right.
$$

Para $t \in \{0,1,\dots,n\}$

$$
P_\theta(X_1=y_1,\dots,X_n=y_n\lvert T(\mathbb{X}_n) = t) =
\left \{ \begin{array}{ll}
\frac{\prod^n_{i=1}\theta^{y_i}(1-\theta)^{1-y_i}}{\binom{n}{t}\cdot\theta^t\cdot(1-\theta)^{n-t}}, & \text{Se}\ t = \sum^n_{i=1}y_i\\
0, & \mathrm{c.c}
\end{array}\right. \forall \theta \in \Theta
$$

Portanto,
$$
P_\theta(X_1=y_1,\dots,X_n=y_n\lvert T(\mathbb{X}_n) = t) =
\left \{ \begin{array}{ll}
\frac{\theta^{\sum^n_{i=1}y_i}(1-\theta)^{\sum^n_{i=1}1-y_i}}{\binom{n}{t}\cdot\theta^t\cdot(1-\theta)^{n-t}}, & \text{Se}\ t = \sum^n_{i=1}y_i \\
0, & \mathrm{c.c}
\end{array}\right. \forall \theta \in \Theta
$$

Concluímos que
$$
\begin{aligned}
P_\theta(X_1=y_1,\dots,X_n=y_n\lvert T(\mathbb{X}_n) = t) =
\left \{ \begin{array}{ll}
\frac{\prod_{i=1}^n \mathbb{1}_{\{0,1\}}(y_i)}{\binom{n}{t}}, & \text{Se}\ t = \sum^n_{i=1}y_i \\
0, & \mathrm{c.c}
\end{array}\right.\\
&\forall \theta \in \Theta, t \in \{0,1,\dots,n\}, \forall y_1,\dots,y_n \in \mathbb{R}
\end{aligned}
$$

A estatística $T(\mathbb{X}_n)$ é suficiente para o modelo estatístico Bernoulli

#### Exemplo 2

Seja $\mathbb{X}_n=(X_1,\dots,X_n)$ uma amostra aleatória de $X\sim\mathrm{Pois}(\theta), \theta \in \Theta = (0,\infty)$.
Verifique se $T(\mathbb{X}_n)= \frac{1}{n}\sum^n_{i=1}X_i$ é uma estatística suficiente para o modelo estatístico.

##### Resposta

$$
P_\theta(X_1=y_1,\dots,X_n=y_n\lvert T(\mathbb{X}_n) = t) =
\frac{P_\theta(X_1=y_1,\dots,X_n=y_n,T(\mathbb{X}_n) = t)}
{P_\theta(T(\mathbb{X}_n)=t}
$$

(Numerador)
$$
\begin{aligned}
&P_\theta(X_1=y_1,\dots,X_n=y_n,T(\mathbb{X}_n)=t) \\
&\stackrel{\mathrm{TPT}}{=}
\overbracket{P_\theta(X_1=y_1,\dots,X_n=y_n)}^{A} \cdot \overbracket{P_\theta(T(\mathbb{X}_n)=t\lvert X_1=y_1,\dots,X_n=y_n)}^B
\end{aligned}
$$

Observe que $A$ é a função probabilidade da amostra e
$$
B = \left \{ \begin{array}{ll}
1, & \text{Se}\ t =\frac{1}{n} \sum^n_{i=1} y_i \\
0, & \mathrm{c.c}
\end{array}\right.
$$

Já sabemos que $\sum^n_{i=1}X_i\sim \mathrm{Pois}(n\cdot\theta)$ (por função geradora de momentos)

$$
P_\theta\left(\frac{1}{n}\sum^n_{i=1}X_i=\frac{k}{n}\right) = \left\{ \begin{array}{ll}
\mathrm{e}^{-n\theta}\cdot \frac{(n\theta)^k}{k!}, & k \in \{0,1,2,\dots\} \\
0, & \mathrm{c.c.}
\end{array}\right.
$$

Tome $t = \frac{k}{n}$, então $k=nt$
$$
P_\theta\left(\sum^n_{i=1}X_i=k\right) = \left\{ \begin{array}{ll}
\mathrm{e}^{-n\theta}\cdot \frac{(n\theta)^{nt}}{(nt)!}, & k \in \{0,\frac{1}{n},\frac{2}{n},\dots\} \\
0, & \mathrm{c.c.}
\end{array}\right.
$$

Portanto, para $t \in \{0, \frac{1}{n}, \frac{2}{n},\dots\}$, temos que
$$
\begin{aligned}
P_\theta(X_1=y_1,\dots,X_n=y_n\lvert T(\mathbb{X}_n)=t) &= \left\{ \begin{array}{ll}
\frac{\prod^n_{i=1}\left\{\mathrm{e}^{-\theta}\cdot\frac{\theta^{y_i}}{y_i!}\cdot \mathbb{1}_{\{0,1,\dots\}}(y_i)\right\}} 
{\mathrm{e}^{-n\theta}\cdot \frac{(n\theta)^{nt}}{(nt)!}}, & t=\frac{1}{n} \sum^n_{i=1} y_i \\
0, & \mathrm{c.c.}
\end{array}\right. \\
&= \left\{\begin{array}{ll}
\frac{\mathrm{e}^{-n\theta}\cdot\theta^{\sum^n_{i=1}y_i}\cdot \prod^n_{i=1}\mathbb{1}_{\{0,1,\dots\}}(y_i)(nt!)} 
{\prod^n_{i=1}(y_i!)\cdot\mathrm{e}^{-n\theta}\cdot (n\theta)^{nt}}, & t=\frac{1}{n} \sum^n_{i=1} y_i \\
0, & \mathrm{c.c.}
\end{array}\right. \\
&= \left\{\begin{array}{ll}
\frac{\prod^n_{i=1}\mathbb{1}_{\{0,1,\dots\}}(y_i)(nt!)} 
{\prod^n_{i=1}(y_i!)(n)^{nt}}, & t=\frac{1}{n} \sum^n_{i=1} y_i \\
0, & \mathrm{c.c.}
\end{array}\right. \\
\forall \theta \in \Theta
\end{aligned} 
$$

Não depende de "$\theta$" para todo $y_1,\dots,y_n \in \mathbb{R}$ e $t \in \{0,\frac{1}{n},\frac{2}{n},\dots\}$

## Caso Contínuo

### Exemplo (Normal)
Seja $\mathbb{X}_n = (X_1,\dots,X_n)$ a.a. de $X\sim N(\theta,1), \theta \in \mathbb{R}$. Verifique se
$T(\mathbb{X}_n) = \sum^n_{i=1}X_i$ é suficiente para o modelo estatístico.

#### Resposta
Por definição
$$
f_\theta^{\mathbb{X}_n\lvert T(\mathbb{X}_n) = t}(y_1,\dots,y_n) = \frac{f_\theta^{\mathbb{X}_n, T(\mathbb{X}_n}(y_1,\dots,y_n,t}
{f_\theta^{T(\mathbb{X}_n}(t)}
$$

(Denominador)
Já sabemos que $T(\mathbb{X}_n) = \sum^n_{i=1} X_i \sim N(n\theta, n)$
$$
\Rightarrow f_\theta^{T(\mathbb{X}_n)}(t) = \frac{1}{\sqrt{2\pi n}} \cdot \mathrm{e}^{-\frac{1}{2n}\cdot (t-n\theta)^2}, t \in \mathbb{R}
$$
(Numerador)
$$
f_\theta^{\mathbb{X}_n,T(\mathbb{X}_n)}(y_1,\dots,y_n,t) = 
f_\theta^{\mathbb{X}_n}(y_1,\dots,y_n) \cdot f_\theta^{T(\mathbb{X}_n\lvert \mathbb{X}_n=(y_1,\dots,y_n}(t)
$$

Note que
$$
\begin{aligned}
&f_\theta^{T(\mathbb{X}_n\lvert \mathbb{X}_n = (y_1,\dots,y_n} = \left\{ \begin{array}{ll}
1, & t = \sum^n_{i=1} y_1 \\
0, & \mathrm{c.c.}
\end{array}\right. \\
\Rightarrow&
f_\theta^{\mathbb{X}_n\lvert T(\mathbb{X}_n) = t}(y_1,\dots,y_n) = \left\{ \begin{array}{ll}
\frac{f_\theta^{\mathbb{X}_n}(y_1,\dots,y_n}
{f_\theta^{T(\mathbb{X}_n)}(t)},& t = \sum^n_{i=1}y_i \\
0, & \mathrm{c.c.}
\end{array} \right.
\end{aligned}
$$

Logo
$$
f_\theta^{T(\mathbb{X}_n\lvert \mathbb{X}_n = (y_1,\dots,y_n} = \left\{ \begin{array}{ll}
\frac{\frac{1}{\sqrt{2\pi\cdot1}^n} \cdot \mathrm{exp}\left\{-\frac{1}{2} \sum^n_i=1 (y_i - \theta)^2\right\}}
{\frac{1}{\sqrt{2\pi\cdot n}}\cdot \mathrm{exp}\left\{-\frac{1}{2n}(t-n\theta)^2\right\}}, & t = \sum y_i \\
0, & \mathrm{c.c.}
\end{array}\right.
$$

Note que $-\frac{1}{2}\sum(y_i-\theta)^2=-\frac{1}{2}\left(\frac{t^2}{n}-2t\theta+n\theta^2\right)$
Logo $f_\theta^{\mathbb{X}_n\lvert T(\mathbb{X}_n)=t}$ não depende de $\theta$ e $\sum X_i$ é suficiente para o modelo
estatístico.

### Problema das funções densidade de probabilidade

*A função densidade de probabilidade não é única.* Entretanto, é única para quase todo ponto (quase certamente).

Por exemplo, $X\sim\mathrm{Exp}(\theta), \theta \in (0,\infty)$
$$
\begin{aligned}
f_\theta(x) &= \left\{\begin{array}{ll}
\theta \cdot \mathrm{e}^{-\theta x},& x \in (0, \infty) \\
0, & x \not \in (0,\infty)
\end{array}\right.\ \ \theta \in \Theta \\
P_\theta(X>2)&=\int^\infty_2 \theta \cdot \mathrm{e}^{-\theta x} dx \\ 
&\text{Se $A$ é enumerável e defina} \\
f_\theta(x)^A &= \left\{\begin{array}{ll}
\theta \cdot \mathrm{e}^{-\theta x},& x \in (0, \infty) \setminus A \\
10, & x \in \mathrm{A} \\
0, & x \not \in (0,\infty)
\end{array}\right. \ \ \theta \in \Theta \\
\end{aligned}
$$

Temos que
$$
\begin{aligned}
f_\theta(x) &= f_\theta^A(x), \forall x \in \mathbb{R} \setminus A, \forall \theta \in \Theta \\
\text{e} \\
f_\theta(x) &\neq f_\theta^A(x), \forall x \in A, \forall \theta \in \Theta
\end{aligned}
$$


Note que $f_\theta$ e $f_\theta^A$ são diferentes, mas produzem as mesmas probabilidades. Dizemos portanto que $f_\theta$
e $f_\theta^A$ são iguais *quase certamente*, ou seja,
$$
P_\theta\left(f_\theta(x)=f_\theta^A(x)\right) = 1, \forall \theta \in \Theta
$$
Ou, de outra forma,
$$
P_\theta\left(f_\theta(x)\neq f_\theta^A(x)\right) = 0, \forall \theta \in \Theta
$$

Notação:
$$
f_\theta(x) = f_\theta^A(x)\ \mathrm{q.c.}\ \forall \theta \in \Theta
$$

No caso contínuo, portanto, a estatística $T(\mathbb{X}_n)$ será suficiente mesmo se
$f_\theta^{\mathbb{X}_n\lvert T(\mathbb{X}_n)=t}(y_1,\dots,y_n)$
depende de "$\theta$" para $(y_1,\dots,y_n) \in A$, **DESDE QUE** $P_\theta(X \in A) = 0, \forall \theta \in \Theta$. Ou seja,
pode depender de "$\theta$" em um conjunto com probabilidade zero.

## Critério da Fatoração de Neyman-Fisher (Caso Simples)

Seja $\mathbb{X}_n=(x_1,\dots,x_n)$ uma [amostra aleatória](populacao-e-amostra.qmd#sec-aa) de $X \sim f_\theta, \theta \in \Theta \subseteq \mathbb{R}^p$
(sendo as $f_\theta, \theta \in \Theta$ do mesmo "tipo", formalmente, dominadas pela mesma medida), em que $p \in \{1,2,\dots\}$.
Uma estatística $T(\mathbb{X}_n)$ é suficiente para o modelo estatístico se, e somente se, existirem funções
$h(\cdot): \mathbb{R}^n\rightarrow \mathbb{R}, m(\cdot,\cdot): \mathrm{Im}(T)\times\theta\rightarrow\mathbb{R}$
(mensuráveis) tais que:
$$
f_\theta^{\mathbb{X}_n}(y_1,\dots,y_n) = h(y_1,\dots,y_n)\cdot m\left(T(y_1,\dots,y_n),\theta\right), \forall \theta \in \Theta\; \mathrm{q.c.}
$$

Obs:

1. $h$ não depende de "$\theta$";

2. $m$ depende de valores amostrais por meio da estatística $T(\mathbb{X}_n)$;

3. $f_\theta^{\mathbb{X}_n} = f_\theta^{(n)}$ é a função (densidade) de probabilidade da amostra aleatória.

4. Note que a [função de verossimilhança](funcao-verossimilhanca.qmd) é obtida calculando $f_\theta^{\mathbb{X}_n}$ na
amostra observada, ou seja,
$$
L_{\mathbb{X}_n}(\theta) = f_\theta^{\mathbb{X}_n}\underbracket{(x_1,\dots,x_n)}^{\mathbb{X}_n}
$$

5. Alguns livros usam a função de verossimilhança no critério da fatoração
$$
L_{\mathbb{X}_n}(\theta) =  h(\mathbb{X}_n) \cdot m(T(\mathbb{X}_n),\theta)\; \mathrm{q.c.} \forall \theta \in \Theta
$$
em que $\mathbb{X}_n = (x_1,\dots,x_n)$ da [amostra observada](populacao-e-amostra.qmd#sec-ao)

### Prova (caso discreto)

#### $\Rightarrow$

Assuma que $T(\mathbb{X}_n)$ seja suficiente. Por definição, $P_\theta(X_1=y_1,\dots,X_n=y_n\lvert T(\mathbb{X}_n) = t)$
não depende de "$\theta$". Logo, podemos escrever:
$$
P_\theta(X_1=y_1,\dots,X_n=y_n\lvert T(\mathbb{X}_n)=t) = h^*(y_1,\dots,y_n,t) \forall \theta \in \Theta
$$ {#eq-podemos}
Note também que,
$$
P_\theta(X_1=y_1,\dots,X_n=y_n\lvert T(\mathbb{X}_n)=t) =
\frac{P_\theta(X_1=y_1,\dots,X_n=y_n, T(\mathbb{X}_n)=t)}
{P_\theta(T(\mathbb{X}_n)=t)}
$$
para valores de $t$ em que a probabilidade condicional exista.

$$
\begin{aligned}
&P_\theta(X_1=y_1,\dots,X_n=y_n, T(\mathbb{X}_n)=t) \\
&=
P_\theta(X_1=y_1,\dots,X_n=y_n)\cdot P_\theta(T(\mathbb{X}_n)=t\lvert X_1=y_1,\dots,X_n=y_n)
\end{aligned}
$$

Como, com $\mathbb{y}_n = y_1,\dots,y_n$,
$$
P_\theta(T(\mathbb{X}_n)=t\lvert X_1=y_1,\dots,X_n=y_n) = \left\{\begin{array}{ll}
1, & T(\mathbb{y}_n) = t \\
0, & \mathrm{cc}
\end{array}\right.
$$
então
$$
P_\theta(X_1=y_1,\dots,X_n=y_n\lvert T(\mathbb{X}_n)=t) =
\frac{P_\theta(X_1=y_1,\dots,X_n=y_n)\cdot \mathbb{1}(T(\mathbb{y}_n)=t)}
{P_\theta(T(\mathbb{X}_n) =t)}
$${#eq-aberto}

Por ([-@eq-podemos]) e ([-@eq-aberto]), temos que
$$
h^*(y_1,\dots,y_n, t) \cdot P_\theta(T(\mathbb{X}_n)=t) = P_\theta(X_1=y_1,\dots,X_n=y_n\lvert \mathbb{1}(T(\mathbb{X}_n)=t))
$$

Para $T(\mathbb{X}_n)=t$, temos que
$$
P_\theta(X_1=y_1,\dots,X_n=y_n) = h^*(y_1,\dots,y_n, T(\mathbb{y}_n)) \cdot P_\theta(T(\mathbb{X}_n)=T(\mathbb{y}_n))
$$

#### $\Leftarrow$
Assuma que existam $h, m$ tais que
$$
P_\theta(X_1=y_1,\dots,X_n=y_n) = h(\mathrm{y}_n) \cdot m(T(\mathbb{y}_n,\theta)
$$
Note que
$$
P_\theta(X_1=y_1,\dots,X_n=y_n\lvert T(\mathbb{X}_n = t) = \left\{ \begin{array}{ll}
\frac{P_\theta(X_1=y_1,\dots,X_n=y_n)}{P_\theta(T(\mathbb{X}_n)=t)}, & T(\mathbb{y}_n)=t \\
0, & \mathrm{c.c.}
\end{array}\right.
$$

Observe que
$$
P_\theta(T(\mathbb{X}_n) = t) = \sum_{(y_1,\dots,y_n) : T(\mathbb{y})_n=t} P_\theta(X_1=y_1,\dots,X_n=y_n)
$$
Por suposição
$$
\begin{aligned}
P_\theta(T(\mathbb{X}_n) =t) &= \sum_{\mathbb{y}_n : T(\mathbb{y}_n)=t} h(\mathbb{y}_n) \cdot m (T(\mathbb{y}_n), \theta) \\
&= \sum_{\mathbb{y}_n : T(\mathbb{y}_n)=t} h(\mathbb{y}_n) \cdot m(t,\theta) \\
&= m(t,\theta) \cdot \sum_{\mathbb{y}_n : T(\mathbb{y}_n)=t} h(\mathbb{y}_n)
\end{aligned}
$$
Portanto
$$
P_\theta(X_1=y_1,\dots,X_n=y_n\lvert T(\mathbb{X}_n)=t) = \left\{\begin{array}{ll}
\frac{h(\mathbb{y}_n)\cdot m(T(\mathbb{y}_n),\theta)}{m(t,\theta) \cdot \sum_{\mathbb{y}_n: T(\mathbb{y}_n) = t} h(\mathbb{y}_n)}, & T(\mathbb{y}_n) = t \\
0, & \mathrm{c.c.}
\end{array}\right.
$$

Logo,
$$
P_\theta(X_1=y_1,\dots,X_n=y_n\lvert T(\mathbb{X}_n)=t) = \left\{\begin{array}{ll}
\frac{h(\mathbb{y}_n)}{\sum_{\mathbb{y}_n: T(\mathbb{y}_n) = t} h(\mathbb{y}_n)}, & T(\mathbb{y}_n) = t \\
0, & \mathrm{c.c.}
\end{array}\right.
$$


### Exemplo (1 do caso discreto)

Seja $\mathbb{X}_n=(X_1,\dots,X_n)$ amostra aleatória de $X\sim\mathrm{Ber}(\theta), \theta \in \Theta = (0,1)$. Verifique
se $T(\mathbb{X}_n)=\sum^n_{i=1}X_i$ é suficiente para o modelo estatístico.

#### Resposta
Observe que
$$
\begin{aligned}
f_\theta^{\mathbb{X}_n}(y_1,\dots,y_n) &= \left\{ \begin{array}{ll}
\prod^n_{i=1}\left\{\theta^{y_i}\cdot(1-\theta)^{1-y_i}\right\}, & y_i \in \{0,1\}, \forall i = 1,\dots,n \\
0, & \mathrm{c.c.}
\end{array}\right. \\
\Rightarrow
f_\theta^{\mathbb{X}_n}(y_1,\dots,y_n) &= \theta^{\sum y_i} \cdot (1-\theta)^{n-\sum y_i} \cdot \prod^n_{i=1}\mathbb{1}_{\{0,1\}} (y_1)
\end{aligned}
$$

Tome $h(y_1,\dots,y_n) = \prod^n_{i=1}\mathbb{1}_{\{0,1\}}(y_1)$ e $m(T(y_1,\dots,y_n),\theta) = \theta^{\sum y_i} \cdot (1-\theta)^{n-\sum y_i}$
em que $T(y_1,\dots,y_n) = \sum^n_{i=1}y_i$. Temos que
$$
f_\theta^{\mathbb{X}_n}(y_1,\dots,y_n) = h(y_1,\dots,y_n) \cdot m(T(y_1,\dots,y_n),\theta),\forall \theta \in \Theta
$$

Pelo critério da fatoração, $T(\mathbb{X}_n) = \sum^n_{i=1}X_i$ é suficiente para o modelo de Bernoulli.
