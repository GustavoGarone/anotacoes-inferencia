---
tags:
  - Inferência
cssclasses:
  - no-embed-border
---
Seja $(X_{1},\dots,X_{n})$ uma amostra aleatória (a.a) de $X\sim f_{\theta,}\theta \in \Theta$

a [[Função Densidade de Probabilidade]] conjunta de $(X_{1}, X_{2},\dots,X_{n})$ é $\forall \theta \in \Theta$,
no caso discreto:
$$P_\theta(X_{1}=k_{1}, X_{2}=k_{2},\dots,X_{n}= k_{n})\stackrel{\text{ind}}{=}\prod^{n}_{i=1}P_\theta(X_{i}=k_{i})\stackrel{\text{id}}{=}\prod^{n}_{i=1}P_\theta(X=k_{i})\stackrel{\text{def}}{=}\prod^{n}_{i=1}f_\theta(k_{i})$$
no caso contínuo:
$$f_\theta^{(x)}(k_{1},k_{2},\dots,k_{n})\stackrel{\text{i.i.d}}{=}\prod^{n}_{i=1}f_{\theta}(k_{i})$$
# Exemplo
Seja $(X_{1},X_{2},X_{3})$ uma a.a  de $X\sim \text{Ber}(\theta), \theta \in (0,1)$
1. Especifique o espaço paramétrico
2. Calcule a função de probabilidade da amostra
3. Encontre as seguintes [[Quantidade de Interesse]] em função de $\theta$
	1. $g(\theta)=E_\theta(X)$
	2. $g(\theta)=P_\theta(X=0)$
	3. $g(\theta)=\mathrm{CV}_\theta(X)$
Resolução
1. $\Theta=(0,1)$
2. Duas resoluções possíveis
	1. Dando valores à amostra$$\begin{aligned}
	     &(X_{1}, X_{2},X_{3})  &P(X_{1}= k_{1},X_{2}= k_{2},X_{3}= k_{3})= \prod ^{3}_{i=1}P_\theta(X=k_{1})\\
	     &(0,0,0) &(1-\theta)^{3}\\
	     &(0,0,1) &(1-\theta)^{2}\theta\\
	     &(0,1,0) &(1-\theta)^{2}\theta\\
	     &(1,0,0) &(1-\theta)^{2}\theta\\
	     &(0,1,1) &(1-\theta)\theta^2\\
	     &(1,0,1) &(1-\theta)\theta^{2}\\
		 &(1,1,0) &(1-\theta)\theta^{2}\\
		 &(1,1,1) &\theta^{3}\\
	     
	     \end{aligned}$$
	    2. Enunciando a função
	     Observe que, se $k \in \{0,1 \}$, $$\begin{aligned}
		 &P_\theta(X=k)=\theta^{k}(1-\theta)^{1-k}\cdot\mathbb{1}_{\{0,1 \}}(k)\Rightarrow\\
			&P_\theta(X_{1}=k,X_{2}=k_{2},X_{3}= k_{3})\stackrel{\text{i.i.d}}{=}\prod^{3}_{i=1}\{\theta^{k_{i}}(1-\theta)^{1-k_{1}}\mathbb{1}_{\{0,1 \}}(k_{1}) \} = \\
			&= \theta^{\sum\limits^{3}_{i=1}k_{1}}(1-\theta)^{3-\sum\limits^{3}_{i=1}k_{1}}\prod^{3}_{i=1}\mathbb{1}_{\{0,1\}}(k_{i})
		 \end{aligned}$$
3. Em função de $\theta$:
	1. $g(\theta)=E_\theta(X)=\theta$
	2. $g(\theta)=P_\theta(X=0)=1-\theta$
	3. $g(\theta)=\mathrm{CV}_\theta(X)=\frac{\sqrt{\theta(1-\theta)}}{\theta}$
- - -
Outro exemplo
Seja $(X_{1},\dots,X_{n})$ uma a.a de $X\sim\text{Ber}(\theta), \theta \in(0,1)$, encontre a f.p conjunta da amostra.
$$\begin{aligned}
P_\theta(X_{1}=k_{1},\dots,X_{n}=k_{n})\stackrel{\text{iid}}{=}\prod ^{n}_{i=1}P_\theta(X=k_{i})=\prod^{n}_{i=1}\{\theta^{k_{i}}(1-\theta)^{1-k_{i}}\mathbb{1}_{\{0,1 \}}(k_{i}) \}\\
\Rightarrow P_\theta(X_{1}=k_{1},\dots,X_{n}=k_{n})=\theta^{\sum\limits^{n}_{i=1}k_{1}}\cdot \theta^{n-\sum\limits^{n}_{i=i}k_{i}}\cdot\mathbb{1}_{\{0,1\}}(k_{i})
\end{aligned}$$
- - - 
Mais um exemplo
Seja $(X_{1},\dots,X_{n})$ uma a.a de $X\sim\text{Pois}(\theta), \theta \in(0,\infty)$, encontre a f.p. conjunta da amostra.
Como esse vetor é uma a.a. (ou seja, variáveis aleatórias independentes e identicamente distribuídas), temos que
$$P_\theta(X_1=k_{1},\dots,X_{n}=k_{n})\stackrel{\text{iid}}{=}\prod^{n}_{i=1}P_\theta(X=k_{i})=\prod^{n}_{i=1}\{\mathrm{e}^{-\theta} \cdot \frac{\theta^{k_{i}}}{k_{i}!}\}$$
Sempre que $k_{i}\in\{0,1,\dots \}, \forall i = 1,\dots,n$
$$\Rightarrow P_\theta(\dots)=\mathrm{e}^{-n \theta} \cdot \frac{\theta^{\sum\limits^{n}_{i=1}k_{i}}}{\prod^{n}_{i=1}(k_{i})!} $$
- - -
Um exemplo para contínua,
Seja $(X_{1},\dots,X_{n})$ uma a.a de $X\sim\text{Exp}(\theta), \theta \in(0,\infty)$, encontre a f.d.p conjunta da amostra.
$$F_{\theta}^{(*)}(k_{1},\dots,k_{n})\stackrel{\text{iid}}{=}\prod^{n}_{i=1}f_\theta(k_{i})=\prod^{n}_{i=1}\{\theta \mathrm{e}^{-\theta k_{i}} \cdot \mathbb{1}_{(0,\infty )}(k_{i}) \}\Rightarrow f_\theta^{(*)}(\dots)=\theta^{n} \cdot \mathrm{e}^{-\theta\sum\limits^{n}_{i=1}k_{i}} \cdot\prod^{n}_{i=1}\mathbb{1}_{(0,\infty)}(k)$$
- - -
Mais um,
Seja $(X_{1},\dots,X_{n})$ uma a.a. (i.i.d) de $X\sim N(\mu,\sigma^{2})$ em que $\theta=(\mu, \sigma^{2}) \in \Theta=\mathbb{R}\times \mathbb{R}_{+}$. Considere $\stackrel{x}{\sim}=(x_{1},\dots,x_{n})$ a amostra observada.
$$
\begin{aligned}
L_{\stackrel{X}{\sim}} \stackrel{\text{iid}}{=} \prod^{n}_{i=1}f_\theta(x_{i})=\prod^{n}_{i=1}\left\{\frac{1}{\sqrt{2 \pi \sigma^{2}}} \mathrm{exp}\{- \frac{1}{2\sigma^{2}}(x_{1}-\mu)^{2}\}\right\} = \frac{1}{(2 \pi \sigma^{2})^{\frac{x}{2}}}\cdot \mathrm{exp}\{- \frac{1}{2 \sigma^{2}} \sum\limits^{n}_{i=1}(x_{i}-\mu)^{2}  \}
\end{aligned}

$$
# Função de Verossimilhança
Obs: quando analisamos a distribuição conjunta da amostra em função de $\theta$ nos valores da amostra observada, temos a **Função de Verossimilhança**
$$\mathrm{L}_{\stackrel{x}{\sim}}(\theta)=P_\theta(X_{1}=x_{1},X_{2}=x_{2},\dots,X_{n}=x_{n})$$
em que $\stackrel{x}{\sim}=(x_{1},x_{2},\dots,x_{n})$ é a amostra observada.
Obs: A função de verossimilhança, no caso discreto, é a probabilidade de observar a amostra observada.

*Exemplo*
Considere $(X_{1},X_{2},X_{3},X_{4})$ a.a de $X\sim \text{Ber}(\theta), \theta \in \{0.1, 0.5, 0.9 \}$. Note que o espaço paramétrico é $\Theta=\{0.1,0.5,0.9 \}$. Considere, ainda, que a amostra observada foi $(0,1,1,1)$. Encontre a função de verossimilhança.
$$L_\stackrel{X}{\sim}(\theta)=P_\theta(X_{1}=x_{1},X_{2}=x_{2},X_{3}=x_{3},X_{4}=x_{4}) = \theta^{\sum\limits^{4}_{i=1}x_{i}}(1-\theta)^{4-\sum\limits^{4}_{i=1}x_{i}}\cdot \cancelto{1}{\prod^{4}_{i=1}\mathbb{1}_{\{0,1 \}}(x_{i})} = \theta^{3}(1-\theta) $$
## Estimação via máxima verossimilhança (EMV)
O valor numérico $\hat\theta_{n}$ que maximiza a função de verossimilhança, ou seja, $L_{\stackrel{x}{\sim}}(\hat\theta_{n}) \geq L_{\stackrel{x}{\sim}}(\theta)\forall \theta \in \Theta$ é dito ser uma [[Estatísticas e Estimadores#Estimativas|estimativa]] de máxima verossimilhança (MV) para $\theta$. Observe que $\hat\theta_{n}$ depende da [[População e amostra#Amostra (Observada)|amostra observada]] e portanto: $\hat\theta_{n} = \hat\theta_(x_{1},x_{2},\dots,x_{n})$.

O [[Estatísticas e Estimadores#Estimadores|estimador]] de máxima verossimilhança é obtido substituindo $(x_{1},\dots,x_{n})$ por $(X_{1},\dots,X_{n})$, ou seja, $\hat\theta_{(X_{1},\dots,X_{n})}$ é o Estimador de Máxima Verossimilhança (EMV)

Exemplo:

Seja $(X_{1},\dots,X_{n})$ [[População e amostra#Amostra Aleatória|a.a]] de $X\sim f_{\theta}, \theta \in \{\frac{1}{3}, \frac{1}{2} \}$ em que $f_\theta$ é uma função de probabilidade que satisfaz:

$$
\begin{array}{|c|c|c|c|}
\hline
X=x: & 0 & 1 & 2\\
\hline
f_{\theta}(x): & \theta & \theta^{2} & 1-\theta-\theta^{2}\\
\hline
\end{array}
$$
Considere que a amostra observada é $\stackrel{x}{\sim}=(0,0,1)$.

a-) Encontre a estimativa da máxima verossimilhança 
Sabemos que
$$
f_{\theta}(x)= \theta^{\mathbb{1}_{\{0\}}(x)} \cdot (\theta^{2})^{\mathbb{1}_{\{1\}}(x)} \cdot (1-\theta-\theta^{2})^{\mathbb{1}_{\{2\}}(x)} \forall \theta \in \Theta
$$
portanto,

$$
L_{\stackrel{x}{\sim}}(\theta)\stackrel{\text{iid}}{=}\prod^{n}_{i=1}f_{\theta}(x_{i})  = \theta^{\sum\limits^{n}_{i=1}\mathbb{1}_{\{0\}}(x_{i})} \cdot (\theta^{2})^{\sum\limits^{n}_{i=1}\mathbb{1}_{\{1\}}(x_{i})} \cdot (1-\theta-\theta^{2})^{\sum\limits^{n}_{i=1}\mathbb{1}_{\{2\}}(x_{i})} \forall \theta \in \Theta
$$
Para $\stackrel{x}{\sim}=(0,0,1)$,
$$
L_{\stackrel{x}{\sim}}(\theta) = \theta^{2} \cdot \theta^{2} \cdot (1-\theta -\theta^{2})^{0} = \theta^{4} \forall \theta \in \Theta
$$
Substituindo $\forall \theta \in \Theta$:
$$
\theta = \frac{1}{2}\Rightarrow L_{\stackrel{x}{\sim}}\left(\frac{1}{2}\right)=\frac{1}{16} ~~~~ \theta = \frac{1}{3}\Rightarrow L_{\stackrel{x}{\sim}}\left(\frac{1}{3}\right)=\frac{1}{81}
$$
Portanto, $\hat\theta_{n}=\frac{1}{2}$ é a estimativa de máxima verossimilhança.

### Invariância dos EMVs
Teorema.
Se $\hat\theta_{(X_{1},\dots,X_{n})}$ for EMV para $\theta$, então $g(\hat\theta_{(X_{1},\dots,X_{n})})$ é o EMV para $g(\theta)$, ou seja, $g(\hat\theta_n)$ é a estimativa de máxima verossimilhança para $g(\theta)$

Mais um exemplo.

Seja $(X_{1},\dots,X_{n})$ a.a. de $X\sim N(\mu, \sigma^2)$ em que $\theta = (\mu, \sigma^{2}) \in \Theta=\mathbb{R}\times \mathbb{R}^{+}$
Assuma que $\stackrel{x}{\sim} = (x_{1},\dots,x_{n})$ é a amostra observada.
Lembrando que estaremos chamando $\theta=(\mu, \sigma^{2})$, mas estes são parâmetros genéricos. Poderíamos, por exemplo, chamá-los de $\theta=(\theta_{1},\theta_{2})$, o que pode facilitar a visualizar algumas derivadas.

a-) Encontre as estimativas de máxima verossimilhança para $\theta = (\mu, \sigma^{2})$:
A Função de Verossimilhança é:
$$
\begin{aligned}
L_{\underset{\sim}{x}}(\theta)&\stackrel{\text{iid}}{=}\prod^{n}_{i=1}f_\theta(x_{i}) = \prod^{n}_{i=1}\left\{ \frac{1}{\sqrt{2 \pi \sigma^{2}}}\cdot \exp\left\{\frac{-1}{2}\cdot\frac{(x_{i}-\mu)^{2}}{\sigma^{2}} \right\} \right\}\\
&= \frac{1}{(2\pi \sigma^{2})^{\frac{n}{2}}} \cdot \exp\left\{ \frac{-1}{2\sigma^{2}}\sum\limits^{n}_{i=1}(x_{i}-\mu)^{2}\right\}
\end{aligned}$$
Podemos derivar para encontrar o máximo da FMV. Para isso, derivaremos e igualamos a zero primeiro em relação a $\mu$ e então a $\sigma^{2}$ (podemos aplicar o logaritmo para facilitar as operações.)
$$
\begin{aligned}
\frac{\partial\ln(L_{\underset{\sim}{x}})}{\partial \mu} &= \frac{1}{\sigma^{2}}\sum\limits^{n}_{i=1}(x_{i}-\mu) =0 \Rightarrow \hat \mu =\frac{1}{n} \sum\limits^{n}_{i=1}x_{i} \\
\frac{\partial \ln(L_{\underset{\sim}{x}})}{\partial \sigma^{2}} &= - \frac{n}{2\sigma^{2}} + \frac{1}{2\sigma^{4}} \sum\limits^{n}_{i=1}(x_{i}-\mu)^{2} =0 \\
\therefore \\
\mathrm{Estimativas~ MV} & = 
\begin{cases}
\mu = \bar{x} \\
-\frac{n}{2\sigma^{2}} + \frac{1}{2 \sigma^{4}} \sum\limits^{n}_{i=1}(x_{i}-\mu)^{2}=0 \\
\end{cases} \\
&\Leftrightarrow \begin{cases}
\hat{\mu}=\bar{x} \\
\hat{\sigma}^{2}= \frac{1}{n}\sum\limits^{n}_{i=1}(x_{i}-\bar{x})^{2}
\end{cases}
\end{aligned}
$$
Estes são os pontos que maximizam a Função de Máxima Verossimilhança. (Provados em cálculo), ou seja, são as estimativas de máxima verossimilhança para $\mu, \sigma^{2}$ respectivamente, e $\hat{\mu}(X_{1},\dots,X_{n})=\bar{X}, \sigma^{2}(X_{1},\dots,X_{n})=\frac{1}{n}\sum\limits^{n}_{i=1}(X_{i}-\bar{X})^{2}$ são os estimadores de máxima verossimilhança.

- - -

Pela propriedade de invariância podemos encontrar o EMV para $g(\theta)= \frac{\sqrt{\mathrm{Var}_\theta(X)}}{E_{\theta(X)}}$:
$$
\widehat{g(\theta)} = \frac{\sqrt{\frac{1}{n}\sum\limits^{n}_{i=1}(X_{i}-\bar{X})}}{\bar{X}}
$$
Observação:
Seja $(X_{1},\dots,X_{n})$ a.a. de $X\sim N(\mu, \sigma^{2})$. Então,
1-) $\bar{X} \underset{\text{Exata!}}{\sim}N\left(\mu, \frac{\sigma^{2}}{n}\right)\forall \mu, \sigma \in \mathbb{R} : \sigma^{2}>0 \text{ e } n\geq 1$
2-) $\sum\limits^{n}_{i=1} \frac{(x_{1}-\bar{X})^{2}}{\sigma^{2}}\underset{\text{Exata!}}{\sim}\chi^{2}_{(n-1)}$ em que $\chi^{2}_{k}$ representa a [[Distribuição Qui-Quadrado]] com $k$ grau de liberdade, cuja função densidade de probabilidade é:
$$
f(x) = \frac{1}{\Gamma(\frac{k}{2})2^{\frac{k}{2}}} \cdot x^{\frac{k}{2}-1} \cdot \exp\left\{\frac{-x}{2}\right\} \cdot \mathbb{1}_{(0, \infty)}(x)
$$
Para qualquer outra distribuição, existe um resultado aproximado pelo ![[Teorema do Limite Central]]
