{
  "hash": "79d8145f261e54b315fdfba8405d636e",
  "result": {
    "engine": "julia",
    "markdown": "\n\n\n# Teste de Hipótese - Aprofundamento\n\nSeja $\\boldsymbol{X}_n = (X_1, \\dots, X_n)$ [amostra aleatória](populacao-e-amostra.qmd#sec-aa) de $X \\sim f_\\theta, \\theta \\in \\Theta$.\n\nNote que, no caso discreto:\n$$\nP_\\theta(X \\in A) = \\sum_{x \\in A} f_\\theta(x), \\forall \\theta \\in \\Theta.\n$$\njá no caso contínuo,\n$$\nP_\\theta(X \\in A) = \\int_{A} f_\\theta(x), \\forall \\theta \\in \\Theta.\n$$\n\nTemos uma família $\\mathcal{P} = \\{P_\\theta : \\theta \\in \\Theta\\}$ de probabilidades que podem explicar o comportamento\ndos dados. Um dos objetivos do teste de hipótese é verificar se podemos reduzir $\\mathcal{P}$ para uma família menor $\\mathcal{P}_0 \\subseteq \\mathcal{P}$:\n$$\n\\mathcal{P}_0 = \\{P_\\theta : \\theta \\in \\Theta_0\\}, \\Theta_0 \\subseteq \\Theta.\n$$\n\nDefinimos uma *hipótese estatística*\n$$\n\\mathcal{H}_0 : \\theta \\in \\Theta_0 \\iff \\mathcal{H}_0 : P_\\theta \\in \\mathcal{P}_0\n$$\n$\\mathcal{H}_0$ afirma que\n\n> \"A medida de probabilidade que explica os dados está em $\\mathcal{P}_0$\"\n\nNo caso em que $\\Theta_0 = \\{\\theta_0\\}$, então a hipótese\n$\\mathcal{H}_0 : \\theta = \\theta_0 ( \\iff \\mathcal{H}_0 : P_{\\theta} = P_{\\theta_0})$ afirma que $P_{\\theta_0}$ explica o\ncomportamento probabilístico dos dados observados.\n\n$\\mathcal{H}_0$ é chamada de *hipótese nula*.\n\nNote que a negação de $\\mathcal{H}_0$ é\n\n> \"Não é o caso que a família $\\mathcal{P}_0$ contenha a medida que explica os dados\".\n\nOu seja, a negação de $\\mathcal{H}_0$ sugere que a medida que explica os dados pode, inclusive, não estar contida em $\\mathcal{P}$.\n\nDefinimos também uma hipótese alternativa,\n$$\n\\mathcal{H}_1: \\theta \\in (\\Theta \\setminus \\Theta_0) \\iff \\mathcal{H}_1 : P_\\theta \\in (\\mathcal{P} \\setminus \\mathcal{P}_0)\n$$\n\n<!-- Em termos de espaço paramétrico, -->\n<!-- TODO: Desenho do Theta maior e theta menor e visualização dos conjuntos e relação com hipóteses e os thetas -->\n\n<!-- Em termos de família de probabilidades,  -->\n<!-- TODO: Desenho do mathcal P maior e P0 e visualização dos conjuntos e relação com hipóteses e os P_thetas -->\n\nObserve que $\\mathcal{H}_0$ e $\\mathcal{H}_1$ não são exaustivas: na prática, ambas podem ser falsas.\n\nSe o analista considerar que $\\mathcal{P}$ contém a medida que explica os dados, então, condicional a essa crença, $\\mathrm{\\mathcal{H}_0}$ e\n$\\mathrm{\\mathcal{H}_1}$ são exaustivas.\n\n## Exemplo\n\nSeja $\\boldsymbol{X}_n$ a.a. de $X\\sim N(\\mu, \\sigma^2), \\theta = (\\mu, \\sigma^2) \\in \\mathbb{R}\\times\\mathbb{R}_+$.\n\nConsidere as seguintes hipóteses\n$$\n\\begin{cases}\n\\mathcal{H}_0 : \\mu = 0 \\\\\n\\mathcal{H}_1 : \\mu \\neq 0.\n\\end{cases}\n$$\nNote que\n$$\n\\begin{cases}\n\\mathcal{H}_0 : \\theta \\in \\Theta_0 \\\\\n\\mathcal{H}_1 : \\theta \\in \\Theta \\setminus \\Theta_0,\n\\end{cases}\n$$\nem que\n$$\n\\begin{aligned}\n\\Theta_0 &= \\{(\\mu, \\sigma^2) \\in \\mathbb{R} \\times \\mathbb{R}_+ : \\mu = 0\\} \\\\\n\\Theta_1 &= \\Theta \\setminus \\Theta_0 = \\{(\\mu, \\sigma^2) \\in \\mathbb{R} \\times \\mathbb{R}_+ : \\mu \\neq 0\\}.\n\\end{aligned}\n$$\nDisso, temos que\n$$\n\\begin{aligned}\n\\Theta &= \\Theta_0 \\cup \\Theta_1 \\\\\n\\Theta_0 &\\cap \\Theta_1 = \\emptyset.\n\\end{aligned}\n$$\n\n\nAs hipóteses acima podem ser reescritas em termos das suas respectivas famílias de medidas de probabilidades da seguinte forma:\n$$\n\\begin{cases}\n\\mathcal{H}_0: P in \\mathcal{P}_0 \\\\\n\\mathcal{H}_1: P \\in \\mathcal{P}_1\n\\end{cases}\n$$,\nem que $\\mathcal{P}_0 = \\{N(\\mu, \\sigma^2): \\ \\mu = 0, \\ \\sigma^2 > 0 \\}$ e\n$\\mathcal{P}_1 = (\\mathcal{P} - \\mathcal{P}_0) = \\{ N(\\mu, \\sigma^2): \\ \\mu \\neq 0, \\ \\sigma^2 > 0 \\}$\n\nObserve que, se assumirmos que a distribuição normal explica os dados, então $\\mathcal{H}_0$ e $\\mathcal{H}_1$ são exaustivas.\nCaso contrário, existe uma terceira opção: $\\neg(\\mathcal{H}_0 \\lor \\mathcal{H}_1)$.\n\n## Hipóteses estatísticas\n\nTOda hipótese estatística é uma interpretação de uma hipótese científica.\n\n| Hipótese científica |     Hipótese estatística   | Hipótese paramétrica |\n|:-------------------:|:--------------------------:|:--------------------:|\n| \"A moeda é honesta\" | $X \\sim \\mathrm{Ber}(0.5)$ |    $\\theta = 0.5$    | \n\nAs hipótese estatísticas são escritas em termos de medidas de probabilidade, mas podem também ser representadas em termos\ndo espaço paramétrico:\n$$\n\\begin{cases}\n\\mathcal{H}_0 : P_\\theta \\in \\mathcal{P}_0 \\\\\n\\mathcal{H}_1 : P_\\theta \\in \\mathcal{P}_1 =  (\\mathcal{P} \\setminus \\mathcal{P}_0),\n\\end{cases}\n$$\nem que $\\mathcal{P}_0 = \\{P_\\theta : \\theta \\in \\Theta_0\\}$ e  $\\mathcal{P}_1 = \\{P_\\theta : \\theta \\in \\Theta_1\\}$ e\n$\\Theta_1 = \\Theta \\setminus \\Theta_0$.\n\nObserve que $\\mathcal{H}_0, \\mathcal{H}_1$ estão sempre restritas a modelo estatístico. Entretanto, a negação de\n$\\mathcal{H}_0, \\neg \\mathcal{H}_0$, não está restrita ao modelo adotado.\n\n:::{.callout-note title=\"Algumas considerações lógicas\"}\n<!-- TODO: desenho das famílias e tal igual da outra aula com o P0 no cantinho e P1 grandão no resto do quadrado uau h0 no p0 e 1 no p1 e ta mas pode ta fora, terceira via... -->\n\n1. Se o analista considerar que $\\mathcal{H}_0$ ou $\\mathcal{H}_1$ são verdadeiros, então ele está considerando que a medida que *explica* ou *gera*\nos dados está em $\\mathcal{P}$. Está é a suposição de que o universo de possibilidades é fechado (suposição de Neyman-Pearson; Teoria da Decisão).\n\n2. $\\mathcal{H}_0$ e $\\mathcal{H}_1$ não pode ser simultaneamente verdadeiras.\n\n3. Se $\\neg \\mathcal{H}_0$ e $\\neg \\mathcal{H}_1$, então a medida que explica/gera os dados não está em $\\mathcal{P}$.\n\n4. $\\neg \\mathcal{H}_0 \\not\\Rightarrow \\mathcal{H}_1$. Isto é, se provarmos que $\\mathcal{H}_0$ é falsa, não necessariamente\n$\\mathcal{H}_1$ é verdadeira.\n\n5. $\\mathcal{H}_1 \\Rightarrow \\neg \\mathcal{H}_0$.\n:::\n\n## Tipos de hipótese\n\nSejam $\\mathcal{H}_0, \\mathcal{H}_1$ as hipóteses nula e alternativa, respectivamente, tais que\n$$\n\\begin{cases}\n\\mathcal{H}_0 : \\theta \\in \\Theta_0 \\\\\n\\mathcal{H}_1 : \\theta \\in \\Theta_1 \\\\\n\\end{cases}\n$$\nem que $\\Theta_0 \\neq \\emptyset, \\Theta_0 \\cup \\Theta 1 = \\Theta, \\Theta_0 \\cap \\Theta_1 = \\emptyset$. Dizemos que $\\mathcal{H}_0$ é\numa **hipótese simples** se $\\# \\Theta_0 = 1$. Caso contrário, dizemos que é uma **hipótese composta**.\n\n### Exemplos\n\n1. Seja $\\boldsymbol{X}_n = (X_1, \\dots, X_n)$ a.a. de $X\\sim \\mathrm{Ber}(\\theta), \\theta \\in \\{0.5, 0.9\\}$. Considere as hipóteses\n$$\n\\begin{cases}\n\\mathcal{H}_0 : \\theta = 0.5 \\\\\n\\mathcal{H}_1 : \\theta = 0.9\n\\end{cases}\\ \\ \\ \\text{Ambas são simples!}\n$$\n\n2. Seja $\\boldsymbol{X}_n = (X_1, \\dots, X_n)$ a.a. de $X\\sim \\mathrm{Ber}(\\theta), \\theta \\in (0,1)$. Considere as hipóteses\n$$\n\\begin{cases}\n\\mathcal{H}_0 : \\theta = 0.5\\ \\  \\text{Hipótese simples} \\\\\n\\mathcal{H}_1 : \\theta \\neq 0.5\\ \\ \\text{Hipótese composta}\n\\end{cases}\n$$\nNote que\n$$\n\\begin{cases}\n\\Theta_0 = \\{0.5\\} \\\\\n\\Theta_1 = (0,0.5) \\cup (0.5, 1) \\\\\n\\end{cases}\n$$\n\n3. Seja $\\boldsymbol{X}_n$ a.a. de $X \\sim N(\\mu, \\sigma^2) \\in \\mathbb{R}\\times\\mathbb{R}_+$\n$$\na)\\ \\begin{cases}\n\\mathcal{H}_0 : \\mu = 0.5\\ \\  \\text{Hipótese composta!} \\\\\n\\mathcal{H}_1 : \\mu \\neq 0.5\\ \\ \\text{Hipótese composta}\n\\end{cases}\n$$\nNote que como\n$$\n\\begin{cases}\n\\Theta_0 = \\{(\\mu, \\sigma^2) \\in \\mathbb{R}\\times\\mathbb{R}_+ : \\mu = 0\\}\n\\Theta_0 = \\{(\\mu, \\sigma^2) \\in \\mathbb{R}\\times\\mathbb{R}_+ : \\mu \\neq 0\\}\n\\end{cases}\n$$\ntêm mais de um elemento, concluímos que ambas hipóteses são compostas.\n\n$$\nb)\\ \\begin{cases}\n\\mathcal{H}_0 : (\\mu, \\sigma^2) = (0,1)\\ \\  \\text{Hipótese simples!} \\\\\n\\mathcal{H}_1 : (\\mu, \\sigma^2) \\neq (0,1)\\ \\ \\text{Hipótese composta}\n\\end{cases}\n$$\n\n### Tipos de decisão sobre as hipóteses\n\n1. Se o espaço de possibilidades for fechado, isto é, $\\mathcal{H}_0$ ou $\\mathcal{H}_1$ é verdadeira, então:\na) Aceitamos $\\mathcal{H}_0$ (rejeitamos $\\mathcal{H}_1$)\nb) Aceitamos $\\mathcal{H}_1$ (rejeitamos $\\mathcal{H}_0$)\n\nObserve que, neste caso, não há terceira opção (abordagem de Neyman-Pearson).\n\n2. Se o espaço de possibilidades for aberto, isto é, temos a terceira opção de que o modelo está equivocado, então:\na) Não rejeitamos $\\mathcal{H}_0$ (**Não** significa que aceitamos $\\mathcal{H}_0$)\nb) Rejeitamos $\\mathcal{H}_0$ (**Não** significa aceitar $\\mathcal{H}_1$, pela existência da terceira opção.)\n\nNeste caso temos uma terceira opção (abordagem de Fisher).\n\nAtualmente, dizemos apenas que há ou não há evidências para rejeitarmos $\\mathcal{H}_0$. **Não** se diz aceitar\n\"$\\mathcal{H}_0$\"\n\n### Tipos de Erro\n\nSe o universo for aberto:\n\n*Erro tipo I*: \"Rejeitar $\\mathcal{H}_0$ quando este é verdadeiro\"\n\n*Erro tipo II*: \"Não rejeitar $\\mathcal{H}_0$ quando este é falso\"\n\n\n| $\\mathcal{H}_0$ | Não rejeitar | Rejeitar |\n|:--------:|:-----------:|:---------------:|\nVerdadeiro |    Acerto   | Erro tipo I |\n   Falso   | Erro tipo II|    Acerto   |\n\nSe o universo for fechado:\n\nErro tipo I: \"Rejeitar $\\mathcal{H}_0$ (ou aceitar $\\mathcal{H}_1$) quando este é verdadeiro\"\n\nErro tipo II: \"Aceitar $\\mathcal{H}_0$ (ou rejeitar $\\mathcal{H}_1$ quando este é falso\"\n\n## Teste de Hipótese de Neyman-Pearson\n\nConsidere o universo fechado.\n\n### Função teste\n\n*Definição*. Seja $\\delta : \\mathfrak{X}^{(n)} \\rightarrow \\{0,1\\}$ uma função tal que\n$$\n\\delta(\\boldsymbol{X}_n) = \\begin{cases}\n0,\\ \\text{Se não rejeitamos}\\ \\mathcal{H}_0 \\\\\n1,\\ \\text{Se rejeitamos}\\ \\mathcal{H}_0 \\\\\n\\end{cases}\n$$\nem que $\\mathfrak{X}^{(n)} = \\{x \\in \\mathbb{R}^n : f_\\theta^{\\boldsymbol{X}_n}(\\boldsymbol{x}) > 0 \\}$, $\\mathcal{H}_0 : \\theta \\in \\Theta_0$,\n$\\mathcal{H}_1 : \\theta \\in \\Theta_1$, $\\Theta_0 \\neq \\emptyset, \\Theta_1 \\neq \\emptyset, \\Theta_0 \\cap \\Theta_1 = \\emptyset, \\Theta_0 \\cup \\Theta_1 = \\Theta$.\n\nDizemos que $\\delta(\\cdot)$ é uma *função teste*.\n\nRegião de rejeição de $\\mathcal{H}_0$:\n$$\nS_{\\delta} = \\{\\boldsymbol{x} \\in \\mathfrak{X}^{(n)} : \\delta(\\boldsymbol{x}) = 1 \\}\n$$\n\nRegião de não rejeição de $\\mathcal{H}_0$:\n$$\nS_{\\delta}^c = \\{\\boldsymbol{x} \\in \\mathfrak{X}^{(n)} : \\delta(\\boldsymbol{x}) = 0 \\}\n$$\n\n<!-- TODO: desenho das regiões dentro do $\\mathfrak{X}$ -->\n\n#### Exemplo\n\nSeja $\\boldsymbol{X}_n$ a.a. de $X\\sim\\mathrm{Ber}(\\theta), \\theta \\in \\{0.1, 0.9\\}$ e as hipóteses\n$$\n\\begin{cases}\n\\mathcal{H}_0 : \\theta = 0.1 \\\\\n\\mathcal{H}_1 : \\theta = 0.9\n\\end{cases}\n$$\n\nDefina $\\delta_i : \\{0,1\\}^n \\rightarrow \\{0, 1\\}, i = 1, 2$ tais que\n\n$$\n\\begin{aligned}\n\\delta_1(\\boldsymbol{x}_n) &= \\begin{cases}\n0, \\bar{x} \\leq 0.5 \\\\\n1, \\bar{x} > 0.5 \\\\\n\\end{cases} \\\\\n\\delta_2(\\boldsymbol{x}_n) &= \\begin{cases}\n0, \\bar{x} \\leq 0.8 \\\\\n1, \\bar{x} > 0.8 \\\\\n\\end{cases}\n\\end{aligned}\n$$\n\n### Função Poder do teste $\\delta$ {#sec-funpoder}\n\n*Definição*. A função poder do teste $\\delta$ é\n$\\pi_\\delta(\\theta) = P_\\theta(S_\\delta)$\n\n:::{.callout-note title=\"Observações\"}\n1.\n$$\n\\pi_\\delta(\\theta), \\forall \\theta \\in \\Theta_0,\n$$\nsão probabilidades de rejeitar $\\mathcal{H}_0$ quando esta é verdadeira.\n\n2.\n$$\n\\pi_\\delta(\\theta), \\forall \\theta \\in \\Theta_1,\n$$\nsão probabilidades de rejeitar $\\mathcal{H}_0$ quando esta é falsa.\n:::\n\n*Definição*. O *nível de significância* é qualquer valor $\\alpha \\in (0,1)$ tal que:\n$$\nP_\\theta(S_\\delta) \\leq \\alpha, \\forall \\theta \\in \\Theta_0.\n$$\nem termos de função poder,\n$$\n\\pi_\\delta(\\theta) \\leq \\alpha, \\forall \\theta \\in \\Theta_0\n$$\n\n\n*Definição*. O tamanho do teste $\\delta$ é\n$$\n\\tau_\\delta = \\sup_{\\theta \\in \\Theta_0} P_\\theta(S_\\delta) \\leq \\alpha\n$$\nem termos de função poder,\n$$\n\\tau_\\delta = \\sup_{\\theta \\in \\Theta_0} \\pi_\\delta(\\theta)\n$$\n\n#### Probabilidade dos Erros I, II\n\n:::{.callout-tip title=\"Tipos de Erro\"}\nRelembrando os tipos de erro:\n$$\n\\begin{cases}\n\\text{Erro tipo I:}\\ \\text{Rejeitar $\\mathcal{H}_0$ quando esta é verdadeira} \\\\\n\\text{Erro tipo II:}\\ \\text{Não rejeitar $\\mathcal{H}_0$ quando esta é falsa}\n\\end{cases}\n$$\n:::\n\n$$\n\\alpha_{\\delta, \\mathrm{max}} = \\tau_\\delta = \\sup_{\\theta \\in \\Theta_0} \\pi_\\delta(\\theta)\n$$\né a probabilidade máxima de cometer o Erro tipo I e\n$$\n\\beta_{\\delta, \\mathrm{max}} = \\sup_{\\theta \\in \\Theta_1} (1-\\pi_\\delta(\\theta))\n$$\né a probabilidade máxima de cometer o Erro tipo II.\n\nQuando $\\mathcal{H}_0, \\mathcal{H}_1$ são simples, temos\n$$\n\\begin{aligned}\n\\alpha_{\\delta, \\max} &= \\pi_\\delta(\\theta_0) = P_\\theta(S_\\delta) \\\\\n\\beta_{\\delta, \\max} &= 1 - \\pi_\\delta(\\theta_1),\n\\end{aligned}\n$$\ncom\n$$\n\\begin{aligned}\n\\mathcal{H}_0 : \\theta = \\theta_0 \\\\\n\\mathcal{H}_1 : \\theta \\neq \\theta_1.\n\\end{aligned}\n$$\n\n:::{.callout-warning title=\"Notação (equivocada) em alguns materiais\"}\nAlguns livros, especialmente para iniciantes ou profissionais de outras áreas, podem escrever:\n$$\n\\begin{aligned}\n\\alpha_{\\delta, \\max} &= P(\\text{Erro tipo I}) \\\\\n&= P(\\text{Rejeitar} | \\text{$\\mathcal{H}_0$ é verdadeira}) \\\\\n&= P(\\delta(\\boldsymbol{X}_n) = 1 | \\text{$\\mathcal{H}_0$ é verdadeira}),\n\\end{aligned}\n$$\n$$\n\\begin{aligned}\n\\beta_{\\delta, \\max} &= P(\\text{Erro tipo II}) \\\\\n&= P(\\text{Não rejeitar} | \\text{$\\mathcal{H}_0$ é falsa}) \\\\\n&= P(\\delta(\\boldsymbol{X}_n) = 0 | \\text{$\\mathcal{H}_0$ é falsa}.\n\\end{aligned}\n$$\nNote que, formalmente, isso **não** faz sentido na estatística clássica! O parâmetro em hipótese, $\\theta$, é desconhecido\nmas **não** aleatório.\n\nNote que, na estatística clássica, as hipóteses são afirmações epistêmicas, e não experimentais (eventos do experimento).\nPortanto, não são elementos da $\\sigma$-álgebra do modelo estatístico. Logo, a notação\n$$\nP(S_\\delta | \\text{$\\mathcal{H}_0$ é verdadeira})\n$$\nnão está bem definida.\n:::\n\n#### Poder do Teste {#sec-poderteste}\n\nSe $\\mathcal{H}_1$ for simples, isto é, $\\mathcal{H}_1 : \\theta = \\theta_1$, então o poder do teste é definido\n$$\n1 - \\beta_{\\delta, \\max} - \\pi_\\delta(\\theta_1)\n$$\n\n:::{.callout-warning title=\"Notação (equivocada) em alguns materiais\"}\nAlguns livros escrevem\n$$\n\\mathrm{poder} = P(\\text{Rejeitar $\\mathcal{H}_0$} | \\text{$\\mathcal{H}_0$ é falsa})\n$$\nIsto está incorreto na interpretação clássica de estatística!\n:::\n\n\n## Lema de Neyman-Pearson\nEstamos interessados em um teste $\\delta^*$ que produza menor $\\alpha_{\\delta^*, \\max}$ e maior poder possível. O Lema\nde Neyman-Pearson apresenta um teste que, dentre os testes de tamanho $\\alpha$, tem maior poder.\n\nSeja $\\boldsymbol{X}_n$ a.a. de $X \\sim f_\\theta, \\theta \\in \\Theta = \\{\\theta_0, \\theta_1\\}$. Considere as hipóteses\n$$\n\\begin{cases}\n\\mathcal{H}_0 : \\theta = \\theta_0 \\\\\n\\mathcal{H}_1 : \\theta = \\theta_1\n\\end{cases}\n$$\nnula e alternativa. A função teste $\\delta^* : \\mathfrak{X}^{(n)} \\rightarrow \\{0,1\\}$ tal que\n$$\n\\delta^*(\\boldsymbol{x}_n) = \\begin{cases}\n0,\\ \\text{se}\\ \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta) < \\eta \\cdot \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta_0) \\\\\n1,\\ \\text{se}\\ \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta) \\geq \\eta \\cdot \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta_0),\n\\end{cases}\n$$\nem que $\\eta$ satisfaz $\\underbracket{P_{\\theta_0}(\\delta^*(\\boldsymbol{X}_n) = 1)}_{\\pi_{\\delta^*}(\\theta_0)} = \\alpha$ para um $\\alpha$ fixado apriori, é o teste mais\npoderoso dentre todo os testes de tamanho $\\alpha$.\n\n### Exemplo (Normal)\n\nSeja $\\boldsymbol{X}_n$ a.a. de $X \\sim N(\\mu, \\sigma^2)$ em que $\\theta = (\\mu, \\sigma^2) \\in \\{(0,1), (0,2)\\}$. Considere\n$$\n\\begin{cases}\n\\mathcal{H}_0 : \\theta = (0,1) \\\\\n\\mathcal{H}_1 : \\theta = (0,2)\n\\end{cases}\n$$\nas hipóteses nula e alternativa.\n\nEncontre o teste mais poderoso de tamanho $\\alpha = 5\\%$.\n\n#### Solução:\n\nDe acordo com o Lema de Neyman-Pearson, o teste mais poderoso de tamanho $\\alpha = 5\\%$ é\n$$\n\\delta^*(\\boldsymbol{x}_n) = \\begin{cases}\n0,\\ \\text{se}\\ \\mathcal{L}_{\\boldsymbol{x}_n}((0,2)) < \\eta \\cdot \\mathcal{L}_{\\boldsymbol{x}_n}((0,1)), \\\\\n1,\\ \\text{se}\\ \\mathcal{L}_{\\boldsymbol{x}_n}((0,2)) \\geq \\eta \\cdot \\mathcal{L}_{\\boldsymbol{x}_n}((0,1)),\n\\end{cases}\n$$\nem que $\\eta$ satisfaz $\\pi_{\\delta^*}((0,1)) = 5\\%$.\n\nA [função de verossimilhança](funcao-verossimilhanca.qmd) é\n$$\n\\begin{aligned}\n\\mathcal{L}_{\\boldsymbol{x}_n}(\\theta) &= \\frac{1}{(\\sqrt{2\\pi\\sigma^2})^n} \\cdot \\mathrm{Exp}\\left\\{\n-\\frac{1}{2} \\sum \\frac{(x_i - \\mu)^2}{\\sigma^2}\\right\\}, \\theta \\in \\{(0,1), (0,2)\\} \\\\\n&= \\frac{1}{(\\sqrt{2\\pi\\sigma^2})^n} \\cdot \\mathrm{Exp}\\left\\{\n-\\frac{1}{2} \\sum \\frac{x_i^2}{\\sigma^2} \\right\\}, \\theta \\in \\{(0,1), (0,2)\\} \\\\\n\\Rightarrow  \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta_0) \n&= \\frac{1}{(\\sqrt{2\\pi})^n} \\cdot \\mathrm{Exp}\\left\\{\n-\\frac{1}{2} \\sum x_i^2 \\right\\} \\\\\n\\mathcal{L}_{\\boldsymbol{x}_n}(\\theta_1) \n&= \\frac{1}{(\\sqrt{4\\pi})^n} \\cdot \\mathrm{Exp}\\left\\{\n-\\frac{1}{2} \\sum \\frac{x_i^2}{2} \\right\\} \\\\\n\\Rightarrow \\mathcal{L}_{\\boldsymbol{x}_n}((0,2)) &\\geq \\eta \\cdot \\mathcal{L}_{\\boldsymbol{x}_n}((0,1)) \\\\\n\\iff \\frac{\\mathcal{L}_{\\boldsymbol{x}_n}((0,2))}{\\mathcal{L}_{\\boldsymbol{x}_n}((0,1))} &\\geq \\eta \\\\\n\\Rightarrow &\\frac{\\frac{1}{(\\sqrt{4\\pi})^n} \\cdot \\mathrm{Exp}\\left\\{\n-\\frac{1}{2} \\sum \\frac{x_i^2}{2} \\right\\}}{\\frac{1}{(\\sqrt{2\\pi})^n} \\cdot \\mathrm{Exp}\\left\\{\n-\\frac{1}{2} \\sum x_i^2 \\right\\}} \\geq \\eta \\\\\n\\iff &\\frac{1}{2^{\\frac{n}{2}}} \\mathrm{e}^{\\frac{1}{4}\\sum x_i^2} \\geq \\eta \\\\\n\\iff &\\mathrm{e}^{\\frac{1}{4}\\sum x_i^2} \\geq 2^{n/2}\\eta \\\\\n\\iff &\\frac{1}{4}\\sum x_i^2 \\geq \\ln(2^{n/2}\\eta) \\\\\n\\iff &\\sum x_i^2 \\geq 4 \\ln(2^{n/2}\\eta).\n\\end{aligned}\n$$\n\nLogo, $\\sum x_i^2 \\geq 4 \\ln(2^{n/2}\\eta)$ é equivalente a $\\frac{\\frac{1}{(\\sqrt{4\\pi})^n} \\cdot \\mathrm{Exp}\\left\\{\n-\\frac{1}{2} \\sum \\frac{x_i^2}{2} \\right\\}}{\\frac{1}{(\\sqrt{2\\pi})^n} \\cdot \\mathrm{Exp}\\left\\{\n-\\frac{1}{2} \\sum x_i^2 \\right\\}}$.\n\nObserve ainda que\n$$\n\\begin{aligned}\n\\pi_{\\delta^*}((0,1)) &= P_{(0,1)}(\\delta^*(\\boldsymbol{X}_n) = 1) \\\\\n&\\stackrel{\\text{Def.}}{=} P_{(0,1)}\\left(\\frac{\\mathcal{L}_{\\boldsymbol{x}_n}((0,2))}{\\mathcal{L}_{\\boldsymbol{x}_n}((0,1))} \\geq \\eta\\right) \\\\\n&\\stackrel{\\text{Desenv.}}{=} P_{(0,1)}\\left(\\sum X_i^2 \\geq \\eta^*\\right),\n\\end{aligned}\n$$\nem que $\\eta^* = 4\\ln(2^{n/2} \\eta)$. Como, sob $\\mathcal{H}_0, \\sum X_i^2 \\sim \\chi^2_n$, temos que\n$$\n\\pi_{\\delta^*}((0,1)) = P_{(0,1)}(\\chi^2_{n} \\geq \\eta^*) = 5\\%.\n$$\n\nCom $n=2$, temos que\n$$\n\\pi_{\\delta^*}((0,1)) = P_{(0,1)}(\\chi^2_{n} \\geq \\eta^*) = 5\\%. \\Rightarrow \\eta^* = 5.991.\n$$\n\nPortanto, o teste mais poderoso é\n$$\n\\delta^*(\\boldsymbol{x}_n) = \\begin{cases}\n0,\\ \\text{se}\\ \\sum x_i^2 < 5.991 \\\\\n1,\\ \\text{se}\\ \\sum x_i^2 \\geq 5.991.\n\\end{cases}\n$$\n\n### Exemplo (Bernoulli)\nSeja $\\boldsymbol{X}_n$ a.a. de $X \\sim \\mathrm{Ber}(\\theta), \\theta \\in \\{0.1, 0.9\\}$. Considere\n$$\n\\begin{cases}\n\\mathcal{H}_0 : \\theta = 0.9 \\\\\n\\mathcal{H}_1 : \\theta = 0.1\n\\end{cases}\n$$\nas hipóteses nula e alternativa e $n=10$. Encontre o Teste Mais Poderoso (TMP) de tamanho $\\alpha = 10\\%$.\n\n#### Resposta\n\nDe acordo com o Lema de Neyman-Pearson, o teste mais poderoso de tamanho $\\alpha = 5\\%$ é\n$$\n\\delta^*(\\boldsymbol{x}_n) = \\begin{cases}\n0,\\ \\text{se}\\ \\mathcal{L}_{\\boldsymbol{x}_n}(0.1) < \\eta \\cdot \\mathcal{L}_{\\boldsymbol{x}_n}(0.9), \\\\\n1,\\ \\text{se}\\ \\mathcal{L}_{\\boldsymbol{x}_n}(0.1) \\geq \\eta \\cdot \\mathcal{L}_{\\boldsymbol{x}_n}(0.9),\n\\end{cases}\n$$\nem que $\\eta$ satisfaz $\\pi_{\\delta^*}(0.9) = 10\\%$.\n\nNote que\n$$\n\\begin{aligned}\n\\mathcal{L}_{\\boldsymbol{x}_n}(\\theta) &= \\theta^{\\sum x_i} (1-\\theta)^{n - \\sum x_i} \\\\\n\\Rightarrow \\frac{\\mathcal{L}_{\\boldsymbol{x}_n}(0.1)}{\\mathcal{L}_{\\boldsymbol{x}_n}(0.9)} &= \\frac{0.1^{\\sum x_i} (0.9)^{n - \\sum x_i}}{0.9^{\\sum x_i} (0.1)^{n - \\sum x_i}} \\\\\n&= \\frac{0.1^{\\sum x_i} \\cdot 0.9^{n} \\cdot 0.1^{\\sum x_i}}{0.9^{\\sum x_i} \\cdot 0.9^{\\sum x_i} \\cdot 0.1^n} \\\\\n&= \\left(\\frac{0.1 \\cdot 0.1}{0.9 \\cdot 0.9}\\right)^{\\sum x_i} \\cdot \\left(\\frac{0.9}{0.1}\\right)^{n} \\geq \\eta \\\\\n\\iff &(\\sum x_i) \\ln \\left(\\frac{0.01}{0.81}\\right) + n \\ln (9) \\geq \\ln \\eta \\\\\n\\iff &(\\sum x_i) \\ln \\left(\\frac{1}{81}\\right) \\geq \\ln \\eta - n \\ln (9) \\\\\n\\iff &(-\\sum x_i) \\ln (81) \\geq \\ln \\eta - n \\ln (9) \\\\\n\\iff &-\\sum x_i \\geq \\frac{\\ln \\eta - n \\ln (9)}{\\ln (81)} \\\\\n\\iff &\\sum x_i \\leq \\frac{-\\ln \\eta + n \\ln (9)}{\\ln (81)}\n\\end{aligned}\n$$\n\nLogo, $\\left(\\frac{0.1 \\cdot 0.1}{0.9 \\cdot 0.9}\\right)^{\\sum x_i} \\cdot \\left(\\frac{0.9}{0.1}\\right)^{n} \\geq \\eta \\iff \\sum x_i \\leq \\frac{-\\ln \\eta + n \\ln (9)}{\\ln (81)}$.\n$$\n\\pi_{\\delta^*}(0.1) = P_{0.1}(\\delta^*(\\boldsymbol{X}_n) = 1) = P_{0.1}(\\sum X_i \\leq \\eta^*) = \\alpha = 10\\%\n$$\n\nNote que, sob $\\mathcal{H}_0, \\sum^10_{i=1} X_i \\sim \\mathrm{Bin}(10, 0.9)$. Ademais,\n$$\nP_{0.9}(\\sum X \\leq 7) = 0.0702,\\ \\ \\\nP_{0.9}(\\sum X \\leq 8) = 0.2639.\n$$\nLogo, não é possível encontrar um valor para $\\eta$ exato com esse tamanho do teste.\n\n## Testes uniformemente mais poderosos (TUMP)\n\nConsidere\n$$\n\\begin{cases}\n\\mathcal{H}_0 : \\theta \\in \\Theta_0 \\\\\n\\mathcal{H}_1 : \\theta \\in \\Theta_1\n\\end{cases}\n$$\nas hipóteses nula e alternativa em que $\\Theta_0 \\cup \\Theta_1 = \\Theta, \\Theta_0 \\cap \\Theta_1 = \\emptyset, \\#(\\Theta_0) \\geq 1, \\#(\\Theta_1) \\geq 1$.\nO teste de tamanho $\\alpha$ $\\delta^* : \\mathfrak{X}^{(n)} \\rightarrow \\{0,1\\}$ é uniformemente mais poderoso se, e somente se,\n\n1. $\\sup\\limits_{\\theta \\in \\Theta_0} \\pi_{\\delta^*}(\\theta) = \\alpha$;\n\n2. Para qualquer outro teste $\\delta$ tal que $\\pi_{\\delta}(\\theta) \\leq \\alpha, \\underbrace{\\forall \\theta \\in \\Theta_0}_{\\text{Sob}\\ \\mathcal{H}_0}$, então\n$$\n\\pi_{\\delta^*}(\\theta) \\geq \\pi_{\\delta}(\\theta), \\underbrace{\\forall \\theta \\in \\Theta_1}_{\\text{Sob}\\ \\mathcal{H}_1}.\n$$\n\n### Teorema de Karlin-Rubin (I)\n*Uma generalização do Lema de Neyman-Pearson.*\n\nSeja $\\boldsymbol{X}_n$ a.a. de $X \\sim f_\\theta, \\theta \\in \\Theta \\subseteq \\mathbb{R}$. Considere\n$$\n\\begin{cases}\n\\mathcal{H}_0 : \\theta \\leq \\theta_0\\\\\n\\mathcal{H}_1 : \\theta > \\theta_0\n\\end{cases}\n$$\nem que $\\theta_0$ é um valor dado. Se a *razão de verossimilhanças*\n$$\n\\mathrm{R}_{\\theta',\\theta''}(T(\\boldsymbol{x}_n)) = \\frac{\\mathcal{L}_{\\boldsymbol{x}_n}(\\theta')}{\\mathcal{L}_{\\boldsymbol{x}_n}(\\theta'')}\n$$\nfor *monótona não-decrescente* e $\\theta' \\geq \\theta''$, em que $T(\\boldsymbol{x}_n)$ é uma [estatística suficiente](estatisticas-suficientes.qmd) para o [modelo](modelo-estatistico.qmd), então\no teste $\\delta^* : \\mathfrak{X}^{(n)} \\rightarrow \\{0, 1\\}$ tal que\n$$\n\\delta^* (\\boldsymbol{x}_n) = \\begin{cases}\n0, T(\\boldsymbol{x}_n) < c \\\\\n1, T(\\boldsymbol{x}_n) \\geq c,\n\\end{cases}\n$$\ne $c$ satisfaz\n$$\n\\pi_{\\delta^*}(\\theta_0) = P_{\\theta_0}(\\delta^*(\\boldsymbol{x}_n) = 1) = \\alpha,\n$$\né o teste uniformemente mais poderoso.\n\n#### Exemplo (Normal)\n\nSeja $\\boldsymbol{X}_n$ a.a. de $X\\sim N(\\theta, 2), \\theta \\in \\Theta \\subseteq \\mathbb{R}$. Considere\n$$\n\\begin{cases}\n\\mathcal{H}_0 : \\theta \\leq 1\\\\\n\\mathcal{H}_1 : \\theta > 1.\n\\end{cases}\n$$\nEncontre o TUMP de tamanho $\\alpha = 10\\%$.\n\n##### Resposta\n\nA função de verossimilhança é dada por\n$$\n\\begin{aligned}\n\\mathcal{L}_{\\boldsymbol{x}_n}(\\theta) &= \\left(\\frac{1}{\\sqrt{4\\pi}}\\right)^n \\mathrm{Exp}\\left\\{\n-\\frac{1}{4} \\sum (x_i - \\theta)^{2} \\right\\} \\\\ &= \n\\left(\\frac{1}{\\sqrt{4\\pi}}\\right)^n \\mathrm{Exp}\\left\\{\n-\\frac{1}{4} \\sum x_i^2 - 2\\theta\\sum x_i + n\\theta^2 \\right\\}.\n\\end{aligned}\n$$\nPelo [Critério de Fatoração de Fisher](estatisticas-suficientes.qmd#sec-crit-fat), $T(\\boldsymbol{X}_n) = \\sum x_i$ é uma estatística suficiente.\nNote que\n$$\n\\begin{aligned}\n\\mathrm{R}_{\\theta',\\theta''}(T(\\boldsymbol{X}_n)) &= \\frac{\\left(\\frac{1}{\\sqrt{4\\pi}}\\right)^n \\mathrm{Exp}\\left\\{\n-\\frac{1}{4} \\sum x_i^2 - 2\\theta'\\sum x_i + n\\theta'^2 \\right\\}}{\\left(\\frac{1}{\\sqrt{4\\pi}}\\right)^n \\mathrm{Exp}\\left\\{\n-\\frac{1}{4} \\sum x_i^2 - 2\\theta''\\sum x_i + n\\theta''^2 \\right\\}} \\\\\n&= \\mathrm{Exp}\\left\\{-\\frac{1}{4}(-\\theta'T(\\boldsymbol{x}_n) + n\\theta'^2 + 2\\theta''T(\\boldsymbol{x}_n) - n\\theta''^2\\right\\} \\\\\n&= \\mathrm{Exp}\\left\\{\\frac{1}{2}(\\theta' - \\theta'')T(\\boldsymbol{x}_n)-\\frac{1}{4} n'\\theta^2 +\\frac{1}{4}\\theta''^2\\right\\}.\n\\end{aligned}\n$$\nComo $\\theta' \\geq \\theta''$, temos que $\\theta' - \\theta'' \\geq 0$. Logo, $\\mathrm{R}_{\\theta',\\theta''}(T(\\boldsymbol{x}_n))$ é uma função\nmonótona *não-decrescente* em $T(\\boldsymbol{x}_n)$. Logo, pelo Teorema de Karlin-Rubin (I), temos que\n$$\n\\delta^*(\\boldsymbol{x}_n) = \\begin{cases}\n0, \\sum x_i < c \\\\\n1, \\sum x_i \\geq c\n\\end{cases}\n$$\nem que $c$ satisfaz\n$$\n\\pi_{\\delta^*}(1) = P_{1}(\\sum X_i \\geq c) = \\alpha.\n$$\n\nCom $n = 5$, temos que, quando $\\theta = 1$, $\\sum X_i \\sim N(1 \\cdot 5, 5 \\cdot 2)$. Assim,\n$$\n\\begin{aligned}\nP_{1}(\\sum X_i \\geq c) &= P_1\\left(\\frac{\\sum x_i - 5}{\\sqrt{10}} \\geq \\frac{c - 5}{\\sqrt{10}}\\right) = 0.1 \\\\\n\\Rightarrow \\frac{c - 5}{\\sqrt{10}} &= 1.28 \\\\\n\\Rightarrow c &= 1.28 \\cdot \\sqrt{10} + 5 = 9.064 \\\\\n\\Rightarrow  \\delta^*(\\boldsymbol{x}_n) &= \\begin{cases}\n0, \\sum x_i < 9.064 \\\\\n1, \\sum x_i \\geq 9.064\n\\end{cases}\n\\end{aligned}\n$$\nPortanto, se $\\sum x_i < 9.064$, dizemos que *não há evidencias* para rejeitarmos $\\mathcal{H}_0$ a $10\\%$ de significância\nestatística. Se $\\sum x_i \\geq 9.064$, dizemos que *há evidencias* para rejeitarmos $\\mathcal{H}_0$ a $10\\%$ de significância\nestatística.\n\n#### Exemplo (Exponencial)\n\nSeja $\\boldsymbol{X}_n$ a.a. de $X\\sim \\mathrm{Exp}(\\theta), \\theta \\in \\Theta \\subseteq \\mathbb{R}_+$. Considere\n$$\n\\begin{cases}\n\\mathcal{H}_0 : \\theta \\leq 2\\\\\n\\mathcal{H}_1 : \\theta > 2.\n\\end{cases}\n$$\nEncontre o TUMP de tamanho $\\alpha = 5\\%$.\n\n##### Resposta\n\nA função de verossimilhança é dada por\n$$\n\\begin{aligned}\n\\mathcal{L}_{\\boldsymbol{x}_n}(\\theta) &= \\theta^n \\cdot \\mathrm{e}^{-\\theta \\sum x_i}.\n\\end{aligned}\n$$\n\nPelo [Critério de Fatoração de Fisher](estatisticas-suficientes.qmd#sec-crit-fat), $T(\\boldsymbol{X}_n) = \\sum X_i$ é uma estatística suficiente.\n$$\n\\begin{aligned}\n\\mathrm{R}_{\\theta',\\theta''}(T(\\boldsymbol{X}_n)) &= \\frac{\\theta'^n \\cdot \\mathrm{e}^{-\\theta' \\sum x_i}}{\\theta''^n \\cdot \\mathrm{e}^{-\\theta'' \\sum x_i}} \\\\\n&= \\left(\\frac{\\theta'}{\\theta''}\\right)^n \\mathrm{Exp} \\left\\{ -\\theta' T(\\boldsymbol{x}_n) + \\theta''T(\\boldsymbol{x}_n) \\right\\} \\\\\n&= \\left(\\frac{\\theta'}{\\theta''}\\right)^n \\mathrm{Exp} \\left\\{ (\\theta'' - \\theta') T(\\boldsymbol{x}_n)\\right\\}, \\theta' \\geq \\theta''.\n\\end{aligned}\n$$\nComo $\\theta' \\geq \\theta''$, temos que $\\mathrm{R}_{\\theta',\\theta''}(T(\\boldsymbol{X}_n))$ é decrescente em $T(\\boldsymbol{x}_n)$.\nTome $T'(\\boldsymbol{x}_n) = - \\sum x_i$, logo\n$$\n\\mathrm{R}_{\\theta',\\theta''}(T(\\boldsymbol{X}_n)) = \\left(\\frac{\\theta'}{\\theta''}\\right)^n \\mathrm{Exp}\n\\left\\{(\\theta' - \\theta'') T(\\boldsymbol{x}_n)\\right\\}, \\theta' \\geq \\theta''.\n$$\né monótona não-decrescente em $T'(\\boldsymbol{x}_n)$. Portanto, pelo Teorema de Karlin-Rubin (I),\n$$\n\\delta^*(\\boldsymbol{x}_n) = \\begin{cases}\n0, \\sum -x_i < c \\\\\n1, \\sum -x_i \\geq c\n\\end{cases} \\Rightarrow\n\\delta^*(\\boldsymbol{x}_n) = \\begin{cases}\n0, \\sum x_i > -c \\\\\n1, \\sum x_i \\leq -c\n\\end{cases}\n$$\nem que $c$ satisfaz\n$$\n\\pi_{\\delta^*}(1) = P_{1}(\\sum X_i \\leq -c) = \\alpha.\n$$\nPara $n=10$, sob $\\theta = 2, \\sum X_i \\sim \\mathrm{Gama}(10,2)$. Computacionalmente, o quantil $0.05$ dessa distribuição\né $2.7127$. Logo, $-c = 2.7127 \\Rightarrow c = -2.7127$. Dessa forma, se $\\sum x_i > 2.7127$, dizemos que *não há evidencias* para rejeitarmos $\\mathcal{H}_0$ a $5\\%$ de significância\nestatística. Se $\\sum x_i \\leq 2.7127$, dizemos que *há evidencias* para rejeitarmos $\\mathcal{H}_0$ a $5\\%$ de significância estatística.\n\n### Teorema de Karlin-Rubin (II)\nSeja $\\boldsymbol{X}_n$ a.a. de $X \\sim f_\\theta, \\theta \\in \\Theta \\subseteq \\mathbb{R}$. Considere\n$$\n\\begin{cases}\n\\mathcal{H}_0 : \\theta \\geq \\theta_0\\\\\n\\mathcal{H}_1 : \\theta < \\theta_0\n\\end{cases}\n$$\nem que $\\theta_0$ é um valor dado. Se a *razão de verossimilhanças*\n$$\n\\mathrm{R}_{\\theta',\\theta''}(T(\\boldsymbol{x}_n)) = \\frac{\\mathcal{L}_{\\boldsymbol{x}_n}(\\theta')}{\\mathcal{L}_{\\boldsymbol{x}_n}(\\theta'')}\n$$\nfor *monótona não-crescente* e $\\theta' \\leq \\theta''$, em que $T(\\boldsymbol{x}_n)$ é uma [estatística suficiente](estatisticas-suficientes.qmd) para o [modelo](modelo-estatistico.qmd), então\no teste $\\delta^* : \\mathfrak{X}^{(n)} \\rightarrow \\{0, 1\\}$ tal que\n$$\n\\delta^* (\\boldsymbol{x}_n) = \\begin{cases}\n0, T(\\boldsymbol{x}_n) > c \\\\\n1, T(\\boldsymbol{x}_n) \\leq c,\n\\end{cases}\n$$\ne $c$ satisfaz\n$$\n\\pi_{\\delta^*}(\\theta_0) = P_{\\theta_0}(\\delta^*(\\boldsymbol{x}_n) = 1) = \\alpha,\n$$\né o teste uniformemente mais poderoso.\n\n#### Exemplo (Normal)\n\nSeja $\\boldsymbol{X}_n$ a.a. de $X\\sim N(\\theta, 2), \\theta \\in \\Theta \\subseteq \\mathbb{R}$. Considere\n$$\n\\begin{cases}\n\\mathcal{H}_0 : \\theta \\geq 1\\\\\n\\mathcal{H}_1 : \\theta < 1.\n\\end{cases}\n$$\nEncontre o TUMP de tamanho $\\alpha$.\n\n##### Resposta\n\nA função de verossimilhança é dada por\n$$\n\\begin{aligned}\n\\mathcal{L}_{\\boldsymbol{x}_n}(\\theta) &= \\left(\\frac{1}{\\sqrt{4\\pi}}\\right)^n \\mathrm{Exp}\\left\\{\n-\\frac{1}{4} \\sum (x_i - \\theta)^{2} \\right\\} \\\\ &= \n\\left(\\frac{1}{\\sqrt{4\\pi}}\\right)^n \\mathrm{Exp}\\left\\{\n-\\frac{1}{4} \\sum x_i^2 - 2\\theta\\sum x_i + n\\theta^2 \\right\\}.\n\\end{aligned}\n$$\nPelo [Critério de Fatoração de Fisher](estatisticas-suficientes.qmd#sec-crit-fat), $T(\\boldsymbol{X}_n) = \\sum x_i$ é uma estatística suficiente.\nNote que\n$$\n\\begin{aligned}\n\\mathrm{R}_{\\theta',\\theta''}(T(\\boldsymbol{X}_n)) &= \\frac{\\left(\\frac{1}{\\sqrt{4\\pi}}\\right)^n \\mathrm{Exp}\\left\\{\n-\\frac{1}{4} \\sum x_i^2 - 2\\theta'\\sum x_i + n\\theta'^2 \\right\\}}{\\left(\\frac{1}{\\sqrt{4\\pi}}\\right)^n \\mathrm{Exp}\\left\\{\n-\\frac{1}{4} \\sum x_i^2 - 2\\theta''\\sum x_i + n\\theta''^2 \\right\\}} \\\\\n&= \\mathrm{Exp}\\left\\{-\\frac{1}{4}(-\\theta'T(\\boldsymbol{x}_n) + n\\theta'^2 + 2\\theta''T(\\boldsymbol{x}_n) - n\\theta''^2\\right\\} \\\\\n&= \\mathrm{Exp}\\left\\{\\frac{1}{2}(\\theta' - \\theta'')T(\\boldsymbol{x}_n)-\\frac{1}{4} n'\\theta^2 +\\frac{1}{4}\\theta''^2\\right\\}.\n\\end{aligned}\n$$\nComo $\\theta' \\leq \\theta''$, temos que $\\theta' - \\theta'' \\leq 0$. Logo, $\\mathrm{R}_{\\theta',\\theta''}(T(\\boldsymbol{x}_n))$ é uma função\nmonótona *não-crescente* em $T(\\boldsymbol{x}_n)$. Logo, pelo Teorema de Karlin-Rubin (II), temos que\n$$\n\\delta^*(\\boldsymbol{x}_n) = \\begin{cases}\n0, \\sum x_i > c \\\\\n1, \\sum x_i \\leq c\n\\end{cases}\n$$\nem que $c$ satisfaz\n$$\n\\pi_{\\delta^*}(1) = P_{1}(\\sum X_i \\leq c) = \\alpha.\n$$\n\n### Simulações\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\n# Seja X a.a. de X ~ Exp()\n# H_0 θ ≤ 2; H_1 θ > 2\n# δ(x) = 1 ⟺ ∑ x ≤ 2.71\nusing Distributions, Random, StatsBase\nRandom.seed!(24)\n\n# Borda do Θ_0\nθ_0 = 2\n# Valores do teste e amostra\nn = 100\nα = 0.05\nMC = 10_000\n\nfunction h0()\n  # Sob H_0\n  θ_00 = rand(Uniform(0, θ_0))\n\n\n  d = Exponential(1/θ_00)\n  dsoma = Gamma(n, 1/θ_00)\n  c = quantile(dsoma, α)\n\n  δ = zeros(MC)\n  for i in 1:MC\n    x = rand(d, n)\n    δ[i] = sum(x) ≤ c\n  end\n  println(\"Testes rejeitados sob H_0: $(mean(δ) * 100)%\")\nend\n\nfunction h1()\n  # Sob H_0\n  θ_11 = rand(Uniform(θ_0, θ_0+10))\n\n\n  d = Exponential(1/θ_11)\n  dsoma = Gamma(n, 1/θ_11)\n  c = quantile(dsoma, α)\n\n  δ = zeros(MC)\n  for i in 1:MC\n    x = rand(d, n)\n    δ[i] = sum(x) ≥ c\n  end\n  println(\"Testes rejeitados sob H_1 (Poder do teste): $(mean(δ) * 100)\")\nend\n\nh0()\nh1()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTestes rejeitados sob H_0: 4.9799999999999995%\nTestes rejeitados sob H_1 (Poder do teste): 95.6\n```\n:::\n:::\n\n\n\n",
    "supporting": [
      "teste-hipotese2_files/figure-latex"
    ],
    "filters": []
  }
}