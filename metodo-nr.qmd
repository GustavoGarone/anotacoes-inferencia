# Método de Newton-Raphson

Nem sempre existem soluções fechadas para [estimadores de máxima verossimilhança](emv2.qmd) ou obtidos pelo [método de
momentos](metodo-momentos.qmd). Por exemplo, $U_n(\theta,\boldsymbol{X}_n) = 0$ não tem solução fechada para o EMV e
$E_\theta(X^k) \frac{1}{n} \sum X_i^k$ para o EMM.

Nos dois casos, estamos interessados em encontrar os valores de "$\theta$" que "zeram" uma função $G(\theta)$,

1. $G(\theta) = U_n(\theta, \boldsymbol{X}_n)$

$$
\begin{aligned}
2.& \ \ \ \ \ \ \ \ \ \ \ \  G(\theta) = \left(
\begin{array}{c}
E_\theta(X) - \frac{1}{n} \sum X_i \\
\vdots \\
E_\theta(X^p) - \frac{1}{n} \sum X_i^p
\end{array}\right)
\end{aligned}
$$

Consideraremos a seguir apenas o caso $p=1$

Seja $G : \Theta \rightarrow \mathbb{R}$ uma função contínua e diferenciável, com derivadas contínuas, tal que
$G'(\theta) \neq 0,\forall \theta \in \Theta$. Temos como objetivo encontrar $\hat{\theta} \in \Theta$
tal que $G(\hat{\theta}) = 0$.

Inicia-se o processo com um valor qualquer $\theta_0 \in \Theta$ e calculamos o valor seguinte por meio de:
$$
\theta^{(j)} = \theta^{(j-1)} - \frac{G(\theta^{(j-1)})}{G'(\theta^{(j-1)})}, j = 1, 2, \dots
$$

Continua-se até que $\lvert \theta^{(j)} - \theta^{(j-1)}\rvert \leq \epsilon$ em que $\epsilon$ é um valor de erro pequeno
fixado.

## Exemplo


[Quando tentamos encontrar](emv2.qmd#sec-expnr) o EMV para $\theta$ de $X \sim f_\theta, \theta \in \Theta$ com
$$
f_\theta(x) = \left\{\begin{array}{ll}
\frac{1}{\Gamma(\theta)} x^{\theta-1} \mathrm{e}^{-x}, & x > 0 \\
0, & \mathrm{c.c.}
\end{array}\right.
$$
não conseguimos finalizar a expressão. Vamos relembrar o [escore](escore.qmd) da amostra igualado a zero:

$$
U_n(\boldsymbol{x}_n,\theta) = \sum \ln x_i -\frac{n}{\Gamma(\theta)}\Gamma'(\theta) + \sum \ln x_i  = 0
$$

Considere os $n=7$ valores observados, $\boldsymbol{x}_7 = (3.1, 4.2, 5.7, 2.3, 7.7, 5.1, 3.5)$
$$
\Rightarrow U_n(\boldsymbol{x}_n,\theta) = \underbracket{\frac{7}{\Gamma(\theta)}}_{\frac{\partial \ln}{\partial \theta} \Gamma(\theta)}
+\sum \ln x_i = 0
$$

Utilizando $\theta^{(0)} = 1$,
$$
\begin{aligned}
\theta^{(j)} &= \theta^{(j-1)} - \frac{U_n(\boldsymbol{x}_n,\theta^{(j-1)}}{U'_n(\boldsymbol{x}_n, \theta^{(j-1)}} \\
&= \theta^{(j-1)} - \frac{-7 \psi_1(\theta^{(j-1)})+ 7 \sum \ln x_i}{-7\psi_2(\theta^{(j-1)})} \\
&= \theta^{(j-1)} - \frac{-\psi_1(\theta^{(j-1)})+ \sum \ln x_i}{-\psi_2(\theta^{(j-1)})}
\end{aligned}
$$

em que $\psi_j(\theta) = \frac{\partial^j \ln \Gamma(\theta)}{\partial \theta^j}$

## Algoritmo

Seja $\boldsymbol{X}_n$ a.a. de $X \sim \Gamma(\theta, 1)$ (exemplo anterior). Ainda considerando os mesmos valores
amostrados, $\boldsymbol{x}_7 = (3.1, 4.2, 5.7, 2.3, 7.7, 5.1, 3.5)$, utilize os métodos numéricos para encontrar a
estimativa de máxima verossimilhança.

### Algoritmo por Newton-Raphson
Considere um erro $\epsilon = 10^{-5}$ e $\hat{\theta}^{(0)} = 1.$
$$
\theta^{(j+1)} = \theta^{(j)} - \frac{U_n(\boldsymbol{x}_n,\theta^{(j)}}{U'_n(\boldsymbol{x}_n, \theta^{(j)}}
$$
em que $U_n(\boldsymbol{x}_n,\theta) = -n\psi_1(\theta^{(j-1)})+ \sum \ln x_i$ e $U_n'(\boldsymbol{x}_n,\theta) = -n\psi_2(\theta^{(j-1)})$
com $\psi_j(\theta) = \frac{\partial^j \ln \Gamma(\theta)}{\partial \theta^j}$ e $n=7$.

Logo,
$$
\theta^{(j+1)} = \theta^{(j)} - \frac{U_n(\boldsymbol{x}_n,\theta^{(j)}}{U'_n(\boldsymbol{x}_n, \theta^{(j)}}
$$

<!-- TODO: melhorar iteracao max e erro com append, declarar vetor theta como de floats -->
```{julia}
#| echo: true
using SpecialFunctions # Funções di e trigamma (derivadas do log da gama)


function main()

  x = [3.1, 4.2, 5.7, 2.3, 7.7, 5.1, 3.5]
  n = length(x)

  theta::Vector{Float64} = []
  theta0 = 1
  append!(theta, theta0)


  erromax = 10^(-5)
  erro = Inf
  i = 1
  # iteracoesMax = 6 # Podemos também definir apenas um erro máximo
  while erro > erromax # && i < iteracoesMax
    append!(theta, theta[i] - (sum(log.(x)) - n * digamma(theta[i]))/
                                             (-n*trigamma(theta[i])))
    erro = abs(theta[i+1] - theta[i])
    println("Erro na iteração $i: $erro")
    i += 1
  end
  println("Vetor theta: $theta")

end

main()

```

Note que, se $U'_n(\pmb{x}_n, \hat{\theta}^{(j+1)}) < 0$, a estimativa $\hat{\theta}^{(j+1)}$ será a de MV.

Seja $\boldsymbol{X}_n$ a.a. de $X\sim \mathrm{Exp}(\theta), \theta \in \Theta = (0,\infty)$. Encontre a estimativa de
MV para $\theta$ pelo método de Newton-Raphson considerando $\boldsymbol{x}_n = (2, 2.5, 3, 3.5, 2, 1.5)$ e $\hat{\theta}^{(0)} = 0.5$
Com erro máximo $\epsilon = 10^{-5}$.

Note que
$$
\begin{aligned}
f_\theta(x) &= \theta \mathrm{e}^{-\theta x} \\
\mathcal{L}_{\boldsymbol{x}_n}(\theta) &= \theta^n \mathrm{e}^{-\theta\sum x_i} \\
\ln \mathcal{L}_{\boldsymbol{x}_n}(\theta) &= n \ln\theta -\theta\sum x_i \\
U_n(\boldsymbol{x}_n,\theta) &= \frac{n}{\theta} - \sum x_i \\
U_n'(\boldsymbol{x}_n,\theta) &= -\frac{n}{\theta^2}
\end{aligned}
$$

Logo
$$
\begin{aligned}
\hat{\theta}^{(j+1)} &= \hat{\theta}^{(j)} - \frac{\frac{n}{\hat{\theta}^{(j)}}- \sum x_i}{-\frac{n}{(\hat{\theta}^{(j)})^2}} \\
&= \hat{\theta}^{(j)} - \frac{[n - \hat{\theta}^{(j)}\sum x_i]\hat{\theta}^{(j)}}{-n}
\end{aligned}
$$

```{julia}
#| echo: true

function main()

  x = [2, 2.5, 3, 3.5, 2, 1.5]
  n = length(x)

  theta::Vector{Float64} = []
  theta0 = 0.5
  append!(theta, theta0)


  erromax = 10^(-5)
  erro = Inf
  i = 1
  # iteracoesMax = 5
  while erro > erromax # && i < iteracoesMax
    append!(theta, theta[i] - ((n - sum(x) *theta[i])*theta[i])/(-n))
    erro = abs(theta[i+1] - theta[i])
    println("Erro na iteração $i: $erro")
    i += 1
  end
  println("Vetor theta: $theta")

end

main()

```

Note que se $\Theta \subseteq \mathbb{R}^P$, o método é dado por
$$
\theta^{(j+1)} = \theta^{(j)} - \left[U_n'(\boldsymbol{x}_n,\theta^{(j)}\right]^{-1} \cdot U_n(\boldsymbol{x}_n, \hat{\theta}^{(j)}), j = 0, 1, \dots
$$


### Método de Scoring de Fisher
Para $\Theta \subseteq \mathbb{R}$
$$
\theta^{(j+1)} = \theta^{(j)} + \frac{U_n(\boldsymbol{x}_n,\theta^{(j)}}{I_n(\theta^{(j)}}
$$
Para $\Theta \subseteq \mathbb{R}^P$
$$
\theta^{(j+1)} = \theta^{(j)} + \left[I_n(\boldsymbol{x}_n,\theta^{(j)}\right]^{-1} \cdot U_n(\boldsymbol{x}_n, \hat{\theta}^{(j)}), j = 0, 1, \dots
$$

### Método de descida do gradiente
$$
\theta^{(j+1)} = \theta^{(j)} - \delta \cdot0 U_n(\boldsymbol{x}_n, \hat{\theta}^{(j)}), j = 0, 1, \dots
$$
em que $\delta \in (0,1)$ é a *taxa de "aprendizado"*.

Para o exemplo da Gama,
```{julia}
#| echo: true
using SpecialFunctions # Funções di e trigamma (derivadas do log da gama)


function main()

  x = [3.1, 4.2, 5.7, 2.3, 7.7, 5.1, 3.5]
  n = length(x)

  theta::Vector{Float64} = []
  theta0 = 1
  append!(theta, theta0)
  delta = 0.01


  erromax = 10^(-5)
  erro = Inf
  i = 1
  # iteracoesMax = 6 # Podemos também definir apenas um erro máximo
  while erro > erromax # && i < iteracoesMax
    append!(theta, theta[i] + delta *(sum(log.(x)) - n * digamma(theta[i])))
    erro = abs(theta[i+1] - theta[i])
    if i % 100 == 0
      println("Erro na iteração $i: $erro")
    end
    i += 1
  end
  println("Theta final: $(theta[length(theta)])")

end

main()

```
