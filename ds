[1mdiff --cc metodo-momentos.qmd[m
[1mindex 5ed03dd,6c48eca..0000000[m
[1m--- a/metodo-momentos.qmd[m
[1m+++ b/metodo-momentos.qmd[m
[36m@@@ -1,4 -1,8 +1,12 @@@[m
[32m++<<<<<<< HEAD[m
[32m +# EstimaÃ§Ã£o pelo mÃ©todo de momentos (EMM)[m
[32m++=======[m
[32m+ ```{julia}[m
[32m+ #| output: false[m
[32m+ using Pkg; Pkg.add(["Distributions", "Plots", "LaTeXStrings", "StatsBase"])[m
[32m+ ```[m
[32m+ # EstimaÃ§Ã£o pelo mÃ©todo de momentos[m
[32m++>>>>>>> bb0c001fcd31bfa2fe99a162e1dfd93db4c31854[m
  [m
  Seja $\boldsymbol{X}_n$ uma [a.a.](populacao-e-amostra.qmd#sec-aa) de $X$ tal que[m
  $$[m
[1mdiff --git a/metodo-nr_files/execute-results/html.json b/metodo-nr_files/execute-results/html.json[m
[1mdeleted file mode 100644[m
[1mindex b5ebd79..0000000[m
[1m--- a/metodo-nr_files/execute-results/html.json[m
[1m+++ /dev/null[m
[36m@@ -1,12 +0,0 @@[m
[31m-{[m
[31m-  "hash": "8d2c6240fc4e5d5f823d52782ba8b5f5",[m
[31m-  "result": {[m
[31m-    "engine": "jupyter",[m
[31m-    "markdown": "# MÃ©todo de Newton-Raphson\n\nNem sempre existem soluÃ§Ãµes fechadas para [estimadores de mÃ¡xima verossimilhanÃ§a](emv2.qmd) ou obtidos pelo [mÃ©todo de\nmomentos](metodo-momentos.qmd). Por exemplo, $U_n(\\theta,\\boldsymbol{X}_n) = 0$ nÃ£o tem soluÃ§Ã£o fechada para o EMV e\n$E_\\theta(X^k) \\frac{1}{n} \\sum X_i^k$ para o EMM.\n\nNos dois casos, estamos interessados em encontrar os valores de \"$\\theta$\" que \"zeram\" uma funÃ§Ã£o $G(\\theta)$,\n\n1. $G(\\theta) = U_n(\\theta, \\boldsymbol{X}_n)$\n\n$$\n\\begin{aligned}\n2.& \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  G(\\theta) = \\left(\n\\begin{array}{c}\nE_\\theta(X) - \\frac{1}{n} \\sum X_i \\\\\n\\vdots \\\\\nE_\\theta(X^p) - \\frac{1}{n} \\sum X_i^p\n\\end{array}\\right)\n\\end{aligned}\n$$\n\nConsideraremos a seguir apenas o caso $p=1$\n\nSeja $G : \\Theta \\rightarrow \\mathbb{R}$ uma funÃ§Ã£o contÃ­nua e diferenciÃ¡vel, com derivadas contÃ­nuas, tal que\n$G'(\\theta) \\neq 0,\\forall \\theta \\in \\Theta$. Temos como objetivo encontrar $\\hat{\\theta} \\in \\Theta$\ntal que $G(\\hat{\\theta}) = 0$.\n\nInicia-se o processo com um valor qualquer $\\theta_0 \\in \\Theta$ e calculamos o valor seguinte por meio de:\n$$\n\\theta^{(j)} = \\theta^{(j-1)} - \\frac{G(\\theta^{(j-1)})}{G'(\\theta^{(j-1)})}, j = 1, 2, \\dots\n$$\n\nContinua-se atÃ© que $\\lvert \\theta^{(j)} - \\theta^{(j-1)}\\rvert \\leq \\epsilon$ em que $\\epsilon$ Ã© um valor de erro pequeno\nfixado.\n\n## Exemplo\n\n\n[Quando tentamos encontrar](emv2.qmd#sec-expnr) o EMV para $\\theta$ de $X \\sim f_\\theta, \\theta \\in \\Theta$ com\n$$\nf_\\theta(x) = \\left\\{\\begin{array}{ll}\n\\frac{1}{\\Gamma(\\theta)} x^{\\theta-1} \\mathrm{e}^{-x}, & x > 0 \\\\\n0, & \\mathrm{c.c.}\n\\end{array}\\right.\n$$\nnÃ£o conseguimos finalizar a expressÃ£o. Vamos relembrar o [escore](escore.qmd) da amostra igualado a zero:\n\n$$\nU_n(\\boldsymbol{x}_n,\\theta) = \\sum \\ln x_i -\\frac{n}{\\Gamma(\\theta)}\\Gamma'(\\theta) + \\sum \\ln x_i  = 0\n$$\n\nConsidere os $n=7$ valores observados, $\\boldsymbol{x}_7 = (3.1, 4.2, 5.7, 2.3, 7.7, 5.1, 3.5)$\n$$\n\\Rightarrow U_n(\\boldsymbol{x}_n,\\theta) = \\underbracket{\\frac{7}{\\Gamma(\\theta)}}_{\\frac{\\partial \\ln}{\\partial \\theta} \\Gamma(\\theta)}\n+\\sum \\ln x_i = 0\n$$\n\nUtilizando $\\theta^{(0)} = 1$,\n$$\n\\begin{aligned}\n\\theta^{(j)} &= \\theta^{(j-1)} - \\frac{U_n(\\boldsymbol{x}_n,\\theta^{(j-1)}}{U'_n(\\boldsymbol{x}_n, \\theta^{(j-1)}} \\\\\n&= \\theta^{(j-1)} - \\frac{-7 \\psi_1(\\theta^{(j-1)})+ 7 \\sum \\ln x_i}{-7\\psi_2(\\theta^{(j-1)})} \\\\\n&= \\theta^{(j-1)} - \\frac{-\\psi_1(\\theta^{(j-1)})+ \\sum \\ln x_i}{-\\psi_2(\\theta^{(j-1)})}\n\\end{aligned}\n$$\n\nem que $\\psi_j(\\theta) = \\frac{\\partial^j \\ln \\Gamma(\\theta)}{\\partial \\theta^j}$\n\n## Algoritmos e outros mÃ©todos\n\nSeja $\\boldsymbol{X}_n$ a.a. de $X \\sim \\Gamma(\\theta, 1)$ (exemplo anterior). Ainda considerando os mesmos valores\namostrados, $\\boldsymbol{x}_7 = (3.1, 4.2, 5.7, 2.3, 7.7, 5.1, 3.5)$, utilize os mÃ©todos numÃ©ricos para encontrar a\nestimativa de mÃ¡xima verossimilhanÃ§a.\n\n### Algoritmo por Newton-Raphson\nConsidere um erro $\\epsilon = 10^{-5}$ e $\\hat{\\theta}^{(0)} = 1.$\n$$\n\\theta^{(j+1)} = \\theta^{(j)} - \\frac{U_n(\\boldsymbol{x}_n,\\theta^{(j)}}{U'_n(\\boldsymbol{x}_n, \\theta^{(j)}}\n$$\nem que $U_n(\\boldsymbol{x}_n,\\theta) = -n\\psi_1(\\theta^{(j-1)})+ \\sum \\ln x_i$ e $U_n'(\\boldsymbol{x}_n,\\theta) = -n\\psi_2(\\theta^{(j-1)})$\ncom $\\psi_j(\\theta) = \\frac{\\partial^j \\ln \\Gamma(\\theta)}{\\partial \\theta^j}$ e $n=7$.\n\nLogo,\n$$\n\\theta^{(j+1)} = \\theta^{(j)} - \\frac{U_n(\\boldsymbol{x}_n,\\theta^{(j)}}{U'_n(\\boldsymbol{x}_n, \\theta^{(j)}}\n$$\n\n::: panel-tabset\n\n#### Julia\n\n::: {#a20a1fa7 .cell execution_count=1}\n``` {.julia .cell-code}\nusing SpecialFunctions # FunÃ§Ãµes di e trigamma (derivadas do log da gama)\n\n\nfunction main()\n\n  x = [3.1, 4.2, 5.7, 2.3, 7.7, 5.1, 3.5]\n  n = length(x)\n\n  theta::Vector{Float64} = []\n  theta0 = 1\n  append!(theta, theta0)\n\n\n  erromax = 10^(-5)\n  erro = Inf\n  i = 1\n  # iteracoesMax = 6 # Podemos tambÃ©m definir apenas um erro mÃ¡ximo\n  while erro > erromax # && i < iteracoesMax\n    append!(theta, theta[i] - (sum(log.(x)) - n * digamma(theta[i]))/\n                                             (-n*trigamma(theta[i])))\n    erro = abs(theta[i+1] - theta[i])\n    println(\"Erro na iteraÃ§Ã£o $i: $erro\")\n    i += 1\n  end\n  println(\"Vetor theta: $theta\")\n  println(\"Total de iteraÃ§Ãµes: $i\")\n\nend\n\nmain()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nErro na iteraÃ§Ã£o 1: 1.2248511275743903\nErro na iteraÃ§Ã£o 2: 1.5558203731733413\nErro na iteraÃ§Ã£o 3: 0.8122385697386227\nErro na iteraÃ§Ã£o 4: 0.10638335531193821\nErro na iteraÃ§Ã£o 5: 0.0013809916201203976\nErro na iteraÃ§Ã£o 6: 2.2502724394968254e-7\nVetor theta: [1.0, 2.2248511275743903, 3.7806715007477316, 4.592910070486354, 4.699293425798293, 4.700674417418413, 4.700674642445657]\nTotal de iteraÃ§Ãµes: 7\n```\n:::\n:::\n\n\n#### Python\n\n```{{Python}}\n# CÃ³digo em Python\n```\n\n#### R\n\n```{{R}}\n# CÃ³digo em R\n```\n\n:::\n\nNote que, se $U'_n(\\pmb{x}_n, \\hat{\\theta}^{(j+1)}) < 0$, a estimativa $\\hat{\\theta}^{(j+1)}$ serÃ¡ a de MV.\n\nSeja $\\boldsymbol{X}_n$ a.a. de $X\\sim \\mathrm{Exp}(\\theta), \\theta \\in \\Theta = (0,\\infty)$. Encontre a estimativa de\nMV para $\\theta$ pelo mÃ©todo de Newton-Raphson considerando $\\boldsymbol{x}_n = (2, 2.5, 3, 3.5, 2, 1.5)$ e $\\hat{\\theta}^{(0)} = 0.5$\nCom erro mÃ¡ximo $\\epsilon = 10^{-5}$.\n\nNote que\n$$\n\\begin{aligned}\nf_\\theta(x) &= \\theta \\mathrm{e}^{-\\theta x} \\\\\n\\mathcal{L}_{\\boldsymbol{x}_n}(\\theta) &= \\theta^n \\mathrm{e}^{-\\theta\\sum x_i} \\\\\n\\ln \\mathcal{L}_{\\boldsymbol{x}_n}(\\theta) &= n \\ln\\theta -\\theta\\sum x_i \\\\\nU_n(\\boldsymbol{x}_n,\\theta) &= \\frac{n}{\\theta} - \\sum x_i \\\\\nU_n'(\\boldsymbol{x}_n,\\theta) &= -\\frac{n}{\\theta^2}\n\\end{aligned}\n$$\n\nLogo\n$$\n\\begin{aligned}\n\\hat{\\theta}^{(j+1)} &= \\hat{\\theta}^{(j)} - \\frac{\\frac{n}{\\hat{\\theta}^{(j)}}- \\sum x_i}{-\\frac{n}{(\\hat{\\theta}^{(j)})^2}} \\\\\n&= \\hat{\\theta}^{(j)} - \\frac{[n - \\hat{\\theta}^{(j)}\\sum x_i]\\hat{\\theta}^{(j)}}{-n}\n\\end{aligned}\n$$\n\n::: {#7d7787d7 .cell execution_count=2}\n``` {.julia .cell-code}\nfunction main()\n\n  x = [2, 2.5, 3, 3.5, 2, 1.5]\n  n = length(x)\n\n  theta::Vector{Float64} = []\n  theta0 = 0.5\n  append!(theta, theta0)\n\n\n  erromax = 10^(-5)\n  erro = Inf\n  i = 1\n  # iteracoesMax = 5\n  while erro > erromax # && i < iteracoesMax\n    append!(theta, theta[i] - ((n - sum(x) *theta[i])*theta[i])/(-n))\n    erro = abs(theta[i+1] - theta[i])\n    println(\"Erro na iteraÃ§Ã£o $i: $erro\")\n    i += 1\n  end\n  println(\"Vetor theta: $theta\")\n  println(\"Total de iteraÃ§Ãµes: $i\")\n\nend\n\nmain()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nErro na iteraÃ§Ã£o 1: 0.10416666666666669\nErro na iteraÃ§Ã£o 2: 0.01718026620370372\nErro na iteraÃ§Ã£o 3: 0.0007780354808986645\nErro na iteraÃ§Ã£o 4: 1.468425129103057e-6\nVetor theta: [0.5, 0.3958333333333333, 0.41301359953703703, 0.4137916350179357, 0.4137931034430648]\nTotal de iteraÃ§Ãµes: 5\n```\n:::\n:::\n\n\nNote que se $\\Theta \\subseteq \\mathbb{R}^P$, o mÃ©todo Ã© dado por\n$$\n\\theta^{(j+1)} = \\theta^{(j)} - \\left[U_n'(\\boldsymbol{x}_n,\\theta^{(j)}\\right]^{-1} \\cdot U_n(\\boldsymbol{x}_n, \\hat{\\theta}^{(j)}), j = 0, 1, \\dots\n$$\n\n\n### MÃ©todo de Scoring de Fisher\nPara $\\Theta \\subseteq \\mathbb{R}$\n$$\n\\theta^{(j+1)} = \\theta^{(j)} + \\frac{U_n(\\boldsymbol{x}_n,\\theta^{(j)}}{I_n(\\theta^{(j)}}\n$$\nPara $\\Theta \\subseteq \\mathbb{R}^P$\n$$\n\\theta^{(j+1)} = \\theta^{(j)} + \\left[I_n(\\boldsymbol{x}_n,\\theta^{(j)}\\right]^{-1} \\cdot U_n(\\boldsymbol{x}_n, \\hat{\\theta}^{(j)}), j = 0, 1, \\dots\n$$\n\n### MÃ©todo de descida do gradiente\n$$\n\\theta^{(j+1)} = \\theta^{(j)} + \\delta \\cdot U_n(\\boldsymbol{x}_n, \\hat{\\theta}^{(j)}), j = 0, 1, \\dots\n$$\nem que $\\delta \\in (0,1)$ Ã© a *taxa de \"aprendizado\"*.\n\nPara o exemplo da Gama,\n\n::: {#354412db .cell execution_count=3}\n``` {.julia .cell-code}\nusing SpecialFunctions # FunÃ§Ãµes di e trigamma (derivadas do log da gama)\n\n\nfunction main()\n\n  x = [3.1, 4.2, 5.7, 2.3, 7.7, 5.1, 3.5]\n  n = length(x)\n\n  theta::Vector{Float64} = []\n  theta0 = 1\n  append!(theta, theta0)\n  delta = 0.01\n\n\n  erromax = 10^(-5)\n  erro = Inf\n  i = 1\n  # iteracoesMax = 6 # Podemos tambÃ©m definir apenas um erro mÃ¡ximo\n  while erro > erromax # && i < iteracoesMax\n    append!(theta, theta[i] + delta *(sum(log.(x)) - n * digamma(theta[i])))\n    erro = abs(theta[i+1] - theta[i])\n    if i % 100 == 0\n      println(\"Erro na iteraÃ§Ã£o $i: $erro\")\n    end\n    i += 1\n  end\n  println(\"Theta final: $(theta[length(theta)])\")\n  println(\"Total de iteraÃ§Ãµes: $i\")\n\nend\n\nmain()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nErro na iteraÃ§Ã£o 100: 0.007983394465004956\nErro na iteraÃ§Ã£o 200: 0.0013675177845966502\nErro na iteraÃ§Ã£o 300: 0.0002527165874308679\nErro na iteraÃ§Ã£o 400: 4.731072948604975e-5\nTheta final: 4.700082915774669\nTotal de iteraÃ§Ãµes: 494\n```\n:::\n:::\n\n\n### Descida estocÃ¡stica do gradiente\n$$\n\\theta^{(j+1, i)} = \\theta^{(j, i)} + \\delta \\cdot U(x_i, \\hat{\\theta}^{(j, i)}), j = 0, 1, \\dots, i \\in \\{1,\\dots,n\\}\n$$\n\nEm que $U(x, \\hat{\\theta}^{(j)})$ Ã© a [funÃ§Ã£o escore](escore.qmd) para uma observaÃ§Ã£o. Geraremos dados aleatÃ³rios de uma\ngama.\n\n<!-- TODO: arrumar cÃ³digo abaixo (usar do professor) -->\n\n::: {#a98718a9 .cell execution_count=4}\n``` {.julia .cell-code}\nusing SpecialFunctions, Distributions, Random\nRandom.seed!(36)\n\n\nfunction main()\n\n\n  theta::Vector{Float64} = []\n  theta0 = 15\n  n = 100\n  x = rand(Gamma(theta0, 1), n)\n  println(length(x))\n  append!(theta, 1)\n  delta = 0.0001\n\n\n  erromax = 10^(-2)\n  erro = Inf\n  i = 1\n  epoca = 0\n  totalit = 0\n  # iteracoesMax = 6 # Podemos tambÃ©m definir apenas um erro mÃ¡ximo\n  while erro > erromax # && i < iteracoesMax\n    itsepoca = 0\n    for i in 1:length(x)\n      append!(theta, theta[i] + delta *(log(x[i]) - n * digamma(theta[i])))\n      erro = abs(theta[i+1] - theta[i])\n      itsepoca = i\n    end\n    totalit += itsepoca\n    if epoca % 10 == 0\n      println(\"Erro na epoca $epoca e iteraÃ§Ã£o $totalit: $erro\")\n      println(\"theta estimado na epoca $epoca: $(theta[totalit])\")\n    end\n    epoca += 1\n  end\n  println(\"Theta final: $(theta[length(theta)])\")\n  println(\"Total de iteraÃ§Ãµes: $i\")\n\nend\n\nmain()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n100\nErro na epoca 0 e iteraÃ§Ã£o 100: 0.001649963400952137\ntheta estimado na epoca 0: 1.3254323939150596\nTheta final: 1.3270823573160118\nTotal de iteraÃ§Ãµes: 1\n```\n:::\n:::\n\n\n",[m
[31m-    "supporting": [[m
[31m-      "metodo-nr_files/figure-html"[m
[31m-    ],[m
[31m-    "filters": [],[m
[31m-    "includes": {}[m
[31m-  }[m
[31m-}[m
\ No newline at end of file[m
